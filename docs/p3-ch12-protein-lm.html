<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Protein Language Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p3-ch13-regulatory.html" rel="next">
<link href="./p3-ch11-dna-lm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="./p3-ch12-protein-lm.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer and Adaptation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-protein-lm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p4--multi-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Systems and Scale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch18-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch19-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Benchmarks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Evaluation Principles</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Confounders and Leakage</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch28-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch29-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Ethical and Frontiers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-esm-model-family" id="toc-the-esm-model-family" class="nav-link active" data-scroll-target="#the-esm-model-family"><span class="header-section-number">12.1</span> The ESM Model Family</a>
  <ul class="collapse">
  <li><a href="#esm-1b-establishing-the-paradigm" id="toc-esm-1b-establishing-the-paradigm" class="nav-link" data-scroll-target="#esm-1b-establishing-the-paradigm"><span class="header-section-number">12.1.1</span> ESM-1b: Establishing the Paradigm</a></li>
  <li><a href="#emergent-biological-knowledge" id="toc-emergent-biological-knowledge" class="nav-link" data-scroll-target="#emergent-biological-knowledge"><span class="header-section-number">12.1.2</span> Emergent Biological Knowledge</a></li>
  <li><a href="#esm-2-scaling-up" id="toc-esm-2-scaling-up" class="nav-link" data-scroll-target="#esm-2-scaling-up"><span class="header-section-number">12.1.3</span> ESM-2: Scaling Up</a></li>
  </ul></li>
  <li><a href="#alternative-architectures" id="toc-alternative-architectures" class="nav-link" data-scroll-target="#alternative-architectures"><span class="header-section-number">12.2</span> Alternative Architectures</a></li>
  <li><a href="#attention-and-evolutionary-coupling" id="toc-attention-and-evolutionary-coupling" class="nav-link" data-scroll-target="#attention-and-evolutionary-coupling"><span class="header-section-number">12.3</span> Attention and Evolutionary Coupling</a></li>
  <li><a href="#esmfold-structure-from-sequence" id="toc-esmfold-structure-from-sequence" class="nav-link" data-scroll-target="#esmfold-structure-from-sequence"><span class="header-section-number">12.4</span> ESMFold: Structure from Sequence</a>
  <ul class="collapse">
  <li><a href="#eliminating-the-alignment-bottleneck" id="toc-eliminating-the-alignment-bottleneck" class="nav-link" data-scroll-target="#eliminating-the-alignment-bottleneck"><span class="header-section-number">12.4.1</span> Eliminating the Alignment Bottleneck</a></li>
  <li><a href="#what-esmfold-reveals-about-plms" id="toc-what-esmfold-reveals-about-plms" class="nav-link" data-scroll-target="#what-esmfold-reveals-about-plms"><span class="header-section-number">12.4.2</span> What ESMFold Reveals About PLMs</a></li>
  </ul></li>
  <li><a href="#function-prediction" id="toc-function-prediction" class="nav-link" data-scroll-target="#function-prediction"><span class="header-section-number">12.5</span> Function Prediction</a></li>
  <li><a href="#variant-effect-prediction" id="toc-variant-effect-prediction" class="nav-link" data-scroll-target="#variant-effect-prediction"><span class="header-section-number">12.6</span> Variant Effect Prediction</a></li>
  <li><a href="#integration-with-structure-prediction" id="toc-integration-with-structure-prediction" class="nav-link" data-scroll-target="#integration-with-structure-prediction"><span class="header-section-number">12.7</span> Integration with Structure Prediction</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="header-section-number">12.8</span> Limitations</a>
  <ul class="collapse">
  <li><a href="#orphan-and-dark-proteins" id="toc-orphan-and-dark-proteins" class="nav-link" data-scroll-target="#orphan-and-dark-proteins"><span class="header-section-number">12.8.1</span> Orphan and Dark Proteins</a></li>
  <li><a href="#novel-folds" id="toc-novel-folds" class="nav-link" data-scroll-target="#novel-folds"><span class="header-section-number">12.8.2</span> Novel Folds</a></li>
  <li><a href="#conformational-flexibility" id="toc-conformational-flexibility" class="nav-link" data-scroll-target="#conformational-flexibility"><span class="header-section-number">12.8.3</span> Conformational Flexibility</a></li>
  <li><a href="#epistasis" id="toc-epistasis" class="nav-link" data-scroll-target="#epistasis"><span class="header-section-number">12.8.4</span> Epistasis</a></li>
  <li><a href="#interpretability" id="toc-interpretability" class="nav-link" data-scroll-target="#interpretability"><span class="header-section-number">12.8.5</span> Interpretability</a></li>
  </ul></li>
  <li><a href="#lessons-for-genomic-foundation-models" id="toc-lessons-for-genomic-foundation-models" class="nav-link" data-scroll-target="#lessons-for-genomic-foundation-models"><span class="header-section-number">12.9</span> Lessons for Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#self-supervision-captures-biological-knowledge" id="toc-self-supervision-captures-biological-knowledge" class="nav-link" data-scroll-target="#self-supervision-captures-biological-knowledge"><span class="header-section-number">12.9.1</span> Self-Supervision Captures Biological Knowledge</a></li>
  <li><a href="#scale-yields-consistent-improvements" id="toc-scale-yields-consistent-improvements" class="nav-link" data-scroll-target="#scale-yields-consistent-improvements"><span class="header-section-number">12.9.2</span> Scale Yields Consistent Improvements</a></li>
  <li><a href="#transfer-learning-is-effective" id="toc-transfer-learning-is-effective" class="nav-link" data-scroll-target="#transfer-learning-is-effective"><span class="header-section-number">12.9.3</span> Transfer Learning is Effective</a></li>
  <li><a href="#architecture-choices-must-match-sequence-properties" id="toc-architecture-choices-must-match-sequence-properties" class="nav-link" data-scroll-target="#architecture-choices-must-match-sequence-properties"><span class="header-section-number">12.9.4</span> Architecture Choices Must Match Sequence Properties</a></li>
  <li><a href="#integration-multiplies-capability" id="toc-integration-multiplies-capability" class="nav-link" data-scroll-target="#integration-multiplies-capability"><span class="header-section-number">12.9.5</span> Integration Multiplies Capability</a></li>
  </ul></li>
  <li><a href="#significance" id="toc-significance" class="nav-link" data-scroll-target="#significance"><span class="header-section-number">12.10</span> Significance</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="./p3-ch12-protein-lm.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-protein-lm" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Evolution is the most thorough experiment ever conducted on protein sequences. Over billions of years, natural selection has tested trillions of amino acid combinations, ruthlessly eliminating those that fail to fold or function while preserving those that work. The sequences that survive to populate modern databases are not random strings but successful solutions to biological problems, each one implicitly encoding information about structure, stability, and function. The central insight of protein language models is that this evolutionary record, comprising hundreds of millions of sequences in databases like UniRef, contains sufficient information to learn the fundamental principles of protein biology without ever being shown a crystal structure or a functional assay.</p>
<p>This insight transformed computational biology. Traditional approaches to understanding proteins required either expensive experimental characterization or physics-based simulations that struggled to capture the full complexity of protein behavior. Multiple sequence alignments (MSAs) could extract evolutionary conservation patterns, but required finding homologous sequences for each protein of interest and could not generalize beyond the specific alignment. Protein language models changed the equation by compressing evolutionary knowledge into neural network parameters that transfer across the entire protein universe. A model trained to predict masked amino acids learns, as a byproduct, which residues contact each other in three-dimensional space, which positions tolerate variation, and which substitutions disrupt function. The physics of protein folding, selected for across evolutionary time, emerges from the statistics of surviving sequences.</p>
<p>This chapter examines how protein language models pioneered biological foundation modeling, establishing principles that would later guide genomic language models. The ESM family demonstrated that transformers can learn protein structure and function from sequence alone, achieving results that rival methods requiring explicit structural supervision. Understanding these successes, and their limitations, provides essential context for the DNA and RNA language models covered in subsequent chapters.</p>
<section id="the-esm-model-family" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="the-esm-model-family"><span class="header-section-number">12.1</span> The ESM Model Family</h2>
<section id="esm-1b-establishing-the-paradigm" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="esm-1b-establishing-the-paradigm"><span class="header-section-number">12.1.1</span> ESM-1b: Establishing the Paradigm</h3>
<p>The Evolutionary Scale Modeling project, developed at Meta AI Research, demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision <span class="citation" data-cites="rives_esm_2021">(<a href="references.html#ref-rives_esm_2021" role="doc-biblioref">Rives et al. 2021</a>)</span>. The approach was strikingly simple: take the BERT architecture that had revolutionized natural language processing, replace words with amino acids, and train on protein sequence databases. The resulting models learned far more than anyone expected.</p>
<p>ESM-1b was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families (see <a href="p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a> for details on sequence database construction and clustering strategies). UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy that would otherwise bias the model toward overrepresented families <span class="citation" data-cites="suzek_uniref_2007">(<a href="references.html#ref-suzek_uniref_2007" role="doc-biblioref">Suzek et al. 2007</a>)</span>. This curation strategy ensures the model encounters diverse evolutionary solutions to protein function rather than memorizing common motifs.</p>
<p>The architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is masked language modeling, the self-supervised strategy introduced in <a href="p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>: randomly mask 15% of amino acids in each sequence, and train the model to predict the masked positions given surrounding context. This objective contains no information about structure, function, or evolution beyond what is implicit in the sequences themselves.</p>
</section>
<section id="emergent-biological-knowledge" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="emergent-biological-knowledge"><span class="header-section-number">12.1.2</span> Emergent Biological Knowledge</h3>
<p>The surprise was not that ESM-1b learned to predict masked amino acids accurately, but what else it learned in the process. Despite never seeing structural or functional labels during training, ESM-1b’s internal representations encode information about protein biology at multiple levels of organization.</p>
<p>Secondary structure emerges in the attention patterns. When researchers analyzed which sequence positions the model attends to when making predictions, they found that attention concentrates along patterns corresponding to alpha helices and beta sheets. The model implicitly learns that certain amino acid sequences form specific structural elements, encoding this knowledge without ever being told what secondary structure is.</p>
<p>More remarkably, ESM-1b captures residue-residue contacts. Amino acids that are distant in the linear sequence but close in three-dimensional space attend to each other in the model’s attention matrices. This emergent capability suggests the model learns aspects of protein folding purely from sequence statistics. When attention weights were converted to contact predictions, they achieved accuracy approaching dedicated contact prediction methods that were explicitly trained for that task.</p>
<p>The model’s masked token predictions correlate strongly with position-specific conservation scores derived from multiple sequence alignments. ESM effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns across 33 million sequences rather than from explicit conservation annotations. Positions where the model confidently predicts specific amino acids correspond to positions that are conserved across protein families.</p>
<p>Perhaps most striking, attention concentrates on functionally important positions. Catalytic residues, binding sites, and other sites of biological importance receive elevated attention even without explicit functional annotation in the training data. The model discovers that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites where nature has constrained variation because they perform essential functions.</p>
</section>
<section id="esm-2-scaling-up" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="esm-2-scaling-up"><span class="header-section-number">12.1.3</span> ESM-2: Scaling Up</h3>
<p>ESM-2 extended the ESM approach across a range of model scales, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity <span class="citation" data-cites="lin_esm-2_2022">(<a href="references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. The results confirmed a pattern familiar from natural language processing: bigger models learn more.</p>
<table class="caption-top table">
<caption>ESM-2 model family spanning four orders of magnitude in parameter count, with architecture details and relative performance on structure-related tasks.</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>Layers</th>
<th>Hidden Dim</th>
<th>Performance Gain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ESM-2 (8M)</td>
<td>8M</td>
<td>6</td>
<td>320</td>
<td>Baseline</td>
</tr>
<tr class="even">
<td>ESM-2 (35M)</td>
<td>35M</td>
<td>12</td>
<td>480</td>
<td>Modest</td>
</tr>
<tr class="odd">
<td>ESM-2 (150M)</td>
<td>150M</td>
<td>30</td>
<td>640</td>
<td>Substantial</td>
</tr>
<tr class="even">
<td>ESM-2 (650M)</td>
<td>650M</td>
<td>33</td>
<td>1280</td>
<td>Large</td>
</tr>
<tr class="odd">
<td>ESM-2 (3B)</td>
<td>3B</td>
<td>36</td>
<td>2560</td>
<td>Near-optimal</td>
</tr>
<tr class="even">
<td>ESM-2 (15B)</td>
<td>15B</td>
<td>48</td>
<td>5120</td>
<td>State-of-the-art</td>
</tr>
</tbody>
</table>
<p>Performance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. The scaling relationship is not linear: doubling parameters does not double accuracy. But gains remain consistent through even the largest models, suggesting that the 15-billion parameter ceiling reflects computational constraints rather than fundamental limits on what sequence statistics can teach.</p>
<p>The scaling behavior mirrors observations in natural language processing, where larger models consistently capture more nuanced patterns. This predictable relationship between scale and capability provides a roadmap for model development: if more biological knowledge is needed, train a larger model on more data. The practical implications shaped how the field approached subsequent genomic foundation models.</p>
</section>
</section>
<section id="alternative-architectures" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="alternative-architectures"><span class="header-section-number">12.2</span> Alternative Architectures</h2>
<p>The success of ESM raised a natural question: how much depends on the specific BERT architecture versus the general approach of self-supervised learning on protein sequences? The ProtTrans family explored this question by applying multiple transformer architectures to protein modeling <span class="citation" data-cites="elnaggar_prottrans_2021">(<a href="references.html#ref-elnaggar_prottrans_2021" role="doc-biblioref">Elnaggar et al. 2021</a>)</span>.</p>
<p>ProtBERT applies the bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences. This training corpus, substantially larger than UniRef50, provides broader coverage at the cost of including more redundant and potentially lower-quality sequences. The architectural choices match ESM closely, enabling direct comparison of training data effects.</p>
<p>ProtT5 adapts the encoder-decoder architecture from T5, enabling both understanding and generation tasks <span class="citation" data-cites="raffel_t5_2019">(<a href="references.html#ref-raffel_t5_2019" role="doc-biblioref">Raffel et al. 2019</a>)</span>. The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture proved valuable for tasks requiring sequence generation, such as structure-conditioned design or sequence completion, though the encoder-only architecture remains dominant for embedding and classification tasks.</p>
<p>ProtXLNet explores permutation language modeling, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training <span class="citation" data-cites="yang_xlnet_2020">(<a href="references.html#ref-yang_xlnet_2020" role="doc-biblioref">Yang et al. 2020</a>)</span>. By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies at the cost of more complex training.</p>
<p>These architectural variants demonstrate that the protein language modeling paradigm generalizes beyond specific design choices. All architectures learn meaningful representations when trained on sufficient data, though performance differences emerge for specific downstream tasks. Encoder-only models excel at classification and embedding tasks where the entire sequence is available. Encoder-decoder models enable generation tasks where outputs must be produced token by token.</p>
</section>
<section id="attention-and-evolutionary-coupling" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="attention-and-evolutionary-coupling"><span class="header-section-number">12.3</span> Attention and Evolutionary Coupling</h2>
<p>The emergence of contact information in ESM’s attention patterns connects to a deeper principle: evolutionary coupling. When two residues must maintain physical contact for a protein to function, mutations at one position create selective pressure for compensatory mutations at the other. Over evolutionary time, these correlated mutations leave statistical signatures in protein families that can be detected through covariance analysis of multiple sequence alignments.</p>
<p>Direct Coupling Analysis (DCA) and related methods extract these coevolutionary signals to predict residue-residue contacts <span class="citation" data-cites="morcos_dca_2011">(<a href="references.html#ref-morcos_dca_2011" role="doc-biblioref"><strong>morcos_dca_2011?</strong></a>)</span>. The approach requires constructing multiple sequence alignments, computing covariance matrices, and applying statistical corrections to distinguish direct from indirect correlations. The resulting contact predictions enabled the first accurate structure predictions for proteins lacking homologs in structural databases.</p>
<p>Protein language models learn to extract similar information through a different route. Rather than computing covariance explicitly, transformers learn attention patterns that capture which positions inform predictions at other positions. When position <span class="math inline">\(i\)</span> strongly attends to position <span class="math inline">\(j\)</span> during masked prediction, the model has learned that knowing the amino acid at <span class="math inline">\(j\)</span> helps predict the amino acid at <span class="math inline">\(i\)</span>. This is precisely the signature of evolutionary coupling: positions that covary because they must maintain physical contact.</p>
<p>The attention-based approach offers several advantages over traditional covariance analysis. Language models generalize across protein families, learning shared principles that transfer to proteins with sparse evolutionary sampling. They handle the statistical challenge of distinguishing direct from indirect correlations implicitly through deep architecture rather than requiring explicit correction. And they provide rich representations beyond binary contact predictions, encoding information about the strength and nature of residue relationships.</p>
<p>Rao and colleagues demonstrated this connection directly by extracting attention weights from ESM and converting them to contact predictions <span class="citation" data-cites="rao_transformer_2021">(<a href="references.html#ref-rao_transformer_2021" role="doc-biblioref"><strong>rao_transformer_2021?</strong></a>)</span>. The resulting predictions approached the accuracy of dedicated contact prediction methods, despite the model never being trained to predict contacts. The attention mechanism, optimized purely for masked token prediction, discovers the coevolutionary structure of protein sequences as a byproduct.</p>
</section>
<section id="esmfold-structure-from-sequence" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="esmfold-structure-from-sequence"><span class="header-section-number">12.4</span> ESMFold: Structure from Sequence</h2>
<section id="eliminating-the-alignment-bottleneck" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="eliminating-the-alignment-bottleneck"><span class="header-section-number">12.4.1</span> Eliminating the Alignment Bottleneck</h3>
<p>The most dramatic demonstration of protein language model capabilities came with ESMFold, which predicts protein 3D structure directly from ESM-2 embeddings without requiring multiple sequence alignments <span class="citation" data-cites="lin_esm-2_2022">(<a href="references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. Traditional structure prediction, including AlphaFold2, relies heavily on MSAs constructed through computationally expensive searches against sequence databases. These searches can take hours per protein, and prediction quality depends critically on finding informative homologs.</p>
<p>ESMFold eliminates this requirement entirely. The architecture couples ESM-2 (using the 15-billion parameter variant) with a structure module adapted from AlphaFold2’s Evoformer and structure module. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates. The model takes a single sequence as input and outputs predicted 3D coordinates for all atoms.</p>
<p>The computational speedup is substantial: approximately 60-fold faster than AlphaFold2 for typical proteins. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive. Metagenomic proteins, often lacking close homologs in existing databases, represent exactly the cases where MSA-based methods struggle and where single-sequence predictions become essential.</p>
<p>ESMFold achieves atomic-level accuracy for many proteins, though slightly below AlphaFold2 for proteins that benefit strongly from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where explicit alignments provide information that single-sequence analysis cannot fully recover. For well-represented protein families, ESMFold approaches AlphaFold2 accuracy at a fraction of the computational cost.</p>
</section>
<section id="what-esmfold-reveals-about-plms" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="what-esmfold-reveals-about-plms"><span class="header-section-number">12.4.2</span> What ESMFold Reveals About PLMs</h3>
<p>ESMFold’s success demonstrates that ESM-2’s internal representations encode sufficient information to determine 3D structure. The language model has learned not merely local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular three-dimensional shape.</p>
<p>This has profound implications for understanding what protein language models learn. The attention patterns that emerge from masked prediction are, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space to maintain stability attend to each other in the transformer’s attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution under physical constraints, encode structural information that sufficiently powerful language models can decode.</p>
<p>The fundamental insight is that evolution has already solved the structure prediction problem, millions of times over, and recorded the solutions in sequence databases. Language models learn to read those solutions, extracting the implicit structural knowledge that selection has embedded in surviving sequences.</p>
</section>
</section>
<section id="function-prediction" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="function-prediction"><span class="header-section-number">12.5</span> Function Prediction</h2>
<p>Beyond structure, protein language models enable prediction of protein function directly from sequence. Function prediction encompasses multiple tasks: predicting Gene Ontology terms that describe molecular function, biological process, and cellular component; classifying enzyme activity; identifying binding sites and interaction partners; and predicting subcellular localization.</p>
<p>Traditional function prediction relied on homology: proteins similar in sequence are assumed to share function. This approach fails for orphan proteins lacking characterized homologs and cannot distinguish functional differences between closely related sequences. PLM-based approaches address both limitations by learning representations that capture functional signatures beyond simple sequence similarity.</p>
<p>For Gene Ontology term prediction, PLM embeddings serve as input features to classification models that predict which GO terms apply to each protein. The embeddings capture evolutionary and structural information relevant to function, enabling accurate predictions even for proteins with limited homology to characterized sequences. Performance improves with embedding quality, suggesting that larger language models capture more functionally relevant information.</p>
<p>Enzyme classification benefits similarly from PLM representations. The Enzyme Commission hierarchy categorizes enzymes by the reactions they catalyze, from broad classes (oxidoreductases, transferases) to specific substrate preferences. PLM embeddings distinguish these categories effectively, capturing the sequence features that determine catalytic activity without requiring explicit structural analysis.</p>
<p>Binding site prediction applies attention analysis to identify which residues participate in ligand binding, protein-protein interactions, or nucleic acid recognition. Positions that the model identifies as important for contextual prediction often correspond to functionally important sites, including binding pockets and catalytic residues. This capability enables rapid identification of functional sites in newly sequenced proteins.</p>
</section>
<section id="variant-effect-prediction" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="variant-effect-prediction"><span class="header-section-number">12.6</span> Variant Effect Prediction</h2>
<p>A critical clinical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely pathogenic or benign. The traditional approach required either direct experimental characterization or computational methods trained on labeled pathogenicity data, both of which scale poorly to the millions of possible variants in each human genome.</p>
<p>ESM-1v demonstrated that PLMs can predict variant effects without any training on variant labels <span class="citation" data-cites="meier_esm-1v_2021">(<a href="references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>)</span>. The approach exploits the masked language modeling objective directly: for a variant at position <span class="math inline">\(i\)</span> changing amino acid <span class="math inline">\(a\)</span> to amino acid <span class="math inline">\(b\)</span>, compute the log-likelihood ratio:</p>
<p><span class="math display">\[\Delta \text{score} = \log P(b \mid \text{context}) - \log P(a \mid \text{context})\]</span></p>
<p>If the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This zero-shot prediction requires no labeled training data. The model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.</p>
<p>The intuition is straightforward. Evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids. Substitutions that violate these preferences are more likely to disrupt function. The language model captures these preferences through training on millions of evolutionarily successful sequences. Variants that the model finds surprising are more likely to be functionally disruptive.</p>
<p>Brandes and colleagues applied ESM-1b to predict effects for all approximately 450 million possible missense variants in the human genome, providing a precomputed resource for clinical variant interpretation <span class="citation" data-cites="brandes_genome-wide_2023">(<a href="references.html#ref-brandes_genome-wide_2023" role="doc-biblioref">Brandes et al. 2023</a>)</span>. On ClinVar benchmarks, ESM-1b outperformed existing methods in classifying variants as pathogenic or benign.</p>
<p>AlphaMissense extended this approach by combining PLM representations with structural context from predicted protein structures <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>. The integration of sequence-based and structure-based signals improves accuracy, particularly for variants affecting protein stability or buried residues. AlphaMissense provides predictions for all approximately 71 million possible single amino acid substitutions in the human proteome.</p>
<p>The detailed comparison of variant effect prediction methods, including how PLM-based scores integrate with clinical classification frameworks, is covered in <a href="p3-ch14-vep-fm.html" class="quarto-xref"><span>Chapter 14</span></a>. Here, the key point is that protein language models provide the foundational representations that make accurate zero-shot variant prediction possible.</p>
</section>
<section id="integration-with-structure-prediction" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="integration-with-structure-prediction"><span class="header-section-number">12.7</span> Integration with Structure Prediction</h2>
<p>Protein language models exist within a broader ecosystem of computational methods for protein analysis. Understanding how PLMs relate to structure prediction systems clarifies their role and capabilities.</p>
<p>AlphaFold2 achieved breakthrough accuracy in structure prediction by combining learned representations with explicit geometric modeling <span class="citation" data-cites="jumper_alphafold2_2021">(<a href="references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. The architecture processes both sequence information through embeddings and evolutionary information through multiple sequence alignments, using an attention-based module (Evoformer) to integrate these signals before predicting atomic coordinates. AlphaFold2’s success depended critically on MSA quality: proteins with many homologs could be predicted accurately, while orphan proteins remained challenging.</p>
<p>ESMFold demonstrated that PLM embeddings can replace MSA-derived features, achieving competitive accuracy without the alignment bottleneck. This finding clarified the relationship between language models and structure prediction: PLMs learn to compress evolutionary information into representations that are functionally equivalent to explicit alignments, at least for proteins with sufficient representation in training databases.</p>
<p>AlphaFold3 extended structure prediction to protein complexes, nucleic acids, and small molecules <span class="citation" data-cites="abramson_alphafold3_2024">(<a href="references.html#ref-abramson_alphafold3_2024" role="doc-biblioref">Abramson et al. 2024</a>)</span>. The architecture incorporates diffusion-based generation, enabling prediction of binding poses and complex assemblies. These capabilities complement PLM-based function prediction by providing structural context for interpreting functional predictions.</p>
<p>Generative protein design methods including RFDiffusion and ProteinMPNN leverage both structural and sequence information <span class="citation" data-cites="watson_rfdiffusion_2023 dauparas_proteinmpnn_2022">(<a href="references.html#ref-watson_rfdiffusion_2023" role="doc-biblioref">Watson et al. 2023</a>; <a href="references.html#ref-dauparas_proteinmpnn_2022" role="doc-biblioref">Dauparas et al. 2022</a>)</span>. RFDiffusion generates novel protein backbones through diffusion processes conditioned on design objectives. ProteinMPNN designs sequences likely to fold into specified structures. Both methods benefit from PLM representations when designing sequences with desired functional properties, demonstrating how language models integrate into the broader protein engineering pipeline.</p>
<p>The trajectory from ESM to ESMFold to integration with design tools illustrates how PLMs serve as a foundation for diverse downstream applications. The representations learned through self-supervised training transfer across tasks, providing a common language for structure prediction, function annotation, variant interpretation, and protein engineering. This pattern of foundation models enabling diverse applications recurs throughout genomic AI, as discussed in <a href="p3-ch10-fm-principles.html" class="quarto-xref"><span>Chapter 10</span></a>.</p>
</section>
<section id="limitations" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="limitations"><span class="header-section-number">12.8</span> Limitations</h2>
<p>Despite their success, protein language models face several limitations that inform the development of genomic models and guide appropriate application.</p>
<section id="orphan-and-dark-proteins" class="level3" data-number="12.8.1">
<h3 data-number="12.8.1" class="anchored" data-anchor-id="orphan-and-dark-proteins"><span class="header-section-number">12.8.1</span> Orphan and Dark Proteins</h3>
<p>PLMs learn from evolutionary statistics, performing best for proteins with rich representation in training databases. Orphan proteins, those unique to specific lineages without detectable homologs, lack the evolutionary context that PLMs exploit. For these proteins, the model has no basis for distinguishing likely from unlikely amino acids at each position, and predictions degrade accordingly.</p>
<p>The problem extends to “dark” proteins that are poorly characterized despite having homologs. If an entire protein family has escaped experimental characterization, PLMs may learn statistical patterns without capturing functional relevance. The model cannot distinguish constraint imposed by function from constraint imposed by historical accident.</p>
</section>
<section id="novel-folds" class="level3" data-number="12.8.2">
<h3 data-number="12.8.2" class="anchored" data-anchor-id="novel-folds"><span class="header-section-number">12.8.2</span> Novel Folds</h3>
<p>Training data shapes what models can predict. PLMs trained on natural protein databases learn the statistical patterns of naturally occurring folds, potentially struggling with designed proteins or hypothetical folds outside the training distribution. When researchers design proteins with novel topologies not found in nature, PLM predictions become less reliable because the relevant sequence patterns were never encountered during training.</p>
</section>
<section id="conformational-flexibility" class="level3" data-number="12.8.3">
<h3 data-number="12.8.3" class="anchored" data-anchor-id="conformational-flexibility"><span class="header-section-number">12.8.3</span> Conformational Flexibility</h3>
<p>Most PLM representations assume a single static structure, but many proteins adopt multiple conformations relevant to function. Allosteric proteins, intrinsically disordered regions, and proteins that undergo conformational changes upon binding present challenges for methods that embed each sequence into a single representation. The language model learns the average properties of sequences but may not capture the dynamic range that determines biological behavior.</p>
</section>
<section id="epistasis" class="level3" data-number="12.8.4">
<h3 data-number="12.8.4" class="anchored" data-anchor-id="epistasis"><span class="header-section-number">12.8.4</span> Epistasis</h3>
<p>Most variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit epistasis, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors model marginal effects at each position but do not explicitly capture higher-order interactions, though the contextual embeddings may represent some epistatic relationships implicitly.</p>
</section>
<section id="interpretability" class="level3" data-number="12.8.5">
<h3 data-number="12.8.5" class="anchored" data-anchor-id="interpretability"><span class="header-section-number">12.8.5</span> Interpretability</h3>
<p>While attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods (<a href="p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 24</span></a>), but PLMs remain partially opaque. For clinical applications where explanations matter, this interpretability gap limits adoption. A prediction that a variant is pathogenic is more useful when accompanied by mechanistic insight into why the variant disrupts function.</p>
</section>
</section>
<section id="lessons-for-genomic-foundation-models" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="lessons-for-genomic-foundation-models"><span class="header-section-number">12.9</span> Lessons for Genomic Foundation Models</h2>
<p>The success of protein language models established principles that guided subsequent development of genomic foundation models. These lessons transfer with appropriate modifications to DNA and RNA modeling.</p>
<section id="self-supervision-captures-biological-knowledge" class="level3" data-number="12.9.1">
<h3 data-number="12.9.1" class="anchored" data-anchor-id="self-supervision-captures-biological-knowledge"><span class="header-section-number">12.9.1</span> Self-Supervision Captures Biological Knowledge</h3>
<p>PLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can exploit. This principle underlies the entire foundation model paradigm: sufficiently large models trained on sufficiently large datasets with appropriate objectives will learn representations that capture biological function.</p>
</section>
<section id="scale-yields-consistent-improvements" class="level3" data-number="12.9.2">
<h3 data-number="12.9.2" class="anchored" data-anchor-id="scale-yields-consistent-improvements"><span class="header-section-number">12.9.2</span> Scale Yields Consistent Improvements</h3>
<p>Performance improves predictably with model size through the range currently explored. The progression from 8 million to 15 billion parameters in ESM-2 showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While scaling cannot continue indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This relationship justified the computational investment in large genomic foundation models.</p>
</section>
<section id="transfer-learning-is-effective" class="level3" data-number="12.9.3">
<h3 data-number="12.9.3" class="anchored" data-anchor-id="transfer-learning-is-effective"><span class="header-section-number">12.9.3</span> Transfer Learning is Effective</h3>
<p>Representations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects, function annotation). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids simultaneously learns about protein structure, function, evolutionary constraint, and disease relevance. The same principle motivates genomic language models: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects.</p>
</section>
<section id="architecture-choices-must-match-sequence-properties" class="level3" data-number="12.9.4">
<h3 data-number="12.9.4" class="anchored" data-anchor-id="architecture-choices-must-match-sequence-properties"><span class="header-section-number">12.9.4</span> Architecture Choices Must Match Sequence Properties</h3>
<p>The BERT-style bidirectional encoder proved effective for proteins, where entire sequences are typically available and lengths rarely exceed a thousand residues. Genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with coding regions being dense while intergenic regions are sparser, and structural features including reverse-complement relationships absent in proteins. These differences motivate architectural adaptations in genomic language models, as explored in <a href="p3-ch11-dna-lm.html" class="quarto-xref"><span>Chapter 11</span></a>.</p>
</section>
<section id="integration-multiplies-capability" class="level3" data-number="12.9.5">
<h3 data-number="12.9.5" class="anchored" data-anchor-id="integration-multiplies-capability"><span class="header-section-number">12.9.5</span> Integration Multiplies Capability</h3>
<p>AlphaMissense demonstrated that PLM embeddings combine effectively with structural and population genetics information, achieving accuracy beyond what any single information source provides. The most powerful methods integrate multiple signals, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace functional annotations, chromatin data, and clinical information.</p>
</section>
</section>
<section id="significance" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="significance"><span class="header-section-number">12.10</span> Significance</h2>
<p>Protein language models established that transformer architectures can learn deep biological knowledge from sequence data alone. ESM’s ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. The framework validated a paradigm: treat biological sequences as a language, train large models to predict masked tokens, and extract functional knowledge from the learned representations.</p>
<p>This success directly motivated the development of genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too. The DNA language models covered in <a href="p3-ch11-dna-lm.html" class="quarto-xref"><span>Chapter 11</span></a> adapt PLM architectures and training strategies to the distinct challenges of genomic sequences: longer contexts, different alphabets, and the full complexity of gene regulation. RNA language models (<a href="p4-ch15-rna.html" class="quarto-xref"><span>Chapter 15</span></a>) occupy an intermediate position, sharing features with both protein and DNA modeling.</p>
<p>The integration path continues beyond sequence modeling. Just as PLM representations feed into structure prediction (ESMFold) and variant effect prediction (AlphaMissense), genomic language model embeddings integrate into regulatory models (<a href="p3-ch13-regulatory.html" class="quarto-xref"><span>Chapter 13</span></a>) and clinical risk prediction (<a href="p6-ch25-clinical-risk.html" class="quarto-xref"><span>Chapter 25</span></a>). Protein design methods (<a href="p6-ch28-design.html" class="quarto-xref"><span>Chapter 28</span></a>) demonstrate how generative modeling builds on the representations that language models provide. Throughout this progression, the principle established by ESM remains: self-supervised learning on biological sequences captures knowledge that transfers across diverse applications, providing a foundation for genomic AI.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-abramson_alphafold3_2024" class="csl-entry" role="listitem">
Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. <span>“[<span>AlphaFold3</span>] <span>Accurate</span> Structure Prediction of Biomolecular Interactions with <span>AlphaFold</span> 3.”</span> <em>Nature</em> 630 (8016): 493–500. <a href="https://doi.org/10.1038/s41586-024-07487-w">https://doi.org/10.1038/s41586-024-07487-w</a>.
</div>
<div id="ref-brandes_genome-wide_2023" class="csl-entry" role="listitem">
Brandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. <span>“Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.”</span> <em>Nature Genetics</em> 55 (9): 1512–22. <a href="https://doi.org/10.1038/s41588-023-01465-0">https://doi.org/10.1038/s41588-023-01465-0</a>.
</div>
<div id="ref-cheng_alphamissense_2023" class="csl-entry" role="listitem">
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. <span>“[<span>AlphaMissense</span>] <span>Accurate</span> Proteome-Wide Missense Variant Effect Prediction with <span>AlphaMissense</span>.”</span> <em>Science</em> 381 (6664): eadg7492. <a href="https://doi.org/10.1126/science.adg7492">https://doi.org/10.1126/science.adg7492</a>.
</div>
<div id="ref-dauparas_proteinmpnn_2022" class="csl-entry" role="listitem">
Dauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. <span>“Robust Deep Learning–Based Protein Sequence Design Using <span>ProteinMPNN</span>.”</span> <em>Science</em> 378 (6615): 49–56. <a href="https://doi.org/10.1126/science.add2187">https://doi.org/10.1126/science.add2187</a>.
</div>
<div id="ref-elnaggar_prottrans_2021" class="csl-entry" role="listitem">
Elnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. <span>“<span>ProtTrans</span>: <span>Towards</span> <span>Cracking</span> the <span>Language</span> of <span>Life</span>’s <span>Code</span> <span>Through</span> <span>Self</span>-<span>Supervised</span> <span>Deep</span> <span>Learning</span> and <span>High</span> <span>Performance</span> <span>Computing</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2007.06225">https://doi.org/10.48550/arXiv.2007.06225</a>.
</div>
<div id="ref-jumper_alphafold2_2021" class="csl-entry" role="listitem">
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. <span>“[<span>AlphaFold2</span>] <span>Highly</span> Accurate Protein Structure Prediction with <span>AlphaFold</span>.”</span> <em>Nature</em> 596 (7873): 583–89. <a href="https://doi.org/10.1038/s41586-021-03819-2">https://doi.org/10.1038/s41586-021-03819-2</a>.
</div>
<div id="ref-lin_esm-2_2022" class="csl-entry" role="listitem">
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. <span>“[<span>ESM</span>-2] <span>Language</span> Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.07.20.500902">https://doi.org/10.1101/2022.07.20.500902</a>.
</div>
<div id="ref-meier_esm-1v_2021" class="csl-entry" role="listitem">
Meier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. <span>“[<span>ESM</span>-1v] <span>Language</span> Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.”</span> bioRxiv. <a href="https://doi.org/10.1101/2021.07.09.450648">https://doi.org/10.1101/2021.07.09.450648</a>.
</div>
<div id="ref-raffel_t5_2019" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. <span>“Exploring the <span>Limits</span> of <span>Transfer</span> <span>Learning</span> with a <span>Unified</span> <span>Text</span>-to-<span>Text</span> <span>Transformer</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1910.10683">https://doi.org/10.48550/arXiv.1910.10683</a>.
</div>
<div id="ref-rives_esm_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“[<span>ESM</span>-1b] <span>Biological</span> Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 118 (15): e2016239118. <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-suzek_uniref_2007" class="csl-entry" role="listitem">
Suzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. <span>“<span>UniRef</span>: Comprehensive and Non-Redundant <span>UniProt</span> Reference Clusters.”</span> <em>Bioinformatics</em> 23 (10): 1282–88. <a href="https://doi.org/10.1093/bioinformatics/btm098">https://doi.org/10.1093/bioinformatics/btm098</a>.
</div>
<div id="ref-watson_rfdiffusion_2023" class="csl-entry" role="listitem">
Watson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. <span>“De Novo Design of Protein Structure and Function with <span>RFdiffusion</span>.”</span> <em>Nature</em> 620 (7976): 1089–1100. <a href="https://doi.org/10.1038/s41586-023-06415-8">https://doi.org/10.1038/s41586-023-06415-8</a>.
</div>
<div id="ref-yang_xlnet_2020" class="csl-entry" role="listitem">
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. <span>“<span>XLNet</span>: <span>Generalized</span> <span>Autoregressive</span> <span>Pretraining</span> for <span>Language</span> <span>Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1906.08237">https://doi.org/10.48550/arXiv.1906.08237</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p3-ch11-dna-lm.html" class="pagination-link" aria-label="DNA Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p3-ch13-regulatory.html" class="pagination-link" aria-label="Regulatory Models">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>