<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>13&nbsp; Protein Language Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p3-ch14-hybrid.html" rel="next">
<link href="./p3-ch12-rna.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p3--architectures.html">Part II: Deep Learning Architectures</a></li><li class="breadcrumb-item"><a href="./p3-ch13-plm.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-pgs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS &amp; Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-cadd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deleteriousness Scores</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-foundation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretrain.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning &amp; Deployment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CNN Sequence-to-Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-plm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-hybrid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">p4–multi-modal_multi-scale.qmd</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-sc-epi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Single-Cell &amp; Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Graphs, Networks, and Biology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Multi-Omics &amp; Systems Biology</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch18-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch19-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-vep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-confound.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Interpretability &amp; Mechanisms</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch23-clinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch24-variants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Pathogenic Variant Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-drugs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Drug Discovery &amp; Biotech</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Future Work &amp; Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-model-list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-esm-model-family" id="toc-the-esm-model-family" class="nav-link active" data-scroll-target="#the-esm-model-family"><span class="header-section-number">13.1</span> The ESM Model Family</a>
  <ul class="collapse">
  <li><a href="#esm-1b-establishing-the-paradigm" id="toc-esm-1b-establishing-the-paradigm" class="nav-link" data-scroll-target="#esm-1b-establishing-the-paradigm"><span class="header-section-number">13.1.1</span> ESM-1b: Establishing the Paradigm</a></li>
  <li><a href="#emergent-biological-knowledge" id="toc-emergent-biological-knowledge" class="nav-link" data-scroll-target="#emergent-biological-knowledge"><span class="header-section-number">13.1.2</span> Emergent Biological Knowledge</a></li>
  <li><a href="#esm-2-scaling-up" id="toc-esm-2-scaling-up" class="nav-link" data-scroll-target="#esm-2-scaling-up"><span class="header-section-number">13.1.3</span> ESM-2: Scaling Up</a></li>
  </ul></li>
  <li><a href="#alternative-architectures-the-prottrans-family" id="toc-alternative-architectures-the-prottrans-family" class="nav-link" data-scroll-target="#alternative-architectures-the-prottrans-family"><span class="header-section-number">13.2</span> Alternative Architectures: The ProtTrans Family</a></li>
  <li><a href="#zero-shot-variant-effect-prediction" id="toc-zero-shot-variant-effect-prediction" class="nav-link" data-scroll-target="#zero-shot-variant-effect-prediction"><span class="header-section-number">13.3</span> Zero-Shot Variant Effect Prediction</a>
  <ul class="collapse">
  <li><a href="#the-zero-shot-paradigm" id="toc-the-zero-shot-paradigm" class="nav-link" data-scroll-target="#the-zero-shot-paradigm"><span class="header-section-number">13.3.1</span> The Zero-Shot Paradigm</a></li>
  <li><a href="#genome-wide-application" id="toc-genome-wide-application" class="nav-link" data-scroll-target="#genome-wide-application"><span class="header-section-number">13.3.2</span> Genome-Wide Application</a></li>
  <li><a href="#the-proteingym-benchmark" id="toc-the-proteingym-benchmark" class="nav-link" data-scroll-target="#the-proteingym-benchmark"><span class="header-section-number">13.3.3</span> The ProteinGym Benchmark</a></li>
  </ul></li>
  <li><a href="#esmfold-structure-from-sequence" id="toc-esmfold-structure-from-sequence" class="nav-link" data-scroll-target="#esmfold-structure-from-sequence"><span class="header-section-number">13.4</span> ESMFold: Structure from Sequence</a>
  <ul class="collapse">
  <li><a href="#eliminating-the-alignment-bottleneck" id="toc-eliminating-the-alignment-bottleneck" class="nav-link" data-scroll-target="#eliminating-the-alignment-bottleneck"><span class="header-section-number">13.4.1</span> Eliminating the Alignment Bottleneck</a></li>
  <li><a href="#what-esmfold-reveals-about-plms" id="toc-what-esmfold-reveals-about-plms" class="nav-link" data-scroll-target="#what-esmfold-reveals-about-plms"><span class="header-section-number">13.4.2</span> What ESMFold Reveals About PLMs</a></li>
  </ul></li>
  <li><a href="#integration-into-variant-interpretation-pipelines" id="toc-integration-into-variant-interpretation-pipelines" class="nav-link" data-scroll-target="#integration-into-variant-interpretation-pipelines"><span class="header-section-number">13.5</span> Integration into Variant Interpretation Pipelines</a>
  <ul class="collapse">
  <li><a href="#cadd-v1.7-plm-features-for-ensemble-methods" id="toc-cadd-v1.7-plm-features-for-ensemble-methods" class="nav-link" data-scroll-target="#cadd-v1.7-plm-features-for-ensemble-methods"><span class="header-section-number">13.5.1</span> CADD v1.7: PLM Features for Ensemble Methods</a></li>
  <li><a href="#alphamissense-combining-plm-and-structure" id="toc-alphamissense-combining-plm-and-structure" class="nav-link" data-scroll-target="#alphamissense-combining-plm-and-structure"><span class="header-section-number">13.5.2</span> AlphaMissense: Combining PLM and Structure</a></li>
  </ul></li>
  <li><a href="#lessons-for-genomic-foundation-models" id="toc-lessons-for-genomic-foundation-models" class="nav-link" data-scroll-target="#lessons-for-genomic-foundation-models"><span class="header-section-number">13.6</span> Lessons for Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#self-supervision-works" id="toc-self-supervision-works" class="nav-link" data-scroll-target="#self-supervision-works"><span class="header-section-number">13.6.1</span> Self-Supervision Works</a></li>
  <li><a href="#scale-matters" id="toc-scale-matters" class="nav-link" data-scroll-target="#scale-matters"><span class="header-section-number">13.6.2</span> Scale Matters</a></li>
  <li><a href="#transfer-learning-is-effective" id="toc-transfer-learning-is-effective" class="nav-link" data-scroll-target="#transfer-learning-is-effective"><span class="header-section-number">13.6.3</span> Transfer Learning is Effective</a></li>
  <li><a href="#architecture-choices-matter" id="toc-architecture-choices-matter" class="nav-link" data-scroll-target="#architecture-choices-matter"><span class="header-section-number">13.6.4</span> Architecture Choices Matter</a></li>
  <li><a href="#integration-with-other-modalities" id="toc-integration-with-other-modalities" class="nav-link" data-scroll-target="#integration-with-other-modalities"><span class="header-section-number">13.6.5</span> Integration with Other Modalities</a></li>
  </ul></li>
  <li><a href="#limitations-and-ongoing-challenges" id="toc-limitations-and-ongoing-challenges" class="nav-link" data-scroll-target="#limitations-and-ongoing-challenges"><span class="header-section-number">13.7</span> Limitations and Ongoing Challenges</a>
  <ul class="collapse">
  <li><a href="#sequence-length-constraints" id="toc-sequence-length-constraints" class="nav-link" data-scroll-target="#sequence-length-constraints"><span class="header-section-number">13.7.1</span> Sequence Length Constraints</a></li>
  <li><a href="#orphan-proteins" id="toc-orphan-proteins" class="nav-link" data-scroll-target="#orphan-proteins"><span class="header-section-number">13.7.2</span> Orphan Proteins</a></li>
  <li><a href="#epistasis" id="toc-epistasis" class="nav-link" data-scroll-target="#epistasis"><span class="header-section-number">13.7.3</span> Epistasis</a></li>
  <li><a href="#interpretability" id="toc-interpretability" class="nav-link" data-scroll-target="#interpretability"><span class="header-section-number">13.7.4</span> Interpretability</a></li>
  </ul></li>
  <li><a href="#beyond-language-models-structure-prediction-and-design" id="toc-beyond-language-models-structure-prediction-and-design" class="nav-link" data-scroll-target="#beyond-language-models-structure-prediction-and-design"><span class="header-section-number">13.8</span> Beyond Language Models: Structure Prediction and Design</a>
  <ul class="collapse">
  <li><a href="#structure-prediction-systems" id="toc-structure-prediction-systems" class="nav-link" data-scroll-target="#structure-prediction-systems"><span class="header-section-number">13.8.1</span> Structure Prediction Systems</a></li>
  <li><a href="#generative-design-methods" id="toc-generative-design-methods" class="nav-link" data-scroll-target="#generative-design-methods"><span class="header-section-number">13.8.2</span> Generative Design Methods</a></li>
  <li><a href="#molecular-docking-and-binding" id="toc-molecular-docking-and-binding" class="nav-link" data-scroll-target="#molecular-docking-and-binding"><span class="header-section-number">13.8.3</span> Molecular Docking and Binding</a></li>
  <li><a href="#infrastructure-and-search-methods" id="toc-infrastructure-and-search-methods" class="nav-link" data-scroll-target="#infrastructure-and-search-methods"><span class="header-section-number">13.8.4</span> Infrastructure and Search Methods</a></li>
  <li><a href="#integration-with-protein-language-models" id="toc-integration-with-protein-language-models" class="nav-link" data-scroll-target="#integration-with-protein-language-models"><span class="header-section-number">13.8.5</span> Integration with Protein Language Models</a></li>
  </ul></li>
  <li><a href="#significance" id="toc-significance" class="nav-link" data-scroll-target="#significance"><span class="header-section-number">13.9</span> Significance</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p3--architectures.html">Part II: Deep Learning Architectures</a></li><li class="breadcrumb-item"><a href="./p3-ch13-plm.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-prot" class="quarto-section-identifier"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>TODO:</strong> - ADD TAPE; ProGen - Add figure: ESM architecture diagram showing transformer layers, attention heads, and masked token prediction - Add figure: ESMFold pipeline diagram showing embedding extraction → structure module - Add figure: AlphaMissense workflow showing integration of PLM embeddings with structural context - Consider adding visualization of attention patterns capturing residue contacts - Add table comparing PLM architectures (ESM, ProtTrans variants, ESM-2 scaling) - Add discussion somewhere (here or VEP chapter) on marginal VEP calculations of log likelihoods</p>
</div>
</div>
<p>Before transformers revolutionized genomic sequence modeling, they first transformed our ability to model proteins. The field built upon decades of computational progress, from David Baker’s Rosetta framework that pioneered physics-based protein structure prediction and design through sampling and energy minimization, to the deep learning revolution of the past decade. This progression moved from early generative models like DeepSequence that captured epistatic interactions in protein families <span class="citation" data-cites="riesselman_deepsequence_2018">(<a href="references.html#ref-riesselman_deepsequence_2018" role="doc-biblioref">Riesselman, Ingraham, and Marks 2018</a>)</span>, to EVE’s unsupervised prediction of disease variants from evolutionary data <span class="citation" data-cites="frazer_eve_2021">(<a href="references.html#ref-frazer_eve_2021" role="doc-biblioref">Frazer et al. 2021</a>)</span>, to the ESM family’s demonstration that transformers trained on massive sequence databases learn representations encoding structure and function <span class="citation" data-cites="rives_esm_2021">(<a href="references.html#ref-rives_esm_2021" role="doc-biblioref">Rives et al. 2021</a>)</span>. The trajectory culminated in AlphaFold2’s solution to the protein structure prediction problem <span class="citation" data-cites="jumper_alphafold2_2021">(<a href="references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span> and AlphaMissense’s proteome-wide variant pathogenicity scoring <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>, establishing that self-supervised learning on biological sequences could match or exceed decades of specialized computational methods. The transformative impact of this work was recognized with the 2024 Nobel Prize in Chemistry, awarded to Demis Hassabis and John Jumper for AlphaFold2 and to David Baker for computational protein design. These advances validated and extended the central dogma’s sequence → structure → function paradigm, demonstrating that deep learning models could compress the entire chain of causation into learned representations that predict functional consequences directly from amino acid sequences.</p>
<p>The success of protein language models (PLMs) established a paradigm that would later inspire genomic foundation models: treat biological sequences as a form of natural language, train large transformer models on massive unlabeled sequence databases, and extract functional knowledge through self-supervised learning. The analogy between protein sequences and natural language runs deeper than mere metaphor. Both encode complex information in linear strings of discrete tokens, whether amino acids or words. Both exhibit hierarchical structure, with motifs combining into domains as words combine into phrases. Both have syntax in the form of structural constraints and semantics in the form of functional meaning. And crucially, both are shaped by evolutionary pressure: natural selection filters protein sequences just as cultural selection shapes language.</p>
<p>This chapter examines how protein language models pioneered biological foundation modeling, from the ESM family’s demonstration that transformers can learn protein structure and function from sequence alone, to their application in variant effect prediction and structure determination. Understanding PLMs provides essential context for the genomic language models covered in subsequent chapters, as many architectural choices and training strategies transfer directly from proteins to DNA.</p>
<section id="the-esm-model-family" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="the-esm-model-family"><span class="header-section-number">13.1</span> The ESM Model Family</h2>
<section id="esm-1b-establishing-the-paradigm" class="level3" data-number="13.1.1">
<h3 data-number="13.1.1" class="anchored" data-anchor-id="esm-1b-establishing-the-paradigm"><span class="header-section-number">13.1.1</span> ESM-1b: Establishing the Paradigm</h3>
<p>The Evolutionary Scale Modeling (ESM) project, developed at Meta AI Research, demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision <span class="citation" data-cites="rives_esm_2021">(<a href="references.html#ref-rives_esm_2021" role="doc-biblioref">Rives et al. 2021</a>)</span>. The key insight was that masked language modeling, the same objective that powers BERT in natural language processing, could be applied directly to amino acid sequences.</p>
<p>ESM-1b was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy <span class="citation" data-cites="suzek_uniref_2007">(<a href="references.html#ref-suzek_uniref_2007" role="doc-biblioref">Suzek et al. 2007</a>)</span>. This curation strategy ensures the model sees diverse evolutionary solutions to protein function rather than memorizing overrepresented families.</p>
<p>The architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is masked language modeling: the model learns to predict randomly masked amino acids given surrounding context. This is analogous to BERT’s masked token prediction, but operates on amino acids rather than words.</p>
</section>
<section id="emergent-biological-knowledge" class="level3" data-number="13.1.2">
<h3 data-number="13.1.2" class="anchored" data-anchor-id="emergent-biological-knowledge"><span class="header-section-number">13.1.2</span> Emergent Biological Knowledge</h3>
<p>Despite never seeing structural or functional labels during training, ESM learns representations that capture fundamental biological properties. This emergent knowledge manifests across multiple levels of protein organization.</p>
<p>At the level of secondary structure, attention patterns in ESM correlate with alpha helices and beta sheets. The model implicitly learns that certain amino acid patterns form specific structural elements, encoding this knowledge in its internal representations without any explicit supervision on structure labels.</p>
<p>ESM’s attention heads also capture residue-residue contacts, identifying amino acids that are distant in sequence but close in three-dimensional space. This emergent capability suggests the model learns aspects of protein folding from sequence statistics alone. When researchers analyzed which sequence positions attend to each other in trained ESM models, they found strong correspondence with experimentally determined contact maps.</p>
<p>The model’s masked token predictions correlate with position-specific conservation scores from multiple sequence alignments. ESM effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns in sequence databases rather than from explicit conservation annotations.</p>
<p>Attention also concentrates on catalytic residues, binding sites, and other functionally important positions, even without explicit functional annotation in the training data. The model discovers that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites of biological importance.</p>
</section>
<section id="esm-2-scaling-up" class="level3" data-number="13.1.3">
<h3 data-number="13.1.3" class="anchored" data-anchor-id="esm-2-scaling-up"><span class="header-section-number">13.1.3</span> ESM-2: Scaling Up</h3>
<p>ESM-2 extended the ESM approach with larger models and improved training <span class="citation" data-cites="lin_esm-2_2022">(<a href="references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. The model family spans several orders of magnitude in scale, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>Layers</th>
<th>Contact Prediction Performance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ESM-2 (8M)</td>
<td>8M</td>
<td>6</td>
<td>Baseline</td>
</tr>
<tr class="even">
<td>ESM-2 (35M)</td>
<td>35M</td>
<td>12</td>
<td>+5%</td>
</tr>
<tr class="odd">
<td>ESM-2 (150M)</td>
<td>150M</td>
<td>30</td>
<td>+8%</td>
</tr>
<tr class="even">
<td>ESM-2 (650M)</td>
<td>650M</td>
<td>33</td>
<td>+12%</td>
</tr>
<tr class="odd">
<td>ESM-2 (3B)</td>
<td>3B</td>
<td>36</td>
<td>+15%</td>
</tr>
<tr class="even">
<td>ESM-2 (15B)</td>
<td>15B</td>
<td>48</td>
<td>State-of-the-art</td>
</tr>
</tbody>
</table>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Validate contact prediction performance values</p>
</div>
</div>
<p>Performance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. This phenomenon mirrors the scaling laws observed in natural language processing, where larger models consistently capture more nuanced patterns and achieve better downstream performance. The predictable scaling relationship suggests that continued investment in model size yields reliable returns in biological accuracy.</p>
</section>
</section>
<section id="alternative-architectures-the-prottrans-family" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="alternative-architectures-the-prottrans-family"><span class="header-section-number">13.2</span> Alternative Architectures: The ProtTrans Family</h2>
<p>The ProtTrans family explored multiple transformer architectures for protein sequences, demonstrating that the protein language modeling paradigm generalizes beyond the specific design choices of ESM <span class="citation" data-cites="elnaggar_prottrans_2021">(<a href="references.html#ref-elnaggar_prottrans_2021" role="doc-biblioref">Elnaggar et al. 2021</a>)</span>.</p>
<p>ProtBERT applies the BERT-style bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences <span class="citation" data-cites="devlin_bert_2019 jumper_alphafold2_2021">(<a href="references.html#ref-devlin_bert_2019" role="doc-biblioref">Devlin et al. 2019</a>; <a href="references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. This massive training corpus, substantially larger than UniRef50, provides even broader coverage of protein sequence space.</p>
<p>ProtT5 adapts the encoder-decoder architecture from T5, enabling both understanding and generation tasks <span class="citation" data-cites="raffel_t5_2019">(<a href="references.html#ref-raffel_t5_2019" role="doc-biblioref">Raffel et al. 2019</a>)</span>. The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture is particularly valuable for tasks that require sequence generation, such as protein design or sequence completion.</p>
<p>ProtXLNet explores permutation language modeling based on XLNet, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training <span class="citation" data-cites="yang_xlnet_2020">(<a href="references.html#ref-yang_xlnet_2020" role="doc-biblioref">Yang et al. 2020</a>)</span>. By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies.</p>
<p>These architectural variants demonstrate that the protein language modeling paradigm generalizes across architectures. The choice between encoder-only (BERT-style) and encoder-decoder (T5-style) models depends on the downstream application: encoders excel at classification and embedding tasks, while encoder-decoders enable sequence generation.</p>
</section>
<section id="zero-shot-variant-effect-prediction" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="zero-shot-variant-effect-prediction"><span class="header-section-number">13.3</span> Zero-Shot Variant Effect Prediction</h2>
<p>A critical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely to be pathogenic or benign. Traditionally, this required either direct experimental characterization or computational methods trained on labeled pathogenicity data.</p>
<section id="the-zero-shot-paradigm" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="the-zero-shot-paradigm"><span class="header-section-number">13.3.1</span> The Zero-Shot Paradigm</h3>
<p>ESM-1v demonstrated that PLMs can predict variant effects without any training on variant labels <span class="citation" data-cites="meier_esm-1v_2021">(<a href="references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>)</span>. The approach exploits the masked language modeling objective: for a variant at position <span class="math inline">\(i\)</span> changing amino acid <span class="math inline">\(a\)</span> to amino acid <span class="math inline">\(b\)</span>, compute the log-likelihood ratio:</p>
<p><span class="math display">\[\Delta \text{score} = \log P(b \mid \text{context}) - \log P(a \mid \text{context})\]</span></p>
<p>If the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This zero-shot prediction requires no labeled training data. The model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.</p>
<p>The intuition is straightforward. If evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids, substitutions that violate these preferences are likely to disrupt function. The language model captures these preferences through its training on millions of evolutionarily successful sequences. Variants that the model finds surprising, in the sense of assigning low probability, are more likely to be functionally disruptive.</p>
</section>
<section id="genome-wide-application" class="level3" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="genome-wide-application"><span class="header-section-number">13.3.2</span> Genome-Wide Application</h3>
<p>Brandes and colleagues applied ESM-1b to predict effects for all approximately 450 million possible missense variants in the human genome <span class="citation" data-cites="brandes_genome-wide_2023">(<a href="references.html#ref-brandes_genome-wide_2023" role="doc-biblioref">Brandes et al. 2023</a>)</span>. This comprehensive annotation covers every position in every human protein multiplied by every possible amino acid substitution, providing precomputed effect scores that can be queried for any missense variant without running the model.</p>
<p>On ClinVar, the database of clinically annotated variants, ESM-1b outperformed existing methods in classifying approximately 150,000 missense variants as pathogenic or benign. The model achieved strong correlation with experimental measurements across 28 deep mutational scanning datasets, demonstrating that PLM predictions capture genuine functional information rather than merely correlating with annotation artifacts.</p>
<p>The analysis also identified approximately 2 million variants annotated as damaging only in specific protein isoforms, highlighting the importance of considering alternative splicing when interpreting variant effects. A variant that disrupts function in one isoform may have no effect if that isoform is not expressed in relevant tissues, underscoring the need to integrate PLM predictions with expression context.</p>
</section>
<section id="the-proteingym-benchmark" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="the-proteingym-benchmark"><span class="header-section-number">13.3.3</span> The ProteinGym Benchmark</h3>
<p>ProteinGym provides a comprehensive benchmark for variant effect predictors, aggregating 217 deep mutational scanning assays covering diverse proteins <span class="citation" data-cites="notin_proteingym_2023">(<a href="references.html#ref-notin_proteingym_2023" role="doc-biblioref">Notin et al. 2023</a>)</span>. Deep mutational scanning experiments systematically measure the functional effects of thousands of variants in a protein, providing ground truth for computational method evaluation.</p>
<table class="caption-top table">
<caption>Performance of protein language models and traditional methods on the ProteinGym deep mutational scanning benchmark <span class="citation" data-cites="notin_proteingym_2023">(<a href="references.html#ref-notin_proteingym_2023" role="doc-biblioref">Notin et al. 2023</a>)</span>. Shown are mean Spearman correlations between predicted and experimentally measured variant effects across 217 assays <span class="citation" data-cites="meier_esm-1v_2021 frazer_eve_2021 riesselman_deepsequence_2018 ng_sift_2003 adzhubei_polyphen_2010">(<a href="references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>; <a href="references.html#ref-frazer_eve_2021" role="doc-biblioref">Frazer et al. 2021</a>; <a href="references.html#ref-riesselman_deepsequence_2018" role="doc-biblioref">Riesselman, Ingraham, and Marks 2018</a>; <a href="references.html#ref-ng_sift_2003" role="doc-biblioref">Ng and Henikoff 2003</a>; <a href="references.html#ref-adzhubei_polyphen_2010" role="doc-biblioref">Adzhubei et al. 2010</a>)</span>.</caption>
<thead>
<tr class="header">
<th>Method</th>
<th>Mean Spearman ρ</th>
<th>Information Sources</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ESM-1v</td>
<td>0.48</td>
<td>Single sequence (PLM)</td>
</tr>
<tr class="even">
<td>EVE (evolutionary model)</td>
<td>0.46</td>
<td>MSA (generative model)</td>
</tr>
<tr class="odd">
<td>DeepSequence</td>
<td>0.44</td>
<td>MSA (VAE)</td>
</tr>
<tr class="even">
<td>PolyPhen-2</td>
<td>0.32</td>
<td>Conservation + structure</td>
</tr>
<tr class="odd">
<td>SIFT</td>
<td>0.30</td>
<td>Conservation</td>
</tr>
</tbody>
</table>
<p>PLMs achieve competitive or superior performance to methods that explicitly model evolutionary conservation from multiple sequence alignments, despite using only single sequences as input. This suggests that transformer attention over large sequence databases captures similar information to traditional alignment-based approaches, but in a form that generalizes more readily to novel sequence contexts.</p>
</section>
</section>
<section id="esmfold-structure-from-sequence" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="esmfold-structure-from-sequence"><span class="header-section-number">13.4</span> ESMFold: Structure from Sequence</h2>
<section id="eliminating-the-alignment-bottleneck" class="level3" data-number="13.4.1">
<h3 data-number="13.4.1" class="anchored" data-anchor-id="eliminating-the-alignment-bottleneck"><span class="header-section-number">13.4.1</span> Eliminating the Alignment Bottleneck</h3>
<p>The most dramatic demonstration of PLM capabilities came with ESMFold, which predicts protein 3D structure directly from ESM-2 embeddings <span class="citation" data-cites="lin_esm-2_2022">(<a href="references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. Traditional structure prediction, including AlphaFold2, relies heavily on multiple sequence alignments (MSAs). These computationally expensive searches against sequence databases can take hours per protein, and the quality of predictions depends critically on finding informative homologs.</p>
<p>ESMFold eliminates this requirement entirely. The architecture couples ESM-2 (15 billion parameters) with a structure module adapted from AlphaFold2. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates.</p>
<p>The computational speedup is substantial: approximately 60-fold faster than AlphaFold2 for typical proteins, enabling metagenomic-scale structure prediction. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive.</p>
<p>ESMFold achieves atomic-level accuracy for many proteins, though slightly below AlphaFold2 for proteins that benefit from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where MSAs provide information that single-sequence analysis cannot recover. For well-represented protein families, ESMFold approaches AlphaFold2 accuracy at a fraction of the computational cost.</p>
</section>
<section id="what-esmfold-reveals-about-plms" class="level3" data-number="13.4.2">
<h3 data-number="13.4.2" class="anchored" data-anchor-id="what-esmfold-reveals-about-plms"><span class="header-section-number">13.4.2</span> What ESMFold Reveals About PLMs</h3>
<p>ESMFold’s success demonstrates that ESM-2’s internal representations encode sufficient information to determine 3D structure. The language model has learned not just local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular shape.</p>
<p>This has profound implications for understanding what PLMs learn. The attention that transformers pay to distant sequence positions during masked prediction is, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space attend to each other in the transformer’s attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution and the physical constraints of protein folding, encode structural information that sufficiently powerful language models can decode.</p>
</section>
</section>
<section id="integration-into-variant-interpretation-pipelines" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="integration-into-variant-interpretation-pipelines"><span class="header-section-number">13.5</span> Integration into Variant Interpretation Pipelines</h2>
<section id="cadd-v1.7-plm-features-for-ensemble-methods" class="level3" data-number="13.5.1">
<h3 data-number="13.5.1" class="anchored" data-anchor-id="cadd-v1.7-plm-features-for-ensemble-methods"><span class="header-section-number">13.5.1</span> CADD v1.7: PLM Features for Ensemble Methods</h3>
<p>The Combined Annotation Dependent Depletion (CADD) framework integrates diverse annotations to score variant deleteriousness (<a href="p1-ch04-cadd.html" class="quarto-xref"><span>Chapter 4</span></a>). CADD v1.7 incorporated ESM-1v predictions as features within its existing integrative architecture <span class="citation" data-cites="schubach_cadd_2024">(<a href="references.html#ref-schubach_cadd_2024" role="doc-biblioref">Schubach et al. 2024</a>)</span>.</p>
<p>The integration approach treats PLM scores as additional annotations alongside conservation scores, functional annotations, and regulatory predictions. For each missense variant, ESM-1v scores are computed and included as features in CADD’s gradient-boosted tree classifier. This allows the ensemble to learn how PLM predictions complement other evidence sources, potentially capturing cases where PLM and conservation signals provide independent information.</p>
<p>Performance gains from PLM integration are consistent across benchmarks. On ClinVar pathogenic versus common variant classification, CADD v1.7 improves from 0.94 to 0.95 AUROC. On deep mutational scanning datasets (31 assays), performance improves from 0.78 to 0.81 Spearman correlation. The PLM features particularly improve scoring for variants in regions with limited evolutionary conservation data, where traditional methods struggle but language models can still extract contextual information.</p>
</section>
<section id="alphamissense-combining-plm-and-structure" class="level3" data-number="13.5.2">
<h3 data-number="13.5.2" class="anchored" data-anchor-id="alphamissense-combining-plm-and-structure"><span class="header-section-number">13.5.2</span> AlphaMissense: Combining PLM and Structure</h3>
<p>AlphaMissense represents the current state-of-the-art in missense variant effect prediction, combining PLM representations with structural context <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>. Rather than treating PLMs as a feature source for an external classifier, AlphaMissense adapts AlphaFold’s architecture directly for pathogenicity prediction.</p>
<p>The model learns to predict pathogenicity by combining three information sources. Sequence embeddings from ESM-style language modeling provide evolutionary context about amino acid preferences at each position. Structural context from predicted protein structures captures whether a position is buried or exposed, in a secondary structure element or loop, near active sites or binding interfaces. Evolutionary information from cross-species comparisons supplements the single-sequence PLM signal with explicit alignment-derived conservation.</p>
<p>The training data comes from population frequency databases, primarily gnomAD <span class="citation" data-cites="gnomAD">(<a href="references.html#ref-gnomAD" role="doc-biblioref"><span>“The <span>Genome</span> <span>Aggregation</span> <span>Database</span> (<span class="nocase">gnomAD</span>)”</span> n.d.</a>)</span>. Common variants, those observed frequently in healthy populations, provide weak labels for benign effects. Variants absent from large population databases, particularly those in constrained positions, provide weak labels for deleterious effects. Critically, AlphaMissense never trains on clinical pathogenicity labels from ClinVar, yet achieves state-of-the-art performance on clinical benchmarks. This demonstrates that the combination of PLM representations, structural context, and population genetics signals captures genuine functional information rather than memorizing clinical annotations.</p>
<p>AlphaMissense provides predictions for all approximately 71 million possible single amino acid substitutions across the human proteome. Of these, 89% are classified as either likely benign or likely pathogenic with sufficient confidence to be actionable, providing interpretable predictions for the vast majority of possible missense variants.</p>
<table class="caption-top table">
<caption>Comparative performance of missense variant effect predictors on clinical (ClinVar) and experimental (deep mutational scanning) benchmarks <span class="citation" data-cites="ng_sift_2003 adzhubei_polyphen_2010 schubach_cadd_2024 meier_esm-1v_2021 cheng_alphamissense_2023">(<a href="references.html#ref-ng_sift_2003" role="doc-biblioref">Ng and Henikoff 2003</a>; <a href="references.html#ref-adzhubei_polyphen_2010" role="doc-biblioref">Adzhubei et al. 2010</a>; <a href="references.html#ref-schubach_cadd_2024" role="doc-biblioref">Schubach et al. 2024</a>; <a href="references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>; <a href="references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>.</caption>
<thead>
<tr class="header">
<th>Method</th>
<th>ClinVar AUC</th>
<th>DMS Correlation</th>
<th>Information Sources</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SIFT</td>
<td>0.78</td>
<td>0.30</td>
<td>Conservation</td>
</tr>
<tr class="even">
<td>PolyPhen-2</td>
<td>0.82</td>
<td>0.32</td>
<td>Conservation + structure</td>
</tr>
<tr class="odd">
<td>CADD v1.7</td>
<td>0.95</td>
<td>0.81</td>
<td>Multi-feature integration</td>
</tr>
<tr class="even">
<td>ESM-1v</td>
<td>0.89</td>
<td>0.48</td>
<td>Sequence only (zero-shot)</td>
</tr>
<tr class="odd">
<td>AlphaMissense</td>
<td>0.94</td>
<td>0.52</td>
<td>PLM + structure + population</td>
</tr>
</tbody>
</table>
<p>AlphaMissense achieves top performance by integrating the strengths of multiple approaches: PLM-derived sequence understanding, AlphaFold-derived structural context, and population genetics-derived evolutionary constraint signals.</p>
</section>
</section>
<section id="lessons-for-genomic-foundation-models" class="level2" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="lessons-for-genomic-foundation-models"><span class="header-section-number">13.6</span> Lessons for Genomic Foundation Models</h2>
<p>The success of protein language models established several principles that inform genomic foundation modeling. These lessons transfer, with appropriate modifications, to the DNA language models covered in subsequent chapters.</p>
<section id="self-supervision-works" class="level3" data-number="13.6.1">
<h3 data-number="13.6.1" class="anchored" data-anchor-id="self-supervision-works"><span class="header-section-number">13.6.1</span> Self-Supervision Works</h3>
<p>PLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can learn to exploit. This principle underlies the entire foundation model paradigm: if sufficiently large models are trained on sufficiently large datasets with appropriate self-supervised objectives, they will learn representations that capture biological function.</p>
</section>
<section id="scale-matters" class="level3" data-number="13.6.2">
<h3 data-number="13.6.2" class="anchored" data-anchor-id="scale-matters"><span class="header-section-number">13.6.2</span> Scale Matters</h3>
<p>Performance improves predictably with model size, motivating the development of larger genomic models. The progression from 8 million to 15 billion parameters in ESM-2 showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While the relationship between scale and performance is not linear indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This scaling relationship justifies the substantial computational investment required to train genomic foundation models.</p>
</section>
<section id="transfer-learning-is-effective" class="level3" data-number="13.6.3">
<h3 data-number="13.6.3" class="anchored" data-anchor-id="transfer-learning-is-effective"><span class="header-section-number">13.6.3</span> Transfer Learning is Effective</h3>
<p>Representations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids is simultaneously learning about protein structure, function, evolutionary constraint, and disease relevance, even though none of these properties appear in the training objective. The same principle applies to genomic sequences: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects.</p>
</section>
<section id="architecture-choices-matter" class="level3" data-number="13.6.4">
<h3 data-number="13.6.4" class="anchored" data-anchor-id="architecture-choices-matter"><span class="header-section-number">13.6.4</span> Architecture Choices Matter</h3>
<p>The BERT-style bidirectional encoder proved highly effective for proteins, where the entire sequence context is typically available. However, genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with proteins being information-dense while intergenic regions are less so, and different symmetries including the reverse-complement structure absent in proteins. These differences motivate architectural adaptations in genomic language models, including hybrid architectures that combine convolutional and attention mechanisms, longer context windows, and specialized tokenization schemes.</p>
</section>
<section id="integration-with-other-modalities" class="level3" data-number="13.6.5">
<h3 data-number="13.6.5" class="anchored" data-anchor-id="integration-with-other-modalities"><span class="header-section-number">13.6.5</span> Integration with Other Modalities</h3>
<p>AlphaMissense showed that PLM embeddings combine effectively with structural information. Similarly, genomic models benefit from integration with epigenomic data, gene annotations, and other biological context. The most powerful variant effect predictors combine multiple information sources, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace other genomic annotations.</p>
</section>
</section>
<section id="limitations-and-ongoing-challenges" class="level2" data-number="13.7">
<h2 data-number="13.7" class="anchored" data-anchor-id="limitations-and-ongoing-challenges"><span class="header-section-number">13.7</span> Limitations and Ongoing Challenges</h2>
<p>Despite their success, protein language models face several limitations that inform the development of genomic models.</p>
<section id="sequence-length-constraints" class="level3" data-number="13.7.1">
<h3 data-number="13.7.1" class="anchored" data-anchor-id="sequence-length-constraints"><span class="header-section-number">13.7.1</span> Sequence Length Constraints</h3>
<p>Most PLMs handle sequences up to 1,000 to 2,000 amino acids. While sufficient for most individual protein domains, this limits modeling of large protein complexes and does not directly transfer to the much longer sequences in genomics. Genomic language models must handle sequences spanning millions of bases, requiring architectural innovations beyond simple scaling of transformer attention.</p>
</section>
<section id="orphan-proteins" class="level3" data-number="13.7.2">
<h3 data-number="13.7.2" class="anchored" data-anchor-id="orphan-proteins"><span class="header-section-number">13.7.2</span> Orphan Proteins</h3>
<p>PLMs struggle with proteins that have few homologs in training databases. Orphan or dark proteins, those unique to specific lineages, lack the evolutionary signal that PLMs exploit. For these proteins, the statistical patterns learned from diverse sequence families provide less informative context. This limitation is less severe for genomic models trained on reference genomes, where even unique sequences exist in the context of conserved flanking regions.</p>
</section>
<section id="epistasis" class="level3" data-number="13.7.3">
<h3 data-number="13.7.3" class="anchored" data-anchor-id="epistasis"><span class="header-section-number">13.7.3</span> Epistasis</h3>
<p>Most variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit epistasis, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors do not explicitly model these interaction effects, though the contextual embeddings may capture some epistatic relationships implicitly.</p>
</section>
<section id="interpretability" class="level3" data-number="13.7.4">
<h3 data-number="13.7.4" class="anchored" data-anchor-id="interpretability"><span class="header-section-number">13.7.4</span> Interpretability</h3>
<p>While attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods (<a href="p5-ch22-interp.html" class="quarto-xref"><span>Chapter 22</span></a>), but PLMs remain partially opaque. For clinical applications where explanations are valued, this interpretability gap limits adoption. Future work must balance the accuracy gains from complex models against the transparency required for clinical decision-making.</p>
</section>
</section>
<section id="beyond-language-models-structure-prediction-and-design" class="level2" data-number="13.8">
<h2 data-number="13.8" class="anchored" data-anchor-id="beyond-language-models-structure-prediction-and-design"><span class="header-section-number">13.8</span> Beyond Language Models: Structure Prediction and Design</h2>
<p>While protein language models demonstrate the power of self-supervised learning on sequences alone, the broader protein modeling landscape encompasses methods that explicitly incorporate structural information, evolutionary constraints, and physical principles. These approaches complement PLMs by addressing tasks where three-dimensional geometry, binding interactions, or design objectives are central.</p>
<section id="structure-prediction-systems" class="level3" data-number="13.8.1">
<h3 data-number="13.8.1" class="anchored" data-anchor-id="structure-prediction-systems"><span class="header-section-number">13.8.1</span> Structure Prediction Systems</h3>
<p>AlphaFold2 revolutionized protein structure prediction by combining learned representations with explicit modeling of protein geometry <span class="citation" data-cites="jumper_alphafold2_2021">(<a href="references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. Unlike pure sequence models, AlphaFold2 processes both sequence information through embeddings and structural information through an iterative refinement process that directly predicts atomic coordinates. The system requires multiple sequence alignments as input, using evolutionary information to infer residue-residue contacts and structural constraints. AlphaFold2 searches for homologous sequences using computationally expensive tools including HHblits against the BFD and Jackhmmer against other databases, a process that can take hours per protein.</p>
<p>AlphaFold3 extends this framework to model protein complexes, nucleic acids, and small molecules <span class="citation" data-cites="abramson_alphafold3_2024">(<a href="references.html#ref-abramson_alphafold3_2024" role="doc-biblioref">Abramson et al. 2024</a>)</span>. The architecture incorporates diffusion-based structure generation, allowing it to predict not only protein structures but also their interactions with other biomolecules. This expansion from single proteins to molecular complexes represents a shift toward modeling entire biological systems rather than isolated components.</p>
<p>ColabFold democratized access to AlphaFold-quality predictions by replacing the slow MSA search with MMseqs2, reducing search time from hours to minutes while maintaining prediction accuracy <span class="citation" data-cites="mirdita_colabfold_2022">(<a href="references.html#ref-mirdita_colabfold_2022" role="doc-biblioref">Mirdita et al. 2022</a>)</span>. The system uses precomputed databases and optimized search algorithms to make structure prediction accessible through free cloud computing resources. ColabFold’s efficiency enabled large-scale structural proteomics, with researchers generating predictions for entire proteomes in practical time frames.</p>
<p>OpenFold provides an open-source reimplementation of AlphaFold2, enabling researchers to modify and extend the architecture for specialized applications <span class="citation" data-cites="ahdritz_openfold_2024">(<a href="references.html#ref-ahdritz_openfold_2024" role="doc-biblioref">Ahdritz et al. 2024</a>)</span>. By making the full training and inference pipeline accessible, OpenFold supports development of domain-specific variants optimized for particular protein families or prediction tasks.</p>
<p>RosettaFold emerged as an alternative to AlphaFold2, demonstrating that similar accuracy could be achieved through different architectural choices <span class="citation" data-cites="baek_rosettafold_2021">(<a href="references.html#ref-baek_rosettafold_2021" role="doc-biblioref">Baek et al. 2021</a>)</span>. The three-track neural network architecture processes sequence, distance, and coordinate information simultaneously, with each track informing the others through carefully designed information exchange. RosettaFold’s modular design facilitated subsequent extensions including RoseTTAFold2 for protein-protein interactions and RoseTTAFoldNA for nucleic acid modeling.</p>
<p>ESMFold, discussed earlier in this chapter, represents a distinct approach by eliminating MSA requirements entirely <span class="citation" data-cites="lin_esm-2_2022">(<a href="references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. The model achieves AlphaFold2-level accuracy for many proteins while being orders of magnitude faster, enabling structural annotation of metagenomic sequences where traditional MSA construction fails due to lack of homologs.</p>
</section>
<section id="generative-design-methods" class="level3" data-number="13.8.2">
<h3 data-number="13.8.2" class="anchored" data-anchor-id="generative-design-methods"><span class="header-section-number">13.8.2</span> Generative Design Methods</h3>
<p>ProteinMPNN applies message-passing neural networks to the inverse folding problem: designing sequences that fold into specified backbone structures <span class="citation" data-cites="dauparas_proteinmpnn_2022">(<a href="references.html#ref-dauparas_proteinmpnn_2022" role="doc-biblioref">Dauparas et al. 2022</a>)</span>. Given a target structure, the model learns to generate amino acid sequences likely to adopt that fold. ProteinMPNN’s success at designing stable, functional proteins demonstrates that neural networks can capture the sequence-structure relationship in both directions, complementing the structure prediction capabilities of AlphaFold and ESM.</p>
<p>RFDiffusion extends diffusion models to protein backbone generation, enabling de novo design of proteins with specified functions <span class="citation" data-cites="watson_rfdiffusion_2023">(<a href="references.html#ref-watson_rfdiffusion_2023" role="doc-biblioref">Watson et al. 2023</a>)</span>. Rather than predicting structure from sequence or sequence from structure, RFDiffusion generates entirely novel protein backbones conditioned on design objectives such as binding a target molecule or forming particular geometric shapes. The method has been used to design proteins with enzymatic activity, binding specificity, and novel folds not found in nature.</p>
<p>Boltz-1 introduced a unified framework for predicting structures of protein-ligand, protein-protein, and protein-nucleic acid complexes using a diffusion-based approach <span class="citation" data-cites="wohlwend_boltz1_2025">(<a href="references.html#ref-wohlwend_boltz1_2025" role="doc-biblioref">Wohlwend et al. 2025</a>)</span>. Boltz-2 builds on this foundation with improved accuracy and broader applicability across different types of biomolecular interactions. These methods address the critical challenge of predicting not just individual protein structures but how proteins interact with other molecules in cellular contexts.</p>
</section>
<section id="molecular-docking-and-binding" class="level3" data-number="13.8.3">
<h3 data-number="13.8.3" class="anchored" data-anchor-id="molecular-docking-and-binding"><span class="header-section-number">13.8.3</span> Molecular Docking and Binding</h3>
<p>DiffDock applies diffusion models to molecular docking, predicting how small molecules bind to protein targets <span class="citation" data-cites="corso_diffdock_2022">(<a href="references.html#ref-corso_diffdock_2022" role="doc-biblioref">Corso et al. 2022</a>)</span>. Traditional docking methods rely on physics-based scoring functions and extensive sampling, often requiring hours of computation per ligand-protein pair. DiffDock learns to generate binding poses directly, achieving comparable or better accuracy in a fraction of the time. This capability is particularly valuable for drug discovery, where thousands of potential compounds must be evaluated against protein targets.</p>
</section>
<section id="infrastructure-and-search-methods" class="level3" data-number="13.8.4">
<h3 data-number="13.8.4" class="anchored" data-anchor-id="infrastructure-and-search-methods"><span class="header-section-number">13.8.4</span> Infrastructure and Search Methods</h3>
<p>The MSA construction pipeline underlying AlphaFold2 represents substantial engineering beyond the neural network architecture itself. HHblits performs iterative profile-profile searches to identify remote homologs, building deep alignments that capture evolutionary constraints <span class="citation" data-cites="remmert_hhblits_2012">(<a href="references.html#ref-remmert_hhblits_2012" role="doc-biblioref">Remmert et al. 2012</a>)</span>. Jackhmmer provides complementary sensitivity using hidden Markov model searches <span class="citation" data-cites="finn_hmmer_2011">(<a href="references.html#ref-finn_hmmer_2011" role="doc-biblioref">Finn, Clements, and Eddy 2011</a>)</span>. These tools process multiple sequence databases including UniRef, BFD, and MGnify, each optimized for different coverage-redundancy tradeoffs.</p>
<p>MMseqs2 revolutionized sequence search by achieving BLAST-level sensitivity at hundreds of times the speed through careful algorithmic optimization and parallelization <span class="citation" data-cites="steinegger_mmseqs2_2017">(<a href="references.html#ref-steinegger_mmseqs2_2017" role="doc-biblioref">Steinegger and Söding 2017</a>)</span>. ColabFold’s adoption of MMseqs2 for MSA construction demonstrated that the computational bottleneck in structure prediction lay not in the neural network but in the database search, motivating the development of faster search algorithms as a critical infrastructure problem.</p>
</section>
<section id="integration-with-protein-language-models" class="level3" data-number="13.8.5">
<h3 data-number="13.8.5" class="anchored" data-anchor-id="integration-with-protein-language-models"><span class="header-section-number">13.8.5</span> Integration with Protein Language Models</h3>
<p>These structure-focused methods increasingly incorporate PLM representations as complementary information sources. AlphaFold3 integrates sequence embeddings from language models with its structure prediction network. RFDiffusion can condition generation on ESM embeddings to guide designs toward particular sequence properties. ProteinMPNN benefits from PLM-derived features when designing sequences for challenging structural targets. This trend toward hybrid architectures suggests that the future of protein modeling lies not in choosing between sequence models and structure models, but in intelligently combining their complementary strengths.</p>
<p>The protein modeling ecosystem thus spans a continuum from pure sequence models like ESM that never explicitly represent structure, through hybrid systems like AlphaFold that combine learned sequence representations with geometric constraints, to physics-based methods that emphasize structural principles. Each approach offers distinct advantages: PLMs provide fast, broadly applicable predictions without MSA construction; structure prediction systems achieve atomic-level accuracy when sufficient evolutionary data exists; generative methods enable design of novel proteins with specified functions. Understanding this landscape helps position genomic foundation models, which face analogous tradeoffs between sequence-only and structure-aware approaches.</p>
</section>
</section>
<section id="significance" class="level2" data-number="13.9">
<h2 data-number="13.9" class="anchored" data-anchor-id="significance"><span class="header-section-number">13.9</span> Significance</h2>
<p>Protein language models established that transformer architectures can learn deep biological knowledge from sequence data alone. ESM’s ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. This success directly motivated the development of genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too.</p>
<p>The genomic language models covered in <a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a> adapt PLM architectures and training strategies to the distinct challenges of DNA sequences: longer contexts, different alphabets, and the full complexity of gene regulation. The integration path continues as well: just as CADD v1.7 and AlphaMissense incorporate PLM predictions, future models will integrate genomic and proteomic language models into unified frameworks for variant interpretation (<a href="p5-ch20-vep.html" class="quarto-xref"><span>Chapter 20</span></a>) and multi-omic modeling (<a href="p4-ch17-systems.html" class="quarto-xref"><span>Chapter 17</span></a>).</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-abramson_alphafold3_2024" class="csl-entry" role="listitem">
Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. <span>“[<span>AlphaFold3</span>] <span>Accurate</span> Structure Prediction of Biomolecular Interactions with <span>AlphaFold</span> 3.”</span> <em>Nature</em> 630 (8016): 493–500. <a href="https://doi.org/10.1038/s41586-024-07487-w">https://doi.org/10.1038/s41586-024-07487-w</a>.
</div>
<div id="ref-adzhubei_polyphen_2010" class="csl-entry" role="listitem">
Adzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky, Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev. 2010. <span>“A Method and Server for Predicting Damaging Missense Mutations.”</span> <em>Nature Methods</em> 7 (4): 248–49. <a href="https://doi.org/10.1038/nmeth0410-248">https://doi.org/10.1038/nmeth0410-248</a>.
</div>
<div id="ref-ahdritz_openfold_2024" class="csl-entry" role="listitem">
Ahdritz, Gustaf, Nazim Bouatta, Christina Floristean, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy J. O’Donnell, et al. 2024. <span>“<span>OpenFold</span>: Retraining <span>AlphaFold2</span> Yields New Insights into Its Learning Mechanisms and Capacity for Generalization.”</span> <em>Nature Methods</em> 21 (8): 1514–24. <a href="https://doi.org/10.1038/s41592-024-02272-z">https://doi.org/10.1038/s41592-024-02272-z</a>.
</div>
<div id="ref-baek_rosettafold_2021" class="csl-entry" role="listitem">
Baek, Minkyung, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, et al. 2021. <span>“Accurate Prediction of Protein Structures and Interactions Using a Three-Track Neural Network.”</span> <em>Science</em> 373 (6557): 871–76. <a href="https://doi.org/10.1126/science.abj8754">https://doi.org/10.1126/science.abj8754</a>.
</div>
<div id="ref-brandes_genome-wide_2023" class="csl-entry" role="listitem">
Brandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. <span>“Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.”</span> <em>Nature Genetics</em> 55 (9): 1512–22. <a href="https://doi.org/10.1038/s41588-023-01465-0">https://doi.org/10.1038/s41588-023-01465-0</a>.
</div>
<div id="ref-cheng_alphamissense_2023" class="csl-entry" role="listitem">
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. <span>“[<span>AlphaMissense</span>] <span>Accurate</span> Proteome-Wide Missense Variant Effect Prediction with <span>AlphaMissense</span>.”</span> <em>Science</em> 381 (6664): eadg7492. <a href="https://doi.org/10.1126/science.adg7492">https://doi.org/10.1126/science.adg7492</a>.
</div>
<div id="ref-corso_diffdock_2022" class="csl-entry" role="listitem">
Corso, Gabriele, Hannes Stärk, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. 2022. <span>“<span>DiffDock</span>: <span>Diffusion</span> <span>Steps</span>, <span>Twists</span>, and <span>Turns</span> for <span>Molecular</span> <span>Docking</span>.”</span> <em>arXiv.org</em>. <a href="https://arxiv.org/abs/2210.01776v2">https://arxiv.org/abs/2210.01776v2</a>.
</div>
<div id="ref-dauparas_proteinmpnn_2022" class="csl-entry" role="listitem">
Dauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. <span>“Robust Deep Learning–Based Protein Sequence Design Using <span>ProteinMPNN</span>.”</span> <em>Science</em> 378 (6615): 49–56. <a href="https://doi.org/10.1126/science.add2187">https://doi.org/10.1126/science.add2187</a>.
</div>
<div id="ref-devlin_bert_2019" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span>Pre</span>-Training of <span>Deep</span> <span>Bidirectional</span> <span>Transformers</span> for <span>Language</span> <span>Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.
</div>
<div id="ref-elnaggar_prottrans_2021" class="csl-entry" role="listitem">
Elnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. <span>“<span>ProtTrans</span>: <span>Towards</span> <span>Cracking</span> the <span>Language</span> of <span>Life</span>’s <span>Code</span> <span>Through</span> <span>Self</span>-<span>Supervised</span> <span>Deep</span> <span>Learning</span> and <span>High</span> <span>Performance</span> <span>Computing</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2007.06225">https://doi.org/10.48550/arXiv.2007.06225</a>.
</div>
<div id="ref-finn_hmmer_2011" class="csl-entry" role="listitem">
Finn, Robert D., Jody Clements, and Sean R. Eddy. 2011. <span>“<span>HMMER</span> Web Server: Interactive Sequence Similarity Searching.”</span> <em>Nucleic Acids Research</em> 39 (suppl_2): W29–37. <a href="https://doi.org/10.1093/nar/gkr367">https://doi.org/10.1093/nar/gkr367</a>.
</div>
<div id="ref-frazer_eve_2021" class="csl-entry" role="listitem">
Frazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021. <span>“[<span>EVE</span>] <span>Disease</span> Variant Prediction with Deep Generative Models of Evolutionary Data.”</span> <em>Nature</em> 599 (7883): 91–95. <a href="https://doi.org/10.1038/s41586-021-04043-8">https://doi.org/10.1038/s41586-021-04043-8</a>.
</div>
<div id="ref-jumper_alphafold2_2021" class="csl-entry" role="listitem">
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. <span>“[<span>AlphaFold2</span>] <span>Highly</span> Accurate Protein Structure Prediction with <span>AlphaFold</span>.”</span> <em>Nature</em> 596 (7873): 583–89. <a href="https://doi.org/10.1038/s41586-021-03819-2">https://doi.org/10.1038/s41586-021-03819-2</a>.
</div>
<div id="ref-lin_esm-2_2022" class="csl-entry" role="listitem">
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. <span>“[<span>ESM</span>-2] <span>Language</span> Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.07.20.500902">https://doi.org/10.1101/2022.07.20.500902</a>.
</div>
<div id="ref-meier_esm-1v_2021" class="csl-entry" role="listitem">
Meier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. <span>“[<span>ESM</span>-1v] <span>Language</span> Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.”</span> bioRxiv. <a href="https://doi.org/10.1101/2021.07.09.450648">https://doi.org/10.1101/2021.07.09.450648</a>.
</div>
<div id="ref-mirdita_colabfold_2022" class="csl-entry" role="listitem">
Mirdita, Milot, Konstantin Schütze, Yoshitaka Moriwaki, Lim Heo, Sergey Ovchinnikov, and Martin Steinegger. 2022. <span>“<span>ColabFold</span>: Making Protein Folding Accessible to All.”</span> <em>Nature Methods</em> 19 (6): 679–82. <a href="https://doi.org/10.1038/s41592-022-01488-1">https://doi.org/10.1038/s41592-022-01488-1</a>.
</div>
<div id="ref-ng_sift_2003" class="csl-entry" role="listitem">
Ng, Pauline C., and Steven Henikoff. 2003. <span>“<span>SIFT</span>: <span>Predicting</span> Amino Acid Changes That Affect Protein Function.”</span> <em>Nucleic Acids Research</em> 31 (13): 3812–14. <a href="https://doi.org/10.1093/nar/gkg509">https://doi.org/10.1093/nar/gkg509</a>.
</div>
<div id="ref-notin_proteingym_2023" class="csl-entry" role="listitem">
Notin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk, Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023. <span>“<span>ProteinGym</span>: <span>Large</span>-<span>Scale</span> <span>Benchmarks</span> for <span>Protein</span> <span>Fitness</span> <span>Prediction</span> and <span>Design</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 36 (December): 64331–79. <a href="https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html">https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html</a>.
</div>
<div id="ref-raffel_t5_2019" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. <span>“Exploring the <span>Limits</span> of <span>Transfer</span> <span>Learning</span> with a <span>Unified</span> <span>Text</span>-to-<span>Text</span> <span>Transformer</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1910.10683">https://doi.org/10.48550/arXiv.1910.10683</a>.
</div>
<div id="ref-remmert_hhblits_2012" class="csl-entry" role="listitem">
Remmert, Michael, Andreas Biegert, Andreas Hauser, and Johannes Söding. 2012. <span>“<span>HHblits</span>: Lightning-Fast Iterative Protein Sequence Searching by <span>HMM</span>-<span>HMM</span> Alignment.”</span> <em>Nature Methods</em> 9 (2): 173–75. <a href="https://doi.org/10.1038/nmeth.1818">https://doi.org/10.1038/nmeth.1818</a>.
</div>
<div id="ref-riesselman_deepsequence_2018" class="csl-entry" role="listitem">
Riesselman, Adam J., John B. Ingraham, and Debora S. Marks. 2018. <span>“Deep Generative Models of Genetic Variation Capture the Effects of Mutations.”</span> <em>Nature Methods</em> 15 (10): 816–22. <a href="https://doi.org/10.1038/s41592-018-0138-4">https://doi.org/10.1038/s41592-018-0138-4</a>.
</div>
<div id="ref-rives_esm_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“[<span>ESM</span>-1b] <span>Biological</span> Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 118 (15): e2016239118. <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-schubach_cadd_2024" class="csl-entry" role="listitem">
Schubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and Martin Kircher. 2024. <span>“<span>CADD</span> V1.7: Using Protein Language Models, Regulatory <span>CNNs</span> and Other Nucleotide-Level Scores to Improve Genome-Wide Variant Predictions.”</span> <em>Nucleic Acids Research</em> 52 (D1): D1143–54. <a href="https://doi.org/10.1093/nar/gkad989">https://doi.org/10.1093/nar/gkad989</a>.
</div>
<div id="ref-steinegger_mmseqs2_2017" class="csl-entry" role="listitem">
Steinegger, Martin, and Johannes Söding. 2017. <span>“<span>MMseqs2</span> Enables Sensitive Protein Sequence Searching for the Analysis of Massive Data Sets.”</span> <em>Nature Biotechnology</em> 35 (11): 1026–28. <a href="https://doi.org/10.1038/nbt.3988">https://doi.org/10.1038/nbt.3988</a>.
</div>
<div id="ref-suzek_uniref_2007" class="csl-entry" role="listitem">
Suzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. <span>“<span>UniRef</span>: Comprehensive and Non-Redundant <span>UniProt</span> Reference Clusters.”</span> <em>Bioinformatics</em> 23 (10): 1282–88. <a href="https://doi.org/10.1093/bioinformatics/btm098">https://doi.org/10.1093/bioinformatics/btm098</a>.
</div>
<div id="ref-gnomAD" class="csl-entry" role="listitem">
<span>“The <span>Genome</span> <span>Aggregation</span> <span>Database</span> (<span class="nocase">gnomAD</span>).”</span> n.d. Accessed July 3, 2025. <a href="https://www.nature.com/immersive/d42859-020-00002-x/index.html">https://www.nature.com/immersive/d42859-020-00002-x/index.html</a>.
</div>
<div id="ref-watson_rfdiffusion_2023" class="csl-entry" role="listitem">
Watson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. <span>“De Novo Design of Protein Structure and Function with <span>RFdiffusion</span>.”</span> <em>Nature</em> 620 (7976): 1089–1100. <a href="https://doi.org/10.1038/s41586-023-06415-8">https://doi.org/10.1038/s41586-023-06415-8</a>.
</div>
<div id="ref-wohlwend_boltz1_2025" class="csl-entry" role="listitem">
Wohlwend, Jeremy, Gabriele Corso, Saro Passaro, Noah Getz, Mateo Reveiz, Ken Leidal, Wojtek Swiderski, et al. 2025. <span>“Boltz-1 <span>Democratizing</span> <span>Biomolecular</span> <span>Interaction</span> <span>Modeling</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2024.11.19.624167">https://doi.org/10.1101/2024.11.19.624167</a>.
</div>
<div id="ref-yang_xlnet_2020" class="csl-entry" role="listitem">
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. <span>“<span>XLNet</span>: <span>Generalized</span> <span>Autoregressive</span> <span>Pretraining</span> for <span>Language</span> <span>Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1906.08237">https://doi.org/10.48550/arXiv.1906.08237</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p3-ch12-rna.html" class="pagination-link" aria-label="RNA &amp; Transcript-Level Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p3-ch14-hybrid.html" class="pagination-link" aria-label="Long-range Hybrid Models">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>