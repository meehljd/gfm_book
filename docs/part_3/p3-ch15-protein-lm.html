<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Protein Language Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_3/p3-ch16-regulatory.html" rel="next">
<link href="../part_3/p3-ch14-dna-lm.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch15-protein-lm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--multi-modal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Systems and Scale</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch15-esm-family" id="toc-sec-ch15-esm-family" class="nav-link active" data-scroll-target="#sec-ch15-esm-family"><span class="header-section-number">15.1</span> ESM Model Family</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-esm1b" id="toc-sec-ch15-esm1b" class="nav-link" data-scroll-target="#sec-ch15-esm1b"><span class="header-section-number">15.1.1</span> ESM-1b: Establishing the Paradigm</a></li>
  <li><a href="#sec-ch15-emergent-knowledge" id="toc-sec-ch15-emergent-knowledge" class="nav-link" data-scroll-target="#sec-ch15-emergent-knowledge"><span class="header-section-number">15.1.2</span> Emergent Biological Knowledge</a></li>
  <li><a href="#sec-ch15-esm2" id="toc-sec-ch15-esm2" class="nav-link" data-scroll-target="#sec-ch15-esm2"><span class="header-section-number">15.1.3</span> ESM-2: Scaling Up</a></li>
  </ul></li>
  <li><a href="#sec-ch15-alternative-architectures" id="toc-sec-ch15-alternative-architectures" class="nav-link" data-scroll-target="#sec-ch15-alternative-architectures"><span class="header-section-number">15.2</span> Alternative Architectures</a></li>
  <li><a href="#sec-ch15-attention-coupling" id="toc-sec-ch15-attention-coupling" class="nav-link" data-scroll-target="#sec-ch15-attention-coupling"><span class="header-section-number">15.3</span> Attention and Evolutionary Coupling</a></li>
  <li><a href="#sec-ch15-esmfold" id="toc-sec-ch15-esmfold" class="nav-link" data-scroll-target="#sec-ch15-esmfold"><span class="header-section-number">15.4</span> ESMFold: Structure from Sequence</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-alignment-free" id="toc-sec-ch15-alignment-free" class="nav-link" data-scroll-target="#sec-ch15-alignment-free"><span class="header-section-number">15.4.1</span> Alignment-Free Prediction</a></li>
  <li><a href="#sec-ch15-esmfold-implications" id="toc-sec-ch15-esmfold-implications" class="nav-link" data-scroll-target="#sec-ch15-esmfold-implications"><span class="header-section-number">15.4.2</span> What ESMFold Reveals About PLMs</a></li>
  </ul></li>
  <li><a href="#sec-ch15-function-prediction" id="toc-sec-ch15-function-prediction" class="nav-link" data-scroll-target="#sec-ch15-function-prediction"><span class="header-section-number">15.5</span> Function Prediction</a></li>
  <li><a href="#sec-ch15-variant-effects" id="toc-sec-ch15-variant-effects" class="nav-link" data-scroll-target="#sec-ch15-variant-effects"><span class="header-section-number">15.6</span> Variant Effect Prediction</a></li>
  <li><a href="#sec-ch15-structure-integration" id="toc-sec-ch15-structure-integration" class="nav-link" data-scroll-target="#sec-ch15-structure-integration"><span class="header-section-number">15.7</span> Integration with Structure Prediction</a></li>
  <li><a href="#sec-ch15-limitations" id="toc-sec-ch15-limitations" class="nav-link" data-scroll-target="#sec-ch15-limitations"><span class="header-section-number">15.8</span> Limitations</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-orphan-proteins" id="toc-sec-ch15-orphan-proteins" class="nav-link" data-scroll-target="#sec-ch15-orphan-proteins"><span class="header-section-number">15.8.1</span> Orphan and Dark Proteins</a></li>
  <li><a href="#sec-ch15-novel-folds" id="toc-sec-ch15-novel-folds" class="nav-link" data-scroll-target="#sec-ch15-novel-folds"><span class="header-section-number">15.8.2</span> Novel Folds</a></li>
  <li><a href="#sec-ch15-conformational-flexibility" id="toc-sec-ch15-conformational-flexibility" class="nav-link" data-scroll-target="#sec-ch15-conformational-flexibility"><span class="header-section-number">15.8.3</span> Conformational Flexibility</a></li>
  <li><a href="#sec-ch15-epistasis" id="toc-sec-ch15-epistasis" class="nav-link" data-scroll-target="#sec-ch15-epistasis"><span class="header-section-number">15.8.4</span> Epistasis</a></li>
  <li><a href="#sec-ch15-interpretability" id="toc-sec-ch15-interpretability" class="nav-link" data-scroll-target="#sec-ch15-interpretability"><span class="header-section-number">15.8.5</span> Interpretability</a></li>
  </ul></li>
  <li><a href="#sec-ch15-lessons" id="toc-sec-ch15-lessons" class="nav-link" data-scroll-target="#sec-ch15-lessons"><span class="header-section-number">15.9</span> Lessons for Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-self-supervised" id="toc-sec-ch15-self-supervised" class="nav-link" data-scroll-target="#sec-ch15-self-supervised"><span class="header-section-number">15.9.1</span> Self-Supervised Biological Knowledge</a></li>
  <li><a href="#sec-ch15-scaling" id="toc-sec-ch15-scaling" class="nav-link" data-scroll-target="#sec-ch15-scaling"><span class="header-section-number">15.9.2</span> Scaling Benefits</a></li>
  <li><a href="#sec-ch15-transfer" id="toc-sec-ch15-transfer" class="nav-link" data-scroll-target="#sec-ch15-transfer"><span class="header-section-number">15.9.3</span> Effective Transfer Learning</a></li>
  <li><a href="#sec-ch15-architecture-matching" id="toc-sec-ch15-architecture-matching" class="nav-link" data-scroll-target="#sec-ch15-architecture-matching"><span class="header-section-number">15.9.4</span> Architecture-Sequence Matching</a></li>
  <li><a href="#sec-ch15-integration" id="toc-sec-ch15-integration" class="nav-link" data-scroll-target="#sec-ch15-integration"><span class="header-section-number">15.9.5</span> Integration Benefits</a></li>
  </ul></li>
  <li><a href="#sec-ch15-conclusion" id="toc-sec-ch15-conclusion" class="nav-link" data-scroll-target="#sec-ch15-conclusion"><span class="header-section-number">15.10</span> Paradigm That Generalized</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch15-protein-lm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch15-protein-lm" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Evolution is the most thorough experiment ever conducted on protein sequences. Over billions of years, natural selection tested trillions of amino acid combinations, ruthlessly eliminating sequences that failed to fold or function while preserving those that worked. The sequences populating modern databases are not random strings but successful solutions to biological problems, each implicitly encoding information about structure, stability, and function. The central insight of protein language models is that this evolutionary record, comprising hundreds of millions of sequences in databases like UniRef, contains sufficient information to learn the fundamental principles of protein biology without ever being shown a crystal structure or a functional assay.</p>
<p>This insight transformed computational biology. Traditional approaches to understanding proteins required either expensive experimental characterization or physics-based simulations that struggled with the complexity of protein behavior. Multiple sequence alignments could extract conservation patterns, but required finding homologs for each protein of interest and could not generalize beyond specific families. Protein language models changed the equation by compressing evolutionary knowledge into neural network parameters that transfer across the entire protein universe. A model trained to predict masked amino acids learns, as a byproduct, which residues contact each other in three-dimensional space, which positions tolerate variation, and which substitutions disrupt function. The physics of protein folding, selected across evolutionary time, emerges from the statistics of surviving sequences.</p>
<p>The <em>ESM</em> family demonstrated that transformers can learn protein structure and function from sequence alone, achieving results that rival methods requiring explicit structural supervision. Evolutionary Scale Modeling, as the name suggests, exploits the scale of evolutionary data to learn representations that generalize across proteins regardless of homology or family membership. Understanding these successes and their limitations provides essential context for genomic language models, where analogous approaches face distinct challenges arising from the multi-scale organization of regulatory information in DNA (see <a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>).</p>
<section id="sec-ch15-esm-family" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="sec-ch15-esm-family"><span class="header-section-number">15.1</span> ESM Model Family</h2>
<p>The <em>ESM</em> (Evolutionary Scale Modeling) family developed at Meta AI Research represents the most influential protein language model lineage, progressing from an initial proof-of-concept to models capable of predicting three-dimensional structure from sequence alone. The progression from <em>ESM-1b</em> through <em>ESM-2</em> illustrates how scaling transformer architectures yields systematic improvements in biological knowledge extraction, while revealing what self-supervised learning on protein sequences can and cannot achieve.</p>
<section id="sec-ch15-esm1b" class="level3" data-number="15.1.1">
<h3 data-number="15.1.1" class="anchored" data-anchor-id="sec-ch15-esm1b"><span class="header-section-number">15.1.1</span> ESM-1b: Establishing the Paradigm</h3>
<p>The Evolutionary Scale Modeling project demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision <span class="citation" data-cites="rives_esm_2021">(<a href="../bib/references.html#ref-rives_esm_2021" role="doc-biblioref">Rives et al. 2021</a>)</span>. The approach was strikingly simple: take the BERT architecture that had revolutionized natural language processing, replace words with amino acids, and train on protein sequence databases. The resulting models learned far more than anyone expected.</p>
<p><em>ESM-1b</em> was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families. The construction and characteristics of protein sequence databases are detailed in <a href="../part_1/p1-ch02-data.html#sec-ch02-protein-databases" class="quarto-xref"><span>Section 2.6</span></a>, with implications for training data curation in <a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a>. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy that would otherwise bias the model toward overrepresented families <span class="citation" data-cites="suzek_uniref_2007">(<a href="../bib/references.html#ref-suzek_uniref_2007" role="doc-biblioref">Suzek et al. 2007</a>)</span>. This curation strategy ensures the model encounters diverse evolutionary solutions to protein function rather than memorizing common motifs.</p>
<p>The architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is <strong>masked language modeling</strong>, the self-supervised strategy introduced in <a href="../part_2/p2-ch08-pretraining.html#sec-ch08-mlm" class="quarto-xref"><span>Section 8.1</span></a>: randomly mask 15% of amino acids in each sequence, and train the model to predict the masked positions given surrounding context. This objective contains no information about structure, function, or evolution beyond what is implicit in the sequences themselves.</p>
</section>
<section id="sec-ch15-emergent-knowledge" class="level3" data-number="15.1.2">
<h3 data-number="15.1.2" class="anchored" data-anchor-id="sec-ch15-emergent-knowledge"><span class="header-section-number">15.1.2</span> Emergent Biological Knowledge</h3>
<p>The surprise was not that <em>ESM-1b</em> learned to predict masked amino acids accurately, but what else it learned in the process. Despite never seeing structural or functional labels during training, <em>ESM-1b’s</em> internal representations encode information about protein biology at multiple levels of organization.</p>
<p>Secondary structure emerges in the attention patterns. When researchers analyzed which sequence positions the model attends to when making predictions, they found that attention concentrates along patterns corresponding to alpha helices and beta sheets. The model implicitly learns that certain amino acid sequences form specific structural elements, encoding this knowledge without ever being told what secondary structure is.</p>
<p>More remarkably, <em>ESM-1b</em> captures residue-residue contacts. Amino acids that are distant in the linear sequence but close in three-dimensional space attend to each other in the model’s attention matrices. This emergent capability suggests the model learns aspects of protein folding purely from sequence statistics. When attention weights were converted to contact predictions, they achieved accuracy approaching dedicated contact prediction methods that were explicitly trained for that task.</p>
<p>The model’s masked token predictions correlate strongly with position-specific conservation scores derived from multiple sequence alignments. <em>ESM</em> effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns across 33 million sequences rather than from explicit conservation annotations. Positions where the model confidently predicts specific amino acids correspond to positions that are conserved across protein families.</p>
<p>Perhaps most striking, attention concentrates on functionally important positions. Catalytic residues, binding sites, and other sites of biological importance receive elevated attention even without explicit functional annotation in the training data. The model identifies that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites where nature has constrained variation because they perform essential functions.</p>
<div id="fig-plm-emergent" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plm-emergent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/01-A-fig-plm-emergent.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/01-B-fig-plm-emergent.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/01-C-fig-plm-emergent.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/01-D-fig-plm-emergent.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER D</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plm-emergent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: [Essential] Multi-panel figure. Panel A (Training Objective): Protein sequence with 15% masked; model predicting from context; “No structure, function, or evolutionary labels.” Panel B (Secondary Structure in Attention): Attention heatmap with alpha helix and beta sheet regions highlighted; attention concentrating along structural patterns. Panel C (Residue Contacts from Attention): Attention weights converted to contact map; ground truth from crystal structure overlaid; strong correspondence. Panel D (Functional Site Discovery): Protein structure cartoon; positions with elevated attention highlighted; overlap with catalytic residues, binding sites.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch15-esm2" class="level3" data-number="15.1.3">
<h3 data-number="15.1.3" class="anchored" data-anchor-id="sec-ch15-esm2"><span class="header-section-number">15.1.3</span> ESM-2: Scaling Up</h3>
<p><em>ESM-2</em> extended the <em>ESM</em> approach across a range of model scales, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. The results confirmed a pattern familiar from natural language processing: bigger models learn more.</p>
<table class="caption-top table">
<caption><em>ESM-2</em> model family spanning four orders of magnitude in parameter count, with architecture details and relative performance on structure-related tasks.</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>Layers</th>
<th>Hidden Dim</th>
<th>Performance Gain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>ESM-2</em> (8M)</td>
<td>8M</td>
<td>6</td>
<td>320</td>
<td>Baseline</td>
</tr>
<tr class="even">
<td><em>ESM-2</em> (35M)</td>
<td>35M</td>
<td>12</td>
<td>480</td>
<td>Modest</td>
</tr>
<tr class="odd">
<td><em>ESM-2</em> (150M)</td>
<td>150M</td>
<td>30</td>
<td>640</td>
<td>Substantial</td>
</tr>
<tr class="even">
<td><em>ESM-2</em> (650M)</td>
<td>650M</td>
<td>33</td>
<td>1280</td>
<td>Large</td>
</tr>
<tr class="odd">
<td><em>ESM-2</em> (3B)</td>
<td>3B</td>
<td>36</td>
<td>2560</td>
<td>Near-optimal</td>
</tr>
<tr class="even">
<td><em>ESM-2</em> (15B)</td>
<td>15B</td>
<td>48</td>
<td>5120</td>
<td>State-of-the-art</td>
</tr>
</tbody>
</table>
<p>Performance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. The scaling relationship is not linear: doubling parameters does not double accuracy. But gains remain consistent through even the largest models, suggesting that the 15-billion parameter ceiling reflects computational constraints rather than fundamental limits on what sequence statistics can teach.</p>
<p>The scaling behavior mirrors observations in natural language processing, where larger models consistently capture more nuanced patterns. This predictable relationship between scale and capability provides a roadmap for model development: if more biological knowledge is needed, train a larger model on more data. The practical implications shaped how the field approached subsequent genomic foundation models, with the scaling law framework and its implications discussed in <a href="p3-ch13-fm-principles.html#sec-ch13-scaling" class="quarto-xref"><span>Section 13.3</span></a>.</p>
<div id="fig-esm2-scaling" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-esm2-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/02-A-fig-esm2-scaling.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/02-B-fig-esm2-scaling.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/02-C-fig-esm2-scaling.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-esm2-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: [Essential] Scaling analysis. Panel A (Performance vs.&nbsp;Parameters): Log-log plot; x-axis parameters (8M → 15B); y-axis performance on structure-related tasks; multiple curves; no sign of saturation. Panel B (Model Family Table): <em>ESM-2</em> variants stacked by size; visual encoding of layers, hidden dimension, performance; annotate where capabilities emerge (~150M useful embeddings, ~650M structural understanding, ~3B near-optimal single-sequence structure, ~15B approaches MSA methods). Panel C (Capability Thresholds): Specific capabilities as step functions; contact prediction gradual; zero-shot structure emergent at ~650M.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch15-alternative-architectures" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="sec-ch15-alternative-architectures"><span class="header-section-number">15.2</span> Alternative Architectures</h2>
<p>The success of <em>ESM</em> raised a natural question: how much depends on the specific BERT architecture versus the general approach of self-supervised learning on protein sequences? The <em>ProtTrans</em> family explored this question by applying multiple transformer architectures to protein modeling <span class="citation" data-cites="elnaggar_prottrans_2021">(<a href="../bib/references.html#ref-elnaggar_prottrans_2021" role="doc-biblioref">Elnaggar et al. 2021</a>)</span>.</p>
<p><em>ProtBERT</em> applies the bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences. This training corpus, substantially larger than UniRef50, provides broader coverage at the cost of including more redundant and potentially lower-quality sequences. The architectural choices match <em>ESM</em> closely, enabling direct comparison of training data effects.</p>
<p><em>ProtT5</em> adapts the encoder-decoder architecture from <em>T5</em>, enabling both understanding and generation tasks <span class="citation" data-cites="raffel_t5_2019">(<a href="../bib/references.html#ref-raffel_t5_2019" role="doc-biblioref">Raffel et al. 2023</a>)</span>. The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture proved valuable for tasks requiring sequence generation, such as structure-conditioned design or sequence completion, though the encoder-only architecture remains dominant for <strong>embedding</strong> and classification tasks.</p>
<p><em>ProtXLNet</em> explores permutation language modeling, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training <span class="citation" data-cites="yang_xlnet_2020">(<a href="../bib/references.html#ref-yang_xlnet_2020" role="doc-biblioref">Yang et al. 2020</a>)</span>. By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies at the cost of more complex training.</p>
<p>These architectural variants demonstrate that the protein language modeling paradigm generalizes beyond specific design choices. All architectures learn meaningful representations when trained on sufficient data, though performance differences emerge for specific downstream tasks. Encoder-only models excel at classification and embedding tasks where the entire sequence is available. Encoder-decoder models enable generation tasks where outputs must be produced token by token.</p>
</section>
<section id="sec-ch15-attention-coupling" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="sec-ch15-attention-coupling"><span class="header-section-number">15.3</span> Attention and Evolutionary Coupling</h2>
<p>The emergence of contact information in <em>ESM’s</em> attention patterns connects to a deeper principle: <strong>evolutionary coupling</strong>. When two residues must maintain physical contact for a protein to function, mutations at one position create selective pressure for compensatory mutations at the other. Over evolutionary time, these correlated mutations leave statistical signatures in protein families that can be detected through covariance analysis of multiple sequence alignments.</p>
<p>Direct Coupling Analysis (DCA) and related methods extract these coevolutionary signals to predict residue-residue contacts <span class="citation" data-cites="morcos_dca_2011">(<a href="../bib/references.html#ref-morcos_dca_2011" role="doc-biblioref">Morcos et al. 2011</a>)</span>. The approach requires constructing multiple sequence alignments, computing covariance matrices, and applying statistical corrections to distinguish direct from indirect correlations. The resulting contact predictions enabled the first accurate structure predictions for proteins lacking homologs in structural databases.</p>
<p>Protein language models learn to extract similar information through a different route. Rather than computing covariance explicitly, transformers learn attention patterns that capture which positions inform predictions at other positions. When position <em>i</em> strongly attends to position <em>j</em> during masked prediction, the model has learned that knowing the amino acid at <em>j</em> helps predict the amino acid at <em>i</em>. This is precisely the signature of evolutionary coupling: positions that covary because they must maintain physical contact.</p>
<p>The attention-based approach offers several advantages over traditional covariance analysis. Language models generalize across protein families, learning shared principles that transfer to proteins with sparse evolutionary sampling. They handle the statistical challenge of distinguishing direct from indirect correlations implicitly through deep architecture rather than requiring explicit correction. And they provide rich representations beyond binary contact predictions, encoding information about the strength and nature of residue relationships.</p>
<p>Rao and colleagues demonstrated this connection directly by extracting attention weights from <em>ESM</em> and converting them to contact predictions <span class="citation" data-cites="rao_transformer_2020">(<a href="../bib/references.html#ref-rao_transformer_2020" role="doc-biblioref">Rao et al. 2020</a>)</span>. The resulting predictions approached the accuracy of dedicated contact prediction methods, despite the model never being trained to predict contacts. The <strong>attention mechanism</strong>, optimized purely for masked token prediction, discovers the coevolutionary structure of protein sequences as a byproduct.</p>
</section>
<section id="sec-ch15-esmfold" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="sec-ch15-esmfold"><span class="header-section-number">15.4</span> ESMFold: Structure from Sequence</h2>
<p>Structure prediction has traditionally required multiple sequence alignments (MSAs) that search protein databases for evolutionary relatives, a process that can take hours per protein and fails entirely for sequences lacking detectable homologs. <em>ESMFold</em> demonstrated that the representations learned by <em>ESM-2</em> contain sufficient evolutionary information to predict three-dimensional structure directly, eliminating the alignment requirement while maintaining competitive accuracy.</p>
<section id="sec-ch15-alignment-free" class="level3" data-number="15.4.1">
<h3 data-number="15.4.1" class="anchored" data-anchor-id="sec-ch15-alignment-free"><span class="header-section-number">15.4.1</span> Alignment-Free Prediction</h3>
<p>The most dramatic demonstration of protein language model capabilities came with <em>ESMFold</em>, which predicts protein 3D structure directly from <em>ESM-2</em> embeddings without requiring multiple sequence alignments <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. Traditional structure prediction, including <em>AlphaFold2</em>, relies heavily on MSAs constructed through computationally expensive searches against sequence databases. These searches can take hours per protein, and prediction quality depends critically on finding informative homologs.</p>
<p><em>ESMFold</em> eliminates this requirement entirely. The architecture couples <em>ESM-2</em> (using the 15-billion parameter variant) with a structure module adapted from <em>AlphaFold2’s</em> Evoformer and structure module. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates. The model takes a single sequence as input and outputs predicted 3D coordinates for all atoms.</p>
<p>The computational speedup is substantial: approximately 60-fold faster than <em>AlphaFold2</em> for typical proteins. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive. Metagenomic proteins, often lacking close homologs in existing databases, represent exactly the cases where MSA-based methods struggle and where single-sequence predictions become essential.</p>
<p><em>ESMFold</em> achieves atomic-level accuracy for many proteins, though slightly below <em>AlphaFold2</em> for proteins that benefit strongly from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where explicit alignments provide information that single-sequence analysis cannot fully recover. For well-represented protein families, <em>ESMFold</em> approaches <em>AlphaFold2</em> accuracy at a fraction of the computational cost.</p>
<div id="fig-esmfold" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-esmfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/03-A-fig-esmfold.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/03-B-fig-esmfold.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/03-C-fig-esmfold.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/03-D-fig-esmfold.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER D</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-esmfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.3: [High] Four-panel figure. Panel A (Architecture Pipeline): Single sequence → <em>ESM-2</em> (15B) embeddings → Structure module → 3D coordinates; “No MSA required.” Panel B (Speed Comparison): Bar chart of <em>AlphaFold2</em> (hours) vs <em>ESMFold</em> (minutes); 60× speedup. Panel C (Accuracy Comparison): Scatter plot <em>ESMFold</em> vs <em>AlphaFold2</em> colored by MSA depth; well-represented proteins both accurate; sparse MSA proteins <em>ESMFold</em> more robust. Panel D (Metagenomic Application): Earth Microbiome Project proteins; many lack homologs; <em>ESMFold</em> enables scale.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch15-esmfold-implications" class="level3" data-number="15.4.2">
<h3 data-number="15.4.2" class="anchored" data-anchor-id="sec-ch15-esmfold-implications"><span class="header-section-number">15.4.2</span> What ESMFold Reveals About PLMs</h3>
<p><em>ESMFold’s</em> success demonstrates that <em>ESM-2’s</em> internal representations encode sufficient information to determine 3D structure. The language model has learned not merely local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular three-dimensional shape.</p>
<p>This has profound implications for understanding what protein language models learn. The attention patterns that emerge from masked prediction are, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space to maintain stability attend to each other in the transformer’s attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution under physical constraints, encode structural information that sufficiently powerful language models can decode.</p>
<p>The fundamental insight is that evolution has already solved the structure prediction problem, millions of times over, and recorded the solutions in sequence databases. Language models learn to read those solutions, extracting the implicit structural knowledge that selection has embedded in surviving sequences.</p>
<div class="callout callout-style-default callout-note callout-titled" title="AlphaFold: Structure Prediction Without Foundation Models">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
AlphaFold: Structure Prediction Without Foundation Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>AlphaFold2’s</em> performance at CASP14 in 2020 solved a 50-year grand challenge, predicting protein structures with accuracy competitive with experimental determination <span class="citation" data-cites="jumper_alphafold2_2021">(<a href="../bib/references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. The achievement transformed structural biology and earned its creators the 2024 Nobel Prize in Chemistry. Yet <em>AlphaFold</em> is not a foundation model in the sense this book uses the term (see <a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>). Understanding why illuminates what makes PLM-based approaches distinctive.</p>
<p><em>AlphaFold</em> requires multiple sequence alignments as input. The Evoformer architecture processes MSA features alongside the query sequence, using attention mechanisms that operate over both the sequence dimension and the alignment dimension. Evolutionary information enters the model explicitly through database search rather than being learned implicitly from sequence data. This design choice has computational consequences: MSA construction can take hours per protein, and prediction quality depends critically on finding informative homologs. For orphan proteins lacking close relatives in sequence databases, <em>AlphaFold’s</em> accuracy degrades substantially.</p>
<p>The architectural innovations that enabled <em>AlphaFold’s</em> success differ fundamentally from the foundation model paradigm. Evoformer’s attention over MSA rows and columns, iterative recycling through the network, and the structure module’s SE(3)-equivariant operations represent expert-designed inductive biases encoding protein physics. These components were engineered specifically for structure prediction, not learned from self-supervised objectives on broad sequence data. The model excels at its designed task but does not produce general-purpose representations transferable to other problems.</p>
<p><em>ESMFold</em> inverts this design philosophy. Rather than requiring explicit evolutionary input, <em>ESMFold</em> couples <em>ESM-2</em> embeddings with a structure module adapted from <em>AlphaFold’s</em> architecture. The language model provides the evolutionary context that the structure module needs, context learned implicitly through masked token prediction on millions of protein sequences. A single sequence goes in; predicted coordinates come out. No MSA construction, no database search, no hours of preprocessing.</p>
<p>The comparison reveals what protein language models have and have not learned. <em>ESMFold</em> approaches <em>AlphaFold</em> accuracy for well-represented protein families where the language model’s training data provided dense evolutionary sampling. The gap widens for proteins where deep MSAs provide information that single-sequence analysis cannot fully recover. <em>ESMFold</em> runs approximately 60-fold faster than <em>AlphaFold</em>, enabling structure prediction at metagenomic scale for the millions of protein sequences emerging from environmental sequencing projects. The two approaches exhibit different failure modes: <em>AlphaFold</em> struggles with orphan proteins that lack homologs; <em>ESMFold</em> struggles with sequences the language model finds surprising (high perplexity), even when homologs exist.</p>
<p><em>AlphaFold3</em> complicates this dichotomy <span class="citation" data-cites="abramson_alphafold3_2024">(<a href="../bib/references.html#ref-abramson_alphafold3_2024" role="doc-biblioref">Abramson et al. 2024</a>)</span>. The updated architecture uses diffusion-based structure generation and handles protein-ligand, protein-nucleic acid, and multi-chain complexes within a unified framework. MSA dependency is reduced in some contexts, and the model moves toward general biomolecular structure prediction rather than single-chain protein folding. Whether this represents convergence between task-specific and foundation model approaches remains an open question.</p>
<p><em>AlphaFold</em> demonstrated that protein structure prediction was computationally tractable; <em>ESMFold</em> demonstrated that foundation models had learned enough biology to solve it differently. Both insights matter. For this book’s purposes, <em>ESMFold</em> illustrates the foundation model paradigm: self-supervised pretraining produces representations that transfer to downstream tasks, including tasks (like structure prediction) that were not part of the training objective. <em>AlphaFold’s</em> success through architectural engineering rather than learned representations represents an alternative path, one that achieved the goal first but may prove less generalizable as the field matures. The <em>AlphaMissense</em> model discussed in <a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a> repurposes <em>AlphaFold’s</em> structure module for variant effect prediction, suggesting that even task-specific architectures can seed broader applications when their components prove useful beyond their original context.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch15-function-prediction" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="sec-ch15-function-prediction"><span class="header-section-number">15.5</span> Function Prediction</h2>
<p>Beyond structure, protein language models enable prediction of protein function directly from sequence. Function prediction encompasses multiple tasks: predicting Gene Ontology terms that describe molecular function, biological process, and cellular component; classifying enzyme activity; identifying binding sites and interaction partners; and predicting subcellular localization.</p>
<p>Traditional function prediction relied on homology: proteins similar in sequence are assumed to share function. This approach fails for orphan proteins lacking characterized homologs and cannot distinguish functional differences between closely related sequences. PLM-based approaches address both limitations by learning representations that capture functional signatures beyond simple sequence similarity.</p>
<p>For Gene Ontology term prediction, PLM embeddings serve as input features to classification models that predict which GO terms apply to each protein. The embeddings capture evolutionary and structural information relevant to function, enabling accurate predictions even for proteins with limited homology to characterized sequences <span class="citation" data-cites="kulmanov_deepgo-se_2024">(<a href="../bib/references.html#ref-kulmanov_deepgo-se_2024" role="doc-biblioref"><strong>kulmanov_deepgo-se_2024?</strong></a>)</span>. Performance improves with embedding quality, suggesting that larger language models capture more functionally relevant information. These embeddings can also serve as node features in biological network analyses (<span class="quarto-unresolved-ref">?sec-ch18-fm-embeddings</span>), and the function predictions inform drug target identification workflows (<span class="quarto-unresolved-ref">?sec-ch27-variant-to-gene</span>).</p>
<p>Enzyme classification benefits similarly from PLM representations. The Enzyme Commission hierarchy categorizes enzymes by the reactions they catalyze, from broad classes (oxidoreductases, transferases) to specific substrate preferences. PLM embeddings distinguish these categories effectively, capturing the sequence features that determine catalytic activity without requiring explicit structural analysis <span class="citation" data-cites="sanderson_deepectransformer_2023">(<a href="../bib/references.html#ref-sanderson_deepectransformer_2023" role="doc-biblioref"><strong>sanderson_deepectransformer_2023?</strong></a>)</span>.</p>
<p>Binding site prediction applies attention analysis to identify which residues participate in ligand binding, protein-protein interactions, or nucleic acid recognition. Positions that the model identifies as important for contextual prediction often correspond to functionally important sites, including binding pockets and catalytic residues <span class="citation" data-cites="fang_deepprosite_2023">(<a href="../bib/references.html#ref-fang_deepprosite_2023" role="doc-biblioref"><strong>fang_deepprosite_2023?</strong></a>)</span>. This capability enables rapid identification of functional sites in newly sequenced proteins.</p>
</section>
<section id="sec-ch15-variant-effects" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="sec-ch15-variant-effects"><span class="header-section-number">15.6</span> Variant Effect Prediction</h2>
<p>A critical clinical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely pathogenic or benign. The traditional approach required either direct experimental characterization or computational methods trained on labeled pathogenicity data, both of which scale poorly to the millions of possible variants in each human genome (see <a href="../part_1/p1-ch04-vep-classical.html" class="quarto-xref"><span>Chapter 4</span></a> for discussion of classical approaches).</p>
<p><em>ESM-1v</em> demonstrated that PLMs can predict variant effects without any training on variant labels <span class="citation" data-cites="meier_esm-1v_2021">(<a href="../bib/references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>)</span>. The approach exploits the masked language modeling objective directly: for a variant at position <span class="math inline">\(i\)</span> changing amino acid <span class="math inline">\(a\)</span> to amino acid <span class="math inline">\(b\)</span>, compute the log-likelihood ratio:</p>
<p><span class="math display">\[
\Delta \text{score} = \log P(b \mid \text{context}) - \log P(a \mid \text{context})
\]</span></p>
<p>If the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This <strong>zero-shot</strong> prediction requires no labeled training data. The model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.</p>
<p>The intuition is straightforward. Evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids. Substitutions that violate these preferences are more likely to disrupt function. The language model captures these preferences through training on millions of evolutionarily successful sequences. Variants that the model finds surprising are more likely to be functionally disruptive.</p>
<div id="fig-plm-variant-scoring" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plm-variant-scoring-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/04-A-fig-plm-variant-scoring.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER A</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/04-B-fig-plm-variant-scoring.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER B</strong></figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/04-C-fig-plm-variant-scoring.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER C</strong></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/04-D-fig-plm-variant-scoring.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER D</strong></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plm-variant-scoring-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.4: [High] Four-panel figure. Panel A (The Scoring Mechanism): Protein sequence with variant position; <span class="math inline">\(P(\mathrm{ref}\mid \mathrm{context})\)</span> vs <span class="math inline">\(P(\mathrm{var}\mid \mathrm{context})\)</span>; Score <span class="math inline">\(= \log P(\mathrm{var}) - \log P(\mathrm{ref})\)</span>. Panel B (Intuition): Evolution tested billions of substitutions; low probability variants <span class="math inline">\(=\)</span> evolutionarily disfavored <span class="math inline">\(=\)</span> likely disruptive. Panel C (Benchmark Performance): ROC curves <em>ESM-1v</em> vs classical methods on deep mutational scanning data; competitive without variant labels. Panel D (<em>AlphaMissense</em> Enhancement): <em>ESM</em> embeddings + <em>AlphaFold2</em> structural features; combined model; <span class="math inline">\(71\text{M}\)</span> precomputed scores; performance boost from structure.
</figcaption>
</figure>
</div>
<p>Brandes and colleagues applied <em>ESM-1b</em> to predict effects for all approximately 450 million possible missense variants in the human genome, providing a precomputed resource for clinical variant interpretation <span class="citation" data-cites="brandes_genome-wide_2023">(<a href="../bib/references.html#ref-brandes_genome-wide_2023" role="doc-biblioref">Brandes et al. 2023</a>)</span>. On ClinVar benchmarks, <em>ESM-1b</em> outperformed existing methods in classifying variants as pathogenic or benign.</p>
<p><em>AlphaMissense</em> extended this approach by combining PLM representations with structural context from predicted protein structures <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="../bib/references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>. The integration of sequence-based and structure-based signals improves accuracy, particularly for variants affecting protein stability or buried residues. <em>AlphaMissense</em> provides predictions for all approximately 71 million possible single amino acid substitutions in the human proteome.</p>
<p>The detailed comparison of variant effect prediction methods, including how PLM-based scores integrate with clinical classification frameworks, is covered in <a href="p3-ch17-vep-fm.html#sec-ch17-alphamissense" class="quarto-xref"><span>Section 17.2.3</span></a>. The calibration of these scores to ACMG criteria appears in <span class="quarto-unresolved-ref">?sec-ch14-acmg-mapping</span>, and integration into rare disease diagnostic workflows in <span class="quarto-unresolved-ref">?sec-ch26-fm-scoring</span>. Here, the key point is that protein language models provide the foundational representations that make accurate zero-shot variant prediction possible.</p>
</section>
<section id="sec-ch15-structure-integration" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="sec-ch15-structure-integration"><span class="header-section-number">15.7</span> Integration with Structure Prediction</h2>
<p>Protein language models exist within a broader ecosystem of computational methods for protein analysis. Understanding how PLMs relate to structure prediction systems clarifies their role and capabilities.</p>
<p><em>AlphaFold2</em> achieved breakthrough accuracy in structure prediction by combining learned representations with explicit geometric modeling <span class="citation" data-cites="jumper_alphafold2_2021">(<a href="../bib/references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. The architecture processes both sequence information through embeddings and evolutionary information through multiple sequence alignments, using an attention-based module (Evoformer) to integrate these signals before predicting atomic coordinates. <em>AlphaFold2’s</em> success depended critically on MSA quality: proteins with many homologs could be predicted accurately, while orphan proteins remained challenging.</p>
<p><em>ESMFold</em> demonstrated that PLM embeddings can replace MSA-derived features, achieving competitive accuracy without the alignment bottleneck. This finding clarified the relationship between language models and structure prediction: PLMs learn to compress evolutionary information into representations that are functionally equivalent to explicit alignments, at least for proteins with sufficient representation in training databases.</p>
<p><em>AlphaFold3</em> extended structure prediction to protein complexes, nucleic acids, and small molecules <span class="citation" data-cites="abramson_alphafold3_2024">(<a href="../bib/references.html#ref-abramson_alphafold3_2024" role="doc-biblioref">Abramson et al. 2024</a>)</span>. The architecture incorporates diffusion-based generation, enabling prediction of binding poses and complex assemblies. These capabilities complement PLM-based function prediction by providing structural context for interpreting functional predictions.</p>
<p>Generative protein design methods including <em>RFDiffusion</em> and <em>ProteinMPNN</em> leverage both structural and sequence information <span class="citation" data-cites="watson_rfdiffusion_2023 dauparas_proteinmpnn_2022">(<a href="../bib/references.html#ref-watson_rfdiffusion_2023" role="doc-biblioref">Watson et al. 2023</a>; <a href="../bib/references.html#ref-dauparas_proteinmpnn_2022" role="doc-biblioref">Dauparas et al. 2022</a>)</span>. <em>RFDiffusion</em> generates novel protein backbones through diffusion processes conditioned on design objectives. <em>ProteinMPNN</em> designs sequences likely to fold into specified structures. Both methods benefit from PLM representations when designing sequences with desired functional properties, demonstrating how language models integrate into the broader protein engineering pipeline (see <a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a> for detailed treatment of sequence design methods).</p>
<p>The trajectory from <em>ESM</em> to <em>ESMFold</em> to integration with design tools illustrates how PLMs serve as a foundation for diverse downstream applications. The representations learned through self-supervised training transfer across tasks, providing a common language for structure prediction, function annotation, variant interpretation, and protein engineering. This pattern of foundation models enabling diverse applications recurs throughout genomic AI, as discussed in <a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
</section>
<section id="sec-ch15-limitations" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="sec-ch15-limitations"><span class="header-section-number">15.8</span> Limitations</h2>
<p>Despite their success, protein language models face several limitations that inform the development of genomic models and guide appropriate application.</p>
<div id="fig-plm-limitations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plm-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch12/05-fig-plm-limitations.png" class="img-fluid figure-img"></p>
<figcaption><strong>FIGURE PLACEHOLDER</strong></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plm-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.5: [Enhancing] Grid of limitation categories with visual examples. Orphan Proteins: Phylogenetic tree with isolated lineage; no homologs = no evolutionary context; performance degradation curve. Novel Folds: Designed protein with non-natural topology; predictions unreliable outside training. Conformational Flexibility: Protein with multiple conformations; PLM produces single embedding. Epistasis: Two distant positions; individual mutations benign; combination deleterious; models assume independence. Interpretability: Attention correlates with biology but mechanism remains opaque.
</figcaption>
</figure>
</div>
<section id="sec-ch15-orphan-proteins" class="level3" data-number="15.8.1">
<h3 data-number="15.8.1" class="anchored" data-anchor-id="sec-ch15-orphan-proteins"><span class="header-section-number">15.8.1</span> Orphan and Dark Proteins</h3>
<p>PLMs learn from evolutionary statistics, performing best for proteins with rich representation in training databases. Orphan proteins, those unique to specific lineages without detectable homologs, lack the evolutionary context that PLMs exploit. For these proteins, the model has no basis for distinguishing likely from unlikely amino acids at each position, and predictions degrade accordingly.</p>
<p>The problem extends to “dark” proteins that are poorly characterized despite having homologs. If an entire protein family has escaped experimental characterization, PLMs may learn statistical patterns without capturing functional relevance. The model cannot distinguish constraint imposed by function from constraint imposed by historical accident.</p>
</section>
<section id="sec-ch15-novel-folds" class="level3" data-number="15.8.2">
<h3 data-number="15.8.2" class="anchored" data-anchor-id="sec-ch15-novel-folds"><span class="header-section-number">15.8.2</span> Novel Folds</h3>
<p>Training data shapes what models can predict. PLMs trained on natural protein databases learn the statistical patterns of naturally occurring folds, potentially struggling with designed proteins or hypothetical folds outside the training distribution. When researchers design proteins with novel topologies not found in nature, PLM predictions become less reliable because the relevant sequence patterns were never encountered during training <span class="citation" data-cites="verkuil_language_2022">(<a href="../bib/references.html#ref-verkuil_language_2022" role="doc-biblioref"><strong>verkuil_language_2022?</strong></a>)</span>.</p>
</section>
<section id="sec-ch15-conformational-flexibility" class="level3" data-number="15.8.3">
<h3 data-number="15.8.3" class="anchored" data-anchor-id="sec-ch15-conformational-flexibility"><span class="header-section-number">15.8.3</span> Conformational Flexibility</h3>
<p>Most PLM representations assume a single static structure, but many proteins adopt multiple conformations relevant to function. Allosteric proteins, intrinsically disordered regions, and proteins that undergo conformational changes upon binding present challenges for methods that embed each sequence into a single representation. The language model learns the average properties of sequences but may not capture the dynamic range that determines biological behavior.</p>
</section>
<section id="sec-ch15-epistasis" class="level3" data-number="15.8.4">
<h3 data-number="15.8.4" class="anchored" data-anchor-id="sec-ch15-epistasis"><span class="header-section-number">15.8.4</span> Epistasis</h3>
<p>Most variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit <strong>epistasis</strong>, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors model marginal effects at each position but do not explicitly capture higher-order interactions, though the contextual embeddings may represent some epistatic relationships implicitly.</p>
</section>
<section id="sec-ch15-interpretability" class="level3" data-number="15.8.5">
<h3 data-number="15.8.5" class="anchored" data-anchor-id="sec-ch15-interpretability"><span class="header-section-number">15.8.5</span> Interpretability</h3>
<p>While attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods, including attention pattern analysis (<a href="../part_5/p5-ch24-interpretability.html#sec-ch24-attention" class="quarto-xref"><span>Section 24.5</span></a>) and probing studies (<a href="../part_5/p5-ch24-interpretability.html#sec-ch24-probing" class="quarto-xref"><span>Section 24.4</span></a>), but PLMs remain partially opaque. The distinction between plausible and faithful explanations, critical for clinical applications, is examined in <a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 24</span></a>. For clinical applications where explanations matter, this interpretability gap limits adoption. A prediction that a variant is pathogenic is more useful when accompanied by mechanistic insight into why the variant disrupts function.</p>
</section>
</section>
<section id="sec-ch15-lessons" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="sec-ch15-lessons"><span class="header-section-number">15.9</span> Lessons for Genomic Foundation Models</h2>
<p>The success of protein language models established principles that guided subsequent development of genomic foundation models. These lessons transfer with appropriate modifications to DNA and RNA modeling.</p>
<section id="sec-ch15-self-supervised" class="level3" data-number="15.9.1">
<h3 data-number="15.9.1" class="anchored" data-anchor-id="sec-ch15-self-supervised"><span class="header-section-number">15.9.1</span> Self-Supervised Biological Knowledge</h3>
<p>PLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can exploit. This principle underlies the entire foundation model paradigm: sufficiently large models trained on sufficiently large datasets with appropriate objectives will learn representations that capture biological function.</p>
</section>
<section id="sec-ch15-scaling" class="level3" data-number="15.9.2">
<h3 data-number="15.9.2" class="anchored" data-anchor-id="sec-ch15-scaling"><span class="header-section-number">15.9.2</span> Scaling Benefits</h3>
<p>Performance improves predictably with model size through the range currently explored. The progression from 8 million to 15 billion parameters in <em>ESM-2</em> showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While scaling cannot continue indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This relationship justified the computational investment in large genomic foundation models (see <a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a> for discussion of scaling laws in genomic contexts).</p>
</section>
<section id="sec-ch15-transfer" class="level3" data-number="15.9.3">
<h3 data-number="15.9.3" class="anchored" data-anchor-id="sec-ch15-transfer"><span class="header-section-number">15.9.3</span> Effective Transfer Learning</h3>
<p>Representations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects, function annotation). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids simultaneously learns about protein structure, function, evolutionary constraint, and disease relevance. The same principle motivates genomic language models: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects. Transfer learning strategies, including fine-tuning approaches and parameter-efficient adaptation, are discussed in detail in <a href="../part_2/p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>, with specific guidance on choosing between these strategies in <span class="quarto-unresolved-ref">?sec-ch09-choosing-strategy</span>.</p>
</section>
<section id="sec-ch15-architecture-matching" class="level3" data-number="15.9.4">
<h3 data-number="15.9.4" class="anchored" data-anchor-id="sec-ch15-architecture-matching"><span class="header-section-number">15.9.4</span> Architecture-Sequence Matching</h3>
<p>The BERT-style bidirectional encoder proved effective for proteins, where entire sequences are typically available and lengths rarely exceed a thousand residues. Genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with coding regions being dense while intergenic regions are sparser, and structural features including reverse-complement relationships absent in proteins. These differences motivate architectural adaptations in genomic language models, as explored in <a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>.</p>
</section>
<section id="sec-ch15-integration" class="level3" data-number="15.9.5">
<h3 data-number="15.9.5" class="anchored" data-anchor-id="sec-ch15-integration"><span class="header-section-number">15.9.5</span> Integration Benefits</h3>
<p><em>AlphaMissense</em> demonstrated that PLM embeddings combine effectively with structural and population genetics information, achieving accuracy beyond what any single information source provides. The most powerful methods integrate multiple signals, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace functional annotations, chromatin data, and clinical information (see <a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a> for variant effect prediction integration strategies).</p>
</section>
</section>
<section id="sec-ch15-conclusion" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="sec-ch15-conclusion"><span class="header-section-number">15.10</span> Paradigm That Generalized</h2>
<p>Protein language models established that transformer architectures can learn deep biological knowledge from sequence alone. <em>ESM’s</em> ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. The framework validated a paradigm: treat biological sequences as language, train large models to predict masked tokens, and extract functional knowledge from learned representations. Attention patterns in these models capture evolutionary constraint, contact prediction, and structural relationships without requiring multiple sequence alignments or explicit structural supervision.</p>
<p>This success directly motivated genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too. The DNA language models examined in <a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a> adapt protein language model architectures and training strategies to the distinct challenges of genomic sequences: longer contexts, different alphabets, ambiguous tokenization, and the full complexity of gene regulation beyond protein coding. RNA language models occupy an intermediate position, sharing features with both protein and DNA modeling while addressing the unique challenges of RNA structure and processing.</p>
<p>The integration path extends beyond sequence modeling. Just as protein language model representations feed into structure prediction (<em>ESMFold</em>) and variant effect prediction (<em>AlphaMissense</em>), genomic language model embeddings integrate into regulatory models (<a href="p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>) and clinical applications (<a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a>, <a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a>). Protein design methods (<a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a>) demonstrate how generative modeling builds on the representations that language models provide. Throughout this progression, the principle that <em>ESM</em> established remains: self-supervised learning on biological sequences captures knowledge that transfers across diverse applications.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-abramson_alphafold3_2024" class="csl-entry" role="listitem">
Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. <span>“[<span>AlphaFold3</span>] <span>Accurate</span> Structure Prediction of Biomolecular Interactions with <span>AlphaFold</span> 3.”</span> <em>Nature</em> 630 (8016): 493–500. <a href="https://doi.org/10.1038/s41586-024-07487-w">https://doi.org/10.1038/s41586-024-07487-w</a>.
</div>
<div id="ref-brandes_genome-wide_2023" class="csl-entry" role="listitem">
Brandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. <span>“Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.”</span> <em>Nature Genetics</em> 55 (9): 1512–22. <a href="https://doi.org/10.1038/s41588-023-01465-0">https://doi.org/10.1038/s41588-023-01465-0</a>.
</div>
<div id="ref-cheng_alphamissense_2023" class="csl-entry" role="listitem">
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. <span>“[<span>AlphaMissense</span>] <span>Accurate</span> Proteome-Wide Missense Variant Effect Prediction with <span>AlphaMissense</span>.”</span> <em>Science</em> 381 (6664): eadg7492. <a href="https://doi.org/10.1126/science.adg7492">https://doi.org/10.1126/science.adg7492</a>.
</div>
<div id="ref-dauparas_proteinmpnn_2022" class="csl-entry" role="listitem">
Dauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. <span>“Robust Deep Learning–Based Protein Sequence Design Using <span>ProteinMPNN</span>.”</span> <em>Science</em> 378 (6615): 49–56. <a href="https://doi.org/10.1126/science.add2187">https://doi.org/10.1126/science.add2187</a>.
</div>
<div id="ref-elnaggar_prottrans_2021" class="csl-entry" role="listitem">
Elnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. <span>“<span>ProtTrans</span>: <span>Towards</span> <span>Cracking</span> the <span>Language</span> of <span>Life</span>’s <span>Code</span> <span>Through</span> <span>Self</span>-<span>Supervised</span> <span>Deep</span> <span>Learning</span> and <span>High</span> <span>Performance</span> <span>Computing</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2007.06225">https://doi.org/10.48550/arXiv.2007.06225</a>.
</div>
<div id="ref-jumper_alphafold2_2021" class="csl-entry" role="listitem">
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. <span>“[<span>AlphaFold2</span>] <span>Highly</span> Accurate Protein Structure Prediction with <span>AlphaFold</span>.”</span> <em>Nature</em> 596 (7873): 583–89. <a href="https://doi.org/10.1038/s41586-021-03819-2">https://doi.org/10.1038/s41586-021-03819-2</a>.
</div>
<div id="ref-lin_esm-2_2022" class="csl-entry" role="listitem">
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. <span>“[<span>ESM</span>-2] <span>Language</span> Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.07.20.500902">https://doi.org/10.1101/2022.07.20.500902</a>.
</div>
<div id="ref-meier_esm-1v_2021" class="csl-entry" role="listitem">
Meier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. <span>“[<span>ESM</span>-1v] <span>Language</span> Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.”</span> bioRxiv. <a href="https://doi.org/10.1101/2021.07.09.450648">https://doi.org/10.1101/2021.07.09.450648</a>.
</div>
<div id="ref-morcos_dca_2011" class="csl-entry" role="listitem">
Morcos, Faruck, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S. Marks, Chris Sander, Riccardo Zecchina, José N. Onuchic, Terence Hwa, and Martin Weigt. 2011. <span>“Direct-Coupling Analysis of Residue Coevolution Captures Native Contacts Across Many Protein Families.”</span> <em>Proceedings of the National Academy of Sciences</em> 108 (49): E1293–1301. <a href="https://doi.org/10.1073/pnas.1111471108">https://doi.org/10.1073/pnas.1111471108</a>.
</div>
<div id="ref-raffel_t5_2019" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. <span>“Exploring the <span>Limits</span> of <span>Transfer</span> <span>Learning</span> with a <span>Unified</span> <span>Text</span>-to-<span>Text</span> <span>Transformer</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1910.10683">https://doi.org/10.48550/arXiv.1910.10683</a>.
</div>
<div id="ref-rao_transformer_2020" class="csl-entry" role="listitem">
Rao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. 2020. <span>“Transformer Protein Language Models Are Unsupervised Structure Learners.”</span> bioRxiv. <a href="https://doi.org/10.1101/2020.12.15.422761">https://doi.org/10.1101/2020.12.15.422761</a>.
</div>
<div id="ref-rives_esm_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“[<span>ESM</span>-1b] <span>Biological</span> Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 118 (15): e2016239118. <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-suzek_uniref_2007" class="csl-entry" role="listitem">
Suzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. <span>“<span>UniRef</span>: Comprehensive and Non-Redundant <span>UniProt</span> Reference Clusters.”</span> <em>Bioinformatics</em> 23 (10): 1282–88. <a href="https://doi.org/10.1093/bioinformatics/btm098">https://doi.org/10.1093/bioinformatics/btm098</a>.
</div>
<div id="ref-watson_rfdiffusion_2023" class="csl-entry" role="listitem">
Watson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. <span>“De Novo Design of Protein Structure and Function with <span>RFdiffusion</span>.”</span> <em>Nature</em> 620 (7976): 1089–1100. <a href="https://doi.org/10.1038/s41586-023-06415-8">https://doi.org/10.1038/s41586-023-06415-8</a>.
</div>
<div id="ref-yang_xlnet_2020" class="csl-entry" role="listitem">
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. <span>“<span>XLNet</span>: <span>Generalized</span> <span>Autoregressive</span> <span>Pretraining</span> for <span>Language</span> <span>Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1906.08237">https://doi.org/10.48550/arXiv.1906.08237</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_3/p3-ch14-dna-lm.html" class="pagination-link" aria-label="DNA Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_3/p3-ch16-regulatory.html" class="pagination-link" aria-label="Regulatory Models">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>