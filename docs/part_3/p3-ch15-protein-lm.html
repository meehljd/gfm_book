<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>15&nbsp; Protein Language Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_3/p3-ch16-regulatory.html" rel="next">
<link href="../part_3/p3-ch14-dna-lm.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch15-protein-lm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch15-esm-family" id="toc-sec-ch15-esm-family" class="nav-link active" data-scroll-target="#sec-ch15-esm-family"><span class="header-section-number">15.1</span> ESM Model Family</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-esm1b" id="toc-sec-ch15-esm1b" class="nav-link" data-scroll-target="#sec-ch15-esm1b"><span class="header-section-number">15.1.1</span> ESM-1b: Establishing the Paradigm</a></li>
  <li><a href="#sec-ch15-emergent-knowledge" id="toc-sec-ch15-emergent-knowledge" class="nav-link" data-scroll-target="#sec-ch15-emergent-knowledge"><span class="header-section-number">15.1.2</span> Emergent Biological Knowledge</a></li>
  <li><a href="#sec-ch15-esm2" id="toc-sec-ch15-esm2" class="nav-link" data-scroll-target="#sec-ch15-esm2"><span class="header-section-number">15.1.3</span> ESM-2: Scaling Up</a></li>
  </ul></li>
  <li><a href="#sec-ch15-alternative-architectures" id="toc-sec-ch15-alternative-architectures" class="nav-link" data-scroll-target="#sec-ch15-alternative-architectures"><span class="header-section-number">15.2</span> Alternative Architectures</a></li>
  <li><a href="#sec-ch15-attention-coupling" id="toc-sec-ch15-attention-coupling" class="nav-link" data-scroll-target="#sec-ch15-attention-coupling"><span class="header-section-number">15.3</span> Attention and Evolutionary Coupling</a></li>
  <li><a href="#sec-ch15-esmfold" id="toc-sec-ch15-esmfold" class="nav-link" data-scroll-target="#sec-ch15-esmfold"><span class="header-section-number">15.4</span> ESMFold: Structure from Sequence</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-alignment-free" id="toc-sec-ch15-alignment-free" class="nav-link" data-scroll-target="#sec-ch15-alignment-free"><span class="header-section-number">15.4.1</span> Alignment-Free Prediction</a></li>
  <li><a href="#sec-ch15-esmfold-implications" id="toc-sec-ch15-esmfold-implications" class="nav-link" data-scroll-target="#sec-ch15-esmfold-implications"><span class="header-section-number">15.4.2</span> What ESMFold Reveals About PLMs</a></li>
  </ul></li>
  <li><a href="#sec-ch15-function-prediction" id="toc-sec-ch15-function-prediction" class="nav-link" data-scroll-target="#sec-ch15-function-prediction"><span class="header-section-number">15.5</span> Function Prediction</a></li>
  <li><a href="#sec-ch15-variant-effects" id="toc-sec-ch15-variant-effects" class="nav-link" data-scroll-target="#sec-ch15-variant-effects"><span class="header-section-number">15.6</span> Variant Effect Prediction</a></li>
  <li><a href="#sec-ch15-structure-integration" id="toc-sec-ch15-structure-integration" class="nav-link" data-scroll-target="#sec-ch15-structure-integration"><span class="header-section-number">15.7</span> Integration with Structure Prediction</a></li>
  <li><a href="#sec-ch15-limitations" id="toc-sec-ch15-limitations" class="nav-link" data-scroll-target="#sec-ch15-limitations"><span class="header-section-number">15.8</span> Limitations</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-orphan-proteins" id="toc-sec-ch15-orphan-proteins" class="nav-link" data-scroll-target="#sec-ch15-orphan-proteins"><span class="header-section-number">15.8.1</span> Orphan and Dark Proteins</a></li>
  <li><a href="#sec-ch15-novel-folds" id="toc-sec-ch15-novel-folds" class="nav-link" data-scroll-target="#sec-ch15-novel-folds"><span class="header-section-number">15.8.2</span> Novel Folds</a></li>
  <li><a href="#sec-ch15-conformational-flexibility" id="toc-sec-ch15-conformational-flexibility" class="nav-link" data-scroll-target="#sec-ch15-conformational-flexibility"><span class="header-section-number">15.8.3</span> Conformational Flexibility</a></li>
  <li><a href="#sec-ch15-epistasis" id="toc-sec-ch15-epistasis" class="nav-link" data-scroll-target="#sec-ch15-epistasis"><span class="header-section-number">15.8.4</span> Epistasis</a></li>
  <li><a href="#sec-ch15-interpretability" id="toc-sec-ch15-interpretability" class="nav-link" data-scroll-target="#sec-ch15-interpretability"><span class="header-section-number">15.8.5</span> Interpretability</a></li>
  </ul></li>
  <li><a href="#sec-ch15-lessons" id="toc-sec-ch15-lessons" class="nav-link" data-scroll-target="#sec-ch15-lessons"><span class="header-section-number">15.9</span> Lessons for Genomic Foundation Models</a>
  <ul class="collapse">
  <li><a href="#sec-ch15-self-supervised" id="toc-sec-ch15-self-supervised" class="nav-link" data-scroll-target="#sec-ch15-self-supervised"><span class="header-section-number">15.9.1</span> Self-Supervised Biological Knowledge</a></li>
  <li><a href="#sec-ch15-scaling" id="toc-sec-ch15-scaling" class="nav-link" data-scroll-target="#sec-ch15-scaling"><span class="header-section-number">15.9.2</span> Scaling Benefits</a></li>
  <li><a href="#sec-ch15-transfer" id="toc-sec-ch15-transfer" class="nav-link" data-scroll-target="#sec-ch15-transfer"><span class="header-section-number">15.9.3</span> Effective Transfer Learning</a></li>
  <li><a href="#sec-ch15-architecture-matching" id="toc-sec-ch15-architecture-matching" class="nav-link" data-scroll-target="#sec-ch15-architecture-matching"><span class="header-section-number">15.9.4</span> Architecture-Sequence Matching</a></li>
  <li><a href="#sec-ch15-integration" id="toc-sec-ch15-integration" class="nav-link" data-scroll-target="#sec-ch15-integration"><span class="header-section-number">15.9.5</span> Integration Benefits</a></li>
  </ul></li>
  <li><a href="#sec-ch15-conclusion" id="toc-sec-ch15-conclusion" class="nav-link" data-scroll-target="#sec-ch15-conclusion"><span class="header-section-number">15.10</span> Paradigm That Generalized</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch15-protein-lm.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch15-protein-lm" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>Evolution already solved the problem. Protein language models learned to read the answers.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled" title="Chapter Overview">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Estimated reading time:</strong> 35-45 minutes</p>
<p><strong>Prerequisites:</strong> Understanding of transformer architectures and self-attention (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>), masked language modeling objectives (<a href="../part_2/p2-ch08-pretraining.html#sec-ch08-mlm" class="quarto-xref"><span>Section 8.1</span></a>), and basic protein biochemistry (amino acids, protein folding). Familiarity with DNA language models (<a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>) provides helpful context but is not required.</p>
<p><strong>Learning Objectives:</strong> After this chapter, you should be able to:</p>
<ol type="1">
<li>Explain how masked language modeling on protein sequences yields emergent biological knowledge (structure, contacts, function)</li>
<li>Describe the ESM model family architecture and how capabilities scale with model size</li>
<li>Articulate why attention patterns in PLMs capture evolutionary coupling and residue contacts</li>
<li>Apply zero-shot variant effect prediction using log-likelihood ratios from PLMs</li>
<li>Evaluate when PLM-based approaches succeed versus when they face fundamental limitations</li>
</ol>
<p><strong>Key Insight:</strong> Evolution has already solved the protein structure and function problem billions of times over. Protein language models learn to read those solutions from sequence statistics alone, extracting structural and functional knowledge that selection has encoded in surviving sequences.</p>
</div>
</div>
<p>Over billions of years, natural selection tested trillions of amino acid combinations, ruthlessly eliminating sequences that failed to fold or function while preserving those that worked. The sequences populating modern databases are not random strings but successful solutions to biological problems, each implicitly encoding information about structure, stability, and function. Think of protein databases as a massive answer key from billions of years of open-book exams. Evolution posed the same question trillions of times—“which sequences fold and function?”—and surviving proteins are the accumulated correct answers. The central insight of protein language models is that this evolutionary record, comprising hundreds of millions of sequences in databases like UniRef, contains sufficient information to learn the fundamental principles of protein biology without ever being shown a crystal structure or a functional assay.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Deep Dive: Protein Structure Levels">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deep Dive: Protein Structure Levels
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>For ML readers:</strong> Protein structure is hierarchically organized:</p>
<p><strong>Primary structure:</strong> The linear sequence of amino acids (e.g., Met-Ala-Lys-Glu…). This is the input to protein language models.</p>
<p><strong>Secondary structure:</strong> Local folding patterns:</p>
<ul>
<li><em>Alpha helix</em>: spiral shape stabilized by hydrogen bonds between nearby residues</li>
<li><em>Beta sheet</em>: extended strands connected by hydrogen bonds</li>
<li><em>Random coil/loops</em>: unstructured regions connecting helices and sheets</li>
</ul>
<p><strong>Tertiary structure:</strong> The complete 3D arrangement of a single protein chain. Determined by:</p>
<ul>
<li>Hydrophobic residues packing into the protein core</li>
<li>Disulfide bonds between cysteines</li>
<li>Salt bridges between charged residues</li>
</ul>
<p><strong>Quaternary structure:</strong> Multiple protein chains assembled into a complex (e.g., hemoglobin has four subunits).</p>
<p><strong>Why this matters for PLMs:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 38%">
<col style="width: 38%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Structure Level</th>
<th>What PLMs Learn</th>
<th>Evidence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Secondary</td>
<td>Attention patterns follow helix/sheet boundaries</td>
<td><a href="#sec-ch15-emergent-knowledge" class="quarto-xref"><span>Section 15.1.2</span></a></td>
</tr>
<tr class="even">
<td>Tertiary</td>
<td>Residue contacts visible in attention matrices</td>
<td>Contact prediction accuracy</td>
</tr>
<tr class="odd">
<td>Conservation</td>
<td>Constrained positions = high prediction confidence</td>
<td>Correlation with alignment-based metrics</td>
</tr>
</tbody>
</table>
<p>The remarkable finding is that training only on primary structure (sequences) yields models that implicitly encode higher-level structural information.</p>
</div>
</div>
<p>This insight transformed computational biology. Traditional approaches to understanding proteins required either expensive experimental characterization or physics-based simulations that struggled with the complexity of protein behavior. Multiple sequence alignments could extract conservation patterns, but required finding homologs for each protein of interest and could not generalize beyond specific families. Protein language models changed the equation by compressing evolutionary knowledge into neural network parameters that transfer across the entire protein universe. A model trained to predict masked amino acids learns, as a byproduct, which residues contact each other in three-dimensional space, which positions tolerate variation, and which substitutions disrupt function. The physics of protein folding, selected across evolutionary time, emerges from the statistics of surviving sequences.</p>
<p>The <em>ESM</em> family demonstrated that transformers can learn protein structure and function from sequence alone, achieving results that rival methods requiring explicit structural supervision. Evolutionary Scale Modeling, as the name suggests, exploits the scale of evolutionary data to learn representations that generalize across proteins regardless of homology or family membership. Understanding these successes and their limitations provides essential context for genomic language models, where analogous approaches face distinct challenges arising from the multi-scale organization of regulatory information in DNA (see <a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>).</p>
<section id="sec-ch15-esm-family" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="sec-ch15-esm-family"><span class="header-section-number">15.1</span> ESM Model Family</h2>
<p>The <em>ESM</em> (Evolutionary Scale Modeling) family developed at Meta AI Research represents the most influential protein language model lineage, progressing from an initial proof-of-concept to models capable of predicting three-dimensional structure from sequence alone. The progression from <em>ESM-1b</em> through <em>ESM-2</em> illustrates how scaling transformer architectures yields systematic improvements in biological knowledge extraction, while revealing what self-supervised learning on protein sequences can and cannot achieve.</p>
<section id="sec-ch15-esm1b" class="level3" data-number="15.1.1">
<h3 data-number="15.1.1" class="anchored" data-anchor-id="sec-ch15-esm1b"><span class="header-section-number">15.1.1</span> ESM-1b: Establishing the Paradigm</h3>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading on, consider: if you wanted to teach a neural network about protein biology without providing any labels (no structure annotations, no function labels, no conservation scores), what training objective might you use? What information is implicitly present in the sequences themselves?</p>
</div>
</div>
<p>The Evolutionary Scale Modeling project demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision <span class="citation" data-cites="rives_esm_2021">(<a href="../bib/references.html#ref-rives_esm_2021" role="doc-biblioref">Rives et al. 2021</a>)</span>. The approach was strikingly simple: take the BERT architecture that had revolutionized natural language processing, replace words with amino acids, and train on protein sequence databases. The resulting models learned far more than anyone expected.</p>
<p><em>ESM-1b</em> was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families. The construction and characteristics of protein sequence databases are detailed in <a href="../part_1/p1-ch02-data.html#sec-ch02-protein-databases" class="quarto-xref"><span>Section 2.6</span></a>, with implications for training data curation in <a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a>. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy that would otherwise bias the model toward overrepresented families <span class="citation" data-cites="suzek_uniref_2007">(<a href="../bib/references.html#ref-suzek_uniref_2007" role="doc-biblioref">Suzek et al. 2007</a>)</span>. Why 50% specifically? This threshold represents a structural inflection point: proteins sharing &gt;50% sequence identity almost always share the same fold, while proteins below 30% identity may have completely different structures despite some sequence similarity. Clustering at 50% thus preserves one representative per structural family while collapsing near-identical orthologs that would otherwise dominate training. This curation strategy ensures the model encounters diverse evolutionary solutions to protein function rather than memorizing common motifs.</p>
<p>The architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is <strong>masked language modeling</strong>, the self-supervised strategy introduced in <a href="../part_2/p2-ch08-pretraining.html#sec-ch08-mlm" class="quarto-xref"><span>Section 8.1</span></a>: randomly mask 15% of amino acids in each sequence, and train the model to predict the masked positions given surrounding context. This objective contains no information about structure, function, or evolution beyond what is implicit in the sequences themselves.</p>
<p>Why does masked language modeling teach the model about protein biology? The key is that proteins are not random sequences—they are products of evolution, shaped by physical and functional constraints. To accurately predict a masked amino acid, the model must learn what makes a sequence “protein-like.” At a buried core position, hydrophobic amino acids are overwhelmingly favored because charged residues would destabilize the fold. At an active site, specific catalytic residues are nearly invariant because alternatives abolish function. At the interface between two domains, co-evolved residues must fit together geometrically. The model cannot distinguish these cases without learning the underlying principles. Predicting masked tokens thus becomes a proxy for understanding protein constraints.</p>
</section>
<section id="sec-ch15-emergent-knowledge" class="level3" data-number="15.1.2">
<h3 data-number="15.1.2" class="anchored" data-anchor-id="sec-ch15-emergent-knowledge"><span class="header-section-number">15.1.2</span> Emergent Biological Knowledge</h3>
<p>The surprise was not that <em>ESM-1b</em> learned to predict masked amino acids accurately, but what else it learned in the process. Despite never seeing structural or functional labels during training, <em>ESM-1b’s</em> internal representations encode information about protein biology at multiple levels of organization.</p>
<p>Secondary structure emerges in the attention patterns. When researchers analyzed which sequence positions the model attends to when making predictions, they found that attention concentrates along patterns corresponding to alpha helices and beta sheets. The model implicitly learns that certain amino acid sequences form specific structural elements, encoding this knowledge without ever being told what secondary structure is.</p>
<p>More remarkably, <em>ESM-1b</em> captures residue-residue contacts. Amino acids that are distant in the linear sequence but close in three-dimensional space attend to each other in the model’s attention matrices. This emergent capability suggests the model learns aspects of protein folding purely from sequence statistics. When attention weights were converted to contact predictions, they achieved accuracy approaching dedicated contact prediction methods that were explicitly trained for that task.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>The model was never told about protein structure, yet structure emerged. This is the central surprise of protein language models: the training objective (predict masked amino acids) is purely about sequence statistics, but the solution the model finds encodes structural relationships. Evolution has embedded so much physical information in sequence constraints that learning sequence patterns is, implicitly, learning structure.</p>
</div>
</div>
<p>The model’s masked token predictions correlate strongly with position-specific conservation scores derived from multiple sequence alignments. <em>ESM</em> effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns across 33 million sequences rather than from explicit conservation annotations. Positions where the model confidently predicts specific amino acids correspond to positions that are conserved across protein families.</p>
<p>Perhaps most striking, attention concentrates on functionally important positions. Catalytic residues, binding sites, and other sites of biological importance receive elevated attention even without explicit functional annotation in the training data. The model identifies that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites where nature has constrained variation because they perform essential functions.</p>
<div id="fig-plm-emergent" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plm-emergent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/01-A-fig-plm-emergent.svg" class="img-fluid figure-img"></p>
<figcaption>Attention captures residue contacts</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/01-B-fig-plm-emergent.svg" class="img-fluid figure-img"></p>
<figcaption>Secondary structure emerges in embedding space</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/01-C-fig-plm-emergent.svg" class="img-fluid figure-img"></p>
<figcaption>Attention concentrates on functional sites</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/01-D-fig-plm-emergent.svg" class="img-fluid figure-img"></p>
<figcaption>Evolutionary relationships encoded in embeddings</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plm-emergent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: Emergent properties of protein language models. (A) Specific attention heads capture residue-residue contacts with high correlation to experimental structures. (B) Secondary structure emerges as distinct clusters in embedding space. (C) Attention concentrates on functionally important positions including catalytic sites and binding pockets. (D) Evolutionary relationships are encoded, with proteins clustering by taxonomic origin. All properties emerge from masked language modeling on protein sequences without explicit structural or functional supervision, demonstrating that evolutionary sequence patterns encode rich biological information.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Without looking back, can you articulate what biological knowledge emerges from <em>ESM-1b</em> despite the model never receiving explicit labels for these properties? List at least three types of emergent knowledge.</p>
<details>
<summary>
Check your answer
</summary>
The model learns: (1) secondary structure patterns (alpha helices, beta sheets) visible in attention matrices, (2) residue-residue contacts between positions distant in sequence but close in 3D space, (3) evolutionary conservation patterns that identify constrained positions, and (4) functional site locations where catalytic residues and binding sites receive elevated attention.
</details>
</div>
</div>
</section>
<section id="sec-ch15-esm2" class="level3" data-number="15.1.3">
<h3 data-number="15.1.3" class="anchored" data-anchor-id="sec-ch15-esm2"><span class="header-section-number">15.1.3</span> ESM-2: Scaling Up</h3>
<p><em>ESM-2</em> extended the <em>ESM</em> approach across a range of model scales, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. The results confirmed a pattern familiar from natural language processing: bigger models learn more.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Predict Before You Look">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Predict Before You Look
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before viewing the table below, make a prediction: How do you expect model performance to scale as parameters increase from 8 million to 15 billion (a ~2000-fold increase)? Will performance gains be linear, accelerating, or diminishing? At what point would you expect performance to plateau?</p>
</div>
</div>
<table class="caption-top table">
<caption><em>ESM-2</em> model family spanning four orders of magnitude in parameter count, with architecture details and relative performance on structure-related tasks.</caption>
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
<th>Layers</th>
<th>Hidden Dim</th>
<th>Performance Gain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>ESM-2</em> (8M)</td>
<td>8M</td>
<td>6</td>
<td>320</td>
<td>Baseline</td>
</tr>
<tr class="even">
<td><em>ESM-2</em> (35M)</td>
<td>35M</td>
<td>12</td>
<td>480</td>
<td>Modest</td>
</tr>
<tr class="odd">
<td><em>ESM-2</em> (150M)</td>
<td>150M</td>
<td>30</td>
<td>640</td>
<td>Substantial</td>
</tr>
<tr class="even">
<td><em>ESM-2</em> (650M)</td>
<td>650M</td>
<td>33</td>
<td>1280</td>
<td>Large</td>
</tr>
<tr class="odd">
<td><em>ESM-2</em> (3B)</td>
<td>3B</td>
<td>36</td>
<td>2560</td>
<td>Near-optimal</td>
</tr>
<tr class="even">
<td><em>ESM-2</em> (15B)</td>
<td>15B</td>
<td>48</td>
<td>5120</td>
<td>State-of-the-art</td>
</tr>
</tbody>
</table>
<p>Performance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. The scaling relationship is not linear: doubling parameters does not double accuracy. But gains remain consistent through even the largest models, suggesting that the 15-billion parameter ceiling reflects computational constraints rather than fundamental limits on what sequence statistics can teach.</p>
<p>Why does performance continue improving with scale? The answer lies in the complexity of the patterns the model must capture. Small models can learn common motifs and obvious constraints—the GT dinucleotide at splice donors, the hydrophobic core of globular proteins. But subtle patterns require more capacity: the coordinated positions of an allosteric network, the epistatic relationships between distant residues, the statistical signatures of rare folds. Each increase in model size allows the model to represent more of these subtle dependencies. The consistent gains across very different tasks (structure, contacts, variant effects) suggest that larger models learn more general biological principles, not just task-specific shortcuts.</p>
<p>The scaling behavior mirrors observations in natural language processing, where larger models consistently capture more nuanced patterns. This predictable relationship between scale and capability provides a roadmap for model development: if more biological knowledge is needed, train a larger model on more data. The practical implications shaped how the field approached subsequent genomic foundation models, with the scaling law framework and its implications discussed in <a href="p3-ch13-fm-principles.html#sec-ch13-scaling" class="quarto-xref"><span>Section 13.3</span></a>.</p>
<div id="fig-esm2-scaling" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-esm2-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/02-A-fig-esm2-scaling.svg" class="img-fluid figure-img"></p>
<figcaption>Loss vs.&nbsp;model size</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/02-B-fig-esm2-scaling.svg" class="img-fluid figure-img"></p>
<figcaption>Downstream performance scales consistently</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/02-C-fig-esm2-scaling.svg" class="img-fluid figure-img"></p>
<figcaption>ESM-2 follows compute-optimal scaling</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-esm2-scaling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.2: Scaling laws for protein language models demonstrated by ESM-2. (A) Perplexity decreases as a power law of parameter count from 8M to 15B parameters. (B) Downstream task performance scales consistently across diverse applications including contact prediction, structure modeling, and variant effect scoring. (C) ESM-2 development followed near-optimal compute scaling, maximizing capability for given resources. These scaling relationships, mirroring observations in natural language models, suggest that continued scaling would yield further improvements in protein understanding.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch15-alternative-architectures" class="level2" data-number="15.2">
<h2 data-number="15.2" class="anchored" data-anchor-id="sec-ch15-alternative-architectures"><span class="header-section-number">15.2</span> Alternative Architectures</h2>
<p>The success of <em>ESM</em> raised a natural question: how much depends on the specific BERT architecture versus the general approach of self-supervised learning on protein sequences? The <em>ProtTrans</em> family explored this question by applying multiple transformer architectures to protein modeling <span class="citation" data-cites="elnaggar_prottrans_2021">(<a href="../bib/references.html#ref-elnaggar_prottrans_2021" role="doc-biblioref">Elnaggar et al. 2021</a>)</span>.</p>
<p><em>ProtBERT</em> applies the bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences. This training corpus, substantially larger than UniRef50, provides broader coverage at the cost of including more redundant and potentially lower-quality sequences. The architectural choices match <em>ESM</em> closely, enabling direct comparison of training data effects.</p>
<p><em>ProtT5</em> adapts the encoder-decoder architecture from <em>T5</em>, enabling both understanding and generation tasks <span class="citation" data-cites="raffel_t5_2019">(<a href="../bib/references.html#ref-raffel_t5_2019" role="doc-biblioref">Raffel et al. 2023</a>)</span>. The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture proved valuable for tasks requiring sequence generation, such as structure-conditioned design or sequence completion, though the encoder-only architecture remains dominant for <strong>embedding</strong> and classification tasks.</p>
<p><em>ProtXLNet</em> explores permutation language modeling, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training <span class="citation" data-cites="yang_xlnet_2020">(<a href="../bib/references.html#ref-yang_xlnet_2020" role="doc-biblioref">Yang et al. 2020</a>)</span>. By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies at the cost of more complex training.</p>
<table class="caption-top table">
<caption>Comparison of protein language model architectures. All learn meaningful representations when trained on sufficient data, but architectural differences affect downstream task suitability.</caption>
<colgroup>
<col style="width: 22%">
<col style="width: 11%">
<col style="width: 21%">
<col style="width: 18%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Model</th>
<th>Key Feature</th>
<th>Strengths</th>
<th>Best Use Cases</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Encoder-only (BERT)</td>
<td><em>ESM</em>, <em>ProtBERT</em></td>
<td>Bidirectional attention, MLM objective</td>
<td>Excellent embeddings, efficient inference</td>
<td>Classification, embedding extraction, variant scoring</td>
</tr>
<tr class="even">
<td>Encoder-decoder (T5)</td>
<td><em>ProtT5</em></td>
<td>Separate encoder/decoder, seq2seq capable</td>
<td>Generation tasks, flexible output</td>
<td>Sequence design, structure-conditioned generation</td>
</tr>
<tr class="odd">
<td>Autoregressive (XLNet)</td>
<td><em>ProtXLNet</em></td>
<td>Permutation language modeling</td>
<td>Richer dependencies, no [MASK] mismatch</td>
<td>Tasks sensitive to pretraining objective</td>
</tr>
</tbody>
</table>
<p>These architectural variants demonstrate that the protein language modeling paradigm generalizes beyond specific design choices. All architectures learn meaningful representations when trained on sufficient data, though performance differences emerge for specific downstream tasks. Encoder-only models excel at classification and embedding tasks where the entire sequence is available. Encoder-decoder models enable generation tasks where outputs must be produced token by token.</p>
</section>
<section id="sec-ch15-attention-coupling" class="level2" data-number="15.3">
<h2 data-number="15.3" class="anchored" data-anchor-id="sec-ch15-attention-coupling"><span class="header-section-number">15.3</span> Attention and Evolutionary Coupling</h2>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider two amino acid positions in a protein that must physically contact each other for the protein to fold correctly. If a mutation at position A disrupts the contact, what evolutionary pressure would exist at position B? How might this leave a signature in sequence databases that a language model could detect?</p>
</div>
</div>
<p>The emergence of contact information in <em>ESM’s</em> attention patterns connects to a deeper principle: <strong>evolutionary coupling</strong>. When two residues must maintain physical contact for a protein to function, mutations at one position create selective pressure for compensatory mutations at the other. Over evolutionary time, these correlated mutations leave statistical signatures in protein families that can be detected through covariance analysis of multiple sequence alignments.</p>
<p>Direct Coupling Analysis (DCA) and related methods extract these coevolutionary signals to predict residue-residue contacts <span class="citation" data-cites="morcos_dca_2011">(<a href="../bib/references.html#ref-morcos_dca_2011" role="doc-biblioref">Morcos et al. 2011</a>)</span>. The approach requires constructing multiple sequence alignments, computing covariance matrices, and applying statistical corrections to distinguish direct from indirect correlations. The resulting contact predictions enabled the first accurate structure predictions for proteins lacking homologs in structural databases.</p>
<p>Protein language models learn to extract similar information through a different route. Rather than computing covariance explicitly, transformers learn attention patterns that capture which positions inform predictions at other positions. When position <em>i</em> strongly attends to position <em>j</em> during masked prediction, the model has learned that knowing the amino acid at <em>j</em> helps predict the amino acid at <em>i</em>. This is precisely the signature of evolutionary coupling: positions that covary because they must maintain physical contact.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional coevolution analysis (DCA) requires computing alignments and covariance matrices for each protein family. PLMs learn to perform this analysis implicitly during training on millions of sequences, compressing the evolutionary logic into attention patterns that apply immediately to any new sequence. The attention mechanism rediscovers coevolution as the optimal strategy for predicting masked tokens.</p>
</div>
</div>
<p>The attention-based approach offers several advantages over traditional covariance analysis. Language models generalize across protein families, learning shared principles that transfer to proteins with sparse evolutionary sampling. They handle the statistical challenge of distinguishing direct from indirect correlations implicitly through deep architecture rather than requiring explicit correction. And they provide rich representations beyond binary contact predictions, encoding information about the strength and nature of residue relationships.</p>
<p>Rao and colleagues demonstrated this connection directly by extracting attention weights from <em>ESM</em> and converting them to contact predictions <span class="citation" data-cites="rao_transformer_2020">(<a href="../bib/references.html#ref-rao_transformer_2020" role="doc-biblioref">Rao et al. 2020</a>)</span>. The resulting predictions approached the accuracy of dedicated contact prediction methods, despite the model never being trained to predict contacts. The <strong>attention mechanism</strong>, optimized purely for masked token prediction, discovers the coevolutionary structure of protein sequences as a byproduct.</p>
</section>
<section id="sec-ch15-esmfold" class="level2" data-number="15.4">
<h2 data-number="15.4" class="anchored" data-anchor-id="sec-ch15-esmfold"><span class="header-section-number">15.4</span> ESMFold: Structure from Sequence</h2>
<p>Structure prediction has traditionally required multiple sequence alignments (MSAs) that search protein databases for evolutionary relatives, a process that can take hours per protein and fails entirely for sequences lacking detectable homologs. <em>ESMFold</em> demonstrated that the representations learned by <em>ESM-2</em> contain sufficient evolutionary information to predict three-dimensional structure directly, eliminating the alignment requirement while maintaining competitive accuracy.</p>
<section id="sec-ch15-alignment-free" class="level3" data-number="15.4.1">
<h3 data-number="15.4.1" class="anchored" data-anchor-id="sec-ch15-alignment-free"><span class="header-section-number">15.4.1</span> Alignment-Free Prediction</h3>
<p>The most dramatic demonstration of protein language model capabilities came with <em>ESMFold</em>, which predicts protein 3D structure directly from <em>ESM-2</em> embeddings without requiring multiple sequence alignments <span class="citation" data-cites="lin_esm-2_2022">(<a href="../bib/references.html#ref-lin_esm-2_2022" role="doc-biblioref">Lin et al. 2022</a>)</span>. Traditional structure prediction, including <em>AlphaFold2</em>, relies heavily on MSAs constructed through computationally expensive searches against sequence databases. These searches can take hours per protein, and prediction quality depends critically on finding informative homologs.</p>
<p><em>ESMFold</em> eliminates this requirement entirely. The architecture couples <em>ESM-2</em> (using the 15-billion parameter variant) with a structure module adapted from <em>AlphaFold2’s</em> Evoformer and structure module. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates. The model takes a single sequence as input and outputs predicted 3D coordinates for all atoms.</p>
<p>The computational speedup is substantial: approximately 60-fold faster than <em>AlphaFold2</em> for typical proteins. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive. Metagenomic proteins, often lacking close homologs in existing databases, represent exactly the cases where MSA-based methods struggle and where single-sequence predictions become essential.</p>
<p><em>ESMFold</em> achieves atomic-level accuracy for many proteins, though slightly below <em>AlphaFold2</em> for proteins that benefit strongly from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where explicit alignments provide information that single-sequence analysis cannot fully recover. For well-represented protein families, <em>ESMFold</em> approaches <em>AlphaFold2</em> accuracy at a fraction of the computational cost.</p>
<table class="caption-top table">
<caption>Comparison of structure prediction approaches. The choice depends on computational constraints and the evolutionary context of target proteins.</caption>
<colgroup>
<col style="width: 11%">
<col style="width: 9%">
<col style="width: 25%">
<col style="width: 29%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Input</th>
<th>Time per Protein</th>
<th>Best Accuracy Cases</th>
<th>Limitation Cases</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>AlphaFold2</em></td>
<td>Sequence + MSA</td>
<td>Hours</td>
<td>Deep MSA available</td>
<td>Orphan proteins</td>
</tr>
<tr class="even">
<td><em>ESMFold</em></td>
<td>Sequence only</td>
<td>Minutes</td>
<td>Well-represented families</td>
<td>Sparse evolutionary sampling</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-note callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before continuing, pause and reflect: ESMFold predicts structure from single sequences in minutes while AlphaFold2 requires hours of MSA construction. What information does ESM-2 encode that makes this possible? Where did that information come from, given that the model was only trained to predict masked amino acids?</p>
</div>
</div>
<div id="fig-esmfold" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-esmfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/03-A-fig-esmfold.svg" class="img-fluid figure-img"></p>
<figcaption>ESMFold eliminates MSA requirement</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/03-B-fig-esmfold.svg" class="img-fluid figure-img"></p>
<figcaption>Speed vs.&nbsp;accuracy trade-off</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/03-C-fig-esmfold.svg" class="img-fluid figure-img"></p>
<figcaption>ESMFold fails for specific protein categories</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/03-D-fig-esmfold.svg" class="img-fluid figure-img"></p>
<figcaption>ESMFold enables proteome-scale prediction</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-esmfold-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.3: ESMFold structure prediction from single sequences. (A) Architecture comparison: ESMFold uses pretrained ESM-2 representations from single sequences, eliminating AlphaFold2’s MSA requirement and achieving 60× speedup. (B) Speed-accuracy trade-off: ESMFold achieves ~85% of AlphaFold2’s accuracy at 50× the speed. (C) Failure modes: orphan proteins, novel folds, and dynamic proteins remain challenging without evolutionary context. (D) Scale application: the ESM Metagenomic Atlas predicted structures for 617 million proteins, demonstrating capabilities impossible with MSA-based methods.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch15-esmfold-implications" class="level3" data-number="15.4.2">
<h3 data-number="15.4.2" class="anchored" data-anchor-id="sec-ch15-esmfold-implications"><span class="header-section-number">15.4.2</span> What ESMFold Reveals About PLMs</h3>
<p><em>ESMFold’s</em> success demonstrates that <em>ESM-2’s</em> internal representations encode sufficient information to determine 3D structure. The language model has learned not merely local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular three-dimensional shape.</p>
<p>This has profound implications for understanding what protein language models learn. The attention patterns that emerge from masked prediction are, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space to maintain stability attend to each other in the transformer’s attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution under physical constraints, encode structural information that sufficiently powerful language models can decode.</p>
<p>The fundamental insight is that evolution has already solved the structure prediction problem, millions of times over, and recorded the solutions in sequence databases. Language models learn to read those solutions, extracting the implicit structural knowledge that selection has embedded in surviving sequences. The analogy to human language is illuminating: just as you learned grammar by reading millions of sentences without formal rules, PLMs learn protein “grammar” by reading millions of sequences without explicit physics lessons. The rules of protein folding emerge from pattern recognition, not from being taught what a hydrogen bond is.</p>
<div class="callout callout-style-default callout-note callout-titled" title="AlphaFold: Structure Prediction Without Foundation Models">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
AlphaFold: Structure Prediction Without Foundation Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>AlphaFold2’s</em> performance at CASP14 in 2020 solved a 50-year grand challenge, predicting protein structures with accuracy competitive with experimental determination <span class="citation" data-cites="jumper_alphafold2_2021">(<a href="../bib/references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. The achievement transformed structural biology and earned its creators the 2024 Nobel Prize in Chemistry. Yet <em>AlphaFold</em> is not a foundation model in the sense this book uses the term (see <a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>). Understanding why illuminates what makes PLM-based approaches distinctive.</p>
<p><em>AlphaFold</em> requires multiple sequence alignments as input. The Evoformer architecture processes MSA features alongside the query sequence, using attention mechanisms that operate over both the sequence dimension and the alignment dimension. Evolutionary information enters the model explicitly through database search rather than being learned implicitly from sequence data. This design choice has computational consequences: MSA construction can take hours per protein, and prediction quality depends critically on finding informative homologs. For orphan proteins lacking close relatives in sequence databases, <em>AlphaFold’s</em> accuracy degrades substantially.</p>
<p>The architectural innovations that enabled <em>AlphaFold’s</em> success differ fundamentally from the foundation model paradigm. Evoformer’s attention over MSA rows and columns, iterative recycling through the network, and the structure module’s SE(3)-equivariant operations represent expert-designed inductive biases encoding protein physics. These components were engineered specifically for structure prediction, not learned from self-supervised objectives on broad sequence data. The model excels at its designed task but does not produce general-purpose representations transferable to other problems.</p>
<p><em>ESMFold</em> inverts this design philosophy. Rather than requiring explicit evolutionary input, <em>ESMFold</em> couples <em>ESM-2</em> embeddings with a structure module adapted from <em>AlphaFold’s</em> architecture. The language model provides the evolutionary context that the structure module needs, context learned implicitly through masked token prediction on millions of protein sequences. A single sequence goes in; predicted coordinates come out. No MSA construction, no database search, no hours of preprocessing.</p>
<p>The comparison reveals what protein language models have and have not learned. <em>ESMFold</em> approaches <em>AlphaFold</em> accuracy for well-represented protein families where the language model’s training data provided dense evolutionary sampling. The gap widens for proteins where deep MSAs provide information that single-sequence analysis cannot fully recover. <em>ESMFold</em> runs approximately 60-fold faster than <em>AlphaFold</em>, enabling structure prediction at metagenomic scale for the millions of protein sequences emerging from environmental sequencing projects. The two approaches exhibit different failure modes: <em>AlphaFold</em> struggles with orphan proteins that lack homologs; <em>ESMFold</em> struggles with sequences the language model finds surprising (high perplexity), even when homologs exist.</p>
<p><em>AlphaFold3</em> complicates this dichotomy <span class="citation" data-cites="abramson_alphafold3_2024">(<a href="../bib/references.html#ref-abramson_alphafold3_2024" role="doc-biblioref">Abramson et al. 2024</a>)</span>. The updated architecture uses diffusion-based structure generation and handles protein-ligand, protein-nucleic acid, and multi-chain complexes within a unified framework. MSA dependency is reduced in some contexts, and the model moves toward general biomolecular structure prediction rather than single-chain protein folding. Whether this represents convergence between task-specific and foundation model approaches remains an open question.</p>
<p><em>AlphaFold</em> demonstrated that protein structure prediction was computationally tractable; <em>ESMFold</em> demonstrated that foundation models had learned enough biology to solve it differently. Both insights matter. For this book’s purposes, <em>ESMFold</em> illustrates the foundation model paradigm: self-supervised pretraining produces representations that transfer to downstream tasks, including tasks (like structure prediction) that were not part of the training objective. <em>AlphaFold’s</em> success through architectural engineering rather than learned representations represents an alternative path, one that achieved the goal first but may prove less generalizable as the field matures. The <em>AlphaMissense</em> model discussed in <a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a> repurposes <em>AlphaFold’s</em> structure module for variant effect prediction, suggesting that even task-specific architectures can seed broader applications when their components prove useful beyond their original context.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch15-function-prediction" class="level2" data-number="15.5">
<h2 data-number="15.5" class="anchored" data-anchor-id="sec-ch15-function-prediction"><span class="header-section-number">15.5</span> Function Prediction</h2>
<p>Beyond structure, protein language models enable prediction of protein function directly from sequence. Function prediction encompasses multiple tasks: predicting Gene Ontology terms that describe molecular function, biological process, and cellular component; classifying enzyme activity; identifying binding sites and interaction partners; and predicting subcellular localization.</p>
<p>Traditional function prediction relied on homology: proteins similar in sequence are assumed to share function. This approach fails for orphan proteins lacking characterized homologs and cannot distinguish functional differences between closely related sequences. PLM-based approaches address both limitations by learning representations that capture functional signatures beyond simple sequence similarity.</p>
<p>For Gene Ontology term prediction, PLM embeddings serve as input features to classification models that predict which GO terms apply to each protein. The embeddings capture evolutionary and structural information relevant to function, enabling accurate predictions even for proteins with limited homology to characterized sequences <span class="citation" data-cites="kulmanov_deepgo-se_2024">(<a href="../bib/references.html#ref-kulmanov_deepgo-se_2024" role="doc-biblioref"><strong>kulmanov_deepgo-se_2024?</strong></a>)</span>. Performance improves with embedding quality, suggesting that larger language models capture more functionally relevant information. These embeddings can also serve as node features in biological network analyses (<span class="quarto-unresolved-ref">?sec-ch18-fm-embeddings</span>), and the function predictions inform drug target identification workflows (<span class="quarto-unresolved-ref">?sec-ch27-variant-to-gene</span>).</p>
<p>Enzyme classification benefits similarly from PLM representations. The Enzyme Commission hierarchy categorizes enzymes by the reactions they catalyze, from broad classes (oxidoreductases, transferases) to specific substrate preferences. PLM embeddings distinguish these categories effectively, capturing the sequence features that determine catalytic activity without requiring explicit structural analysis <span class="citation" data-cites="sanderson_deepectransformer_2023">(<a href="../bib/references.html#ref-sanderson_deepectransformer_2023" role="doc-biblioref"><strong>sanderson_deepectransformer_2023?</strong></a>)</span>.</p>
<p>Binding site prediction applies attention analysis to identify which residues participate in ligand binding, protein-protein interactions, or nucleic acid recognition. Positions that the model identifies as important for contextual prediction often correspond to functionally important sites, including binding pockets and catalytic residues <span class="citation" data-cites="fang_deepprosite_2023">(<a href="../bib/references.html#ref-fang_deepprosite_2023" role="doc-biblioref"><strong>fang_deepprosite_2023?</strong></a>)</span>. This capability enables rapid identification of functional sites in newly sequenced proteins.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>A researcher has a novel protein sequence from an environmental sample with no detectable homologs in existing databases. Which PLM-based predictions would you expect to be most reliable, and which would be most uncertain? Consider: structure prediction, function prediction, and variant effect prediction.</p>
<details>
<summary>
Consider before revealing
</summary>
For orphan proteins, <strong>variant effect prediction</strong> may be most uncertain because the model has limited evolutionary context for what is “normal” at each position. <strong>Function prediction</strong> via GO terms will also struggle if the protein represents a truly novel functional category. <strong>Structure prediction</strong> via ESMFold may perform surprisingly well if the protein uses common structural motifs even without close homologs, though unusual folds will be challenging. The fundamental limitation is that all PLM capabilities ultimately derive from patterns learned across evolutionary data, making truly novel proteins inherently difficult.
</details>
</div>
</div>
</section>
<section id="sec-ch15-variant-effects" class="level2" data-number="15.6">
<h2 data-number="15.6" class="anchored" data-anchor-id="sec-ch15-variant-effects"><span class="header-section-number">15.6</span> Variant Effect Prediction</h2>
<p>A critical clinical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely pathogenic or benign. The traditional approach required either direct experimental characterization or computational methods trained on labeled pathogenicity data, both of which scale poorly to the millions of possible variants in each human genome (see <a href="../part_1/p1-ch04-vep-classical.html" class="quarto-xref"><span>Chapter 4</span></a> for discussion of classical approaches).</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Mathematical Detail">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Detail
</div>
</div>
<div class="callout-body-container callout-body">
<p>The following section uses log-likelihood ratios from the language model to score variants. If you are unfamiliar with log probabilities, the intuition is: the model assigns a probability to each possible amino acid at each position. If the model thinks the mutant amino acid is less likely than the wild-type, the variant receives a negative score (predicted deleterious). If the mutant is equally or more likely, the variant receives a zero or positive score (predicted benign).</p>
</div>
</div>
<p><em>ESM-1v</em> demonstrated that PLMs can predict variant effects without any training on variant labels <span class="citation" data-cites="meier_esm-1v_2021">(<a href="../bib/references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>)</span>. The approach exploits the masked language modeling objective directly: for a variant at position <span class="math inline">\(i\)</span> changing amino acid <span class="math inline">\(a\)</span> to amino acid <span class="math inline">\(b\)</span>, compute the log-likelihood ratio:</p>
<p><span class="math display">\[
\Delta \text{score} = \log P(b \mid \text{context}) - \log P(a \mid \text{context})
\]</span></p>
<p>If the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This <strong>zero-shot</strong> prediction requires no labeled training data. The model’s evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.</p>
<p>The intuition is straightforward. Evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids. Substitutions that violate these preferences are more likely to disrupt function. The language model captures these preferences through training on millions of evolutionarily successful sequences. Variants that the model finds surprising are more likely to be functionally disruptive.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a protein position where the wild-type amino acid is glycine (small, flexible) and a variant introduces tryptophan (large, bulky). Without knowing anything about the protein’s function, why might a protein language model assign low probability to this substitution? What evolutionary patterns would the model have learned during training that make this substitution “surprising”?</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>Zero-shot variant prediction inverts the typical ML workflow. Instead of training on labeled variant data (benign vs.&nbsp;pathogenic), the model learns what “normal” looks like from evolutionary data. Abnormal variants are detected as deviations from learned expectations, requiring no pathogenicity labels whatsoever. This explains why PLM scores generalize across proteins: they measure violation of evolutionary constraint rather than matching patterns of known pathogenic variants.</p>
</div>
</div>
<div id="fig-plm-variant-scoring" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plm-variant-scoring-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/04-A-fig-plm-variant-scoring.svg" class="img-fluid figure-img"></p>
<figcaption>Log-likelihood ratio scoring mechanism</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/04-B-fig-plm-variant-scoring.svg" class="img-fluid figure-img"></p>
<figcaption>Evolutionary intuition behind zero-shot scoring</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/04-C-fig-plm-variant-scoring.svg" class="img-fluid figure-img"></p>
<figcaption>Deep mutational scanning correlation</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/04-D-fig-plm-variant-scoring.svg" class="img-fluid figure-img"></p>
<figcaption>ClinVar pathogenicity discrimination</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plm-variant-scoring-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.4: Zero-shot variant effect prediction from protein language models. (A) The scoring mechanism computes log-likelihood ratio between variant and reference amino acids given sequence context. More negative scores indicate variants the model finds surprising—evolutionarily disfavored substitutions. (B) The evolutionary intuition: billions of years of natural selection have tested trillions of mutations. Low-probability variants violate learned evolutionary constraints, suggesting functional disruption. (C) Correlation with deep mutational scanning experiments validates that PLM scores capture functional effects. (D) Discrimination of ClinVar pathogenic vs.&nbsp;benign variants approaches methods trained with supervised labels, demonstrating that evolutionary constraint encodes pathogenicity information.
</figcaption>
</figure>
</div>
<p>Brandes and colleagues applied <em>ESM-1b</em> to predict effects for all approximately 450 million possible missense variants in the human genome, providing a precomputed resource for clinical variant interpretation <span class="citation" data-cites="brandes_genome-wide_2023">(<a href="../bib/references.html#ref-brandes_genome-wide_2023" role="doc-biblioref">Brandes et al. 2023</a>)</span>. On ClinVar benchmarks, <em>ESM-1b</em> outperformed existing methods in classifying variants as pathogenic or benign.</p>
<p><em>AlphaMissense</em> extended this approach by combining PLM representations with structural context from predicted protein structures <span class="citation" data-cites="cheng_alphamissense_2023">(<a href="../bib/references.html#ref-cheng_alphamissense_2023" role="doc-biblioref">Cheng et al. 2023</a>)</span>. The integration of sequence-based and structure-based signals improves accuracy, particularly for variants affecting protein stability or buried residues. <em>AlphaMissense</em> provides predictions for all approximately 71 million possible single amino acid substitutions in the human proteome.</p>
<p>The detailed comparison of variant effect prediction methods, including how PLM-based scores integrate with clinical classification frameworks, is covered in <a href="p3-ch17-vep-fm.html#sec-ch17-alphamissense" class="quarto-xref"><span>Section 17.2.3</span></a>. The calibration of these scores to ACMG criteria appears in <span class="quarto-unresolved-ref">?sec-ch14-acmg-mapping</span>, and integration into rare disease diagnostic workflows in <span class="quarto-unresolved-ref">?sec-ch26-fm-scoring</span>. Here, the key point is that protein language models provide the foundational representations that make accurate zero-shot variant prediction possible.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Practical Guidance: Choosing a PLM for Variant Effect Prediction">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Choosing a PLM for Variant Effect Prediction
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>For quick screening:</strong> Use precomputed scores from <em>ESM-1b</em> (450M variants) or <em>AlphaMissense</em> (71M proteome-wide). Available through databases and APIs.</li>
<li><strong>For novel proteins:</strong> Run <em>ESM-1v</em> or <em>ESM-2</em> inference directly. Larger models (650M+) generally perform better.</li>
<li><strong>For structural context:</strong> Consider <em>AlphaMissense</em> when protein structure is relevant to the variant mechanism (buried residues, stability effects).</li>
<li><strong>For integration with clinical pipelines:</strong> PLM scores are one line of evidence among many. See <a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a> for integration with ACMG criteria and other computational evidence.</li>
<li><strong>Limitations to remember:</strong> Zero-shot scores reflect evolutionary constraint, not direct pathogenicity. A constrained position is likely functionally important, but not all variants at important positions cause disease.</li>
</ul>
</div>
</div>
</section>
<section id="sec-ch15-structure-integration" class="level2" data-number="15.7">
<h2 data-number="15.7" class="anchored" data-anchor-id="sec-ch15-structure-integration"><span class="header-section-number">15.7</span> Integration with Structure Prediction</h2>
<p>Protein language models exist within a broader ecosystem of computational methods for protein analysis. Understanding how PLMs relate to structure prediction systems clarifies their role and capabilities.</p>
<p><em>AlphaFold2</em> achieved breakthrough accuracy in structure prediction by combining learned representations with explicit geometric modeling <span class="citation" data-cites="jumper_alphafold2_2021">(<a href="../bib/references.html#ref-jumper_alphafold2_2021" role="doc-biblioref">Jumper et al. 2021</a>)</span>. The architecture processes both sequence information through embeddings and evolutionary information through multiple sequence alignments, using an attention-based module (Evoformer) to integrate these signals before predicting atomic coordinates. <em>AlphaFold2’s</em> success depended critically on MSA quality: proteins with many homologs could be predicted accurately, while orphan proteins remained challenging.</p>
<p><em>ESMFold</em> demonstrated that PLM embeddings can replace MSA-derived features, achieving competitive accuracy without the alignment bottleneck. This finding clarified the relationship between language models and structure prediction: PLMs learn to compress evolutionary information into representations that are functionally equivalent to explicit alignments, at least for proteins with sufficient representation in training databases.</p>
<p><em>AlphaFold3</em> extended structure prediction to protein complexes, nucleic acids, and small molecules <span class="citation" data-cites="abramson_alphafold3_2024">(<a href="../bib/references.html#ref-abramson_alphafold3_2024" role="doc-biblioref">Abramson et al. 2024</a>)</span>. The architecture incorporates diffusion-based generation, enabling prediction of binding poses and complex assemblies. These capabilities complement PLM-based function prediction by providing structural context for interpreting functional predictions.</p>
<p>Generative protein design methods including <em>RFDiffusion</em> and <em>ProteinMPNN</em> leverage both structural and sequence information <span class="citation" data-cites="watson_rfdiffusion_2023 dauparas_proteinmpnn_2022">(<a href="../bib/references.html#ref-watson_rfdiffusion_2023" role="doc-biblioref">Watson et al. 2023</a>; <a href="../bib/references.html#ref-dauparas_proteinmpnn_2022" role="doc-biblioref">Dauparas et al. 2022</a>)</span>. <em>RFDiffusion</em> generates novel protein backbones through diffusion processes conditioned on design objectives. <em>ProteinMPNN</em> designs sequences likely to fold into specified structures. Both methods benefit from PLM representations when designing sequences with desired functional properties, demonstrating how language models integrate into the broader protein engineering pipeline (see <a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a> for detailed treatment of sequence design methods).</p>
<p>The trajectory from <em>ESM</em> to <em>ESMFold</em> to integration with design tools illustrates how PLMs serve as a foundation for diverse downstream applications. The representations learned through self-supervised training transfer across tasks, providing a common language for structure prediction, function annotation, variant interpretation, and protein engineering. This pattern of foundation models enabling diverse applications recurs throughout genomic AI, as discussed in <a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
</section>
<section id="sec-ch15-limitations" class="level2" data-number="15.8">
<h2 data-number="15.8" class="anchored" data-anchor-id="sec-ch15-limitations"><span class="header-section-number">15.8</span> Limitations</h2>
<p>Despite their success, protein language models face several limitations that inform the development of genomic models and guide appropriate application.</p>
<div id="fig-plm-limitations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-plm-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch15/05-fig-plm-limitations.svg" class="img-fluid figure-img"></p>
<figcaption>Limitations and failure modes of protein language models</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-plm-limitations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.5: Fundamental limitations of protein language models. Orphan proteins lacking evolutionary context produce unreliable predictions. Novel folds designed computationally lie outside the training distribution. Intrinsically disordered proteins and dynamic conformations cannot be captured in single embeddings. Epistatic interactions between mutations are not modeled by independent position scoring. Post-translational modifications that alter function are invisible to sequence-only models. These limitations guide appropriate application: PLMs excel for well-characterized protein families but require caution for edge cases where evolutionary learning provides insufficient guidance.
</figcaption>
</figure>
</div>
<section id="sec-ch15-orphan-proteins" class="level3" data-number="15.8.1">
<h3 data-number="15.8.1" class="anchored" data-anchor-id="sec-ch15-orphan-proteins"><span class="header-section-number">15.8.1</span> Orphan and Dark Proteins</h3>
<p>PLMs learn from evolutionary statistics, performing best for proteins with rich representation in training databases. Orphan proteins, those unique to specific lineages without detectable homologs, lack the evolutionary context that PLMs exploit. For these proteins, the model has no basis for distinguishing likely from unlikely amino acids at each position, and predictions degrade accordingly.</p>
<p>The problem extends to “dark” proteins that are poorly characterized despite having homologs. If an entire protein family has escaped experimental characterization, PLMs may learn statistical patterns without capturing functional relevance. The model cannot distinguish constraint imposed by function from constraint imposed by historical accident.</p>
</section>
<section id="sec-ch15-novel-folds" class="level3" data-number="15.8.2">
<h3 data-number="15.8.2" class="anchored" data-anchor-id="sec-ch15-novel-folds"><span class="header-section-number">15.8.2</span> Novel Folds</h3>
<p>Training data shapes what models can predict. PLMs trained on natural protein databases learn the statistical patterns of naturally occurring folds, potentially struggling with designed proteins or hypothetical folds outside the training distribution. When researchers design proteins with novel topologies not found in nature, PLM predictions become less reliable because the relevant sequence patterns were never encountered during training <span class="citation" data-cites="verkuil_language_2022">(<a href="../bib/references.html#ref-verkuil_language_2022" role="doc-biblioref"><strong>verkuil_language_2022?</strong></a>)</span>.</p>
</section>
<section id="sec-ch15-conformational-flexibility" class="level3" data-number="15.8.3">
<h3 data-number="15.8.3" class="anchored" data-anchor-id="sec-ch15-conformational-flexibility"><span class="header-section-number">15.8.3</span> Conformational Flexibility</h3>
<p>Most PLM representations assume a single static structure, but many proteins adopt multiple conformations relevant to function. Allosteric proteins, intrinsically disordered regions, and proteins that undergo conformational changes upon binding present challenges for methods that embed each sequence into a single representation. The language model learns the average properties of sequences but may not capture the dynamic range that determines biological behavior.</p>
</section>
<section id="sec-ch15-epistasis" class="level3" data-number="15.8.4">
<h3 data-number="15.8.4" class="anchored" data-anchor-id="sec-ch15-epistasis"><span class="header-section-number">15.8.4</span> Epistasis</h3>
<p>Most variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit <strong>epistasis</strong>, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors model marginal effects at each position but do not explicitly capture higher-order interactions, though the contextual embeddings may represent some epistatic relationships implicitly.</p>
<p>Why is epistasis so difficult to model? The challenge is combinatorial: with 20 amino acids at each position, predicting all pairwise interactions for a 300-residue protein requires assessing <span class="math inline">\(20^2 \times \binom{300}{2} \approx 18\)</span> million combinations. Higher-order interactions grow exponentially worse. PLMs sidestep this by learning correlations from data rather than enumerating possibilities, but the training data rarely includes systematic measurements of multi-mutant effects. Deep mutational scanning typically measures single-mutant effects; double and higher-order mutants are exponentially rarer in experimental datasets. The model thus learns what it can observe, and complex epistatic interactions remain largely unobserved.</p>
</section>
<section id="sec-ch15-interpretability" class="level3" data-number="15.8.5">
<h3 data-number="15.8.5" class="anchored" data-anchor-id="sec-ch15-interpretability"><span class="header-section-number">15.8.5</span> Interpretability</h3>
<p>While attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods, including attention pattern analysis (<a href="../part_5/p5-ch24-interpretability.html#sec-ch24-attention" class="quarto-xref"><span>Section 24.5</span></a>) and probing studies (<a href="../part_5/p5-ch24-interpretability.html#sec-ch24-probing" class="quarto-xref"><span>Section 24.4</span></a>), but PLMs remain partially opaque. The distinction between plausible and faithful explanations, critical for clinical applications, is examined in <a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 24</span></a>. For clinical applications where explanations matter, this interpretability gap limits adoption. A prediction that a variant is pathogenic is more useful when accompanied by mechanistic insight into why the variant disrupts function.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Stop and Think">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider applying a PLM to predict variant effects in two scenarios: (1) a well-characterized human enzyme with thousands of known homologs, and (2) a viral protein from a newly emerged pathogen with only a few known relatives. Which PLM limitations are most relevant in each case? How might you adjust your interpretation of the predictions accordingly?</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before moving forward, test your understanding of PLM limitations:</p>
<ol type="1">
<li>Why do orphan proteins pose challenges for protein language models?</li>
<li>If you designed a completely novel protein fold not found in nature, why might ESMFold struggle to predict its structure accurately?</li>
<li>How does the independence assumption in variant effect prediction fail to capture epistatic interactions?</li>
</ol>
<details>
<summary>
Check your understanding
</summary>
<ol type="1">
<li><p><strong>Orphan proteins</strong> lack evolutionary context. PLMs learn from statistical patterns across protein families—they identify which amino acids are likely at each position based on what evolution has “tried” and preserved. For proteins unique to specific lineages without homologs, the model has no evolutionary data to learn from, making predictions unreliable.</p></li>
<li><p><strong>Novel folds</strong> lie outside the training distribution. The model learned sequence-structure relationships from natural proteins in its training data. A computationally designed fold with topology not found in nature represents patterns the model never encountered during training, reducing prediction reliability.</p></li>
<li><p><strong>Epistatic interactions</strong> occur when variant effects depend on sequence context. The standard scoring approach treats each position independently, computing the effect of mutation A without considering whether mutation B is present. But two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. The combinatorial explosion of pairwise and higher-order interactions makes explicit modeling intractable, and training data rarely includes systematic multi-mutant measurements.</p></li>
</ol>
</details>
</div>
</div>
</section>
</section>
<section id="sec-ch15-lessons" class="level2" data-number="15.9">
<h2 data-number="15.9" class="anchored" data-anchor-id="sec-ch15-lessons"><span class="header-section-number">15.9</span> Lessons for Genomic Foundation Models</h2>
<p>The success of protein language models established principles that guided subsequent development of genomic foundation models. These lessons transfer with appropriate modifications to DNA and RNA modeling.</p>
<section id="sec-ch15-self-supervised" class="level3" data-number="15.9.1">
<h3 data-number="15.9.1" class="anchored" data-anchor-id="sec-ch15-self-supervised"><span class="header-section-number">15.9.1</span> Self-Supervised Biological Knowledge</h3>
<p>PLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can exploit. This principle underlies the entire foundation model paradigm: sufficiently large models trained on sufficiently large datasets with appropriate objectives will learn representations that capture biological function.</p>
</section>
<section id="sec-ch15-scaling" class="level3" data-number="15.9.2">
<h3 data-number="15.9.2" class="anchored" data-anchor-id="sec-ch15-scaling"><span class="header-section-number">15.9.2</span> Scaling Benefits</h3>
<p>Performance improves predictably with model size through the range currently explored. The progression from 8 million to 15 billion parameters in <em>ESM-2</em> showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While scaling cannot continue indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This relationship justified the computational investment in large genomic foundation models (see <a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a> for discussion of scaling laws in genomic contexts).</p>
</section>
<section id="sec-ch15-transfer" class="level3" data-number="15.9.3">
<h3 data-number="15.9.3" class="anchored" data-anchor-id="sec-ch15-transfer"><span class="header-section-number">15.9.3</span> Effective Transfer Learning</h3>
<p>Representations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects, function annotation). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids simultaneously learns about protein structure, function, evolutionary constraint, and disease relevance. The same principle motivates genomic language models: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects. Transfer learning strategies, including fine-tuning approaches and parameter-efficient adaptation, are discussed in detail in <a href="../part_2/p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>, with specific guidance on choosing between these strategies in <span class="quarto-unresolved-ref">?sec-ch09-choosing-strategy</span>.</p>
</section>
<section id="sec-ch15-architecture-matching" class="level3" data-number="15.9.4">
<h3 data-number="15.9.4" class="anchored" data-anchor-id="sec-ch15-architecture-matching"><span class="header-section-number">15.9.4</span> Architecture-Sequence Matching</h3>
<p>The BERT-style bidirectional encoder proved effective for proteins, where entire sequences are typically available and lengths rarely exceed a thousand residues. Genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with coding regions being dense while intergenic regions are sparser, and structural features including reverse-complement relationships absent in proteins. These differences motivate architectural adaptations in genomic language models, as explored in <a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>.</p>
</section>
<section id="sec-ch15-integration" class="level3" data-number="15.9.5">
<h3 data-number="15.9.5" class="anchored" data-anchor-id="sec-ch15-integration"><span class="header-section-number">15.9.5</span> Integration Benefits</h3>
<p><em>AlphaMissense</em> demonstrated that PLM embeddings combine effectively with structural and population genetics information, achieving accuracy beyond what any single information source provides. The most powerful methods integrate multiple signals, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace functional annotations, chromatin data, and clinical information (see <a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a> for variant effect prediction integration strategies).</p>
</section>
</section>
<section id="sec-ch15-conclusion" class="level2" data-number="15.10">
<h2 data-number="15.10" class="anchored" data-anchor-id="sec-ch15-conclusion"><span class="header-section-number">15.10</span> Paradigm That Generalized</h2>
<p>Protein language models established that transformer architectures can learn deep biological knowledge from sequence alone. <em>ESM’s</em> ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. The framework validated a paradigm: treat biological sequences as language, train large models to predict masked tokens, and extract functional knowledge from learned representations. Attention patterns in these models capture evolutionary constraint, contact prediction, and structural relationships without requiring multiple sequence alignments or explicit structural supervision.</p>
<p>This success directly motivated genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too. The DNA language models examined in <a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a> adapt protein language model architectures and training strategies to the distinct challenges of genomic sequences: longer contexts, different alphabets, ambiguous tokenization, and the full complexity of gene regulation beyond protein coding. RNA language models occupy an intermediate position, sharing features with both protein and DNA modeling while addressing the unique challenges of RNA structure and processing.</p>
<p>The integration path extends beyond sequence modeling. Just as protein language model representations feed into structure prediction (<em>ESMFold</em>) and variant effect prediction (<em>AlphaMissense</em>), genomic language model embeddings integrate into regulatory models (<a href="p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>) and clinical applications (<a href="../part_6/p6-ch28-rare-disease.html" class="quarto-xref"><span>Chapter 28</span></a>, <a href="../part_6/p6-ch27-clinical-risk.html" class="quarto-xref"><span>Chapter 27</span></a>). Protein design methods (<a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a>) demonstrate how generative modeling builds on the representations that language models provide. Throughout this progression, the principle that <em>ESM</em> established remains: self-supervised learning on biological sequences captures knowledge that transfers across diverse applications.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Chapter Summary">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<div class="callout callout-style-default callout-tip callout-titled" title="Test Yourself">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reviewing the summary, test your recall:</p>
<ol type="1">
<li>How does masked language modeling on protein sequences lead to emergent knowledge about protein structure and function? What specific types of biological information emerge without explicit supervision?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-18-contents" aria-controls="callout-18" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-18" class="callout-18-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Masked language modeling requires the model to predict hidden amino acids from surrounding context. To succeed, the model must learn what makes sequences “protein-like”—the constraints imposed by evolution. <strong>Emergent knowledge</strong> includes: (1) <strong>secondary structure</strong> (alpha helices, beta sheets visible in attention patterns), (2) <strong>residue-residue contacts</strong> (spatially close amino acids attend to each other), (3) <strong>evolutionary conservation</strong> (constrained positions receive confident predictions), and (4) <strong>functional sites</strong> (catalytic residues, binding sites receive elevated attention). These properties emerge because evolutionary and physical constraints shape which amino acid combinations survive, embedding structural and functional information in sequence statistics.</p>
</div>
</div>
</div>
<ol start="2" type="1">
<li>Explain the connection between attention patterns in protein language models and evolutionary coupling. Why do attention weights correlate with residue-residue contacts in 3D structure?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-19-contents" aria-controls="callout-19" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-19" class="callout-19-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Evolutionary coupling</strong> occurs when two residues must maintain physical contact for protein function. If mutation at position A disrupts the contact, selection favors compensatory mutations at position B. This creates <strong>correlated mutations</strong> across evolutionary time. Protein language models learn that knowing the amino acid at position B helps predict the amino acid at position A—precisely the signature of evolutionary coupling. The model learns to attend to positions that provide predictive information, and these often correspond to spatially proximate residues that coevolve. The attention mechanism thus rediscovers coevolution as the optimal strategy for masked token prediction.</p>
</div>
</div>
</div>
<ol start="3" type="1">
<li>How does ESMFold predict protein structure without multiple sequence alignments? What enables this, and what are the performance tradeoffs compared to AlphaFold2?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>ESMFold uses <strong>ESM-2 embeddings</strong> as input to a structure prediction module adapted from AlphaFold2’s architecture. The language model embeddings, learned from masked token prediction on millions of sequences, encode evolutionary information that replaces explicit MSA-derived features. This works because the model has compressed evolutionary patterns into its parameters during pretraining. <strong>Tradeoffs</strong>: ESMFold is ~60× faster (minutes vs.&nbsp;hours) and handles orphan proteins better, but achieves slightly lower accuracy for proteins where deep MSAs provide information that single-sequence analysis cannot fully recover. The gap is largest for proteins with sparse evolutionary sampling.</p>
</div>
</div>
</div>
<ol start="4" type="1">
<li>Describe how protein language models perform zero-shot variant effect prediction using log-likelihood ratios. Why does this approach work for distinguishing pathogenic from benign variants?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Zero-shot scoring</strong> computes: Δscore = log P(mutant | context) - log P(wild-type | context). If the model assigns lower probability to the mutant amino acid than the wild-type, the variant is predicted deleterious. This works because the model learned evolutionary preferences from millions of successful sequences. Evolution eliminates functionally disruptive variants, leaving statistical signatures that the model captures. Variants the model finds “surprising” (low probability) often violate evolutionary constraints, indicating likely functional disruption. The approach requires <strong>no pathogenicity labels</strong>—it measures deviation from learned evolutionary expectations rather than matching patterns of known pathogenic variants.</p>
</div>
</div>
</div>
<ol start="5" type="1">
<li>What are three fundamental limitations of protein language models? For each limitation, explain what type of information the model lacks and why self-supervised sequence training cannot provide it.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>(1) Orphan proteins:</strong> Proteins without homologs lack evolutionary context. PLMs learn from statistical patterns across families, but orphan proteins provide no family data to learn from. <strong>(2) Novel folds:</strong> Designed proteins with topologies not found in nature lie outside the training distribution. The model learned sequence-structure relationships from natural proteins; completely novel folds represent patterns never encountered. <strong>(3) Epistatic interactions:</strong> Standard scoring assumes variant effects are independent, but real proteins show epistasis where effects depend on sequence context. The combinatorial explosion makes explicit modeling intractable, and training data rarely includes systematic multi-mutant measurements to learn higher-order interactions.</p>
</div>
</div>
</div>
</div>
</div>
<p><strong>Key Concepts Covered:</strong></p>
<ul>
<li><strong>ESM model family:</strong> Transformer-based protein language models trained on masked language modeling, from ESM-1b (650M parameters) through ESM-2 (up to 15B)</li>
<li><strong>Emergent knowledge:</strong> Structure, contacts, conservation, and functional sites emerge from sequence training without explicit labels</li>
<li><strong>Evolutionary coupling:</strong> Attention patterns learn coevolutionary relationships between residues that must maintain physical contact</li>
<li><strong>ESMFold:</strong> Single-sequence structure prediction using PLM embeddings, 60x faster than AlphaFold2</li>
<li><strong>Zero-shot variant prediction:</strong> Log-likelihood ratios from PLMs score variants without pathogenicity training data</li>
<li><strong>Limitations:</strong> Orphan proteins, novel folds, conformational flexibility, epistasis, and interpretability</li>
</ul>
<p><strong>Main Takeaways:</strong></p>
<ol type="1">
<li><p>Evolution has encoded protein structure and function in sequence statistics. PLMs learn to decode this information through self-supervised training.</p></li>
<li><p>The training objective (predict masked amino acids) is simple, but what the model learns to achieve this objective is profound: structural relationships, evolutionary constraints, and functional importance.</p></li>
<li><p>PLM capabilities scale predictably with model size, providing a roadmap for development.</p></li>
<li><p>Zero-shot variant prediction works because the model has learned what “normal” looks like from evolutionary data. Abnormal variants are detected as deviations from expectations.</p></li>
<li><p>PLMs face fundamental limitations for sequences outside evolutionary training distributions (orphan proteins, novel folds) and struggle with dynamic properties (conformational flexibility, epistasis).</p></li>
</ol>
<p><strong>Connections to Other Chapters:</strong></p>
<ul>
<li>DNA language models (<a href="p3-ch14-dna-lm.html" class="quarto-xref"><span>Chapter 14</span></a>) adapt this paradigm to genomic sequences</li>
<li>Variant effect prediction methods (<a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a>) integrate PLM scores with other evidence</li>
<li>Protein design (<a href="../part_6/p6-ch30-design.html" class="quarto-xref"><span>Chapter 30</span></a>) builds on PLM representations for generative tasks</li>
<li>Interpretability methods (<a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 24</span></a>) probe what PLMs have learned</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-abramson_alphafold3_2024" class="csl-entry" role="listitem">
Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, et al. 2024. <span>“[<span>AlphaFold3</span>] <span>Accurate</span> Structure Prediction of Biomolecular Interactions with <span>AlphaFold</span> 3.”</span> <em>Nature</em> 630 (8016): 493–500. <a href="https://doi.org/10.1038/s41586-024-07487-w">https://doi.org/10.1038/s41586-024-07487-w</a>.
</div>
<div id="ref-brandes_genome-wide_2023" class="csl-entry" role="listitem">
Brandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and Vasilis Ntranos. 2023. <span>“Genome-Wide Prediction of Disease Variant Effects with a Deep Protein Language Model.”</span> <em>Nature Genetics</em> 55 (9): 1512–22. <a href="https://doi.org/10.1038/s41588-023-01465-0">https://doi.org/10.1038/s41588-023-01465-0</a>.
</div>
<div id="ref-cheng_alphamissense_2023" class="csl-entry" role="listitem">
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė, Taylor Applebaum, Alexander Pritzel, et al. 2023. <span>“[<span>AlphaMissense</span>] <span>Accurate</span> Proteome-Wide Missense Variant Effect Prediction with <span>AlphaMissense</span>.”</span> <em>Science</em> 381 (6664): eadg7492. <a href="https://doi.org/10.1126/science.adg7492">https://doi.org/10.1126/science.adg7492</a>.
</div>
<div id="ref-dauparas_proteinmpnn_2022" class="csl-entry" role="listitem">
Dauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, et al. 2022. <span>“Robust Deep Learning–Based Protein Sequence Design Using <span>ProteinMPNN</span>.”</span> <em>Science</em> 378 (6615): 49–56. <a href="https://doi.org/10.1126/science.add2187">https://doi.org/10.1126/science.add2187</a>.
</div>
<div id="ref-elnaggar_prottrans_2021" class="csl-entry" role="listitem">
Elnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. <span>“<span>ProtTrans</span>: <span>Towards</span> <span>Cracking</span> the <span>Language</span> of <span>Life</span>’s <span>Code</span> <span>Through</span> <span>Self</span>-<span>Supervised</span> <span>Deep</span> <span>Learning</span> and <span>High</span> <span>Performance</span> <span>Computing</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2007.06225">https://doi.org/10.48550/arXiv.2007.06225</a>.
</div>
<div id="ref-jumper_alphafold2_2021" class="csl-entry" role="listitem">
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. <span>“[<span>AlphaFold2</span>] <span>Highly</span> Accurate Protein Structure Prediction with <span>AlphaFold</span>.”</span> <em>Nature</em> 596 (7873): 583–89. <a href="https://doi.org/10.1038/s41586-021-03819-2">https://doi.org/10.1038/s41586-021-03819-2</a>.
</div>
<div id="ref-lin_esm-2_2022" class="csl-entry" role="listitem">
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, et al. 2022. <span>“[<span>ESM</span>-2] <span>Language</span> Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.07.20.500902">https://doi.org/10.1101/2022.07.20.500902</a>.
</div>
<div id="ref-meier_esm-1v_2021" class="csl-entry" role="listitem">
Meier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. <span>“[<span>ESM</span>-1v] <span>Language</span> Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.”</span> bioRxiv. <a href="https://doi.org/10.1101/2021.07.09.450648">https://doi.org/10.1101/2021.07.09.450648</a>.
</div>
<div id="ref-morcos_dca_2011" class="csl-entry" role="listitem">
Morcos, Faruck, Andrea Pagnani, Bryan Lunt, Arianna Bertolino, Debora S. Marks, Chris Sander, Riccardo Zecchina, José N. Onuchic, Terence Hwa, and Martin Weigt. 2011. <span>“Direct-Coupling Analysis of Residue Coevolution Captures Native Contacts Across Many Protein Families.”</span> <em>Proceedings of the National Academy of Sciences</em> 108 (49): E1293–1301. <a href="https://doi.org/10.1073/pnas.1111471108">https://doi.org/10.1073/pnas.1111471108</a>.
</div>
<div id="ref-raffel_t5_2019" class="csl-entry" role="listitem">
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. <span>“Exploring the <span>Limits</span> of <span>Transfer</span> <span>Learning</span> with a <span>Unified</span> <span>Text</span>-to-<span>Text</span> <span>Transformer</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1910.10683">https://doi.org/10.48550/arXiv.1910.10683</a>.
</div>
<div id="ref-rao_transformer_2020" class="csl-entry" role="listitem">
Rao, Roshan, Joshua Meier, Tom Sercu, Sergey Ovchinnikov, and Alexander Rives. 2020. <span>“Transformer Protein Language Models Are Unsupervised Structure Learners.”</span> bioRxiv. <a href="https://doi.org/10.1101/2020.12.15.422761">https://doi.org/10.1101/2020.12.15.422761</a>.
</div>
<div id="ref-rives_esm_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“[<span>ESM</span>-1b] <span>Biological</span> Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 118 (15): e2016239118. <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-suzek_uniref_2007" class="csl-entry" role="listitem">
Suzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and Cathy H. Wu. 2007. <span>“<span>UniRef</span>: Comprehensive and Non-Redundant <span>UniProt</span> Reference Clusters.”</span> <em>Bioinformatics</em> 23 (10): 1282–88. <a href="https://doi.org/10.1093/bioinformatics/btm098">https://doi.org/10.1093/bioinformatics/btm098</a>.
</div>
<div id="ref-watson_rfdiffusion_2023" class="csl-entry" role="listitem">
Watson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. <span>“De Novo Design of Protein Structure and Function with <span>RFdiffusion</span>.”</span> <em>Nature</em> 620 (7976): 1089–1100. <a href="https://doi.org/10.1038/s41586-023-06415-8">https://doi.org/10.1038/s41586-023-06415-8</a>.
</div>
<div id="ref-yang_xlnet_2020" class="csl-entry" role="listitem">
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2020. <span>“<span>XLNet</span>: <span>Generalized</span> <span>Autoregressive</span> <span>Pretraining</span> for <span>Language</span> <span>Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1906.08237">https://doi.org/10.48550/arXiv.1906.08237</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_3/p3-ch14-dna-lm.html" class="pagination-link" aria-label="DNA Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_3/p3-ch16-regulatory.html" class="pagination-link" aria-label="Regulatory Models">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>