<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>14&nbsp; DNA Language Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_3/p3-ch15-protein-lm.html" rel="next">
<link href="../part_3/p3-ch13-fm-principles.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch14-dna-lm.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Sequence Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch10-adaptation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmarks and Evaluation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch12-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch14-dna-lm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch15-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch16-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch17-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch19-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch20-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch21-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch22-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Trust</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch24-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch25-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Causal Inference with Foundation Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch26-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Clinical Translation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch28-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch29-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch30-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch31-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch14-task-specific-to-general" id="toc-sec-ch14-task-specific-to-general" class="nav-link active" data-scroll-target="#sec-ch14-task-specific-to-general"><span class="header-section-number">14.1</span> From Task-Specific CNNs to General-Purpose Language Models</a></li>
  <li><a href="#sec-ch14-dnabert" id="toc-sec-ch14-dnabert" class="nav-link" data-scroll-target="#sec-ch14-dnabert"><span class="header-section-number">14.2</span> <em>DNABERT</em>: The First DNA Language Model</a></li>
  <li><a href="#sec-ch14-nucleotide-transformer" id="toc-sec-ch14-nucleotide-transformer" class="nav-link" data-scroll-target="#sec-ch14-nucleotide-transformer"><span class="header-section-number">14.3</span> <em>Nucleotide Transformer</em>: Scaling Data and Model Diversity</a></li>
  <li><a href="#sec-ch14-gpn" id="toc-sec-ch14-gpn" class="nav-link" data-scroll-target="#sec-ch14-gpn"><span class="header-section-number">14.4</span> <em>GPN</em>: Cross-Species Pretraining for Variant Effect Prediction</a></li>
  <li><a href="#sec-ch14-long-context" id="toc-sec-ch14-long-context" class="nav-link" data-scroll-target="#sec-ch14-long-context"><span class="header-section-number">14.5</span> Long-Context Revolution</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-hyenadna" id="toc-sec-ch14-hyenadna" class="nav-link" data-scroll-target="#sec-ch14-hyenadna"><span class="header-section-number">14.5.1</span> <em>HyenaDNA</em>: Megabase Context via Implicit Convolutions</a></li>
  <li><a href="#sec-ch14-caduceus" id="toc-sec-ch14-caduceus" class="nav-link" data-scroll-target="#sec-ch14-caduceus"><span class="header-section-number">14.5.2</span> <em>Caduceus</em>: Bidirectional Processing with Reverse-Complement Equivariance</a></li>
  <li><a href="#sec-ch14-evo2" id="toc-sec-ch14-evo2" class="nav-link" data-scroll-target="#sec-ch14-evo2"><span class="header-section-number">14.5.3</span> <em>Evo 2</em>: Genome-Scale Modeling Across the Tree of Life</a></li>
  </ul></li>
  <li><a href="#sec-ch14-training-data" id="toc-sec-ch14-training-data" class="nav-link" data-scroll-target="#sec-ch14-training-data"><span class="header-section-number">14.6</span> Training Data and What Models Learn</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-corpus-composition" id="toc-sec-ch14-corpus-composition" class="nav-link" data-scroll-target="#sec-ch14-corpus-composition"><span class="header-section-number">14.6.1</span> Training Corpus Composition</a></li>
  <li><a href="#sec-ch14-probing" id="toc-sec-ch14-probing" class="nav-link" data-scroll-target="#sec-ch14-probing"><span class="header-section-number">14.6.2</span> Probing What Models Learn</a></li>
  <li><a href="#sec-ch14-limitations-learned" id="toc-sec-ch14-limitations-learned" class="nav-link" data-scroll-target="#sec-ch14-limitations-learned"><span class="header-section-number">14.6.3</span> What Models Do Not Learn</a></li>
  </ul></li>
  <li><a href="#sec-ch14-benchmarks" id="toc-sec-ch14-benchmarks" class="nav-link" data-scroll-target="#sec-ch14-benchmarks"><span class="header-section-number">14.7</span> Benchmark Performance and Evaluation</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-benchmark-suites" id="toc-sec-ch14-benchmark-suites" class="nav-link" data-scroll-target="#sec-ch14-benchmark-suites"><span class="header-section-number">14.7.1</span> Major Benchmark Suites</a></li>
  <li><a href="#sec-ch14-benchmark-limitations" id="toc-sec-ch14-benchmark-limitations" class="nav-link" data-scroll-target="#sec-ch14-benchmark-limitations"><span class="header-section-number">14.7.2</span> Benchmark Limitations</a></li>
  </ul></li>
  <li><a href="#sec-ch14-annotation-aware" id="toc-sec-ch14-annotation-aware" class="nav-link" data-scroll-target="#sec-ch14-annotation-aware"><span class="header-section-number">14.8</span> Annotation-Aware Extensions</a></li>
  <li><a href="#sec-ch14-practical-use" id="toc-sec-ch14-practical-use" class="nav-link" data-scroll-target="#sec-ch14-practical-use"><span class="header-section-number">14.9</span> Using DNA Language Models in Practice</a>
  <ul class="collapse">
  <li><a href="#sec-ch14-embeddings" id="toc-sec-ch14-embeddings" class="nav-link" data-scroll-target="#sec-ch14-embeddings"><span class="header-section-number">14.9.1</span> Embeddings as Universal Features</a></li>
  <li><a href="#sec-ch14-fine-tuning" id="toc-sec-ch14-fine-tuning" class="nav-link" data-scroll-target="#sec-ch14-fine-tuning"><span class="header-section-number">14.9.2</span> Fine-Tuning and Adaptation</a></li>
  <li><a href="#sec-ch14-zero-shot" id="toc-sec-ch14-zero-shot" class="nav-link" data-scroll-target="#sec-ch14-zero-shot"><span class="header-section-number">14.9.3</span> Zero-Shot and Few-Shot Scoring</a></li>
  </ul></li>
  <li><a href="#sec-ch14-open-challenges" id="toc-sec-ch14-open-challenges" class="nav-link" data-scroll-target="#sec-ch14-open-challenges"><span class="header-section-number">14.10</span> Limitations and Open Challenges</a></li>
  <li><a href="#sec-ch14-soft-landing" id="toc-sec-ch14-soft-landing" class="nav-link" data-scroll-target="#sec-ch14-soft-landing"><span class="header-section-number">14.11</span> Representations Without Predictions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--architectures.html">Part III: Foundation Model Families</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch14-dna-lm.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch14-dna-lm" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Prerequisites:</strong> Before reading this chapter, you should be familiar with:</p>
<ul>
<li>Sequence representations and tokenization strategies (<a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>)</li>
<li>Convolutional neural networks for genomics (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>)</li>
<li>Attention mechanisms and transformers (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>)</li>
<li>Pretraining objectives (masked language modeling, autoregressive) (<a href="../part_2/p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>)</li>
</ul>
<p><strong>Learning Objectives:</strong> After completing this chapter, you will be able to:</p>
<ol type="1">
<li>Explain why self-supervised learning on DNA sequences can discover regulatory grammar</li>
<li>Compare the architectural innovations of major DNA language models (DNABERT, Nucleotide Transformer, HyenaDNA, Caduceus, Evo 2)</li>
<li>Evaluate the tradeoffs between context length, computational complexity, and biological inductive biases</li>
<li>Apply DNA language models for embedding extraction, fine-tuning, and zero-shot variant scoring</li>
<li>Identify the fundamental limitations of sequence-only models</li>
</ol>
<p><strong>Key Insight:</strong> DNA language models learn <em>representations</em>, not <em>predictions</em>. They capture what patterns exist in genomic sequence but not what those patterns do in cellular context. Understanding this distinction is essential for knowing when and how to use these models.</p>
</div>
</div>
<p>A regulatory element in the genome looks like random sequence to the untrained eye: ACGTACGTACGT… indistinguishable from noise. Yet hidden within these letters is a grammar—rules governing which proteins bind where, how signals propagate across kilobases, why some mutations devastate while neighbors remain silent. For decades, researchers cataloged fragments of this grammar one experiment at a time: a binding site here, a splice signal there, each discovery hard-won and narrow in scope.</p>
<p>What if a model could discover this regulatory grammar automatically, simply by reading the genome?</p>
<p>The transformer revolution in natural language processing suggested this might be possible. Statistical patterns in unlabeled text contain information about grammar, semantics, and even world knowledge. Train a model to predict masked words from context, and it learns not just vocabulary but the structure of language itself. <em>BERT</em>, <em>GPT</em>, and their successors demonstrated that self-supervised learning on raw text yields representations useful for tasks the model was never explicitly trained to perform. Proteins proved amenable to the same approach: models trained to predict masked amino acids learned evolutionary constraints, structural properties, and functional relationships without explicit supervision (<a href="p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a>). DNA presents the analogous opportunity. If genomes encode a regulatory language, perhaps self-supervised learning on raw nucleotide sequence could discover its grammar.</p>
<p>DNA language models import this paradigm to nucleotide sequences. Rather than training separate models for each genomic prediction task, as the <strong>convolutional neural network (CNN)</strong> era required (<a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a>), these approaches learn general-purpose representations from unlabeled genomes that transfer across applications. A single pretrained backbone can support regulatory element classification, variant effect prediction, cross-species analysis, and sequence generation through different downstream heads or adaptation strategies. After <strong>fine-tuning</strong> (<a href="../part_2/p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>), the same model that learns to predict masked nucleotides can predict chromatin accessibility in cell types it never saw during <strong>pretraining</strong> (<a href="../part_2/p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>). It can also identify splice sites without splice-specific training data and score variant effects using evolutionary patterns learned from billions of nucleotides.</p>
<p>The opportunity is substantial but not guaranteed to succeed. Protein sequences have clear functional units (domains, secondary structures, binding sites) that language model representations can capture. DNA sequences present a different challenge: regulatory grammar operates at multiple scales simultaneously, from six-nucleotide transcription factor binding sites through kilobase-scale enhancers to megabase chromatin domains. Whether self-supervised learning can discover this multi-scale grammar remains an empirical question.</p>
<section id="sec-ch14-task-specific-to-general" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="sec-ch14-task-specific-to-general"><span class="header-section-number">14.1</span> From Task-Specific CNNs to General-Purpose Language Models</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about the limitations of task-specific CNNs, consider: if you trained a model specifically to predict chromatin accessibility in one cell type, what challenges might you face when applying it to a new cell type or a different prediction task?</p>
</div>
</div>
<p>The convolutional neural networks examined in <a href="../part_2/p2-ch06-cnn.html" class="quarto-xref"><span>Chapter 6</span></a> achieved strong performance on specific genomic prediction tasks. They faced, however, the feature ceiling limitation discussed in <a href="../part_1/p1-ch04-vep-classical.html#sec-ch04-features-to-representations" class="quarto-xref"><span>Section 4.6.4</span></a>: performance bounded by what architectural choices and training data could capture. <em>DeepSEA</em> predicted chromatin marks from sequence; <em>SpliceAI</em> identified splice junctions with clinical utility; <em>ExPecto</em> estimated expression effects of variants. Each model was engineered for its particular application, with architectural choices (filter sizes, dilation patterns, pooling strategies) optimized for the task at hand.</p>
<div id="fig-dna-lm-timeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dna-lm-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/01-fig-dna-lm-timeline.svg" class="img-fluid figure-img"></p>
<figcaption>Evolution of DNA language models from 2021-2025</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dna-lm-timeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Evolution of DNA language models from 2021-2025. The timeline traces key milestones in architectural capability. DNABERT (2021) demonstrated proof-of-concept with 512-token contexts using k-mer tokenization. Nucleotide Transformer (2023) scaled to 2.5 billion parameters with 6kb context and multi-species pretraining. HyenaDNA (2023) broke through the quadratic attention barrier, achieving 1 megabase context through sub-quadratic Hyena operators. Caduceus (2024) introduced reverse-complement equivariance through bidirectional Mamba architectures. Evo 2 (2024-2025) scaled to 40 billion parameters with million-base contexts, enabling pan-genomic understanding and sequence generation. Upper track shows exponential growth in context length; lower track highlights architectural innovations enabling each advance.
</figcaption>
</figure>
</div>
<p>This paradigm succeeded but imposed three constraints that limited scalability. Every new assay, cell type, or phenotype required fresh labeled data; a model trained on ENCODE chromatin data could not predict histone modifications in a new cell type without additional labeled examples. Model architecture was bound to specific prediction problems: <em>SpliceAI’s</em> dilated convolutions were tailored for splice junction detection, and <em>ExPecto’s</em> spatial transformation was designed for the distance-dependent relationship between regulatory elements and transcription start sites. These architectural choices, while effective, did not transfer naturally to other problems. Features learned for one task could not easily support others; a model that learned to recognize transcription factor binding sites during chromatin accessibility training could not directly apply those representations to variant effect prediction without substantial re-engineering.</p>
<p>Protein language models demonstrated an alternative. <em>ESM</em> and related models trained on massive corpora of protein sequences using <strong>masked language modeling</strong> (predicting held-out amino acids from context) or <strong>autoregressive</strong> objectives (predicting the next amino acid). The resulting representations transferred to structure prediction, function annotation, and variant effect scoring without architecture changes (<a href="p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a>). DNA language models import this recipe: pretrain on large collections of genomic sequences using self-supervised objectives, then adapt the learned representations to downstream tasks through probing, fine-tuning, or zero-shot scoring.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Deep Dive: Self-Supervised Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Deep Dive: Self-Supervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>For biology readers:</strong> Self-supervised learning creates training labels from the data itself, without manual annotation:</p>
<p><strong>The insight:</strong> Instead of requiring expensive labeled data (e.g., “this variant is pathogenic”), self-supervised learning generates labels automatically from raw data.</p>
<p><strong>Two main strategies for sequence models:</strong></p>
<p><strong>Masked Language Modeling (MLM):</strong> Hide some tokens, predict them from context.</p>
<ul>
<li>Input: “The CTCF motif [MASK] gene expression”</li>
<li>Target: predict the masked word</li>
<li>Genomic version: mask nucleotides, predict from surrounding sequence</li>
</ul>
<p><strong>Autoregressive (Next-Token Prediction):</strong> Predict each token from all previous tokens.</p>
<ul>
<li>Given: “ACGT”</li>
<li>Predict: the next nucleotide</li>
<li>Used by GPT-style models</li>
</ul>
<p><strong>Why it works:</strong></p>
<ol type="1">
<li>Creates unlimited training data from unlabeled sequences</li>
<li>Forces model to learn statistical patterns that capture biological structure</li>
<li>Positions with strong predictions = evolutionarily constrained positions</li>
<li>Patterns useful for masked prediction transfer to other tasks</li>
</ol>
<p><strong>The key insight:</strong> Predicting masked nucleotides requires understanding what patterns are “allowed” in genomic sequence—which is exactly what determines variant effects.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: The Foundation Model Paradigm Shift
</div>
</div>
<div class="callout-body-container callout-body">
<p>The core shift from task-specific CNNs to DNA language models is this: instead of building specialized architectures for each task, train a single model to understand DNA sequence through self-supervision, then adapt that understanding to any downstream application. This inverts the traditional workflow from <em>task-first</em> (design architecture for task, train from scratch) to <em>representation-first</em> (learn general representations, adapt to tasks).</p>
</div>
</div>
<p>The practical workflow begins with training a language model on unlabeled genomic sequences to predict masked or subsequent nucleotides. From the trained model, <strong>embeddings</strong> are extracted for sequences of interest (windows around variants, regulatory elements, or entire genes). These embeddings then support downstream tasks through probing with lightweight classifiers, fine-tuning for specific applications, or zero-shot scoring via probability comparisons. Once a sufficiently powerful backbone exists, it becomes the default starting point for nearly any DNA-level prediction problem.</p>
</section>
<section id="sec-ch14-dnabert" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="sec-ch14-dnabert"><span class="header-section-number">14.2</span> <em>DNABERT</em>: The First DNA Language Model</h2>
<p><em>DNABERT</em> applied the <em>BERT</em> masked language modeling framework to genomic sequences, establishing proof of concept for DNA self-supervision <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. The model used overlapping k-mers (typically 6-mers) as tokens, creating a vocabulary of 4,096 tokens from the <span class="math inline">\(4^6\)</span> possible hexamers. This tokenization strategy, detailed in <a href="../part_2/p2-ch05-representations.html#sec-ch05-kmer" class="quarto-xref"><span>Section 5.2</span></a>, provided computational efficiency at the cost of positional ambiguity for variants. Training on the human reference genome, <em>DNABERT</em> learned to predict masked tokens from surrounding context using the standard <em>BERT</em> architecture.</p>
<p>The design choices reflected computational constraints of the time. The <span class="math inline">\(k\)</span>-mer <strong>tokenization</strong> provided some sequence compression compared to single-nucleotide representations, but the overlapping nature (each nucleotide participates in multiple adjacent k-mers) meant the compression was modest and created ambiguity about precise variant positions. Context windows were limited to 512 tokens, corresponding to a few hundred base pairs of genomic sequence. The standard <strong>transformer architecture</strong> with quadratic attention complexity made longer contexts computationally prohibitive, a limitation examined in <a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a> and resolved by the architectural innovations in <a href="#sec-ch14-hyenadna" class="quarto-xref"><span>Section 14.5.1</span></a> and <a href="#sec-ch14-caduceus" class="quarto-xref"><span>Section 14.5.2</span></a>.</p>
<p>Despite these limitations, <em>DNABERT</em> demonstrated several important principles. Fine-tuning on downstream tasks (promoter classification, splice site prediction, transcription factor binding site identification) achieved competitive performance with task-specific models trained from scratch. Learned embeddings captured biologically meaningful patterns, with similar sequences clustering together in embedding space even when trained only on the reference genome. The <em>BERT</em>-style architecture could be reused across multiple tasks with modest adaptation.</p>
<p><em>DNABERT-2</em> addressed the tokenization limitations through improved approaches including BPE-style token merging that better compressed repetitive sequences <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="../bib/references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. The resulting model could represent longer genomic contexts within the same number of tokens, improving computational efficiency. On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring (<a href="../part_2/p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a>), <em>DNABERT-2</em> achieved consistent gains over both the original <em>DNABERT</em> and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications (see <a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a> for detailed discussion of tokenization strategies).</p>
<p>The <em>DNABERT</em> family collectively established that self-supervision on DNA works, that tokenization choices substantially affect performance, and that masked language model training produces reusable representations for diverse sequence tasks. The foundation model paradigm transfers effectively from natural language to genomic sequence.</p>
</section>
<section id="sec-ch14-nucleotide-transformer" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="sec-ch14-nucleotide-transformer"><span class="header-section-number">14.3</span> <em>Nucleotide Transformer</em>: Scaling Data and Model Diversity</h2>
<p><em>DNABERT</em> demonstrated feasibility but operated at modest scale relative to the size of genomes. The <em>Nucleotide Transformer</em> family pushed substantially further, emphasizing diversity in both training data and model architecture <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>.</p>
<p>The training corpus spanned genomic data from multiple species and human populations, exposing models to diverse sequence patterns, different regulatory architectures, and evolutionary constraints recurring across lineages. This cross-species pretraining mirrors the use of large multi-species alignments in protein language models but operates directly on raw DNA without explicit alignment. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over <em>DNABERT</em> while still using dense transformer attention. The training objective remained masked language modeling on subsequences sampled from genomes.</p>
<p>The <em>Nucleotide Transformer</em> project introduced a benchmark panel that has become a standard yardstick for evaluating DNA language models. Tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, splice site identification, and regulatory element type classification. Models are evaluated through linear probes or light fine-tuning on standardized train/validation/test splits. This benchmark infrastructure enabled systematic comparison across models and established the evaluation protocols now used throughout the field (see <a href="../part_2/p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a> for comprehensive discussion of genomic benchmarks).</p>
<p>Scaling experiments revealed predictable relationships between model size, training data, and performance. Larger models with more pretraining data and longer context windows achieved better downstream performance, following patterns observed in natural language and protein modeling. These scaling trends suggest that continued investment in larger genomic language models will yield further improvements, though the optimal allocation between parameters, data, and compute remains an active research question (<a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>).</p>
</section>
<section id="sec-ch14-gpn" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="sec-ch14-gpn"><span class="header-section-number">14.4</span> <em>GPN</em>: Cross-Species Pretraining for Variant Effect Prediction</h2>
<p>While the <em>Nucleotide Transformer</em> demonstrated the value of scaling, the Genomic Pretrained Network (<em>GPN</em>) explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes <span class="citation" data-cites="benegas_gpn_2023">(<a href="../bib/references.html#ref-benegas_gpn_2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. Rather than scaling to maximum size, <em>GPN</em> asked whether self-supervision could yield useful variant effect predictors even in constrained settings.</p>
<p><em>GPN</em> was trained on unaligned reference genomes from <em>Arabidopsis thaliana</em> and seven related species within the Brassicales order using masked language modeling. Despite this modest training corpus, analysis revealed emergent encoding of gene structure (exon-intron boundaries, splice sites) and DNA sequence motifs (transcription factor binding patterns) without explicit supervision. The model discovered these patterns purely from statistical regularities of genomic sequence across related species.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>GPN</em> learns from only eight plant genomes, yet it outperforms conservation scores derived from alignments across dozens of species. What might explain this surprising result? Consider what information is preserved versus lost in multiple sequence alignment.</p>
</div>
</div>
<p>For variant effect prediction, <em>GPN</em> used a <strong>likelihood ratio</strong> approach. Given reference and alternate alleles at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood (relative to the reference) are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked Example: Zero-Shot Variant Scoring
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider scoring a variant at position 1000 in a gene. The workflow:</p>
<ol type="1">
<li><strong>Extract context window:</strong> Take the sequence from positions 500-1500 (1 kb centered on variant)</li>
<li><strong>Compute reference likelihood:</strong> Feed sequence with reference allele (e.g., A) to model; record log-probability <span class="math inline">\(\log P(\text{seq}|\text{A})\)</span></li>
<li><strong>Compute alternate likelihood:</strong> Feed sequence with alternate allele (e.g., G) to model; record log-probability <span class="math inline">\(\log P(\text{seq}|\text{G})\)</span></li>
<li><strong>Calculate likelihood ratio:</strong> <span class="math inline">\(\Delta = \log P(\text{seq}|\text{ref}) - \log P(\text{seq}|\text{alt})\)</span></li>
<li><strong>Interpret:</strong> Positive <span class="math inline">\(\Delta\)</span> means alternate reduces sequence likelihood, suggesting disruption. Larger values indicate stronger constraint violation.</li>
</ol>
<p>This approach requires no variant-specific training data and works for any position the model can process.</p>
</div>
</div>
<p>Evaluated on <em>A. thaliana</em> variants using allele frequencies from the 1001 Genomes Project, <em>GPN</em> outperformed traditional conservation scores including phyloP and phastCons <span class="citation" data-cites="benegas_gpn_2023">(<a href="../bib/references.html#ref-benegas_gpn_2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. This was notable because phyloP and phastCons require explicit multiple sequence alignments and evolutionary models, while <em>GPN</em> learned its representations from unaligned sequences through self-supervision alone. The later <em>GPN-MSA</em> extended this approach to mammalian genomes by incorporating multi-species alignments, achieving strong performance on human variant benchmarks (<span class="quarto-unresolved-ref">?sec-ch14-dna-lm-vep</span>). The success of this approach informed subsequent development of zero-shot variant scoring methods for clinical applications (<span class="quarto-unresolved-ref">?sec-ch26-fm-scoring</span>).</p>
<p><em>GPN</em> established several important principles. Cross-species pretraining captures evolutionary constraints transferable to variant effect prediction. Relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures within that group. The masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.</p>
</section>
<section id="sec-ch14-long-context" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="sec-ch14-long-context"><span class="header-section-number">14.5</span> Long-Context Revolution</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mathematical Content Ahead
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section discusses computational complexity notation (O notation) for comparing algorithms. If you are not familiar with this notation: <span class="math inline">\(O(L^2)\)</span> means computational cost grows with the square of sequence length <span class="math inline">\(L\)</span>, while <span class="math inline">\(O(L)\)</span> or <span class="math inline">\(O(L \log L)\)</span> means cost grows linearly or near-linearly. The key takeaway is that quadratic scaling makes long sequences impractical, while linear scaling enables megabase processing.</p>
</div>
</div>
<p>Quadratic attention complexity limits transformer context to tens of kilobases at best. Processing a 100 kb sequence with dense attention requires on the order of <span class="math inline">\(10^{10}\)</span> computations per layer. Yet regulatory phenomena routinely span larger distances: enhancer-promoter interactions extend 50-200 kb, topologically associating domains organize chromatin at the megabase scale, and some gene regulation involves even longer-range dependencies. The three-dimensional organization of chromatin that enables these long-range contacts is examined in <a href="../part_4/p4-ch20-3d-genome.html" class="quarto-xref"><span>Chapter 20</span></a>; here we focus on how linear sequence models can capture information about these interactions. The mismatch between biological context and computational context represented a fundamental architectural limitation.</p>
<div id="fig-long-context-revolution" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-long-context-revolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/02-A-fig-long-context-revolution.svg" class="img-fluid figure-img"></p>
<figcaption>The quadratic attention bottleneck limits standard transformers</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/02-B-fig-long-context-revolution.svg" class="img-fluid figure-img"></p>
<figcaption>Sub-quadratic architectures enable million-base contexts</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-long-context-revolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2: Breaking the attention bottleneck for genomic modeling. (A) Standard self-attention requires computing all pairwise interactions between positions, creating quadratic O(L²) memory and compute requirements. This limits practical contexts to approximately 10-50 kilobases, falling short of enhancer-promoter distances (50-100kb) and topologically associating domain scales (~1Mb). (B) Sub-quadratic architectures achieve longer contexts through different strategies: Hyena uses learnable long convolutions (O(L log L)); Mamba employs selective state-space models with linear scaling (O(L)); linear attention approximates the attention mechanism with kernel methods (O(L)). These innovations enabled DNA language models to process million-base contexts, accessing biological relationships invisible to shorter-context models.
</figcaption>
</figure>
</div>
<section id="sec-ch14-hyenadna" class="level3" data-number="14.5.1">
<h3 data-number="14.5.1" class="anchored" data-anchor-id="sec-ch14-hyenadna"><span class="header-section-number">14.5.1</span> <em>HyenaDNA</em>: Megabase Context via Implicit Convolutions</h3>
<p><em>HyenaDNA</em> addressed this limitation by replacing attention with implicit convolutions that scale sub-quadratically <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. The Hyena architecture parameterizes long convolutional filters through neural networks rather than storing explicit filter weights, achieving <span class="math inline">\(O(L \log L)\)</span> complexity through efficient FFT-based convolution compared to <span class="math inline">\(O(L^2)\)</span> for standard attention. The result was a 500-fold increase in context length: <em>HyenaDNA</em> processes sequences up to 1 Mb while maintaining single-nucleotide resolution.</p>
<p>Processing megabase-scale windows allows the model to capture entire gene bodies plus flanking regulatory regions, long-range enhancer-promoter interactions, and topologically associating domain structure. Despite the long context, single-nucleotide tokens preserve maximum resolution for variant effect prediction. Each nucleotide is independently represented without the ambiguity introduced by <span class="math inline">\(k\)</span>-mer tokenization.</p>
<p>On <em>Nucleotide Transformer</em> benchmarks, <em>HyenaDNA</em> achieved state-of-the-art results on the majority of tasks with orders of magnitude fewer parameters. On GenomicBenchmarks, it surpassed prior state-of-the-art on seven of eight datasets <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="../bib/references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. <em>HyenaDNA</em> also demonstrated <strong>in-context learning</strong> in genomics: performance improved when examples were included in the input context without updating model weights. This capability, familiar from large language models, had not previously been observed for genomic sequences and suggests that sufficient context length combined with appropriate architecture enables qualitatively new forms of biological reasoning.</p>
</section>
<section id="sec-ch14-caduceus" class="level3" data-number="14.5.2">
<h3 data-number="14.5.2" class="anchored" data-anchor-id="sec-ch14-caduceus"><span class="header-section-number">14.5.2</span> <em>Caduceus</em>: Bidirectional Processing with Reverse-Complement Equivariance</h3>
<p>DNA is double-stranded, and any sequence can be read from either strand. The reverse complement of a sequence encodes the same information from the opposite strand’s perspective. For many biological processes, predictions should be identical or related consistently regardless of which strand is presented. Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even with data augmentation during training.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a model predicting transcription factor binding. A binding site on the forward strand (5’-GAATTC-3’) has reverse complement (5’-GAATTC-3’ on the reverse strand). Should a model’s prediction differ based on which strand you query? Why or why not? What about for genes, which have defined orientations?</p>
</div>
</div>
<p><em>Caduceus</em> addressed this challenge by building <strong>reverse-complement equivariance</strong> directly into the architecture <span class="citation" data-cites="schiff_caduceus_2024">(<a href="../bib/references.html#ref-schiff_caduceus_2024" role="doc-biblioref">Schiff et al. 2024</a>)</span>. The model extends the <em>Mamba</em> state space architecture (which achieves O(L) complexity) to support both bidirectional processing and strand equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures mathematically related predictions for sequences and their reverse complements.</p>
<p>On downstream benchmarks, <em>Caduceus</em> outperformed previous long-range models. On challenging long-range variant effect prediction tasks, it exceeded models with ten times as many parameters that lacked bidirectionality or equivariance <span class="citation" data-cites="schiff_caduceus_2024">(<a href="../bib/references.html#ref-schiff_caduceus_2024" role="doc-biblioref">Schiff et al. 2024</a>)</span>. The key insight was that incorporating appropriate biological inductive biases can substitute for raw scale. Strand symmetry is a known property of DNA; building it into the architecture avoids wasting model capacity learning what could be specified directly.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Inductive Biases vs.&nbsp;Scale
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Caduceus</em> demonstrates a fundamental principle: when you know something about your domain, encode it in the architecture rather than hoping the model learns it from data. Strand symmetry is mathematically specifiable; building it in yields better performance with fewer parameters than training a larger model to approximate it. This principle applies broadly: whenever you have domain knowledge that can be expressed architecturally, doing so typically improves efficiency and generalization.</p>
</div>
</div>
</section>
<section id="sec-ch14-evo2" class="level3" data-number="14.5.3">
<h3 data-number="14.5.3" class="anchored" data-anchor-id="sec-ch14-evo2"><span class="header-section-number">14.5.3</span> <em>Evo 2</em>: Genome-Scale Modeling Across the Tree of Life</h3>
<p>The original <em>Evo</em> model demonstrated that DNA language models could operate at unprecedented scale <span class="citation" data-cites="nguyen_sequence_2024">(<a href="../bib/references.html#ref-nguyen_sequence_2024" role="doc-biblioref">Nguyen et al. 2024</a>)</span>. Trained on 2.7 million prokaryotic and phage genomes comprising 300 billion nucleotides, <em>Evo</em> processed sequences up to 131 kilobases using the StripedHyena architecture, a hybrid design combining state-space models with attention mechanisms. The 7 billion parameter model exhibited emergent biological understanding: predicting gene essentiality, identifying functional elements, and generating synthetic sequences with plausible biological properties. <em>Evo</em> demonstrated that training on raw DNA sequence alone, without annotation, could yield models that captured fundamental aspects of genome organization.</p>
<p><em>Evo 2</em> extends this foundation to the entire tree of life <span class="citation" data-cites="brixi_evo_2025">(<a href="../bib/references.html#ref-brixi_evo_2025" role="doc-biblioref">Brixi et al. 2025</a>)</span>. Where <em>Evo</em> focused primarily on prokaryotes and phages, <em>Evo 2</em> incorporates eukaryotic genomes with their dramatically different organization: extensive noncoding regions, complex regulatory architectures, and intronic sequences that comprise the majority of gene length in many species. This expansion required both larger models and longer context windows to capture the sprawling regulatory landscapes characteristic of eukaryotic genomes.</p>
<div id="fig-caduceus-equivariance" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-caduceus-equivariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/03-A-fig-caduceus-equivariance.svg" class="img-fluid figure-img"></p>
<figcaption>DNA reverse complement equivalence</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/03-B-fig-caduceus-equivariance.svg" class="img-fluid figure-img"></p>
<figcaption>Standard models break RC equivalence</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/03-C-fig-caduceus-equivariance.svg" class="img-fluid figure-img"></p>
<figcaption>Caduceus architecture guarantees equivariance</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-caduceus-equivariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.3: Reverse complement equivariance as architectural constraint. (A) DNA’s double-stranded nature creates biological equivalence: a regulatory element on one strand has the same function when read as its reverse complement from the other strand. (B) Standard models violate this equivalence, producing different predictions for forward and reverse complement sequences despite their biological identity. (C) Caduceus uses bidirectional Mamba state-space models with equivariant architecture to guarantee identical predictions for equivalent sequences. This architectural constraint eliminates strand bias without data augmentation, improving both biological correctness and data efficiency.
</figcaption>
</figure>
</div>
<p>The training corpus draws from the OpenGenome2 dataset comprising 9.3 trillion DNA tokens across all domains of life, a 30-fold increase over the original <em>Evo</em> training data. This massive scale exposes the model to the full spectrum of genomic organization: compact prokaryotic gene arrangements, sprawling eukaryotic regulatory landscapes with extensive noncoding sequence, viral genomes with overlapping reading frames, and the diversity of regulatory architectures across evolution. The model comes in 7 billion and 40 billion parameter variants, with the larger model extending well beyond the original <em>Evo</em>’s scale.</p>
<p>The architecture builds on StripedHyena 2, refining the hybrid design that proved effective in the original <em>Evo</em>. The combination of convolutional operations with selective attention mechanisms enables processing of sequences up to 1 million nucleotides, nearly an order of magnitude beyond <em>Evo</em>’s 131 kilobase context. Like its predecessor, <em>Evo 2</em> uses an autoregressive training objective (predicting the next base given all previous bases), which differs from the masked language modeling used in <em>DNABERT</em> and related models. Autoregressive training may provide complementary strengths for sequence generation and likelihood-based scoring, since the model learns to generate plausible sequences in addition to discriminating between them (<a href="../part_2/p2-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>).</p>
<p><em>Evo 2</em> exhibits several forms of emergent biological knowledge despite training only on raw sequence, extending the capabilities first observed in <em>Evo</em>. The model learns to identify exon-intron boundaries without explicit annotation, identifies transcription factor binding site patterns matching known motifs, captures aspects of protein secondary and tertiary structure when processing coding sequences, and identifies prophage insertion regions in bacterial genomes. Where <em>Evo</em> demonstrated these capabilities primarily in prokaryotic contexts, <em>Evo 2</em> generalizes them across eukaryotic genomes with their more complex gene structures.</p>
<p>For variant effect prediction, <em>Evo 2</em> enables zero-shot scoring through likelihood ratios. Variants can be scored for consistency with learned genomic patterns by comparing model probabilities for reference versus alternate sequences. On benchmarks of pathogenic versus benign variants, zero-shot scores achieve competitive performance with specialized supervised methods, though calibration remains necessary before clinical application (<span class="quarto-unresolved-ref">?sec-ch14-dna-lm-vep</span>). The calibration methods required for clinical deployment are examined in <a href="../part_5/p5-ch23-uncertainty.html#sec-ch23-calibration" class="quarto-xref"><span>Section 23.2</span></a>, and integration into diagnostic workflows in <span class="quarto-unresolved-ref">?sec-ch26-fm-scoring</span>. The model also supports classification of variants of uncertain significance through simple classifiers trained on its embeddings.</p>
<p>The pan-species training enables cross-species applications that extend <em>Evo</em>’s prokaryotic focus to the full breadth of biology. Variant interpretation extends naturally to non-model organisms, supporting conservation genomics and agricultural breeding where labeled training data is scarce. Model representations cluster sequences by phylogenetic relationships even without explicit evolutionary modeling. Beyond discriminative tasks, <em>Evo 2</em> demonstrates generative capabilities building on <em>Evo</em>’s initial demonstrations: synthesizing plausible mitochondrial genomes, prokaryotic operons, and eukaryotic regulatory regions with coherence across kilobase to megabase scales.</p>
<p>The following table summarizes the key characteristics of the major DNA language models discussed in this chapter:</p>
<div id="tbl-dna-lm-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-dna-lm-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.1: Major DNA language models compared by context length, architecture, and key innovations.
</figcaption>
<div aria-describedby="tbl-dna-lm-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 7%">
<col style="width: 10%">
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 22%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Year</th>
<th>Context</th>
<th>Parameters</th>
<th>Architecture</th>
<th>Training Objective</th>
<th>Key Innovation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>DNABERT</em></td>
<td>2021</td>
<td>~500 bp</td>
<td>110M</td>
<td>Transformer</td>
<td>Masked LM</td>
<td>Proof of concept</td>
</tr>
<tr class="even">
<td><em>DNABERT-2</em></td>
<td>2024</td>
<td>~500 bp</td>
<td>117M</td>
<td>Transformer</td>
<td>Masked LM</td>
<td>BPE tokenization</td>
</tr>
<tr class="odd">
<td><em>Nucleotide Transformer</em></td>
<td>2023</td>
<td>6 kb</td>
<td>500M-2.5B</td>
<td>Transformer</td>
<td>Masked LM</td>
<td>Multi-species, scaling</td>
</tr>
<tr class="even">
<td><em>GPN</em></td>
<td>2023</td>
<td>512 bp</td>
<td>25M</td>
<td>Transformer</td>
<td>Masked LM</td>
<td>Cross-species VEP</td>
</tr>
<tr class="odd">
<td><em>HyenaDNA</em></td>
<td>2023</td>
<td>1 Mb</td>
<td>1.6M-6.6M</td>
<td>Hyena</td>
<td>Next token</td>
<td>Sub-quadratic attention</td>
</tr>
<tr class="even">
<td><em>Caduceus</em></td>
<td>2024</td>
<td>131 kb</td>
<td>1.6M-6.6M</td>
<td>Mamba</td>
<td>Next token</td>
<td>RC-equivariance</td>
</tr>
<tr class="odd">
<td><em>Evo</em></td>
<td>2024</td>
<td>131 kb</td>
<td>7B</td>
<td>StripedHyena</td>
<td>Next token</td>
<td>Prokaryotic scale</td>
</tr>
<tr class="even">
<td><em>Evo 2</em></td>
<td>2025</td>
<td>1 Mb</td>
<td>7B-40B</td>
<td>StripedHyena 2</td>
<td>Next token</td>
<td>Pan-genomic, largest scale</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-ch14-training-data" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="sec-ch14-training-data"><span class="header-section-number">14.6</span> Training Data and What Models Learn</h2>
<p>DNA language models are trained on diverse corpora ranging from single reference genomes to pan-genomic collections spanning the tree of life. Understanding what training data is used and what models learn from it is essential for anticipating model capabilities and limitations.</p>
<section id="sec-ch14-corpus-composition" class="level3" data-number="14.6.1">
<h3 data-number="14.6.1" class="anchored" data-anchor-id="sec-ch14-corpus-composition"><span class="header-section-number">14.6.1</span> Training Corpus Composition</h3>
<p>Early models like <em>DNABERT</em> trained primarily on the human reference genome (GRCh38), providing exposure to approximately 3 billion nucleotides from a single individual. The <em>Nucleotide Transformer</em> expanded to include multiple species and human population variation from resources like the 1000 Genomes Project (<a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a>). <em>Evo 2</em> scaled to 9.3 trillion tokens spanning all domains of life, including complete bacterial chromosomes, eukaryotic genomes, viral sequences, and metagenomic assemblies.</p>
<div id="fig-evo2-training" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-evo2-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/04-A-fig-evo2-training.svg" class="img-fluid figure-img"></p>
<figcaption>Pan-genomic training data for Evo 2</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/04-B-fig-evo2-training.svg" class="img-fluid figure-img"></p>
<figcaption>Context length curriculum for stable training</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/04-C-fig-evo2-training.svg" class="img-fluid figure-img"></p>
<figcaption>Evo 2 generates biologically coherent sequences</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-evo2-training-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.4: Evo 2 training regime for pan-genomic understanding and generation. (A) Training data spans 2.7 trillion nucleotides across all domains of life, enabling learning of universal and lineage-specific sequence patterns. (B) Context length curriculum progressively extends from 8k to 1M tokens, maintaining stable optimization through each transition. (C) The resulting model generates novel sequences with emergent biological coherence: appropriate gene structure, regulatory elements, and statistical properties matching natural genomes. This combination of massive scale, architectural innovation, and careful training regime enables both understanding and generation of genomic sequence.
</figcaption>
</figure>
</div>
<p>The composition of training data shapes what models learn. Reference-only training captures the genome’s architecture but not population variation. Multi-individual training exposes models to common polymorphisms but may underrepresent rare variants. Cross-species training provides evolutionary context (constrained regions are conserved, variable regions diverge) but may not capture species-specific regulatory patterns. Training on functional genomics data (<em>GROVER</em>-style approaches) teaches regulatory activity patterns but ties models to specific assays and cell types.</p>
<p>A tension exists between generality and specificity. Models trained on broader corpora learn more general representations that transfer across species and contexts, but may underperform narrower models on specific applications. Models trained on focused datasets may capture task-relevant patterns more effectively but transfer less well. The optimal training strategy depends on intended applications.</p>
</section>
<section id="sec-ch14-probing" class="level3" data-number="14.6.2">
<h3 data-number="14.6.2" class="anchored" data-anchor-id="sec-ch14-probing"><span class="header-section-number">14.6.2</span> Probing What Models Learn</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>If a model was trained only to predict masked nucleotides (never seeing any labels), how could we determine whether it learned to recognize splice sites, transcription factor binding sites, or gene boundaries? What experiments would reveal this?</p>
</div>
</div>
<p><strong>Linear probing</strong> experiments reveal what information is encoded in model representations without task-specific fine-tuning. By training simple classifiers (logistic regression, single-layer perceptrons) on frozen embeddings to predict known annotations, researchers can assess whether models have learned biologically meaningful patterns. The methodology for such probing experiments is detailed in <a href="../part_2/p2-ch09-transfer.html#sec-ch09-probing-representations" class="quarto-xref"><span>Section 9.3.3</span></a>, with implications for interpretability examined in <a href="../part_5/p5-ch24-interpretability.html#sec-ch24-probing" class="quarto-xref"><span>Section 24.4</span></a>.</p>
<p>DNA language models consistently learn to recognize several categories of genomic features. Models learn patterns corresponding to known transcription factor binding sites, splice signals, and other sequence motifs without explicit supervision; probing for specific motif presence shows that model embeddings can distinguish sequences containing binding sites from those lacking them. Representations also encode gene structure: models distinguish coding from noncoding regions, identify exon-intron boundaries, and recognize splice donor and acceptor sites. This knowledge emerges from sequence statistics alone, suggesting that the compositional and structural differences between genomic region types are learnable from DNA sequence.</p>
<p>Evolutionary constraints are implicitly captured, particularly in models trained on multi-species data. Positions under purifying selection (constrained across evolution) show different embedding patterns than neutral positions. This provides a self-supervised analog to traditional conservation scoring, though the relationship between model-learned and alignment-based conservation measures varies across genomic contexts.</p>
<p>More complex patterns like regulatory grammar (the syntax governing how transcription factors combine to specify expression) show mixed evidence. Models capture some aspects of regulatory logic, such as the spacing preferences between binding sites, but may not fully represent the combinatorial complexity of enhancer function. Similarly, long-range dependencies (enhancer-promoter interactions across tens of kilobases) are accessible to long-context models but require extensive probing to assess whether they are actually leveraged.</p>
</section>
<section id="sec-ch14-limitations-learned" class="level3" data-number="14.6.3">
<h3 data-number="14.6.3" class="anchored" data-anchor-id="sec-ch14-limitations-learned"><span class="header-section-number">14.6.3</span> What Models Do Not Learn</h3>
<p>Equally important is recognizing what current DNA language models struggle to represent. Sequence-only models cannot capture epigenetic context: DNA methylation, histone modifications, and chromatin accessibility all affect gene regulation but are not encoded in primary sequence. Some models (like <em>GROVER</em>) address this by incorporating functional genomics data, but this ties them to specific cell types and experimental conditions.</p>
<p>The three-dimensional structure of chromatin affects which regulatory elements can physically interact, but linear sequence models cannot represent folding (<a href="../part_4/p4-ch20-3d-genome.html" class="quarto-xref"><span>Chapter 20</span></a>). Cell-type specificity of gene regulation depends on transcription factor expression levels and chromatin state, not just sequence; models trained on sequence alone can predict potential regulatory activity but not its realization in specific contexts.</p>
<p>Complex variant patterns beyond single nucleotide changes remain challenging. Indels, structural variants, repeat expansions, and epistatic interactions between distant loci are either not representable (depending on tokenization) or poorly predicted. Most benchmark tasks focus on SNVs, leaving multi-nucleotide effects underexplored.</p>
<p>The following table summarizes what DNA language models can and cannot learn from sequence alone:</p>
<div id="tbl-what-models-learn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-what-models-learn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.2: What DNA language models can and cannot learn from sequence alone. The fundamental limitation is that sequence encodes <em>potential</em>, not <em>realization</em> in specific cellular contexts.
</figcaption>
<div aria-describedby="tbl-what-models-learn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 31%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Category</th>
<th>Can Learn</th>
<th>Cannot Learn</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Sequence motifs</strong></td>
<td>TF binding sites, splice signals, promoter elements</td>
<td>Cell-type-specific activity of motifs</td>
</tr>
<tr class="even">
<td><strong>Gene structure</strong></td>
<td>Exon-intron boundaries, coding vs.&nbsp;noncoding</td>
<td>Alternative splicing patterns in specific tissues</td>
</tr>
<tr class="odd">
<td><strong>Evolutionary constraint</strong></td>
<td>Conservation patterns from cross-species training</td>
<td>Recent selection not captured in training data</td>
</tr>
<tr class="even">
<td><strong>Regulatory grammar</strong></td>
<td>Spacing preferences between motifs</td>
<td>Full combinatorial logic of enhancers</td>
</tr>
<tr class="odd">
<td><strong>Epigenetic state</strong></td>
<td>—</td>
<td>DNA methylation, histone modifications, chromatin accessibility</td>
</tr>
<tr class="even">
<td><strong>3D structure</strong></td>
<td>—</td>
<td>Chromatin folding, enhancer-promoter contacts</td>
</tr>
<tr class="odd">
<td><strong>Complex variants</strong></td>
<td>SNVs (with limitations)</td>
<td>Indels, structural variants, repeat expansions</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<div id="fig-dna-lm-probing" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dna-lm-probing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/05-A-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>DNA LMs encode genomic element identity</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/05-B-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>Model attention correlates with conservation</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/05-C-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>DNA LMs learn regulatory grammar</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/05-D-fig-dna-lm-probing.svg" class="img-fluid figure-img"></p>
<figcaption>Probing reveals fundamental limitations</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dna-lm-probing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.5: Probing analysis reveals what DNA language models learn—and what they cannot. (A) Frozen embeddings support high-accuracy linear classification of genomic element types including promoters, enhancers, and splice sites. (B) Model attention correlates with evolutionary conservation, suggesting learned focus on functionally important positions. (C) Attention patterns capture long-range regulatory relationships between enhancers and promoters. (D) However, probing fails for context-dependent properties: tissue-specific activity, environmental responses, and 3D chromatin organization cannot be predicted from sequence representations alone. DNA language models learn sequence potential; realizing that potential requires cellular context they cannot access.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch14-benchmarks" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="sec-ch14-benchmarks"><span class="header-section-number">14.7</span> Benchmark Performance and Evaluation</h2>
<p>Standardized benchmarks enable systematic comparison across DNA language models, though each benchmark captures only part of what we care about. Understanding benchmark construction and limitations is essential for interpreting performance claims.</p>
<section id="sec-ch14-benchmark-suites" class="level3" data-number="14.7.1">
<h3 data-number="14.7.1" class="anchored" data-anchor-id="sec-ch14-benchmark-suites"><span class="header-section-number">14.7.1</span> Major Benchmark Suites</h3>
<p>The BEND (Benchmark for Nucleotide Deep learning) suite provides a unified framework with tasks including gene finding, enhancer annotation, chromatin state prediction, and variant effect scoring <span class="citation" data-cites="marin_bend_2024">(<a href="../bib/references.html#ref-marin_bend_2024" role="doc-biblioref">Marin et al. 2024</a>)</span>. Standardized splits and metrics enable fair comparison. BEND specifically evaluates whether models capture biologically meaningful features at different resolution scales.</p>
<p>Genomic Benchmarks focus on regulatory element classification tasks: distinguishing promoters from nonpromoters, identifying active enhancers, predicting histone mark presence <span class="citation" data-cites="gresova_genomic-benchmarks_2023">(<a href="../bib/references.html#ref-gresova_genomic-benchmarks_2023" role="doc-biblioref"><strong>gresova_genomic-benchmarks_2023?</strong></a>)</span>. These tasks test whether model representations encode basic genomic annotations. Most current DNA language models achieve high accuracy on these tasks, suggesting benchmark saturation for simpler classification problems.</p>
<p>The Long Range Benchmark (LRB) and DNALongBench evaluate long-context modeling capabilities <span class="citation" data-cites="cheng_dnalongbench_2024">(<a href="../bib/references.html#ref-cheng_dnalongbench_2024" role="doc-biblioref">Cheng et al. 2024</a>)</span>. Tasks include predicting distal enhancer-promoter interactions, modeling chromatin structure across hundreds of kilobases, and integrating information over extended genomic windows. These benchmarks specifically test whether long-context architectures provide meaningful advantages over shorter-context models.</p>
<p>Comparative evaluations across model families reveal that no single architecture dominates all tasks <span class="citation" data-cites="manzo_comparative_2025">(<a href="../bib/references.html#ref-manzo_comparative_2025" role="doc-biblioref">Manzo, Borkowski, and Ovcharenko 2025</a>)</span>. Performance varies substantially depending on task characteristics (local motif recognition versus long-range integration), training data composition, and architectural choices. <em>HyenaDNA</em> and <em>Caduceus</em> excel on long-range tasks where their architectural innovations matter; <em>DNABERT-2</em> and <em>Nucleotide Transformer</em> perform well on shorter-range regulatory classification; <em>Evo 2</em> shows advantages on cross-species tasks and variant effect prediction.</p>
</section>
<section id="sec-ch14-benchmark-limitations" class="level3" data-number="14.7.2">
<h3 data-number="14.7.2" class="anchored" data-anchor-id="sec-ch14-benchmark-limitations"><span class="header-section-number">14.7.2</span> Benchmark Limitations</h3>
<p>Several systematic issues affect benchmark interpretation. Many benchmarks have reached saturation, where multiple models achieve near-perfect performance and discriminative power disappears. This has happened for simpler classification tasks in Genomic Benchmarks. Data leakage arises when training and test sequences share homology, allowing models to succeed through memorization rather than generalization; the homology-aware splitting strategies required to prevent this are detailed in <a href="../part_2/p2-ch11-benchmarks.html#sec-ch11-homology-aware-splitting" class="quarto-xref"><span>Section 12.2</span></a>. Careful sequence clustering (using tools like MMseqs2 or CD-HIT) is required, but many older benchmarks lack rigorous split design. The comprehensive treatment of benchmark construction and evaluation methodology appears in <a href="../part_2/p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a> and <a href="../part_2/p2-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 12</span></a>.</p>
<p>Distribution shift between benchmark data and real-world applications means strong benchmark performance may not predict deployment success. Most benchmarks derive from well-studied regions of well-characterized genomes; performance on understudied regions, rare variants, or non-European populations may differ substantially. The systematic treatment of such confounding factors appears in <a href="../part_2/p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a>, with specific attention to ancestry-related performance disparities in <span class="quarto-unresolved-ref">?sec-ch22-ancestry-confounding</span>.</p>
<p>The choice of evaluation metric affects what gets optimized. auROC favors discrimination regardless of calibration; Spearman correlation measures rank ordering but not absolute effect size prediction. Clinical applications may require well-calibrated probability estimates or accurate quantitative predictions, neither of which standard metrics directly assess (<a href="../part_5/p5-ch23-uncertainty.html" class="quarto-xref"><span>Chapter 23</span></a>). The gap between benchmark performance and deployment utility remains substantial for most genomic applications.</p>
<div id="fig-dna-lm-benchmarks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dna-lm-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch14/06-fig-dna-lm-benchmarks.svg" class="img-fluid figure-img"></p>
<figcaption>Benchmark comparison across DNA language models</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dna-lm-benchmarks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.6: Benchmark comparison across DNA language models. Heatmap shows relative performance on standardized genomic prediction tasks. Larger, newer models generally achieve higher performance, with particularly strong gains on tasks requiring long-range context (enhancer activity, gene expression) where architectural innovations enabling million-base contexts provide clear advantages. Performance on local tasks (splice prediction, promoter classification) is more saturated, with smaller models approaching larger model performance. These benchmarks primarily test frozen embedding quality; fine-tuning would yield different relative rankings.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch14-annotation-aware" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="sec-ch14-annotation-aware"><span class="header-section-number">14.8</span> Annotation-Aware Extensions</h2>
<p>Recent work explores enriching DNA language models with explicit biological structure beyond raw sequence. These approaches represent early steps toward multi-modal genomic foundation models.</p>
<p><em>Life-Code</em> proposes central-dogma-informed tokenization, treating coding and noncoding regions differently <span class="citation" data-cites="liu_life-code_2025">(<a href="../bib/references.html#ref-liu_life-code_2025" role="doc-biblioref">Liu et al. 2025</a>)</span>. Coding regions use codon tokens (three-nucleotide units specifying amino acids), respecting the genetic code’s fundamental structure. Noncoding regions use learned subword units optimized during training. Knowledge distillation from protein language models imports protein-level structural knowledge into DNA representations. <em>Life-Code</em> achieves competitive results across DNA, RNA, and protein tasks, suggesting that encoding biological structure into tokenization provides useful inductive bias (<a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>).</p>
<p><em>BioToken</em> extends tokenization to include explicit genomic annotations <span class="citation" data-cites="medvedev_biotoken_2025">(<a href="../bib/references.html#ref-medvedev_biotoken_2025" role="doc-biblioref">Medvedev et al. 2025</a>)</span>. Rather than representing regions purely as nucleotide strings, <em>BioToken</em> creates composite tokens encoding sequence content, variant presence, structural annotations (exon, intron, UTR), and functional context. The associated <em>BioFM</em> model achieves state-of-the-art performance across genomic benchmarks with substantially fewer parameters (265M), suggesting that annotation-aware representations improve parameter efficiency.</p>
<p>These approaches foreshadow the multi-modal foundation models discussed in Part IV (<a href="../part_4/p4-ch22-multi-omics.html" class="quarto-xref"><span>Chapter 22</span></a>), where sequence is only one of many integrated information streams.</p>
</section>
<section id="sec-ch14-practical-use" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="sec-ch14-practical-use"><span class="header-section-number">14.9</span> Using DNA Language Models in Practice</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Choosing a Usage Pattern
</div>
</div>
<div class="callout-body-container callout-body">
<p>The right approach depends on your data and computational constraints:</p>
<ul>
<li><strong>Little/no labeled data:</strong> Use zero-shot scoring or few-shot in-context learning</li>
<li><strong>Moderate labeled data (hundreds to thousands of examples):</strong> Extract embeddings, train lightweight downstream classifier</li>
<li><strong>Substantial labeled data (thousands+):</strong> Fine-tune with LoRA or full fine-tuning</li>
<li><strong>Computational constraints:</strong> Prefer embedding extraction (frozen model, one forward pass per sequence)</li>
<li><strong>Maximum performance:</strong> Full fine-tuning with sufficient data</li>
</ul>
</div>
</div>
<p>DNA language models support multiple usage patterns for different applications.</p>
<section id="sec-ch14-embeddings" class="level3" data-number="14.9.1">
<h3 data-number="14.9.1" class="anchored" data-anchor-id="sec-ch14-embeddings"><span class="header-section-number">14.9.1</span> Embeddings as Universal Features</h3>
<p>The simplest approach extracts embeddings from a pretrained model and uses them as features for downstream classifiers. The workflow involves extracting embeddings for windows around loci of interest, pooling or selecting positions relevant to the task, and training lightweight downstream models (linear layers, shallow MLPs, gradient boosting) on the extracted features.</p>
<p>This approach supports diverse applications. Regulatory element classification distinguishes promoters, enhancers, silencers, and insulators based on learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence. Variant effect scoring replaces or augments hand-crafted features in frameworks like <em>CADD</em> with language model features (analogous to <em>CADD</em> v1.7’s incorporation of protein language model features, as discussed in <a href="../part_1/p1-ch04-vep-classical.html" class="quarto-xref"><span>Chapter 4</span></a>). The integration of these features into comprehensive variant effect prediction workflows is detailed in <span class="quarto-unresolved-ref">?sec-ch14-combining-evidence</span>. Splicing analysis combines embeddings with specialized architectures.</p>
<p>Because the language model remains frozen, this approach is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor supporting many downstream applications.</p>
</section>
<section id="sec-ch14-fine-tuning" class="level3" data-number="14.9.2">
<h3 data-number="14.9.2" class="anchored" data-anchor-id="sec-ch14-fine-tuning"><span class="header-section-number">14.9.2</span> Fine-Tuning and Adaptation</h3>
<p>When sufficient labeled data exists, fine-tuning typically outperforms frozen embedding approaches (<a href="../part_2/p2-ch09-transfer.html" class="quarto-xref"><span>Chapter 9</span></a>). Updating all language model parameters for a specific task allows representations to specialize, achieving highest performance but requiring more compute and risking catastrophic forgetting of general knowledge.</p>
<p>Parameter-efficient methods like LoRA (Low-Rank Adaptation) offer a middle path, inserting small trainable modules into each layer while keeping the backbone mostly frozen <span class="citation" data-cites="hu_lora_2021">(<a href="../bib/references.html#ref-hu_lora_2021" role="doc-biblioref">Hu et al. 2021</a>)</span>. These approaches achieve most of the performance gains of full fine-tuning while maintaining computational efficiency and preserving general capabilities. Adapter-based methods similarly add small bottleneck modules tuned for specific tasks.</p>
</section>
<section id="sec-ch14-zero-shot" class="level3" data-number="14.9.3">
<h3 data-number="14.9.3" class="anchored" data-anchor-id="sec-ch14-zero-shot"><span class="header-section-number">14.9.3</span> Zero-Shot and Few-Shot Scoring</h3>
<p>For variant interpretation, language models enable zero-shot scoring based on sequence likelihood. Compute the model’s probability for a sequence containing the reference allele, compare to probability for the sequence with the alternative allele, and interpret variants reducing probability as more disruptive. This approach requires no variant-specific training and can score any single-nucleotide variant the model can represent.</p>
<p>Zero-shot scoring quality depends on how well the model’s learned distribution captures biological constraints. Performance tends to improve with model scale and training data diversity (<a href="p3-ch13-fm-principles.html" class="quarto-xref"><span>Chapter 13</span></a>). Few-shot approaches include task examples in the input context, allowing in-context learning without parameter updates. <em>HyenaDNA</em> demonstrated this capability for genomic tasks, suggesting that sufficiently large models with long context can adapt through prompts rather than training.</p>
</section>
</section>
<section id="sec-ch14-open-challenges" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="sec-ch14-open-challenges"><span class="header-section-number">14.10</span> Limitations and Open Challenges</h2>
<p>Despite substantial progress, DNA language models face several fundamental limitations.</p>
<p>The tradeoff between context length and representational fidelity persists. Long-context models like <em>HyenaDNA</em> and <em>Evo 2</em> can process megabase sequences but require efficient architectures that may not capture all the relationships dense attention would learn. Whether these architectural tradeoffs matter for specific applications remains task-dependent.</p>
<p>Most tokenization schemes represent insertions and deletions awkwardly or not at all. Structural variants spanning kilobases, repeat expansions, and complex rearrangements fall outside what current models can process (<a href="../part_2/p2-ch05-representations.html" class="quarto-xref"><span>Chapter 5</span></a>). Epistatic interactions between variants at distant loci are not captured even by long-context models trained solely on single sequences.</p>
<p>Training data composition shapes model capabilities in underexplored ways. Models trained primarily on European-ancestry genomes may perform poorly on variants common in other populations (<a href="../part_2/p2-ch12-confounding.html" class="quarto-xref"><span>Chapter 12</span></a>). Ascertainment bias in training databases (enrichment for coding regions, well-studied genes, specific diseases) propagates to learned representations. The field lacks systematic evaluation of performance disparities across populations.</p>
<p>Interpretability remains limited (<a href="../part_5/p5-ch24-interpretability.html" class="quarto-xref"><span>Chapter 24</span></a>). While probing studies reveal what models encode, explaining why a specific variant receives a particular score in terms connecting to biological mechanism is difficult. Attention patterns and gradient-based attribution provide some insight but often fail to identify the specific sequence features driving predictions.</p>
<p>Integration with other modalities is nascent. DNA sequence provides necessary but insufficient information for predicting gene regulation. Epigenomic state, three-dimensional chromatin structure, transcription factor concentrations, and cellular context all matter. Current DNA language models cannot represent these factors; multi-modal approaches (discussed in Part IV) aim to address this limitation.</p>
</section>
<section id="sec-ch14-soft-landing" class="level2" data-number="14.11">
<h2 data-number="14.11" class="anchored" data-anchor-id="sec-ch14-soft-landing"><span class="header-section-number">14.11</span> Representations Without Predictions</h2>
<p>DNA language models capture sequence patterns, regulatory motifs, and evolutionary constraints through self-supervised pretraining on genomic sequence. The progression from early proof-of-concept models through architectural innovations enabling megabase context demonstrates that the paradigm works: models trained to predict masked nucleotides learn representations that transfer across diverse downstream tasks. Biological inductive biases (strand symmetry, codon structure, cross-species training) can substitute for raw scale on appropriate tasks, creating opportunities for efficient models that encode domain knowledge.</p>
<p>Yet DNA language models have inherent limitations. They produce representations, not predictions. A language model can embed a sequence in a space where similar regulatory elements cluster together, but it cannot directly output the expression level that sequence will produce or the chromatin accessibility it will exhibit. The models capture what patterns exist in genomic sequence but not what those patterns do in cellular context. They cannot represent epigenomic state, three-dimensional chromatin organization, or cell-type-specific regulation without additional inputs beyond sequence.</p>
<p>These limitations define the complementary relationship between language models and sequence-to-function models. Where DNA language models learn representations from sequence statistics, regulatory models like <em>Enformer</em> and <em>Borzoi</em> predict molecular phenotypes from sequence context (<a href="p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>). The regulatory models provide quantitative outputs (expression levels, chromatin tracks, splice probabilities) that language models alone cannot produce. For variant effect prediction (<a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a>), both representation quality and phenotypic prediction matter: language model embeddings capture evolutionary constraint while regulatory models predict functional consequences. Understanding what each model family provides is prerequisite to combining them effectively.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>What we covered:</strong></p>
<ol type="1">
<li><p><strong>The paradigm shift</strong> from task-specific CNNs to general-purpose DNA language models that learn reusable representations from self-supervised pretraining</p></li>
<li><p><strong>Model evolution</strong> from DNABERT (512 tokens, proof of concept) through Evo 2 (1 Mb context, 40B parameters, pan-genomic)</p></li>
<li><p><strong>Architectural innovations</strong> that enable long contexts: implicit convolutions (HyenaDNA), state-space models (Caduceus), and hybrid designs (Evo 2)</p></li>
<li><p><strong>Biological inductive biases</strong> like reverse-complement equivariance that can substitute for raw scale</p></li>
<li><p><strong>What models learn</strong> (motifs, gene structure, evolutionary constraint) and <strong>what they cannot learn</strong> (epigenetic state, 3D structure, cell-type specificity)</p></li>
<li><p><strong>Practical usage patterns:</strong> embeddings as features, fine-tuning, and zero-shot variant scoring</p></li>
</ol>
<p><strong>Key takeaways:</strong></p>
<ul>
<li>DNA language models produce <em>representations</em>, not <em>predictions</em>. They capture sequence patterns but not cellular context.</li>
<li>No single model dominates all tasks; model choice depends on context length requirements and available training data.</li>
<li>Benchmark performance may not predict real-world deployment success due to distribution shift and metric limitations.</li>
</ul>
<p><strong>Looking ahead:</strong></p>
<ul>
<li>Protein language models (<a href="p3-ch15-protein-lm.html" class="quarto-xref"><span>Chapter 15</span></a>) apply similar principles to amino acid sequences</li>
<li>Regulatory models (<a href="p3-ch16-regulatory.html" class="quarto-xref"><span>Chapter 16</span></a>) predict molecular phenotypes that DNA-LMs cannot</li>
<li>Variant effect prediction (<a href="p3-ch17-vep-fm.html" class="quarto-xref"><span>Chapter 17</span></a>) combines both representation and prediction</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-benegas_gpn_2023" class="csl-entry" role="listitem">
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. <span>“[<span>GPN</span>] <span>DNA</span> Language Models Are Powerful Predictors of Genome-Wide Variant Effects.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (44): e2311219120. <a href="https://doi.org/10.1073/pnas.2311219120">https://doi.org/10.1073/pnas.2311219120</a>.
</div>
<div id="ref-brixi_evo_2025" class="csl-entry" role="listitem">
Brixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. <span>“[<span>Evo</span> 2] <span>Genome</span> Modeling and Design Across All Domains of Life with <span>Evo</span> 2.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.02.18.638918">https://doi.org/10.1101/2025.02.18.638918</a>.
</div>
<div id="ref-cheng_dnalongbench_2024" class="csl-entry" role="listitem">
Cheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu Yang, Lei Li, and Jian Ma. 2024. <span>“<span>DNALONGBENCH</span>: <span>A</span> <span>Benchmark</span> <span>Suite</span> <span>For</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Prediction</span> <span>Tasks</span>,”</span> October. <a href="https://openreview.net/forum?id=opv67PpqLS">https://openreview.net/forum?id=opv67PpqLS</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-hu_lora_2021" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. <span>“<span>LoRA</span>: <span>Low</span>-<span>Rank</span> <span>Adaptation</span> of <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-liu_life-code_2025" class="csl-entry" role="listitem">
Liu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. <span>“Life-<span>Code</span>: <span>Central</span> <span>Dogma</span> <span>Modeling</span> with <span>Multi</span>-<span>Omics</span> <span>Sequence</span> <span>Unification</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.07299">https://doi.org/10.48550/arXiv.2502.07299</a>.
</div>
<div id="ref-manzo_comparative_2025" class="csl-entry" role="listitem">
Manzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025. <span>“Comparative <span>Analysis</span> of <span>Deep</span> <span>Learning</span> <span>Models</span> for <span>Predicting</span> <span>Causative</span> <span>Regulatory</span> <span>Variants</span>.”</span> <em>bioRxiv: The Preprint Server for Biology</em>, June, 2025.05.19.654920. <a href="https://doi.org/10.1101/2025.05.19.654920">https://doi.org/10.1101/2025.05.19.654920</a>.
</div>
<div id="ref-marin_bend_2024" class="csl-entry" role="listitem">
Marin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. <span>“<span>BEND</span>: <span>Benchmarking</span> <span>DNA</span> <span>Language</span> <span>Models</span> on Biologically Meaningful Tasks.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2311.12570">https://doi.org/10.48550/arXiv.2311.12570</a>.
</div>
<div id="ref-medvedev_biotoken_2025" class="csl-entry" role="listitem">
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. <span>“<span>BioToken</span> and <span>BioFM</span> – <span>Biologically</span>-<span>Informed</span> <span>Tokenization</span> <span>Enables</span> <span>Accurate</span> and <span>Efficient</span> <span>Genomic</span> <span>Foundation</span> <span>Models</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.03.27.645711">https://doi.org/10.1101/2025.03.27.645711</a>.
</div>
<div id="ref-nguyen_sequence_2024" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Matthew G. Durrant, Brian Kang, Dhruva Katrekar, David B. Li, Liam J. Bartie, et al. 2024. <span>“Sequence Modeling and Design from Molecular to Genome Scale with <span>Evo</span>.”</span> <em>Science</em> 386 (6723): eado9336. <a href="https://doi.org/10.1126/science.ado9336">https://doi.org/10.1126/science.ado9336</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-schiff_caduceus_2024" class="csl-entry" role="listitem">
Schiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. <span>“Caduceus: <span>Bi</span>-<span>Directional</span> <span>Equivariant</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Sequence</span> <span>Modeling</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2403.03234">https://doi.org/10.48550/arXiv.2403.03234</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_3/p3-ch13-fm-principles.html" class="pagination-link" aria-label="Foundation Model Paradigm">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_3/p3-ch15-protein-lm.html" class="pagination-link" aria-label="Protein Language Models">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>