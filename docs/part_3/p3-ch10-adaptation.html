<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Adaptation Strategies – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../part_3/p3-ch11-benchmarks.html" rel="next">
<link href="../part_3/p3-ch09-transfer.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--learning.html">Part III: Learning &amp; Evaluation</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch10-adaptation.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_1/p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Data Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch03-gwas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS and Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_1/p1-ch04-vep-classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classical Variant Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_2/p2--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch05-representations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Tokens and Embeddings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch06-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Convolutional Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_2/p2-ch07-attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Transformers and Attention</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_3/p3--learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Learning &amp; Evaluation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch08-pretraining.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch10-adaptation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch11-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmark Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch12-evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Evaluation Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_3/p3-ch13-confounding.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Confounding and Data Leakage</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_4/p4--fm-families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Foundation Model Families</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch14-fm-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Foundation Model Paradigm</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch15-dna-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">DNA Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch16-protein-lm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch17-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Regulatory Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_4/p4-ch18-vep-fm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_5/p5--cellular-context.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Cellular Context</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch19-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">RNA Structure and Function</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch20-single-cell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Single-Cell Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch21-3d-genome.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">3D Genome Organization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch22-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Graph and Network Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_5/p5-ch23-multi-omics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Multi-Omics Integration</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_6/p6--responsible-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI: Responsible Deployment</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch24-uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Uncertainty Quantification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch25-interpretability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Interpretability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch26-causal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Causality</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_6/p6-ch27-regulatory.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Regulatory and Governance</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../part_7/p7--applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VII: Applications &amp; Frontiers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch28-clinical-risk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch29-rare-disease.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Rare Disease Diagnosis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch30-drug-discovery.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Drug Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch31-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../part_7/p7-ch32-frontiers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Frontiers and Synthesis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../bib/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-b-compute.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Deployment and Compute</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-c-data-curation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Data Curation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-d-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Model Reference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-e-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../appendix/app-f-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-ch10-peft" id="toc-sec-ch10-peft" class="nav-link active" data-scroll-target="#sec-ch10-peft"><span class="header-section-number">10.1</span> Parameter-Efficient Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#sec-ch10-lora" id="toc-sec-ch10-lora" class="nav-link" data-scroll-target="#sec-ch10-lora"><span class="header-section-number">10.1.1</span> Low-Rank Adaptation</a></li>
  <li><a href="#sec-ch10-lora-config" id="toc-sec-ch10-lora-config" class="nav-link" data-scroll-target="#sec-ch10-lora-config"><span class="header-section-number">10.1.2</span> Configuring Low-Rank Adaptation</a></li>
  </ul></li>
  <li><a href="#sec-ch10-layer-selection" id="toc-sec-ch10-layer-selection" class="nav-link" data-scroll-target="#sec-ch10-layer-selection"><span class="header-section-number">10.2</span> Layer Selection for Embedding Extraction</a>
  <ul class="collapse">
  <li><a href="#the-encoder-advantage" id="toc-the-encoder-advantage" class="nav-link" data-scroll-target="#the-encoder-advantage"><span class="header-section-number">10.2.1</span> The Encoder Advantage</a></li>
  <li><a href="#the-decoder-dilemma" id="toc-the-decoder-dilemma" class="nav-link" data-scroll-target="#the-decoder-dilemma"><span class="header-section-number">10.2.2</span> The Decoder Dilemma</a></li>
  <li><a href="#practical-consequences" id="toc-practical-consequences" class="nav-link" data-scroll-target="#practical-consequences"><span class="header-section-number">10.2.3</span> Practical Consequences</a></li>
  <li><a href="#layer-averaging-and-weighted-combinations" id="toc-layer-averaging-and-weighted-combinations" class="nav-link" data-scroll-target="#layer-averaging-and-weighted-combinations"><span class="header-section-number">10.2.4</span> Layer Averaging and Weighted Combinations</a></li>
  <li><a href="#systematic-layer-probing" id="toc-systematic-layer-probing" class="nav-link" data-scroll-target="#systematic-layer-probing"><span class="header-section-number">10.2.5</span> Systematic Layer Probing</a></li>
  <li><a href="#implications-for-model-selection" id="toc-implications-for-model-selection" class="nav-link" data-scroll-target="#implications-for-model-selection"><span class="header-section-number">10.2.6</span> Implications for Model Selection</a></li>
  <li><a href="#cross-reference-to-pretraining-objectives" id="toc-cross-reference-to-pretraining-objectives" class="nav-link" data-scroll-target="#cross-reference-to-pretraining-objectives"><span class="header-section-number">10.2.7</span> Cross-Reference to Pretraining Objectives</a></li>
  </ul></li>
  <li><a href="#sec-ch10-full-finetuning" id="toc-sec-ch10-full-finetuning" class="nav-link" data-scroll-target="#sec-ch10-full-finetuning"><span class="header-section-number">10.3</span> Full Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#sec-ch10-full-finetuning-practice" id="toc-sec-ch10-full-finetuning-practice" class="nav-link" data-scroll-target="#sec-ch10-full-finetuning-practice"><span class="header-section-number">10.3.1</span> Making Full Fine-Tuning Work</a></li>
  <li><a href="#sec-ch10-cls-token" id="toc-sec-ch10-cls-token" class="nav-link" data-scroll-target="#sec-ch10-cls-token"><span class="header-section-number">10.3.2</span> The <code>[CLS]</code> Token and Sequence Aggregation</a></li>
  <li><a href="#mean-pooling-and-alternatives" id="toc-mean-pooling-and-alternatives" class="nav-link" data-scroll-target="#mean-pooling-and-alternatives"><span class="header-section-number">10.3.3</span> Mean Pooling and Alternatives</a></li>
  <li><a href="#practical-considerations-for-genomic-sequences" id="toc-practical-considerations-for-genomic-sequences" class="nav-link" data-scroll-target="#practical-considerations-for-genomic-sequences"><span class="header-section-number">10.3.4</span> Practical Considerations for Genomic Sequences</a></li>
  </ul></li>
  <li><a href="#sec-ch10-choosing-strategy" id="toc-sec-ch10-choosing-strategy" class="nav-link" data-scroll-target="#sec-ch10-choosing-strategy"><span class="header-section-number">10.4</span> Choosing an Adaptation Strategy</a></li>
  <li><a href="#sec-ch10-domain-shift" id="toc-sec-ch10-domain-shift" class="nav-link" data-scroll-target="#sec-ch10-domain-shift"><span class="header-section-number">10.5</span> Domain Shift and Cross-Context Transfer</a>
  <ul class="collapse">
  <li><a href="#sec-ch10-domain-shift-types" id="toc-sec-ch10-domain-shift-types" class="nav-link" data-scroll-target="#sec-ch10-domain-shift-types"><span class="header-section-number">10.5.1</span> Types of Domain Shift in Genomics</a></li>
  <li><a href="#sec-ch10-detecting-shift" id="toc-sec-ch10-detecting-shift" class="nav-link" data-scroll-target="#sec-ch10-detecting-shift"><span class="header-section-number">10.5.2</span> Detecting and Mitigating Shift</a></li>
  </ul></li>
  <li><a href="#sec-ch10-minimal-data" id="toc-sec-ch10-minimal-data" class="nav-link" data-scroll-target="#sec-ch10-minimal-data"><span class="header-section-number">10.6</span> Minimal-Data and Emerging Transfer Paradigms</a>
  <ul class="collapse">
  <li><a href="#sec-ch10-few-shot" id="toc-sec-ch10-few-shot" class="nav-link" data-scroll-target="#sec-ch10-few-shot"><span class="header-section-number">10.6.1</span> Few-Shot Learning with Minimal Examples</a></li>
  <li><a href="#sec-ch10-zero-shot" id="toc-sec-ch10-zero-shot" class="nav-link" data-scroll-target="#sec-ch10-zero-shot"><span class="header-section-number">10.6.2</span> Zero-Shot Transfer Without Task-Specific Data</a></li>
  <li><a href="#sec-ch10-emerging-approaches" id="toc-sec-ch10-emerging-approaches" class="nav-link" data-scroll-target="#sec-ch10-emerging-approaches"><span class="header-section-number">10.6.3</span> Emerging Approaches</a></li>
  <li><a href="#sec-ch10-theory" id="toc-sec-ch10-theory" class="nav-link" data-scroll-target="#sec-ch10-theory"><span class="header-section-number">10.6.4</span> Toward Theoretical Foundations</a></li>
  </ul></li>
  <li><a href="#sec-ch10-label-imbalance" id="toc-sec-ch10-label-imbalance" class="nav-link" data-scroll-target="#sec-ch10-label-imbalance"><span class="header-section-number">10.7</span> Label and Class Imbalance</a>
  <ul class="collapse">
  <li><a href="#manifestations-during-transfer" id="toc-manifestations-during-transfer" class="nav-link" data-scroll-target="#manifestations-during-transfer"><span class="header-section-number">10.7.1</span> Manifestations During Transfer</a></li>
  <li><a href="#mitigation-strategies" id="toc-mitigation-strategies" class="nav-link" data-scroll-target="#mitigation-strategies"><span class="header-section-number">10.7.2</span> Mitigation Strategies</a></li>
  <li><a href="#evaluation-under-imbalance" id="toc-evaluation-under-imbalance" class="nav-link" data-scroll-target="#evaluation-under-imbalance"><span class="header-section-number">10.7.3</span> Evaluation Under Imbalance</a></li>
  <li><a href="#imbalance-as-fundamental-constraint" id="toc-imbalance-as-fundamental-constraint" class="nav-link" data-scroll-target="#imbalance-as-fundamental-constraint"><span class="header-section-number">10.7.4</span> Imbalance as Fundamental Constraint</a></li>
  </ul></li>
  <li><a href="#sec-ch10-diagnosing-transfer" id="toc-sec-ch10-diagnosing-transfer" class="nav-link" data-scroll-target="#sec-ch10-diagnosing-transfer"><span class="header-section-number">10.8</span> Diagnosing Transfer: Validation and Failure Modes</a>
  <ul class="collapse">
  <li><a href="#sec-ch10-negative-transfer" id="toc-sec-ch10-negative-transfer" class="nav-link" data-scroll-target="#sec-ch10-negative-transfer"><span class="header-section-number">10.8.1</span> Diagnosing Negative Transfer</a></li>
  <li><a href="#sec-ch10-validation-pitfalls" id="toc-sec-ch10-validation-pitfalls" class="nav-link" data-scroll-target="#sec-ch10-validation-pitfalls"><span class="header-section-number">10.8.2</span> Validation and Common Pitfalls</a></li>
  <li><a href="#sec-ch10-spurious-success" id="toc-sec-ch10-spurious-success" class="nav-link" data-scroll-target="#sec-ch10-spurious-success"><span class="header-section-number">10.8.3</span> Sources of Spurious Success</a></li>
  </ul></li>
  <li><a href="#sec-ch10-case-studies" id="toc-sec-ch10-case-studies" class="nav-link" data-scroll-target="#sec-ch10-case-studies"><span class="header-section-number">10.9</span> Case Studies in Transfer Learning</a>
  <ul class="collapse">
  <li><a href="#sec-ch10-case-success" id="toc-sec-ch10-case-success" class="nav-link" data-scroll-target="#sec-ch10-case-success"><span class="header-section-number">10.9.1</span> Successful Transfer: Alignment Between Pretraining and Task</a></li>
  <li><a href="#sec-ch10-case-failure" id="toc-sec-ch10-case-failure" class="nav-link" data-scroll-target="#sec-ch10-case-failure"><span class="header-section-number">10.9.2</span> When Transfer Fails: Cross-Species Prediction</a></li>
  </ul></li>
  <li><a href="#sec-ch10-conclusion" id="toc-sec-ch10-conclusion" class="nav-link" data-scroll-target="#sec-ch10-conclusion"><span class="header-section-number">10.10</span> What Transfers, What Breaks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../part_3/p3--learning.html">Part III: Learning &amp; Evaluation</a></li><li class="breadcrumb-item"><a href="../part_3/p3-ch10-adaptation.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ch10-adaptation" class="quarto-section-identifier"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Adaptation Strategies</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>The question is not whether to adapt, but how much.</p>
</blockquote>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Overview
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Estimated reading time:</strong> 35-45 minutes</p>
<p><strong>Prerequisites:</strong> Before reading this chapter, you should be familiar with:</p>
<ul>
<li>Transformer architecture fundamentals (<a href="../part_2/p2-ch07-attention.html" class="quarto-xref"><span>Chapter 7</span></a>)</li>
<li>Pretraining objectives: masked language modeling vs.&nbsp;next-token prediction (<a href="p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>)</li>
<li>The distinction between encoder and decoder architectures (<a href="../appendix/app-a-dl.html#sec-apx-a-attention" class="quarto-xref"><span>Section A.5</span></a>)</li>
<li>Basic concepts of overfitting and regularization (<a href="../appendix/app-a-dl.html#sec-apx-a-training" class="quarto-xref"><span>Section A.2</span></a>)</li>
</ul>
<p><strong>Learning objectives:</strong> After completing this chapter, you will be able to:</p>
<ol type="1">
<li>Select appropriate adaptation strategies (frozen features, LoRA, full fine-tuning) based on data availability and task requirements</li>
<li>Diagnose and address the layer hunting problem when using decoder models</li>
<li>Choose between <code>[CLS]</code> token and mean pooling for sequence aggregation</li>
<li>Recognize and mitigate domain shift across species, tissues, and populations</li>
<li>Apply strategies for handling extreme class imbalance in variant classification</li>
<li>Validate transfer learning claims and detect common pitfalls</li>
</ol>
<p><strong>Why This Matters:</strong> When you deploy a foundation model to a new clinical population, tissue type, or prediction task, “frozen features” may underperform while “full fine-tuning” may catastrophically overfit. Knowing how to adapt models efficiently is the difference between deploying a useful tool and wasting months on failed experiments. The skills in this chapter apply to every clinical genomics project that builds on pretrained models, which, increasingly, means every project.</p>
</div>
</div>
<p>A research hospital developing tissue-specific expression predictors faces an impossible choice. Frozen features from <em>Enformer</em> provide reasonable baselines, but full fine-tuning for each of fifty tissue types would require months of GPU time and risk overfitting the thousands of tissue-specific training examples. The team needs an intermediate approach: enough flexibility to improve over frozen features, enough constraint to prevent overfitting, enough efficiency to iterate across dozens of tissues.</p>
<section id="sec-ch10-peft" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="sec-ch10-peft"><span class="header-section-number">10.1</span> Parameter-Efficient Fine-Tuning</h2>
<section id="sec-ch10-lora" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="sec-ch10-lora"><span class="header-section-number">10.1.1</span> Low-Rank Adaptation</h3>
<p><strong>Low-Rank Adaptation (LoRA)</strong> has emerged as the dominant PEFT technique in genomic applications because it directly operationalizes this insight. Think of it like adding a correction lens to a camera rather than rebuilding the optics from scratch: the original lens (pretrained weights) stays intact, while a small, carefully shaped addition adjusts the focus for your specific purpose. Rather than updating a large weight matrix <span class="math inline">\(W\)</span> directly, LoRA introduces two smaller matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> whose product approximates the desired weight change: <span class="math inline">\(W' = W + BA\)</span> <span class="citation" data-cites="hu_lora_2021">(<a href="../bib/references.html#ref-hu_lora_2021" role="doc-biblioref">Hu et al. 2021</a>)</span>. During fine-tuning, <span class="math inline">\(W\)</span> remains frozen while only <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> receive gradient updates. The rank of these matrices (typically 8 to 64 for genomic models) controls adaptation expressiveness: lower ranks introduce fewer parameters and stronger implicit regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Why Low-Rank Works
</div>
</div>
<div class="callout-body-container callout-body">
<p>The success of LoRA rests on an important empirical finding: task-specific adaptations often lie in a low-dimensional subspace of weight space. You do not need to modify all 500 million parameters to adapt a model for a new tissue type. Instead, a low-rank update (modifying perhaps 2-5 million parameters) captures most of the required adaptation while providing implicit regularization against overfitting. This is analogous to principal component analysis, where a few components often capture most variance in high-dimensional data.</p>
</div>
</div>
<p>The efficiency gains prove substantial. A transformer with 500 million parameters might require updating only 2 to 5 million LoRA parameters (representing the low-rank decompositions applied to attention weight matrices), reducing memory requirements by an order of magnitude compared with full fine-tuning. This efficiency enables training on consumer GPUs for models that would otherwise require specialized infrastructure, and enables systematic hyperparameter search that would be prohibitive with full parameter updates. Zhou et al.&nbsp;demonstrated that LoRA adapters on <em>Nucleotide Transformer</em> enable tissue-specific chromatin accessibility prediction, where separate low-rank matrices capture tissue-specific regulatory patterns while the pretrained backbone encodes general sequence understanding <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="../bib/references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. Clinical applications of parameter-efficient fine-tuning for risk prediction appear in <a href="../part_7/p7-ch28-clinical-risk.html" class="quarto-xref"><span>Chapter 28</span></a>.</p>
<div id="fig-lora-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lora-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/01-fig-lora-architecture.svg" class="img-fluid figure-img"></p>
<figcaption>Low-Rank Adaptation (LoRA) architecture</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lora-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.1: Low-Rank Adaptation (LoRA) architecture. Rather than updating the full weight matrix W directly, LoRA introduces two smaller matrices A and B whose product approximates the desired weight change: W’ = W + BA. During fine-tuning, W remains frozen (gray, 500 million parameters typical) while only A and B receive gradient updates (blue, 2-5 million parameters). The rank r (typically 8-64) controls adaptation expressiveness: lower ranks introduce fewer parameters and stronger implicit regularization; higher ranks enable more flexible task-specific modification at greater overfitting risk.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch10-lora-config" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="sec-ch10-lora-config"><span class="header-section-number">10.1.2</span> Configuring Low-Rank Adaptation</h3>
<p>Selecting LoRA hyperparameters requires balancing expressiveness against overfitting risk, with optimal choices depending on task alignment and available data. The rank parameter controls how many dimensions of modification are possible. Ranks of 4 to 16 typically suffice for tasks closely aligned with pretraining objectives, where small perturbations to pretrained weights capture the required adaptation. When target tasks diverge more substantially from pretraining, ranks of 32 to 64 may prove necessary, though higher ranks approach the parameter count where full fine-tuning becomes competitive. Empirical comparison across ranks on held-out validation data remains the most reliable selection method; theoretical guidance for optimal rank given task characteristics does not yet exist.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about layer selection, consider: if you had to choose between adapting (a) only the first few layers, (b) only the middle layers, or (c) only the final layers of a pretrained transformer for a new classification task, which would you choose and why? How might your answer differ for encoder vs.&nbsp;decoder models?</p>
</div>
</div>
<p>The question of which layers to adapt depends critically on whether the foundation model uses encoder or decoder architecture. Encoder models like <em>DNABERT</em> and <em>Nucleotide Transformer</em> process entire sequences bidirectionally, building representations that integrate context from both directions at every layer. For these models, middle and later layers typically encode the most task-relevant features: early layers capture local sequence patterns (motifs, k-mer statistics) while deeper layers integrate these into higher-order representations (see <a href="p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a> for discussion of layer-wise representation learning). Adapting only the final third of transformer layers often achieves most of the performance gain at a fraction of the parameter cost. Linear probing experiments across layers can identify where task-relevant information concentrates before committing to adapter placement.</p>
<p>Decoder models like <em>HyenaDNA</em> in autoregressive mode and GPT-style genomic models present different considerations. These architectures process sequences left-to-right, with each position attending only to preceding context. The causal attention mask means that later layers have seen more integrated context, but the unidirectional flow creates different feature hierarchies than bidirectional encoders. For decoder models, adapting attention layers proves particularly important because the causal structure means attention patterns determine what contextual information flows forward. Practitioners often find that adapting both attention projections (queries, keys, values, and output) and feed-forward layers in decoder models yields better results than attention-only adaptation that works well for encoders.</p>
<p>Within layers, LoRA can be applied to query, key, value, and output projection matrices in attention, and to the two weight matrices in feed-forward blocks. Attention weight adaptation alone often suffices for encoder models on classification tasks, where the key adaptation involves changing what information the model attends to. Feed-forward adaptation becomes more important when the required transformation involves learning new feature combinations rather than reweighting existing attention patterns. When computational budget permits, adapting all weight matrices with lower rank often outperforms adapting fewer matrices with higher rank.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: LoRA Configuration Checklist
</div>
</div>
<div class="callout-body-container callout-body">
<p>When configuring LoRA for a new task:</p>
<ol type="1">
<li><strong>Start with rank 8-16</strong> for tasks similar to pretraining; increase to 32-64 if validation performance plateaus</li>
<li><strong>For encoder models:</strong> adapt final third of layers; prioritize attention weights</li>
<li><strong>For decoder models:</strong> adapt attention <em>and</em> feed-forward weights; expect to search across layers</li>
<li><strong>Always validate:</strong> compare against frozen features baseline and from-scratch training on identical data</li>
<li><strong>Monitor for overfitting:</strong> use early stopping based on validation loss, not training loss</li>
</ol>
</div>
</div>
<p>These heuristics provide starting points, not guarantees. The interaction between model architecture, pretraining objective, target task, and available data creates a combinatorial space that resists simple rules. Systematic hyperparameter search over rank, layer selection, and weight matrix targeting, guided by validation performance on data matching the deployment distribution, remains the most reliable path to effective adaptation.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Worked Example: Configuring LoRA for Tissue-Specific Prediction">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Worked Example: Configuring LoRA for Tissue-Specific Prediction
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider adapting <em>Nucleotide Transformer</em> (500M parameters) for predicting chromatin accessibility in liver tissue, with 2,000 labeled training examples.</p>
<p><strong>Step 1: Choose initial rank.</strong> With moderate data (2,000 examples) and a task similar to pretraining (sequence to chromatin state), start with rank 16.</p>
<p><strong>Step 2: Select layers.</strong> <em>Nucleotide Transformer</em> is an encoder model with 24 layers. Apply LoRA to layers 17-24 (final third), prioritizing attention weights (Q, K, V projections).</p>
<p><strong>Step 3: Count parameters.</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 30%">
<col style="width: 36%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Calculation</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Original attention (per layer)</td>
<td><span class="math inline">\(4 \times d \times d = 4 \times 1024 \times 1024\)</span></td>
<td>4.2M</td>
</tr>
<tr class="even">
<td>LoRA matrices (per layer)</td>
<td><span class="math inline">\(4 \times (d \times r + r \times d) = 4 \times 2 \times 1024 \times 16\)</span></td>
<td>131K</td>
</tr>
<tr class="odd">
<td>8 adapted layers</td>
<td><span class="math inline">\(8 \times 131K\)</span></td>
<td>~1M trainable</td>
</tr>
</tbody>
</table>
<p>Result: ~1M trainable parameters vs.&nbsp;500M frozen, a 500-fold reduction.</p>
<p><strong>Step 4: Validate.</strong> Compare against frozen features baseline. If validation performance plateaus, try rank 32 or extend to feed-forward weights. If performance matches frozen features, the task may not require adaptation beyond linear probing.</p>
</div>
</div>
</section>
</section>
<section id="sec-ch10-layer-selection" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="sec-ch10-layer-selection"><span class="header-section-number">10.2</span> Layer Selection for Embedding Extraction</h2>
<p>A research team attempting to use <em>HyenaDNA</em> for splice site classification discovers an unexpected problem. Following standard practice from encoder models, they extract embeddings from the final transformer layer and train a linear classifier. Performance barely exceeds random guessing. Frustrated, they try layer 6 of 12 on a hunch and accuracy jumps by 15 percentage points. Layer 4 performs better still for their particular task. The team has stumbled onto a systematic challenge that distinguishes decoder-based foundation models from their encoder counterparts: the optimal layer for embedding extraction varies dramatically by task, and the final layer is often the worst choice.</p>
<p>This phenomenon, sometimes called the <strong>layer hunting problem</strong>, arises from a fundamental asymmetry between how encoder and decoder models are trained. Encoder models like <em>DNABERT</em> and <em>Nucleotide Transformer</em> are optimized to produce representations useful for reconstructing masked tokens from bidirectional context. Every layer contributes to this reconstruction, and the final layer aggregates information specifically designed to support prediction. The <code>[CLS]</code> token or mean-pooled final layer representations work reliably across diverse downstream tasks because the pretraining objective directly shaped these representations for general utility.</p>
<p>Decoder models face a different optimization pressure. The next-token prediction objective trains the final layer specifically to predict vocabulary distributions over the next token. This specialization is precisely what enables fluent generation, but it creates representations optimized for a narrow purpose rather than general-purpose embeddings. The final layer learns to transform rich intermediate representations into the specific format needed for token prediction, discarding information irrelevant to that task but potentially critical for downstream classification or regression.</p>
<section id="the-encoder-advantage" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="the-encoder-advantage"><span class="header-section-number">10.2.1</span> The Encoder Advantage</h3>
<p>Jawahar et al. <span class="citation" data-cites="jawahar_what_2019">(<a href="../bib/references.html#ref-jawahar_what_2019" role="doc-biblioref">Jawahar, Sagot, and Seddah 2019</a>)</span> demonstrated that <em>BERT</em> develops an interpretable layer hierarchy: lower layers encode surface features (word length, capitalization), middle layers capture syntactic structure (constituency, dependency relations), and upper layers represent semantic content (coreference, semantic roles). This progression means practitioners can make principled choices about layer selection based on task requirements. Tasks requiring syntactic understanding benefit from middle layers; tasks requiring semantic similarity benefit from upper layers. Crucially, the final layer remains a reasonable default because it integrates information from all levels while retaining semantic content useful for most applications.</p>
<p>The bidirectional attention mechanism ensures that every position’s representation incorporates information from the entire sequence at every layer. A nucleotide’s representation in layer 12 reflects constraints from both upstream promoter elements and downstream coding sequence. This global integration makes encoder representations naturally suited for tasks where context on both sides matters, which describes most genomic classification problems. Variant effect prediction, transcription factor binding, and splice site recognition all benefit from knowing what lies both before and after the position of interest.</p>
<p>Encoder models also exhibit relatively stable layer-wise performance for frozen feature extraction. While middle layers sometimes outperform final layers for specific tasks, the differences are typically modest (a few percentage points) and the final layer rarely fails catastrophically. Practitioners can extract final-layer embeddings with reasonable confidence that performance will be competitive, reserving layer search for optimization rather than treating it as a requirement for basic functionality.</p>
</section>
<section id="the-decoder-dilemma" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="the-decoder-dilemma"><span class="header-section-number">10.2.2</span> The Decoder Dilemma</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Difficulty Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section explains why decoder models require careful layer selection for classification tasks. The underlying concepts involve understanding how causal attention and next-token prediction objectives shape representations differently than bidirectional encoders. If you are less familiar with these architectural differences, consider reviewing <a href="../appendix/app-a-dl.html#sec-apx-a-attention" class="quarto-xref"><span>Section A.5</span></a> before proceeding.</p>
</div>
</div>
<p>Decoder models trained with causal attention create fundamentally different representation hierarchies. Each layer can only integrate information from preceding positions due to the causal mask. Position 500 in a 1000-token sequence has rich representations of the first 499 tokens but no information about the following 500. This asymmetry propagates through layers, creating representations that emphasize historical context over global sequence properties.</p>
<p>The next-token prediction objective compounds this asymmetry by specializing the final layers for a specific output format. Consider what the final layer must learn: transform the current hidden state into a probability distribution over vocabulary tokens. This transformation discards information about the input sequence that is irrelevant for predicting the immediate next token. Evolutionary conservation patterns 200 positions upstream, motif co-occurrence statistics, and global sequence composition may all inform intermediate representations but contribute nothing to next-token prediction and can be safely discarded by the final layer.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>A decoder model has 12 layers. You are using it for promoter classification (binary: promoter vs.&nbsp;non-promoter). Based on the layer hunting problem, which layers would you expect to perform best for this task, and why? What would you predict about the final layer’s performance?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>You would expect intermediate layers (roughly layers 4-8, the middle third of the network) to perform best for promoter classification. These layers contain general-purpose sequence representations before they are transformed into next-token predictions. The final layer would likely perform poorly because it has been specialized to produce token probability distributions, discarding information about global sequence properties like “is this a promoter?” that are irrelevant for predicting the immediate next token but important for classification.</p>
</div>
</div>
</div>
</div>
</div>
<p>Empirically, practitioners using decoder models for classification consistently find that intermediate layers outperform final layers, often dramatically. For <em>HyenaDNA</em> on regulatory element classification, layers in the middle third of the network frequently achieve the best linear probing accuracy. For GPT-style genomic models, the optimal layer can vary by 30-50% of network depth depending on the specific downstream task. A splice site classifier might perform best with layer 4 representations while a promoter classifier using the same model achieves optimal performance at layer 8. The task-dependence of optimal layer selection adds a hyperparameter dimension that does not exist for encoder models.</p>
<p>The following table summarizes how encoder and decoder architectures differ for embedding extraction:</p>
<div id="tbl-encoder-decoder-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-encoder-decoder-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10.1: Encoder vs.&nbsp;Decoder Models for Embedding Extraction
</figcaption>
<div aria-describedby="tbl-encoder-decoder-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 38%">
<col style="width: 38%">
</colgroup>
<thead>
<tr class="header">
<th>Property</th>
<th>Encoder Models</th>
<th>Decoder Models</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Attention pattern</strong></td>
<td>Bidirectional (all positions see all positions)</td>
<td>Causal (positions see only preceding context)</td>
</tr>
<tr class="even">
<td><strong>Pretraining objective</strong></td>
<td>Masked token reconstruction</td>
<td>Next-token prediction</td>
</tr>
<tr class="odd">
<td><strong>Final layer purpose</strong></td>
<td>General-purpose representations</td>
<td>Token probability distribution</td>
</tr>
<tr class="even">
<td><strong>Best layer for classification</strong></td>
<td>Final layer (usually)</td>
<td>Middle layers (varies by task)</td>
</tr>
<tr class="odd">
<td><strong>Layer search required?</strong></td>
<td>Rarely; final layer works well</td>
<td>Yes; performance varies 15-30% across layers</td>
</tr>
<tr class="even">
<td><strong>Example models</strong></td>
<td>DNABERT, Nucleotide Transformer, ESM</td>
<td>HyenaDNA (autoregressive), GPT-style genomic models</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="practical-consequences" class="level3" data-number="10.2.3">
<h3 data-number="10.2.3" class="anchored" data-anchor-id="practical-consequences"><span class="header-section-number">10.2.3</span> Practical Consequences</h3>
<p>The layer hunting problem creates concrete challenges for deploying decoder-based foundation models. First, it increases computational cost: practitioners must evaluate downstream performance across all layers (or a representative subset) before committing to an adaptation strategy. A 12-layer model requires 12 separate linear probing experiments rather than one. Second, it complicates model comparison: reporting results from the best layer for each model can obscure whether the improvement comes from the model or from more thorough hyperparameter search. Third, it limits reproducibility: papers that report only final-layer performance for decoder models may dramatically underestimate achievable accuracy, while papers that report best-layer performance without specifying the layer make replication difficult.</p>
<p>The problem intensifies when decoder models grow deeper. A 12-layer model has 12 candidate extraction points; a 48-layer model has 48. The search space grows linearly with depth, and there is no theoretical guidance for narrowing the search a priori. Heuristics like “try middle layers first” help but do not eliminate the need for empirical validation on each new task.</p>
</section>
<section id="layer-averaging-and-weighted-combinations" class="level3" data-number="10.2.4">
<h3 data-number="10.2.4" class="anchored" data-anchor-id="layer-averaging-and-weighted-combinations"><span class="header-section-number">10.2.4</span> Layer Averaging and Weighted Combinations</h3>
<p>Several strategies address the layer hunting problem without exhaustive search. <strong>Layer averaging</strong> computes embeddings as the mean across all layers (or a subset), combining information from different levels of abstraction. This approach works surprisingly well in practice because it captures both syntactic features from early layers and more abstract features from later layers. The cost is that averaging dilutes task-specific signal present in particular layers, sometimes underperforming the optimal single layer by several percentage points.</p>
<p><strong>Weighted layer combinations</strong> learn task-specific weights for each layer’s contribution to the final embedding. Given layer representations <span class="math inline">\(h_1, h_2, \ldots, h_L\)</span>, the combined representation is <span class="math inline">\(h = \sum_{l=1}^{L} \alpha_l h_l\)</span> where <span class="math inline">\(\alpha_l\)</span> are learned weights (often softmax-normalized to sum to one). Why learn weights rather than simply averaging? Different tasks extract different types of information from pretrained models. A splice site classifier needs the local motif patterns strongly encoded in early layers; an enhancer-promoter association task needs the integrated contextual features from deeper layers. Uniform averaging dilutes task-relevant signal by mixing it with irrelevant representations. Learned weights allow the model to emphasize the layers where task-relevant information concentrates while suppressing layers that add noise. This approach was popularized by ELMo <span class="citation" data-cites="peters_deep_2018">(<a href="../bib/references.html#ref-peters_deep_2018" role="doc-biblioref">Peters et al. 2018</a>)</span> and remains effective for foundation model adaptation. The weights themselves become informative: high weights on early layers suggest the task relies on surface features; high weights on late-middle layers suggest reliance on contextual integration.</p>
<p>Learned layer weights add minimal parameters (one scalar per layer) while substantially reducing the manual hyperparameter search. The weights can be trained jointly with the downstream classifier using the same labeled data, requiring no additional supervision. For decoder models where optimal layer varies by task, learned combinations often match or exceed single-layer performance while eliminating the need to identify the optimal layer through trial and error.</p>
</section>
<section id="systematic-layer-probing" class="level3" data-number="10.2.5">
<h3 data-number="10.2.5" class="anchored" data-anchor-id="systematic-layer-probing"><span class="header-section-number">10.2.5</span> Systematic Layer Probing</h3>
<p>When using decoder models for transfer learning, systematic layer probing should precede any adaptation strategy that depends on embedding quality. The procedure is straightforward: extract representations from each layer for the downstream task’s training data, train identical lightweight classifiers (linear or shallow MLP) on each layer’s representations, and evaluate on held-out validation data. The layer achieving the best validation performance indicates where task-relevant information concentrates.</p>
<p>This probing step reveals not just the optimal layer but the shape of performance across layers. A sharp peak suggests highly localized task-relevant features; broad performance across middle layers suggests distributed representation. Monotonically increasing performance toward middle layers (then decreasing toward the final layer) is the typical pattern for decoder models on classification tasks. Anomalous patterns (best performance at layer 1, or best performance at the final layer) warrant investigation: they may indicate task-pretraining alignment issues or data quality problems.</p>
<p>For genomic models specifically, probing results often correlate with task properties. Tasks requiring recognition of local sequence motifs (transcription factor binding) show best performance in earlier layers where positional patterns are most directly encoded. Tasks requiring integration of broader context (enhancer-promoter association, long-range regulatory effects) show best performance in deeper middle layers where more context has been accumulated through the causal attention stack. Tasks most aligned with the next-token prediction objective (predicting the next nucleotide) show best performance in later layers, as expected.</p>
<div id="fig-layer-probing-decoder" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-layer-probing-decoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/02-A-fig-layer-probing-decoder.svg" class="img-fluid figure-img"></p>
<figcaption>Encoder model layer-wise performance</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/02-B-fig-layer-probing-decoder.svg" class="img-fluid figure-img"></p>
<figcaption>Decoder model layer-wise performance</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-layer-probing-decoder-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.2: Layer-wise probing reveals fundamental differences between encoder and decoder architectures. (A) Encoder models like DNABERT show relatively stable performance across layers, with the final layer providing reliable representations for classification. The bidirectional attention mechanism integrates information from the entire sequence at every layer. (B) Decoder models like HyenaDNA show an inverted-U pattern, with peak performance at intermediate layers and degraded performance at the final layer. The next-token prediction objective specializes final layers for a narrow purpose, creating what practitioners call the “layer hunting problem”: optimal embedding extraction requires systematic search across layers rather than defaulting to final-layer representations.
</figcaption>
</figure>
</div>
</section>
<section id="implications-for-model-selection" class="level3" data-number="10.2.6">
<h3 data-number="10.2.6" class="anchored" data-anchor-id="implications-for-model-selection"><span class="header-section-number">10.2.6</span> Implications for Model Selection</h3>
<p>The layer hunting problem should inform model architecture selection, not just adaptation strategy. When downstream applications primarily involve classification, regression, or embedding-based retrieval (most clinical genomics applications), encoder architectures offer practical advantages beyond their representational benefits. The reliable performance of final-layer embeddings simplifies deployment pipelines, reduces hyperparameter search burden, and improves reproducibility. The bidirectional context that encoders provide aligns naturally with variant interpretation, where surrounding sequence on both sides determines functional impact.</p>
<p>Decoder architectures remain essential when generation is the primary goal: designing novel regulatory sequences, sampling protein variants, or producing synthetic training data. For these applications, the final layer’s specialization for next-token prediction is a feature rather than a bug. Hybrid strategies that use decoder models for generation but encoder models (or carefully selected decoder layers) for classification can capture benefits of both architectures, though at the cost of maintaining multiple models.</p>
<p>When decoder models must be used for classification (perhaps because they offer superior long-context handling or because they are the only available pretrained model for a particular sequence type), the layer hunting cost should be budgeted explicitly. Plan for layer-wise probing experiments. Consider learned layer weighting from the start. Report which layer produced reported results, and consider reporting performance across layers to enable fair comparison with future work.</p>
</section>
<section id="cross-reference-to-pretraining-objectives" class="level3" data-number="10.2.7">
<h3 data-number="10.2.7" class="anchored" data-anchor-id="cross-reference-to-pretraining-objectives"><span class="header-section-number">10.2.7</span> Cross-Reference to Pretraining Objectives</h3>
<p>The layer hunting problem is a direct consequence of pretraining objective choice, connecting this practical deployment consideration back to the foundational decisions examined in <a href="p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>. Masked language modeling trains all layers to support bidirectional context integration, producing representations useful for diverse downstream tasks throughout the network. Next-token prediction trains final layers for a specific output format, creating the representation collapse that makes layer search necessary. Understanding this connection helps practitioners anticipate adaptation challenges before committing to a foundation model: if your downstream tasks are primarily predictive rather than generative, the reliable final-layer embeddings of encoder models may outweigh other architectural considerations.</p>
</section>
</section>
<section id="sec-ch10-full-finetuning" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="sec-ch10-full-finetuning"><span class="header-section-number">10.3</span> Full Fine-Tuning</h2>
<p>When Avsec et al.&nbsp;sought to predict gene expression from sequence across hundreds of cell types, they required a model capturing tissue-specific regulatory logic unavailable from any generic pretrained representation <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. With millions of labeled examples spanning thousands of genomic tracks, they could afford to update all model parameters, reshaping internal representations entirely for their specific predictive task. Constrained adaptation would have left tissue-specific regulatory patterns unlearned.</p>
<p>Full fine-tuning offers maximum flexibility: every parameter becomes tunable, enabling the model to learn whatever features the target task requires regardless of pretraining emphasis. This flexibility comes with risks proportional to its power.</p>
<section id="sec-ch10-full-finetuning-practice" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="sec-ch10-full-finetuning-practice"><span class="header-section-number">10.3.1</span> Making Full Fine-Tuning Work</h3>
<p>Full fine-tuning updates all model parameters during adaptation but requires careful attention to optimization dynamics. Learning rates must be substantially lower than during pretraining (often 10 to 100 times smaller) to avoid catastrophically disrupting learned representations in early training steps <span class="citation" data-cites="howard_universal_2018">(<a href="../bib/references.html#ref-howard_universal_2018" role="doc-biblioref">Howard and Ruder 2018</a>)</span>. <strong>Gradual unfreezing</strong>, where top layers update first and deeper layers progressively join training, helps preserve low-level features (local motifs, basic sequence statistics) while allowing high-level task-specific adjustment. Why does unfreezing order matter? Early layers encode fundamental sequence patterns (dinucleotide frequencies, basic motifs, compositional statistics) that are broadly useful across tasks. These patterns are hard to relearn and easy to destroy with large gradient updates. Later layers encode task-specific combinations of these features and are the layers that most need adaptation. By training top layers first while keeping bottom layers frozen, gradual unfreezing allows task-specific learning to proceed while protecting transferable low-level features. When bottom layers eventually unfreeze, the top layers have already learned stable task-specific patterns, producing smaller and more structured gradients that refine rather than overwrite the preserved features. Regularization through weight decay, dropout, and early stopping on validation data prevents overfitting to target datasets.</p>
<p>The approach suits scenarios when labeled datasets are large (tens of thousands of examples or more), when the target task diverges substantially from pretraining such that constrained adaptation proves insufficient, or when performance requirements justify computational investment. <em>Enformer</em> fine-tuning for new chromatin assays requires updating most parameters to capture assay-specific signal characteristics distinct from original training conditions. Expression prediction across novel cell types benefits from full adaptation when sufficient tissue-specific data exists.</p>
<p>The risks of unconstrained adaptation are proportional to its flexibility. <strong>Catastrophic forgetting</strong> occurs when fine-tuning overwrites general knowledge useful for related tasks or out-of-distribution inputs; a model fine-tuned aggressively on lymphocyte data may lose performance on epithelial cells it previously handled well <span class="citation" data-cites="mccloskey_catastrophic_1989">(<a href="../bib/references.html#ref-mccloskey_catastrophic_1989" role="doc-biblioref">McCloskey and Cohen 1989</a>)</span>. Overfitting afflicts small target datasets, where the model memorizes training examples rather than learning generalizable patterns. Computational expense can be prohibitive for models with billions of parameters. When negative transfer occurs (pretraining initialization actually hurts optimization), full fine-tuning may underperform training from scratch despite the additional expense.</p>
<p>The conservative strategy is to start with simpler methods and escalate only when they demonstrably fail. Establish frozen feature baselines first. If frozen features outperform random initialization, try PEFT methods before committing to full fine-tuning. Compare fine-tuned models against properly-tuned from-scratch baselines on the same target data. Monitor for overfitting through validation curves and early stopping. The goal is achieving required performance with minimal adaptation complexity.</p>
</section>
<section id="sec-ch10-cls-token" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="sec-ch10-cls-token"><span class="header-section-number">10.3.2</span> The <code>[CLS]</code> Token and Sequence Aggregation</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a 512-nucleotide sequence processed by a transformer, producing 512 embedding vectors (one per position). If you need to classify the entire sequence as “promoter” or “non-promoter,” how would you convert these 512 vectors into a single prediction? What are the tradeoffs of different approaches?</p>
</div>
</div>
<p>Sequence classification requires a fixed-size representation regardless of input length. A promoter classifier must produce the same prediction format whether the input spans 200 or 2,000 nucleotides. A pathogenicity model must output a single score whether analyzing a 50-residue peptide or a 3,000-residue multidomain protein. Transformers produce per-position representations: for a 512-token input, the model generates 512 embedding vectors, one for each position. Converting these variable-length outputs into fixed-size vectors suitable for classification constitutes the <strong>sequence aggregation</strong> problem.</p>
<p>The <code>[CLS]</code> token provides one solution, introduced in <em>BERT</em> and adopted widely across encoder architectures including <em>DNABERT</em> and <em>Nucleotide Transformer</em> <span class="citation" data-cites="devlin_bert_2019">(<a href="../bib/references.html#ref-devlin_bert_2019" role="doc-biblioref">Devlin et al. 2019</a>)</span>. The approach works like a note-taker sitting at the front of a meeting: this special token attends to everything being said (all sequence positions) and distills the discussion into a summary that captures the essential points. The approach prepends a special classification token to every input sequence before processing. This token participates in attention computations like any other position, attending to all sequence positions and being attended to by them. Unlike content tokens that represent actual nucleotides or amino acids, the <code>[CLS]</code> token carries no intrinsic meaning. Its representation emerges entirely from aggregating information across the sequence through the attention mechanism.</p>
<p>The critical insight is that training shapes the <code>[CLS]</code> representation specifically for sequence-level tasks. During <em>BERT</em>’s pretraining, the <code>[CLS]</code> token was used for next-sentence prediction, requiring it to encode information sufficient to determine whether two sentences were contiguous in the original text. This training pressure transforms the <code>[CLS]</code> position into a natural aggregation point: its final-layer representation captures sequence-level properties distilled from positional representations throughout the network. When <em>DNABERT</em> applies this architecture to genomic sequences, the <code>[CLS]</code> token learns to aggregate regulatory signals, motif patterns, and compositional features into a single vector suitable for downstream classification.</p>
<p>The computational mechanism is straightforward. For an input sequence of <span class="math inline">\(n\)</span> tokens, the model prepends <code>[CLS]</code> to create an <span class="math inline">\((n+1)\)</span>-token input. After processing through all transformer layers, the final representation at position 0 (the <code>[CLS]</code> position) serves as the sequence embedding. A linear classifier or shallow neural network trained on this embedding produces the final prediction. The <code>[CLS]</code> approach adds exactly one token to the input, creating negligible computational overhead while providing a principled aggregation mechanism shaped by pretraining.</p>
<p>The <code>[CLS]</code> token’s effectiveness depends on the pretraining objective. When pretraining includes explicit sequence-level tasks (next-sentence prediction in <em>BERT</em>, similar objectives in some genomic models), the <code>[CLS]</code> representation receives direct training signal to encode sequence properties. When pretraining uses only token-level objectives like masked language modeling, the <code>[CLS]</code> representation is shaped indirectly through its participation in attention. <em>DNABERT</em> used masked language modeling without an explicit sequence-level pretraining task, yet its <code>[CLS]</code> representations still proved effective for downstream classification, suggesting that attention-based aggregation suffices even without task-specific pretraining signal <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>.</p>
</section>
<section id="mean-pooling-and-alternatives" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="mean-pooling-and-alternatives"><span class="header-section-number">10.3.3</span> Mean Pooling and Alternatives</h3>
<p>Mean pooling offers a simpler alternative: average all per-position embeddings to obtain a single sequence representation. For a sequence with token representations <span class="math inline">\(h_1, h_2, \ldots, h_n\)</span>, the pooled representation is simply <span class="math inline">\(\bar{h} = \frac{1}{n}\sum_{i=1}^{n} h_i\)</span>. This approach requires no special tokens, no architectural modifications, and no assumptions about which position aggregates sequence information. Every position contributes equally to the final representation.</p>
<p>Mean pooling often matches or exceeds <code>[CLS]</code> performance for genomic and protein sequences, particularly when pretraining did not include explicit sequence-level objectives <span class="citation" data-cites="naderializadeh_aggregating_2025">(<a href="../bib/references.html#ref-naderializadeh_aggregating_2025" role="doc-biblioref">NaderiAlizadeh and Singh 2025</a>)</span>. The explanation lies in information distribution across positions. In natural language, sentence meaning concentrates in specific tokens: the subject, main verb, and key modifiers carry most semantic content while articles and prepositions contribute less. The <code>[CLS]</code> token can learn to weight positions according to their informativeness. In genomic sequences, relevant information may distribute more uniformly: every nucleotide in a transcription factor binding site contributes to recognition, every residue in a protein domain contributes to function. Mean pooling captures this distributed signal naturally, while <code>[CLS]</code> must learn through attention what mean pooling provides by construction.</p>
<p>Max pooling takes element-wise maxima across positions, capturing the strongest activation for each embedding dimension regardless of where it occurs. For regulatory element classification, max pooling can identify whether any position contains a strong motif signal, potentially outperforming mean pooling when a single strong feature determines the label. The tradeoff is sensitivity to outliers and potential loss of information about feature co-occurrence: max pooling cannot distinguish a sequence with one strong signal from a sequence with many moderate signals.</p>
<p>Attention-based pooling learns to weight positions dynamically, computing attention scores that determine each position’s contribution to the final representation <span class="citation" data-cites="hoang_locality-aware_2025">(<a href="../bib/references.html#ref-hoang_locality-aware_2025" role="doc-biblioref">Hoang and Singh 2025</a>)</span>. This generalizes both mean pooling (uniform weights) and <code>[CLS]</code> aggregation (learned weights through attention to a special token). Attention pooling adds parameters but can capture position-dependent importance when the downstream task requires it. For protein sequences, attention pooling has shown advantages over both <code>[CLS]</code> and mean pooling for tasks where specific regions (active sites, binding interfaces) determine function, allowing the model to focus on relevant positions while downweighting uninformative regions.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: Matching Aggregation to Information Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p>The optimal aggregation strategy depends on how task-relevant information distributes across the sequence:</p>
<ul>
<li><strong>Localized signal</strong> (e.g., a single binding motif determines the label): Max pooling or attention pooling excels</li>
<li><strong>Distributed signal</strong> (e.g., overall sequence composition matters): Mean pooling works well</li>
<li><strong>Learned weighting</strong> (e.g., some positions matter more than others): <code>[CLS]</code> token or attention pooling adapts</li>
</ul>
<p>For genomic sequences, information often distributes more uniformly than in natural language, making mean pooling surprisingly competitive with the more complex <code>[CLS]</code> approach.</p>
</div>
</div>
</section>
<section id="practical-considerations-for-genomic-sequences" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="practical-considerations-for-genomic-sequences"><span class="header-section-number">10.3.4</span> Practical Considerations for Genomic Sequences</h3>
<p>Genomic sequence properties create specific considerations for aggregation strategy choice. Sequences often contain substantial length variation: promoter regions might span hundreds of base pairs while enhancer elements vary from tens to thousands. Mean pooling implicitly normalizes for length (the denominator scales with sequence size), while <code>[CLS]</code> representations can encode absolute length information through attention patterns. For tasks where length itself is informative, this distinction matters.</p>
<p>Repetitive elements present another challenge. Genomic sequences frequently contain Alu elements, LINE repeats, and other repetitive content that may dominate mean-pooled representations simply through their abundance. The <code>[CLS]</code> token can learn to downweight repetitive regions if they are uninformative for the classification task, while mean pooling treats all positions equally regardless of their uniqueness or informativeness.</p>
<p>The choice between <code>[CLS]</code> and mean pooling often comes down to empirical validation on the specific task. For <em>DNABERT</em> applied to chromatin accessibility prediction, Ji et al.&nbsp;found that <code>[CLS]</code> representations achieved strong performance, but subsequent work with other DNA language models has shown comparable or superior results with mean pooling <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="../bib/references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. The <em>Nucleotide Transformer</em> evaluation suite includes comparisons across pooling strategies, generally finding modest differences that vary by task. A pragmatic approach extracts both <code>[CLS]</code> and mean-pooled representations during initial experiments, selecting the better performer for production deployment.</p>
<p>For protein language models, the comparison is similarly equivocal. <em>ESM-2</em> embeddings work well with both <code>[CLS]</code> and mean pooling for most classification tasks. Recent work on optimal transport-based aggregation suggests that both standard approaches lose information present in per-residue representations, motivating more sophisticated aggregation schemes for tasks requiring fine-grained sequence understanding <span class="citation" data-cites="naderializadeh_aggregating_2025">(<a href="../bib/references.html#ref-naderializadeh_aggregating_2025" role="doc-biblioref">NaderiAlizadeh and Singh 2025</a>)</span>. These advanced methods add complexity and computational cost; whether the improvement justifies the overhead depends on task requirements and deployment constraints.</p>
<p>When using decoder models for classification, the aggregation question becomes more complex. Decoder architectures typically lack a <code>[CLS]</code> token because their training objective (next-token prediction) does not require sequence-level representations. The final token’s representation aggregates information from all preceding positions due to causal attention, making it a natural candidate for sequence embedding. Mean pooling over decoder representations faces the asymmetry problem discussed in <a href="#sec-ch10-layer-selection" class="quarto-xref"><span>Section 10.2</span></a>: later positions have richer context than earlier positions, creating systematic bias in averaged representations. Some practitioners mean-pool only the final portion of the sequence where representations have accumulated sufficient context, though optimal truncation points vary by model and task.</p>
<p>The interaction between pooling strategy and layer selection deserves attention. For encoder models, <code>[CLS]</code> and mean pooling both work reasonably well with final-layer representations because the pretraining objective shaped all positions for general utility. For decoder models, the optimal layer for <code>[CLS]</code>-style aggregation (using the final token) may differ from the optimal layer for mean pooling, adding another dimension to the hyperparameter search. When computational budget permits, systematic evaluation across both pooling strategies and layer choices provides the most reliable path to effective transfer.</p>
</section>
</section>
<section id="sec-ch10-choosing-strategy" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="sec-ch10-choosing-strategy"><span class="header-section-number">10.4</span> Choosing an Adaptation Strategy</h2>
<p>The preceding sections described what each adaptation approach does; here we address when to use each. Two factors dominate the decision: how much labeled data exists, and how closely the target task aligns with pretraining objectives. <a href="#fig-adaptation-decision-tree" class="quarto-xref">Figure&nbsp;<span>10.3</span></a> provides a decision framework, but the underlying logic is straightforward.</p>
<p>Data quantity determines what is possible. With fewer than 500 labeled examples, linear probing represents the only viable approach; more complex adaptation overfits catastrophically. Between 500 and 5,000 examples, PEFT methods offer favorable tradeoffs, introducing enough flexibility to improve over frozen features while maintaining implicit regularization. Above 10,000 examples, full fine-tuning becomes viable and may be necessary when target tasks diverge substantially from pretraining. Task similarity determines what is necessary. When targets closely resemble pretraining patterns (predicting transcription factor binding after sequence pretraining), frozen features often suffice. When tasks diverge moderately (tissue-specific expression after genome-wide pretraining), PEFT enables selective adaptation. When tasks fundamentally differ from pretraining (three-dimensional chromatin contacts from sequence-only pretraining), full fine-tuning may be required to learn features the pretraining objective never emphasized. Computational constraints impose practical limits: linear probing requires minutes on CPUs, LoRA requires hours on single GPUs, and full fine-tuning of large models requires days on multiple GPUs.</p>
<p>The following table summarizes the key tradeoffs across adaptation strategies:</p>
<div id="tbl-adaptation-strategies" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-adaptation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10.2: Adaptation Strategy Selection Guide
</figcaption>
<div aria-describedby="tbl-adaptation-strategies-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 22%">
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 19%">
<col style="width: 15%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Trainable Parameters</th>
<th>Data Required</th>
<th>Compute Cost</th>
<th>Overfitting Risk</th>
<th>Best When…</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Linear probing</strong></td>
<td>Task head only (thousands)</td>
<td>&lt; 500 examples</td>
<td>Minutes (CPU)</td>
<td>Low</td>
<td>Task aligns with pretraining; limited data</td>
</tr>
<tr class="even">
<td><strong>LoRA / Adapters</strong></td>
<td>1-5% of model (millions)</td>
<td>500-5,000 examples</td>
<td>Hours (1 GPU)</td>
<td>Moderate</td>
<td>Moderate data; need some flexibility</td>
</tr>
<tr class="odd">
<td><strong>Full fine-tuning</strong></td>
<td>100% of model (billions)</td>
<td>&gt; 10,000 examples</td>
<td>Days (multi-GPU)</td>
<td>High</td>
<td>Large data; task diverges from pretraining</td>
</tr>
<tr class="even">
<td><strong>From scratch</strong></td>
<td>100% of model</td>
<td>&gt; 100,000 examples</td>
<td>Days-weeks</td>
<td>Task-dependent</td>
<td>No suitable pretrained model; abundant data</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>These heuristics indicate which strategies merit trying first, but empirical validation supersedes any rule. No formula reliably predicts which approach will succeed for a specific combination of model, task, and data. The conservative path is to establish frozen feature baselines first, escalate to PEFT when frozen features prove insufficient, and reserve full fine-tuning for cases where simpler methods demonstrably fail and sufficient data exists to justify the risk.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Graduated Practice: Adaptation Strategy Selection">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Graduated Practice: Adaptation Strategy Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p>Work through these scenarios in order. Each builds on the previous, with progressively less scaffolding.</p>
<p><strong>Scenario 1 (Guided):</strong> You have 200 labeled splice site variants and access to <em>Nucleotide Transformer</em> (encoder, 500M parameters). The pretraining objective was masked language modeling on genomic sequences.</p>
<p><em>Scaffolding:</em> Apply the decision framework: (1) Data quantity → 200 &lt; 500, so linear probing. (2) Verify task alignment → splice sites involve local motifs similar to MLM patterns, good alignment. (3) Confirm: linear probing is appropriate.</p>
<p><strong>Scenario 2 (Partial guidance):</strong> You have 3,000 labeled examples for predicting enhancer-promoter interactions spanning 50kb windows. You have access to <em>HyenaDNA</em> (decoder, 1.4B parameters).</p>
<p><em>Hint:</em> Consider data quantity threshold, but also: What does “decoder model” imply for layer selection? What does “50kb windows” suggest about context requirements?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Scenario 2">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Scenario 2
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Data quantity (3,000) puts you in LoRA range (500-5,000). However, two complications: (1) <em>HyenaDNA</em> is a decoder, so you face the layer hunting problem; budget for systematic layer probing before committing to adaptation layers. (2) Long-range interactions (50kb) may stress even long-context architectures. Strategy: LoRA with rank 16-32, applied across middle layers, with explicit layer probing experiment first. Compare against frozen features from multiple layers.</p>
</div>
</div>
</div>
<p><strong>Scenario 3 (Minimal guidance):</strong> A hospital has 50,000 pathogenicity labels for <em>BRCA1/BRCA2</em> variants. They want to deploy a classifier trained on <em>ESM-2</em> embeddings. But the hospital primarily serves an African-ancestry population, while ClinVar training data is 78% European.</p>
<p><em>Your task:</em> Identify the appropriate adaptation strategy AND the additional considerations that data quantity alone does not capture.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Scenario 3">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Scenario 3
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Data quantity (50,000) supports full fine-tuning. However, three complications demand attention: (1) <strong>Domain shift</strong>: Population mismatch between training (European-dominated) and deployment (African-ancestry) means even high training accuracy may not transfer. Strategy: fine-tune with domain-adaptive techniques, not just standard fine-tuning. (2) <strong>Validation</strong>: Must evaluate on African-ancestry holdout specifically, not just aggregate metrics. (3) <strong>Class imbalance</strong>: Pathogenic variants in BRCA1/2 are rare; 50,000 labels likely means ~47,000 benign and ~3,000 pathogenic. Strategy: weighted loss, threshold calibration for deployment prevalence. The “right” answer requires addressing all three, not just selecting an adaptation strategy from the table.</p>
</div>
</div>
</div>
</div>
</div>
<div id="fig-adaptation-decision-tree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-adaptation-decision-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/03-fig-adaptation-decision-tree.svg" class="img-fluid figure-img"></p>
<figcaption>Decision framework for adaptation strategy selection</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-adaptation-decision-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.3: Decision framework for adaptation strategy selection. Data quantity dominates the decision: with fewer than 500 labeled examples, only linear probing avoids catastrophic overfitting; between 500 and 5,000 examples, parameter-efficient methods like LoRA offer favorable tradeoffs; above 10,000 examples, full fine-tuning becomes viable. Task similarity to pretraining and computational constraints further refine the choice. Terminal nodes indicate recommended strategies with expected tradeoffs: linear probing requires minimal compute but limits flexibility; LoRA balances adaptation capacity with regularization; full fine-tuning offers maximum flexibility at highest overfitting risk; from-scratch training remains appropriate when no suitable pretrained model exists.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch10-domain-shift" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="sec-ch10-domain-shift"><span class="header-section-number">10.5</span> Domain Shift and Cross-Context Transfer</h2>
<p>The <em>CYP2D6</em> gene encodes a cytochrome P450 enzyme metabolizing approximately 25% of clinically used drugs, including codeine (where poor metabolizers experience no analgesic effect) and tamoxifen (where poor metabolizers show reduced breast cancer treatment efficacy) <span class="citation" data-cites="gaedigk_pharmacogene_2017">(<a href="../bib/references.html#ref-gaedigk_pharmacogene_2017" role="doc-biblioref">Gaedigk et al. 2017</a>)</span>. <em>CYP2D6</em> poses particular challenges for variant interpretation due to its complex structural variation and population-specific haplotypes (see <a href="../part_7/p7-ch29-rare-disease.html" class="quarto-xref"><span>Chapter 29</span></a> for clinical workflow considerations). A foundation model trained on human genomic data and adapted for <em>CYP2D6</em> variant classification might achieve 90% accuracy on common variants well-represented in training data. But the variants most important clinically are rare: novel star alleles in underrepresented populations, structural variants creating gene duplications or deletions, population-specific haplotypes absent from reference databases. Domain shift between training and deployment distributions creates systematic blind spots precisely where clinical stakes are highest.</p>
<section id="sec-ch10-domain-shift-types" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="sec-ch10-domain-shift-types"><span class="header-section-number">10.5.1</span> Types of Domain Shift in Genomics</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reading about specific types of domain shift, consider: if you trained a variant pathogenicity model on European-ancestry samples and deployed it on African-ancestry samples, what kinds of failures might you expect? Why might performance degrade even if the underlying biology is the same?</p>
</div>
</div>
<p>Three types of <strong>domain shift</strong> commonly afflict genomic transfer learning, each creating different patterns of failure.</p>
<p>Evolutionary divergence creates the most fundamental barrier to cross-species transfer. When models trained on human sequences are applied to other organisms, differences in regulatory syntax, motif grammar, and functional constraints can undermine predictions entirely. Human-to-mouse regulatory prediction works reasonably for conserved housekeeping genes but fails for rodent-specific enhancers that never existed in the human training distribution. Strategies for cross-species success include pretraining on multi-species data to learn conservation patterns, fine-tuning with species-specific adapters, and focusing on highly conserved features (core promoter elements, splice site consensus sequences) that transfer more readily than species-specific innovations <span class="citation" data-cites="kelley_basenji2_2020">(<a href="../bib/references.html#ref-kelley_basenji2_2020" role="doc-biblioref">Kelley 2020</a>)</span>.</p>
<p>Tissue-specific regulatory programs create equally severe challenges for cross-tissue transfer. Chromatin accessibility varies dramatically across tissues, with thousands of tissue-specific enhancers and repressors controlling cell-type identity. Models trained predominantly on one tissue may miss regulatory logic specific to others. Effective approaches include shared backbones with tissue-specific prediction heads (each head learns tissue-specific transformations of shared representations), tissue-conditional models accepting tissue identity as additional input, and meta-learning frameworks training across many tissues to extract general principles applicable to novel tissue types <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>.</p>
<p>Population structure introduces a form of domain shift with direct clinical consequences. Models trained predominantly on European-ancestry data perform systematically worse when applied to other populations, with polygenic scores showing 40 to 75 percent reductions in prediction accuracy for African-ancestry individuals (<a href="../part_1/p1-ch03-gwas.html#sec-ch03-portability" class="quarto-xref"><span>Section 3.7</span></a>). The mechanisms are both technical and biological: linkage disequilibrium patterns differ across populations (making tag SNPs poor proxies for causal variants in non-training ancestries), allele frequencies shift (variants common in training data may be rare elsewhere), and effect sizes may genuinely differ due to gene-environment interactions or genetic background effects <span class="citation" data-cites="martin_clinical_2019">(<a href="../bib/references.html#ref-martin_clinical_2019" role="doc-biblioref">Martin et al. 2019</a>)</span>. Foundation models offer potential improvement by learning sequence-based features that transfer across ancestries without relying on population-specific LD patterns, but this potential remains unrealized without explicit evaluation across diverse populations. Multi-ancestry pretraining and ancestry-stratified fine-tuning represent emerging approaches, though the fundamental data imbalance (78% of GWAS participants are European despite comprising 16% of the global population) constrains what any model can learn about underrepresented groups. The broader implications of ancestry confounding receive comprehensive treatment in <a href="p3-ch13-confounding.html#sec-ch13-ancestry-confounding" class="quarto-xref"><span>Section 13.2.1</span></a>.</p>
<p>Technical variation across sequencing platforms, library preparation protocols, and analysis pipelines creates batch effects that masquerade as biological signal. Different instruments produce distinct error profiles; capture kits determine which regions receive adequate coverage; alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology <span class="citation" data-cites="yu_assessing_2024">(<a href="../bib/references.html#ref-yu_assessing_2024" role="doc-biblioref">Yu et al. 2024</a>)</span>. Foundation models are not immune: a DNA language model pretrained on data from one sequencing platform may encode platform-specific artifacts in its representations, producing embeddings that cluster by sequencing center rather than by biological phenotype. Detection requires comparing embeddings across batches using visualization or statistical divergence measures; mitigation strategies include explicit batch correction during preprocessing, domain-adversarial training that penalizes batch-predictive features, and careful data curation ensuring batch balance across labels <span class="citation" data-cites="dockes_preventing_2021">(<a href="../bib/references.html#ref-dockes_preventing_2021" role="doc-biblioref">Dockès, Varoquaux, and Poline 2021</a>)</span>. The relationship between batch effects and institutional confounding is explored further in <a href="p3-ch13-confounding.html#sec-ch13-batch-effects" class="quarto-xref"><span>Section 13.2.2</span></a>.</p>
<p>Differences in molecular readout technology create a more subtle form of shift that affects cross-assay transfer. ChIP-seq and ATAC-seq both measure chromatin state but with different biochemistry, resolution, and signal characteristics. Models trained on one assay may learn assay-specific artifacts rather than underlying biology, producing predictions that fail when applied to related assays measuring the same phenomenon differently. Multi-task pretraining across assays helps models distinguish biological signal from assay-specific noise.</p>
</section>
<section id="sec-ch10-detecting-shift" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="sec-ch10-detecting-shift"><span class="header-section-number">10.5.2</span> Detecting and Mitigating Shift</h3>
<p>Detecting domain shift before deployment prevents silent clinical failures. Statistical divergence measures comparing source and target distributions quantify distribution differences. Embedding visualizations (t-SNE or UMAP projections) reveal whether target examples fall within the source distribution or occupy unfamiliar regions of representation space. Monitoring performance on canary examples (known easy cases that should always be predicted correctly) provides early warning of severe shift during deployment.</p>
<p>When domain shift is detected, mitigation strategies include domain-adaptive fine-tuning, importance weighting of training examples, and explicit modeling of shift through domain-adversarial training <span class="citation" data-cites="ganin_domain-adversarial_2016">(<a href="../bib/references.html#ref-ganin_domain-adversarial_2016" role="doc-biblioref">Ganin et al. 2016</a>)</span>. When shift is severe and cannot be mitigated, acknowledging that transfer is inappropriate for this context prevents overconfident deployment of models that will fail.</p>
<div id="fig-domain-shift-detection" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-domain-shift-detection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/04-A-fig-domain-shift-detection.svg" class="img-fluid figure-img"></p>
<figcaption>Embedding space visualization</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/04-B-fig-domain-shift-detection.svg" class="img-fluid figure-img"></p>
<figcaption>Calibration comparison</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/04-C-fig-domain-shift-detection.svg" class="img-fluid figure-img"></p>
<figcaption>Performance degradation curve</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-domain-shift-detection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.4: Detecting domain shift before deployment prevents silent clinical failures. (A) UMAP visualization of embedding space shows test examples colored by distance from training distribution. Out-of-distribution examples (red) occupy regions where model predictions cannot be trusted. (B) Calibration analysis reveals that in-distribution predictions follow the diagonal (well-calibrated) while out-of-distribution predictions fall below (overconfident). This overconfidence makes OOD failures particularly dangerous. (C) Performance degradation quantifies the relationship between distributional distance and accuracy, enabling threshold-based decisions about when to trust predictions versus abstain.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-ch10-minimal-data" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="sec-ch10-minimal-data"><span class="header-section-number">10.6</span> Minimal-Data and Emerging Transfer Paradigms</h2>
<p>A geneticist studying a newly characterized neurodevelopmental disorder has identified 15 patients with variants in a previously unstudied gene. Functional studies confirm pathogenicity for 8 variants; the remaining 7 are benign. Training a classifier from 15 examples using standard supervised learning would be absurd, yet the clinical need for variant interpretation is immediate. Parents are waiting for diagnoses. <strong>Few-shot learning</strong> and <strong>zero-shot learning</strong> address these extreme data scarcity scenarios that characterize many genomic applications, where clinical urgency outpaces data availability. The rare disease diagnosis workflow in <a href="../part_7/p7-ch29-rare-disease.html" class="quarto-xref"><span>Chapter 29</span></a> illustrates how these methods integrate into clinical practice.</p>
<section id="sec-ch10-few-shot" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="sec-ch10-few-shot"><span class="header-section-number">10.6.1</span> Few-Shot Learning with Minimal Examples</h3>
<p>When only 10 to 100 examples per class exist, standard adaptation overfits catastrophically. The core insight of few-shot learning is that models can be trained explicitly for rapid adaptation by optimizing across many few-shot tasks during a meta-training phase <span class="citation" data-cites="finn_model-agnostic_2017">(<a href="../bib/references.html#ref-finn_model-agnostic_2017" role="doc-biblioref">Finn, Abbeel, and Levine 2017</a>)</span>. <strong>Model-Agnostic Meta-Learning (MAML)</strong> exemplifies this approach by finding parameter initializations that can be fine-tuned effectively from minimal data. The initialization represents a point in parameter space from which a few gradient steps reach good task-specific solutions. At deployment, the meta-trained model adapts to new tasks from a handful of labeled examples, having learned during meta-training what features are generally useful and how to adapt quickly.</p>
<p>A simpler alternative avoids gradient updates at deployment entirely. <strong>Prototypical networks</strong> learn to embed sequences such that examples from the same class cluster together <span class="citation" data-cites="snell_prototypical_2017">(<a href="../bib/references.html#ref-snell_prototypical_2017" role="doc-biblioref">Snell, Swersky, and Zemel 2017</a>)</span>. At inference, class prototypes are computed as the mean embedding of the few available examples per class, and novel sequences are classified based on distance to prototypes. With 10 pathogenic and 10 benign variants as prototypes, novel variants are classified by which prototype cluster they fall nearest in embedding space. The approach requires no parameter updates during deployment, only forward passes to compute embeddings and distances.</p>
</section>
<section id="sec-ch10-zero-shot" class="level3" data-number="10.6.2">
<h3 data-number="10.6.2" class="anchored" data-anchor-id="sec-ch10-zero-shot"><span class="header-section-number">10.6.2</span> Zero-Shot Transfer Without Task-Specific Data</h3>
<p>The most extreme adaptation scenario eliminates task-specific examples entirely. Zero-shot transfer makes predictions using only the pretrained model’s outputs, without any task-specific adaptation. For protein variant effect prediction, <em>ESM</em> log-likelihood ratios score variants by how much they reduce the model’s probability of the observed sequence <span class="citation" data-cites="meier_esm-1v_2021">(<a href="../bib/references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>)</span>. Variants that violate the model’s learned expectations for natural proteins (disrupting conserved residues, introducing destabilizing substitutions) receive low likelihood ratios, flagging them as potentially deleterious. This approach proves competitive with supervised methods for ClinVar pathogenicity prediction because evolutionary constraint (what masked language modeling learns to predict) correlates with functional importance (what pathogenicity classification measures). The <em>ESM</em> variant scoring methodology and its calibration to clinical thresholds are examined in <a href="../part_4/p4-ch18-vep-fm.html" class="quarto-xref"><span>Chapter 18</span></a>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight: When Zero-Shot Works
</div>
</div>
<div class="callout-body-container callout-body">
<p>Zero-shot transfer succeeds when the pretraining objective implicitly captures the target task. For protein variant effect prediction, masked language modeling learns evolutionary constraint, and evolutionary constraint correlates with pathogenicity. The model never saw pathogenicity labels during training, yet its predictions are clinically useful because it learned something closely related.</p>
<p>This insight guides model selection: ask not just “what was this model trained to do?” but “what did it implicitly learn that might transfer to my task?”</p>
</div>
</div>
<p>Zero-shot methods require strong alignment between pretraining objectives and target tasks. When this alignment exists (evolutionary constraint predicts pathogenicity), zero-shot approaches provide immediate predictions without any labeled data. When alignment is weaker (tissue-specific regulatory activity depends on factors beyond sequence conservation), few-shot learning with even a handful of examples typically outperforms zero-shot baselines. For most practical genomic applications, some labeled data improves predictions; few-shot rather than true zero-shot represents the realistic minimal-data regime.</p>
</section>
<section id="sec-ch10-emerging-approaches" class="level3" data-number="10.6.3">
<h3 data-number="10.6.3" class="anchored" data-anchor-id="sec-ch10-emerging-approaches"><span class="header-section-number">10.6.3</span> Emerging Approaches</h3>
<p>Several paradigms are extending the boundaries of what minimal-data transfer can achieve. Very large language models exhibit the capacity for <strong>in-context learning</strong>, performing tasks by observing demonstrations rather than through explicit fine-tuning <span class="citation" data-cites="brown_language_2020">(<a href="../bib/references.html#ref-brown_language_2020" role="doc-biblioref">Brown et al. 2020</a>)</span>. Early evidence suggests genomic foundation models at sufficient scale may exhibit similar behavior, classifying variants based on a few pathogenic and benign examples included in the input prompt. This could transform deployment: rather than training adapters or fine-tuning parameters, practitioners would provide examples of desired behavior at inference time.</p>
<p>Adaptation need not be confined to training time. <strong>Test-time adaptation</strong> updates models during inference based on characteristics of test examples rather than freezing parameters after training <span class="citation" data-cites="wang_tent_2021">(<a href="../bib/references.html#ref-wang_tent_2021" role="doc-biblioref">D. Wang et al. 2021</a>)</span>. For genomic applications facing distribution shift between development and deployment populations, test-time adaptation could adjust model behavior to match deployment-specific characteristics without requiring labeled examples from the deployment distribution. A model developed on European-ancestry data could adapt its uncertainty calibration when encountering African-ancestry variants that differ from training distributions.</p>
<p>Privacy constraints have motivated development of <strong>federated transfer learning</strong>, which enables collaborative model development across institutions without sharing raw genomic data <span class="citation" data-cites="rieke_future_2020">(<a href="../bib/references.html#ref-rieke_future_2020" role="doc-biblioref">Rieke et al. 2020</a>)</span>. Institutions train local models on private patient data and share only aggregated parameter updates, enabling foundation models to learn from far more diverse data than any single institution can access while preserving patient privacy. This approach could help address the population bias in current genomic datasets by enabling contributions from institutions serving underrepresented populations (see <a href="../part_1/p1-ch02-data.html" class="quarto-xref"><span>Chapter 2</span></a> for discussion of population representation in genomic databases).</p>
</section>
<section id="sec-ch10-theory" class="level3" data-number="10.6.4">
<h3 data-number="10.6.4" class="anchored" data-anchor-id="sec-ch10-theory"><span class="header-section-number">10.6.4</span> Toward Theoretical Foundations</h3>
<p>Theoretical foundations for predicting transfer success based on measurable properties of source and target domains would reduce trial-and-error <span class="citation" data-cites="ben-david_theory_2010">(<a href="../bib/references.html#ref-ben-david_theory_2010" role="doc-biblioref">Ben-David et al. 2010</a>)</span>. Currently practitioners must empirically test whether transfer helps; theoretical guidance specifying when transfer will succeed based on domain divergence measures, task similarity metrics, or representation analysis could focus effort on promising applications and avoid wasted investment in doomed transfer attempts.</p>
</section>
</section>
<section id="sec-ch10-label-imbalance" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="sec-ch10-label-imbalance"><span class="header-section-number">10.7</span> Label and Class Imbalance</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Difficulty Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>This section addresses a pervasive challenge in clinical genomics: class imbalance. The material requires careful attention because imbalance affects every stage of the machine learning pipeline differently, and standard metrics can obscure catastrophic failures. If you are less familiar with precision, recall, and ROC/PR curves, consider reviewing <a href="p3-ch12-evaluation.html#sec-ch12-discrimination-metrics" class="quarto-xref"><span>Section 12.6.1</span></a> first.</p>
</div>
</div>
<p>The clinically important variants are precisely the ones that rarely appear in training data. A pathogenicity classifier might train on thousands of benign polymorphisms but only dozens of confirmed pathogenic variants in any given gene family. A splice disruption predictor might see millions of canonical splice sites but only hundreds of cryptic sites that cause disease. This extreme imbalance is not a data curation failure but a reflection of biology: pathogenic variants are rare because purifying selection removes them from populations, and rare variants are rarely observed because they are, by definition, rare. The challenge for transfer learning is that models adapted on imbalanced target data learn to predict the majority class with high confidence while failing on the minority class that matters most.</p>
<p>Class imbalance creates problems at every stage of adaptation. During fine-tuning, gradient updates are dominated by the abundant class because most training examples belong to that class. A model fine-tuned on 10,000 benign variants and 100 pathogenic variants receives 100 times more gradient signal pushing it toward correct benign predictions than toward correct pathogenic predictions. Standard cross-entropy loss weights each example equally, so the model learns that predicting “benign” is almost always correct. Early stopping based on aggregate accuracy reinforces this bias: a model that classifies everything as benign achieves 99% accuracy and appears to have converged, yet provides no clinical value whatsoever.</p>
<p>The imbalance problem compounds across the genomic landscape. ClinVar contains roughly ten times more benign than pathogenic variants for well-studied genes like <em>BRCA1</em>, but the ratio exceeds 100:1 for less-studied genes where expert review is rare <span class="citation" data-cites="landrum_clinvar_2018">(<a href="../bib/references.html#ref-landrum_clinvar_2018" role="doc-biblioref">Landrum et al. 2018</a>)</span>. Population databases like gnomAD contain millions of variants, the vast majority benign common polymorphisms, while disease-causing variants constitute a tiny fraction concentrated in specific functional regions. When foundation models are adapted using these resources, the inherited imbalance creates systematic underconfidence for pathogenic predictions and systematic overconfidence for benign predictions.</p>
<section id="manifestations-during-transfer" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="manifestations-during-transfer"><span class="header-section-number">10.7.1</span> Manifestations During Transfer</h3>
<p>Imbalance affects different adaptation strategies in different ways. Linear probing on frozen representations inherits whatever class structure exists in the embedding space. If pretrained representations do not separate rare pathogenic variants from common benign variants (because the pretraining objective emphasized patterns common in the training corpus, which are by definition not rare), no amount of reweighting during probe training can recover the missing information. The probing classifier may achieve perfect separation on training data through overfitting while failing completely on held-out pathogenic variants that occupy different regions of embedding space.</p>
<p>Parameter-efficient fine-tuning methods like LoRA can partially address imbalance by learning task-specific transformations, but they remain susceptible when the pathogenic signal is weak relative to the benign signal. If only a small number of adapter parameters are tuned and most gradients come from the majority class, the adapters learn transformations that improve majority-class predictions without capturing minority-class patterns. Increasing adapter rank provides more capacity but also more opportunity for overfitting to the few minority examples.</p>
<p>Full fine-tuning offers the most flexibility to reshape representations for imbalanced tasks but carries the greatest overfitting risk. With 100 pathogenic examples and millions of model parameters, the model can memorize every pathogenic example while learning nothing generalizable about what makes variants pathogenic. The resulting model performs perfectly on training pathogenic variants and randomly on held-out pathogenic variants, a form of catastrophic overfitting invisible to training metrics dominated by the benign class.</p>
</section>
<section id="mitigation-strategies" class="level3" data-number="10.7.2">
<h3 data-number="10.7.2" class="anchored" data-anchor-id="mitigation-strategies"><span class="header-section-number">10.7.2</span> Mitigation Strategies</h3>
<div class="callout callout-style-default callout-tip callout-titled" title="Knowledge Check">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Knowledge Check
</div>
</div>
<div class="callout-body-container callout-body">
<p>Consider a variant classifier trained on 10,000 benign variants and 100 pathogenic variants. The model achieves 99% accuracy on the test set. Is this model performing well? What additional information would you need to assess its clinical utility?</p>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answer">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>No, 99% accuracy does not indicate good performance in this imbalanced scenario. A trivial baseline that predicts “benign” for every variant would achieve 99% accuracy (10,000/10,100 correct). You would need to see:</p>
<ol type="1">
<li><p>Sensitivity and specificity separately to know if the model actually detects pathogenic variants.</p></li>
<li><p>auPRC (not just auROC) which is sensitive to class imbalance.</p></li>
<li><p>Confusion matrix showing true/false positives and negatives.</p></li>
<li><p>Performance at clinically relevant decision thresholds.</p></li>
</ol>
<p>The model might be achieving high accuracy by simply predicting benign for everything, making it clinically useless.</p>
</div>
</div>
</div>
</div>
</div>
<p>Addressing class imbalance requires intervention at data, loss, and evaluation levels. No single strategy suffices; effective pipelines combine multiple approaches.</p>
<p>Resampling strategies modify the training distribution to achieve more balanced class representation. Oversampling the minority class by duplicating rare examples increases their influence on gradients, though excessive oversampling causes overfitting to specific minority examples. SMOTE and related methods generate synthetic minority examples by interpolating between existing examples, but interpolation in sequence space or embedding space may produce biologically implausible variants that mislead the model <span class="citation" data-cites="chawla_smote_2002">(<a href="../bib/references.html#ref-chawla_smote_2002" role="doc-biblioref">Chawla et al. 2002</a>)</span>. Undersampling the majority class reduces imbalance but discards potentially useful information; stratified undersampling that preserves the diversity of benign variants across variant types and genomic contexts performs better than random undersampling.</p>
<p>Loss reweighting assigns higher penalties to minority-class errors. Inverse frequency weighting multiplies the loss for each class by the inverse of its training frequency, so a class comprising 1% of training data receives 100 times the loss weight. Why does reweighting help when the gradient directions remain unchanged? Consider what drives learning: the model parameters adjust to reduce loss, and the magnitude of adjustment scales with loss value. Without reweighting, 99 benign examples each contributing loss 0.1 produce aggregate gradient magnitude 9.9 from the benign class, while 1 pathogenic example contributing loss 0.1 produces aggregate gradient magnitude 0.1 from the pathogenic class. The benign signal overwhelms the pathogenic signal by 99:1. With inverse frequency weighting, that single pathogenic example contributes weighted loss <span class="math inline">\(0.1 \times 100 = 10\)</span>, matching the benign aggregate. Now both classes contribute equally to gradient updates, and the model can learn both decision boundaries. Class-balanced loss variants address the diminishing returns of adding more majority-class examples, weighting by effective number of samples rather than raw counts <span class="citation" data-cites="cui_class-balanced_2019">(<a href="../bib/references.html#ref-cui_class-balanced_2019" role="doc-biblioref">Cui et al. 2019</a>)</span>. Focal loss downweights easy examples (confident correct predictions) to focus learning on hard examples, many of which are minority-class instances that the model currently misclassifies <span class="citation" data-cites="lin_focal_2020">(<a href="../bib/references.html#ref-lin_focal_2020" role="doc-biblioref">Lin et al. 2020</a>)</span>. These loss modifications change gradient magnitudes without changing gradient directions, so they work best when the model has sufficient capacity to learn minority-class patterns if given appropriate training signal.</p>
<p>Threshold adjustment and calibration address the deployment manifestation of imbalance. A model trained on imbalanced data learns decision boundaries skewed toward predicting the majority class. Adjusting the classification threshold post-training, typically by lowering the threshold for minority-class predictions, can recover sensitivity without retraining. Platt scaling and temperature scaling recalibrate predicted probabilities to match observed frequencies, essential when downstream applications depend on accurate probability estimates rather than just rankings (see <a href="../part_6/p6-ch24-uncertainty.html#sec-ch24-post-hoc-calibration" class="quarto-xref"><span>Section 24.3</span></a> for calibration methods). The appropriate threshold depends on deployment prevalence, which may differ from training prevalence; a variant predictor trained on balanced batches but deployed where 0.1% of variants are pathogenic requires threshold adjustment to avoid overwhelming clinical workflows with false positives.</p>
<p>Two-stage approaches train separate models for different aspects of the problem. A first-stage model distinguishes clearly benign variants from potentially interesting variants, filtering the vast majority of benign variants at high specificity. A second-stage model, trained on the filtered subset where class balance is more favorable, distinguishes pathogenic from uncertain among the remaining candidates. This cascade architecture reduces imbalance at each stage while maintaining overall pipeline sensitivity, though it requires careful coordination to avoid error compounding across stages.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Practical Guidance: Imbalance Mitigation Checklist
</div>
</div>
<div class="callout-body-container callout-body">
<p>When adapting models for imbalanced genomic classification:</p>
<ol type="1">
<li><strong>Never trust accuracy alone.</strong> A 99% accurate model may be clinically useless.</li>
<li><strong>Report sensitivity and specificity separately.</strong> These reveal what aggregate metrics hide.</li>
<li><strong>Use auPRC, not just auROC.</strong> auPRC is sensitive to imbalance; auROC is not.</li>
<li><strong>Combine strategies:</strong> Use class-weighted loss <em>and</em> threshold calibration <em>and</em> stratified evaluation.</li>
<li><strong>Test at deployment prevalence.</strong> Training-time metrics do not reflect clinical performance.</li>
<li><strong>Confidence intervals matter more for rare classes.</strong> With 50 pathogenic test examples, even a good model shows high variance.</li>
</ol>
</div>
</div>
</section>
<section id="evaluation-under-imbalance" class="level3" data-number="10.7.3">
<h3 data-number="10.7.3" class="anchored" data-anchor-id="evaluation-under-imbalance"><span class="header-section-number">10.7.3</span> Evaluation Under Imbalance</h3>
<p>Standard metrics obscure imbalance-driven failures. Accuracy, which measures the fraction of correct predictions, reaches 99% when a model predicts “benign” for everything in a dataset with 1% pathogenic variants. This apparent success masks complete failure on the task that matters: identifying pathogenic variants.</p>
<p>The auPRC provides a more informative view for imbalanced classification. Unlike auROC, which measures pairwise ranking between one positive and one negative example, auPRC measures precision across recall levels where precision is explicitly sensitive to the number of false positives relative to true positives. When positives are rare, achieving high precision requires correctly ranking the vast majority of negatives below positives, not just typical negatives. A model moving from a balanced validation set to a 1000:1 imbalanced deployment setting will show stable auROC but collapsing auPRC, mirroring the explosion in false discovery rate that clinical users will experience (<a href="p3-ch12-evaluation.html#sec-ch12-discrimination-metrics" class="quarto-xref"><span>Section 12.6.1</span></a> examines this distinction in detail).</p>
<p>Stratified evaluation by class reveals whether aggregate metrics hide minority-class failure. Report sensitivity (true positive rate) and specificity (true negative rate) separately rather than combining them into a single number. Report precision at clinically relevant recall thresholds: if a diagnostic pipeline requires 95% sensitivity for pathogenic variants, what precision does the model achieve at that operating point? This stratified reporting reveals the tradeoffs that aggregate metrics obscure.</p>
<p>Confidence intervals matter more for minority-class metrics. With 100 pathogenic variants in the test set, sensitivity estimates have wide confidence intervals purely from sampling variation. A model with true 80% sensitivity might show anywhere from 70% to 90% sensitivity on a particular test set by chance alone. Multiple test sets, bootstrap confidence intervals, or analytic interval calculations (Wilson score intervals for proportions) provide appropriate uncertainty quantification. Presenting point estimates without intervals overstates confidence in minority-class performance.</p>
</section>
<section id="imbalance-as-fundamental-constraint" class="level3" data-number="10.7.4">
<h3 data-number="10.7.4" class="anchored" data-anchor-id="imbalance-as-fundamental-constraint"><span class="header-section-number">10.7.4</span> Imbalance as Fundamental Constraint</h3>
<p>Class imbalance in genomic transfer learning reflects a fundamental biological reality rather than a correctable data curation problem. Pathogenic variants are rare because evolution works. Deleterious mutations are removed from populations by natural selection, so the variants that remain and accumulate in databases are predominantly benign. This creates a tension: the variants we most need to classify are the variants we have the least data to learn from.</p>
<p>This constraint shapes realistic expectations for transfer learning. A foundation model pretrained on typical genomic sequence has learned patterns of typical sequence, not patterns of pathogenic deviation from typical sequence. Transfer to pathogenic variant classification asks the model to extrapolate to a distribution it has never seen. Techniques for handling class imbalance mitigate but cannot eliminate this fundamental challenge. When only 50 pathogenic variants exist for a gene family, no amount of loss reweighting or sampling strategy can substitute for the missing data. The most honest response may be appropriate uncertainty quantification and abstention on predictions where evidence is insufficient (see <a href="../part_6/p6-ch24-uncertainty.html#sec-ch24-selective-prediction" class="quarto-xref"><span>Section 24.7</span></a>).</p>
<p>The clinical implications extend beyond model performance to workflow design. If a variant classifier has 80% sensitivity for pathogenic variants, 20% of disease-causing variants will be missed. For a rare disease diagnosis where a single pathogenic variant determines diagnosis, this false negative rate translates directly to missed diagnoses and delayed treatment. Understanding class imbalance as a structural constraint rather than a tunable hyperparameter is essential for setting appropriate clinical expectations and designing safety-net workflows that catch the variants models miss.</p>
</section>
</section>
<section id="sec-ch10-diagnosing-transfer" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="sec-ch10-diagnosing-transfer"><span class="header-section-number">10.8</span> Diagnosing Transfer: Validation and Failure Modes</h2>
<p>The research team had done everything right. They selected a state-of-the-art DNA foundation model pretrained on diverse genomic sequences. They applied LoRA adaptation using 5,000 carefully curated training examples. Validation accuracy reached 88%. But when deployed on prospectively collected samples, performance collapsed to 62%, barely better than chance for a binary classification task. Transfer had failed. For the patients whose variants were misclassified during those weeks before the failure was detected, the consequences were real: delayed diagnoses, inappropriate treatments, unnecessary anxiety. Detecting transfer failure before deployment requires understanding its root causes.</p>
<section id="sec-ch10-negative-transfer" class="level3" data-number="10.8.1">
<h3 data-number="10.8.1" class="anchored" data-anchor-id="sec-ch10-negative-transfer"><span class="header-section-number">10.8.1</span> Diagnosing Negative Transfer</h3>
<p>Negative transfer occurs when pretraining actively hurts performance, producing adapted models worse than those trained from scratch on target data alone. Pretraining on human coding sequences may encode codon usage patterns and amino acid preferences that create false expectations when applied to bacterial sequences with different GC content and codon bias. Pretraining on healthy tissue samples may learn features of normal cellular function that prove misleading for cancer samples where regulatory programs are fundamentally altered. The pretrained initialization, rather than providing a useful starting point, creates an optimization landscape that leads to poor task-specific solutions <span class="citation" data-cites="wang_characterizing_2018">(<a href="../bib/references.html#ref-wang_characterizing_2018" role="doc-biblioref">Z. Wang et al. 2018</a>)</span>.</p>
<p>Diagnostic steps identify whether transfer helps or hurts. The most fundamental comparison pits the adapted model against a from-scratch baseline trained on identical target data with equivalent hyperparameter tuning; if the pretrained model does not meaningfully outperform from-scratch training, transfer provides no benefit and the computational overhead of working with large pretrained models is wasted. Adaptation complexity should also scale appropriately: if linear probing fails, full fine-tuning rarely succeeds unless target data is very large, so simpler strategies should be exhausted before investing in more complex ones. Embedding visualization using dimensionality reduction can reveal whether pretrained representations contain task-relevant features; if target task classes are not separated in embedding space, the pretrained model may lack the representations the task requires. Finally, ablating pretraining entirely by comparing against randomly initialized models of identical architecture isolates whether pretrained weights provide value or whether architectural choices alone drive performance.</p>
<p>When diagnostics reveal fundamental mismatches, several remediation strategies apply. Task-specific pretraining on data more closely aligned with target requirements can bridge the gap; pretraining specifically on regulatory regions for regulatory prediction tasks rather than genome-wide pretraining may produce more suitable representations. Hybrid architectures combining pretrained and randomly-initialized components allow selective use of transfer where it helps while avoiding its limitations elsewhere. Trying alternative foundation models whose pretraining objectives better match task requirements may reveal that the problem was model selection rather than transfer learning generally. And accepting that transfer provides no benefit for this specific task, proceeding with from-scratch training, remains a valid conclusion when evidence supports it.</p>
</section>
<section id="sec-ch10-validation-pitfalls" class="level3" data-number="10.8.2">
<h3 data-number="10.8.2" class="anchored" data-anchor-id="sec-ch10-validation-pitfalls"><span class="header-section-number">10.8.2</span> Validation and Common Pitfalls</h3>
<p>A research group reports that their foundation model adaptation achieves 95% accuracy for splice variant classification, far exceeding previous methods. Six months later, clinical deployment reveals performance closer to 70%, with systematic failures on the rare variants that matter most. The initial evaluation was not wrong, but it was misleading. Proper validation separates genuine transfer success from evaluation artifacts that dissolve on contact with clinical reality. The benchmark landscape and its limitations are examined in <a href="p3-ch11-benchmarks.html" class="quarto-xref"><span>Chapter 11</span></a>, evaluation methodology in <a href="p3-ch12-evaluation.html" class="quarto-xref"><span>Chapter 12</span></a>, and systematic sources of inflated performance in <a href="p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>.</p>
<div id="fig-validation-pitfalls" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-validation-pitfalls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/05-A-fig-validation-pitfalls.svg" class="img-fluid figure-img"></p>
<figcaption>Data leakage through overlap</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/05-B-fig-validation-pitfalls.svg" class="img-fluid figure-img"></p>
<figcaption>Temporal leakage</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/05-C-fig-validation-pitfalls.svg" class="img-fluid figure-img"></p>
<figcaption>Baseline comparison issues</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../figs/part_3/ch10/05-D-fig-validation-pitfalls.svg" class="img-fluid figure-img"></p>
<figcaption>Stratified performance hidden</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-validation-pitfalls-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10.5: Common validation pitfalls that inflate transfer learning claims. (A) Overlap between pretraining data and benchmarks creates leakage, allowing models to “remember” test examples. (B) Temporal leakage occurs when training includes information from after benchmark creation. (C) Weak baseline comparisons exaggerate transfer benefits; fair evaluation requires properly-tuned from-scratch baselines. (D) Aggregate metrics conceal stratified failures; rare variants may show near-random performance while being masked by high accuracy on common variants. Together, these pitfalls can make ineffective transfer appear successful until clinical deployment reveals the failures.
</figcaption>
</figure>
</div>
</section>
<section id="sec-ch10-spurious-success" class="level3" data-number="10.8.3">
<h3 data-number="10.8.3" class="anchored" data-anchor-id="sec-ch10-spurious-success"><span class="header-section-number">10.8.3</span> Sources of Spurious Success</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Stop and Think
</div>
</div>
<div class="callout-body-container callout-body">
<p>A paper reports 95% accuracy for a new variant classifier, substantially exceeding prior methods. Before accepting this claim, what questions would you ask about how the evaluation was conducted? What forms of data leakage or evaluation bias might inflate these results?</p>
</div>
</div>
<p>Test set overlap with pretraining data creates artificial performance inflation. Foundation models trained on massive genomic corpora may inadvertently include sequences later used for evaluation. When benchmarking on variants that appeared in pretraining data (even if unlabeled at pretraining time), the model has seen the sequences and may have memorized relevant patterns. Verifying that test sequences were excluded from pretraining requires careful provenance tracking, which published benchmarks often lack <span class="citation" data-cites="sainz_nlp_2023">(<a href="../bib/references.html#ref-sainz_nlp_2023" role="doc-biblioref">Sainz et al. 2023</a>)</span>. Chromosome-based splits help but do not fully address the problem when pretraining spans multiple species or includes population-level diversity (see <a href="p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a> for detailed treatment of data leakage).</p>
<p>Temporal leakage uses future information unavailable at prediction time. Evaluating a variant pathogenicity model on variants annotated after training data was collected creates an unrealistically favorable setting; the model may have seen related variants or learned from the same evidence that later informed annotations. Temporal splits (training on variants discovered before a cutoff, evaluating on variants discovered afterward) provide more realistic assessment of prospective performance <span class="citation" data-cites="landrum_clinvar_2018">(<a href="../bib/references.html#ref-landrum_clinvar_2018" role="doc-biblioref">Landrum et al. 2018</a>)</span>.</p>
<p>Inappropriate baselines inflate apparent transfer benefits. Comparing adapted foundation models against weak or poorly-tuned from-scratch baselines makes transfer look more valuable than warranted. Strong baselines require equivalent hyperparameter tuning, appropriate architectures for the task, and sufficient training on the same target data. When properly-tuned CNNs match or exceed foundation model performance for a task, the additional complexity of pretrained models may not be justified.</p>
<p>Evaluation practices that reveal true performance counter these pitfalls. Single-metric reporting obscures important performance characteristics: a model achieving 90% overall accuracy may show 95% accuracy on common variants and 50% accuracy on rare variants, with the clinically important rare cases hidden by aggregate metrics. Stratified evaluation by allele frequency, variant type, gene family, and other clinically relevant categories reveals whether transfer benefits generalize or concentrate in particular subgroups. Confidence interval reporting and multiple training runs reveal performance variability, since a single training run may produce misleadingly good or bad results through random initialization effects or data sampling. Testing on multiple independent datasets rather than a single benchmark reveals whether gains generalize beyond the specific evaluation setting.</p>
</section>
</section>
<section id="sec-ch10-case-studies" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="sec-ch10-case-studies"><span class="header-section-number">10.9</span> Case Studies in Transfer Learning</h2>
<p>Transfer succeeds when pretraining objectives align with downstream requirements; it fails when they diverge. Three successes and one failure illustrate the conditions that distinguish effective transfer from wasted effort.</p>
<section id="sec-ch10-case-success" class="level3" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="sec-ch10-case-success"><span class="header-section-number">10.9.1</span> Successful Transfer: Alignment Between Pretraining and Task</h3>
<p><em>DNABERT</em> demonstrates how pretraining-task alignment enables efficient transfer. Ji et al.&nbsp;pretrained the model using masked language modeling on <span class="math inline">\(k\)</span>-mer tokenized human genomic sequence <span class="citation" data-cites="ji_dnabert_2021">(<a href="../bib/references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. For ATAC-seq peak classification, linear probes on [CLS] token embeddings achieved competitive performance with CNNs trained from scratch while requiring approximately 10 times less labeled data. Success reflected alignment between pretraining and target: both involve recognizing local sequence motifs (transcription factor binding sites, nucleosome positioning signals) that determine chromatin state. The pretrained representations already encoded the relevant patterns; the linear probe simply learned to separate accessible from inaccessible regions in this well-structured embedding space.</p>
<p><em>ESM</em> illustrates how implicit alignment can support even zero-shot transfer. Rives et al.&nbsp;pretrained <em>ESM</em> on UniRef protein sequences using masked language modeling <span class="citation" data-cites="rives_esm-1b_2021">(<a href="../bib/references.html#ref-rives_esm-1b_2021" role="doc-biblioref">Rives et al. 2021</a>)</span>. For ClinVar pathogenicity classification, Meier et al.&nbsp;showed that zero-shot scoring based on variant effects on sequence likelihood proved competitive with supervised methods <span class="citation" data-cites="meier_esm-1v_2021">(<a href="../bib/references.html#ref-meier_esm-1v_2021" role="doc-biblioref">Meier et al. 2021</a>)</span>. Adding a linear probe on <em>ESM</em> embeddings improved performance further, but the zero-shot baseline was already strong. Success reflected implicit alignment: evolutionary constraint (what masked language modeling captures) correlates with functional importance (what pathogenicity measures). The pretraining objective, though never explicitly targeting variant classification, learned representations directly relevant to it.</p>
<p><em>Enformer</em> shows that transfer can succeed even when substantial fine-tuning is required, given sufficient data. Avsec et al.&nbsp;pretrained the model on thousands of chromatin and expression tracks spanning dozens of cell types <span class="citation" data-cites="avsec_enformer_2021">(<a href="../bib/references.html#ref-avsec_enformer_2021" role="doc-biblioref">Avsec et al. 2021</a>)</span>. Fine-tuning with tissue-specific prediction heads captured regulatory logic unavailable from frozen features, outperforming both frozen <em>Enformer</em> and from-scratch models trained on individual tissues. Success required both the large scale of pretraining (establishing general sequence-to-function mappings) and extensive fine-tuning data (enabling tissue-specific adaptation). With smaller fine-tuning datasets, the approach would have overfit; without diverse pretraining, the model would have lacked transferable regulatory knowledge.</p>
</section>
<section id="sec-ch10-case-failure" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="sec-ch10-case-failure"><span class="header-section-number">10.9.2</span> When Transfer Fails: Cross-Species Prediction</h3>
<p>Models pretrained on human regulatory sequences and applied to zebrafish enhancer prediction often underperform zebrafish-specific models despite the apparent relevance of regulatory sequence patterns <span class="citation" data-cites="kelley_basenji2_2020">(<a href="../bib/references.html#ref-kelley_basenji2_2020" role="doc-biblioref">Kelley 2020</a>)</span>. The failure reflects both sequence divergence (zebrafish regulatory motifs differ from human) and lineage-specific regulatory innovations (teleost-specific enhancers have no human homologs from which to transfer). Cross-species contrastive learning approaches (<a href="p3-ch08-pretraining.html#sec-ch08-contrastive" class="quarto-xref"><span>Section 8.5</span></a>) offer one strategy for building representations that emphasize conserved features over species-specific patterns, but fundamental distributional mismatch cannot always be bridged.</p>
<p>The boundary between success and failure corresponds to evolutionary conservation: patterns shared across species transfer; species-specific patterns do not. Transfer succeeds for deeply conserved elements (core promoters, splice sites) but fails for lineage-specific regulatory logic. This case illustrates a general principle: when the target domain contains features absent from pretraining data, no adaptation strategy can manufacture missing knowledge. Recognizing these limits before deployment prevents confident predictions that are systematically wrong.</p>
</section>
</section>
<section id="sec-ch10-conclusion" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="sec-ch10-conclusion"><span class="header-section-number">10.10</span> What Transfers, What Breaks</h2>
<p>Transfer learning amplifies the value of pretrained models by connecting learned representations to specific applications. A foundation model pretrained on billions of sequences encodes patterns that would require orders of magnitude more labeled data to learn from scratch. Effective transfer realizes this investment; ineffective transfer inherits hidden limitations without the promised benefits.</p>
<p>The risks are concrete. Domain shift between pretraining and deployment contexts causes silent failures: models trained on research cohorts may miscalibrate on clinical populations, models trained on one species may fail unpredictably on another, models trained on one assay technology may not generalize to its successor. These failures produce confident predictions that are systematically wrong, often in ways that correlate with clinically relevant subgroups. Detection through distribution divergence measures and embedding visualization can identify shift before deployment, but mitigation requires either domain-adaptive fine-tuning or acceptance that some shifts cannot be bridged.</p>
<p>Validating transfer claims requires adversarial rigor. Test for contamination between pretraining and evaluation data through sequence-level deduplication. Implement temporal splits that respect real-world prediction scenarios. Compare against properly-tuned baselines trained from scratch with equivalent effort. Stratify performance by ancestry, variant type, and other clinically meaningful categories. The goal is establishing whether transfer provides genuine benefit under realistic deployment conditions, not optimizing for favorable benchmarks. Foundation model applications assume that transfer succeeds; the methods here determine whether that assumption holds for specific contexts.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Test Yourself">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Before reviewing the summary, test your recall:</p>
<ol type="1">
<li>How does LoRA achieve parameter-efficient adaptation through low-rank decomposition, and why does constraining adaptation to low-rank subspaces provide implicit regularization?</li>
<li>Explain the layer hunting problem for decoder models. Why do intermediate layers often outperform final layers for classification, and how does this differ from encoder models?</li>
<li>A classifier trained on 10,000 benign variants and 100 pathogenic variants achieves 99% accuracy. Why might this model be clinically useless despite high accuracy?</li>
<li>What is domain shift, and how does it differ from data leakage? Give an example where a model experiences distribution shift without any leakage.</li>
<li>When should you use linear probing versus LoRA versus full fine-tuning? What data quantity thresholds guide this decision?</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled" title="Check Your Answers">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Check Your Answers
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Low-Rank Adaptation</strong>: LoRA introduces two smaller matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> whose product approximates the desired weight change: <span class="math inline">\(W' = W + BA\)</span>. During fine-tuning, only <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> receive gradient updates while <span class="math inline">\(W\)</span> remains frozen. This reduces trainable parameters from hundreds of millions to just 2-5 million, achieving a 500-fold reduction in parameter count. The low-rank constraint provides implicit regularization because task-specific adaptations empirically lie in low-dimensional subspaces; forcing adaptation through limited dimensions prevents overfitting while capturing most required changes.</p></li>
<li><p><strong>Layer Hunting Problem</strong>: Decoder models are trained with next-token prediction, which specializes final layers to produce token probability distributions rather than general-purpose representations. This optimization discards information irrelevant for predicting the immediate next token but important for classification tasks. Intermediate layers (typically middle third of the network) contain richer general-purpose sequence representations before this specialization occurs, often outperforming final layers by 15-30%. Encoder models avoid this problem because bidirectional masked language modeling shapes all layers for general utility, making final-layer representations reliably effective for downstream tasks.</p></li>
<li><p><strong>Class Imbalance</strong>: With a 100:1 ratio of benign to pathogenic variants, a trivial baseline that predicts “benign” for every variant achieves 99% accuracy (10,000/10,100 correct). High accuracy masks complete failure on the clinically important minority class. The model might have zero sensitivity for pathogenic variants while achieving near-perfect specificity on benign variants, making it useless for clinical diagnosis. Proper evaluation requires sensitivity and specificity reported separately, auPRC (not just auROC), and performance at clinically relevant decision thresholds.</p></li>
<li><p><strong>Domain Shift vs.&nbsp;Leakage</strong>: Domain shift occurs when training and deployment distributions differ due to biological or technical variation (e.g., training on European-ancestry samples and deploying on African-ancestry samples, training on one tissue type and deploying on another). This creates systematic failures even when datasets are properly separated with no overlap. Data leakage, by contrast, occurs when test examples appear in training data or when future information unavailable at prediction time influences training. Example: a model trained on liver chromatin data shows poor performance on heart tissue (domain shift) even though the heart test sequences were never seen during training (no leakage).</p></li>
<li><p><strong>Adaptation Strategy Selection</strong>: Use linear probing with fewer than 500 labeled examples; more complex adaptation will overfit catastrophically. Between 500 and 5,000 examples, LoRA and other PEFT methods offer favorable tradeoffs between flexibility and regularization. With more than 10,000 examples, full fine-tuning becomes viable if the target task diverges substantially from pretraining. However, data quantity alone does not determine strategy; task alignment with pretraining matters equally. Always establish frozen feature baselines first, then escalate to more complex methods only when simpler approaches demonstrably fail.</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chapter Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Core concepts:</strong></p>
<ul>
<li><strong>Parameter-efficient fine-tuning (PEFT):</strong> LoRA and adapters update only 1-5% of model parameters, enabling adaptation on limited data and compute while providing implicit regularization</li>
<li><strong>Layer hunting problem:</strong> Decoder models require systematic layer search for optimal embedding extraction; encoder models reliably use final-layer representations</li>
<li><strong>Sequence aggregation:</strong> <code>[CLS]</code> tokens and mean pooling both work for classification; choice depends on how information distributes across positions</li>
<li><strong>Domain shift:</strong> Species, tissue, population, and technical batch effects create systematic failures when training and deployment distributions differ</li>
</ul>
<p><strong>Key decision rules:</strong></p>
<ul>
<li>&lt;500 examples: Linear probing only</li>
<li>500-5,000 examples: LoRA/adapters recommended</li>
<li>&gt;10,000 examples: Full fine-tuning viable</li>
<li>Always compare against from-scratch baselines</li>
</ul>
<p><strong>Critical warnings:</strong></p>
<ul>
<li>Class imbalance hides failures: A 99% accurate model may miss most pathogenic variants</li>
<li>Test set contamination inflates results: Verify separation between pretraining and evaluation data</li>
<li>Aggregate metrics mislead: Stratify by ancestry, variant type, and allele frequency</li>
<li>Transfer can hurt: Negative transfer produces models worse than random initialization</li>
</ul>
<p><strong>Connections to other chapters:</strong></p>
<ul>
<li>Pretraining objectives shape adaptation requirements (<a href="p3-ch08-pretraining.html" class="quarto-xref"><span>Chapter 8</span></a>)</li>
<li>Evaluation methodology separates real from spurious gains (<a href="p3-ch12-evaluation.html" class="quarto-xref"><span>Chapter 12</span></a>)</li>
<li>Confounding sources inflate benchmarks (<a href="p3-ch13-confounding.html" class="quarto-xref"><span>Chapter 13</span></a>)</li>
<li>Uncertainty quantification enables appropriate abstention (<a href="../part_6/p6-ch24-uncertainty.html" class="quarto-xref"><span>Chapter 24</span></a>)</li>
<li>Clinical deployment requires calibration to deployment prevalence (<a href="../part_7/p7-ch28-clinical-risk.html" class="quarto-xref"><span>Chapter 28</span></a>)</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-avsec_enformer_2021" class="csl-entry" role="listitem">
Avsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A. Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet Kohli, and David R. Kelley. 2021. <span>“[<span>Enformer</span>] <span>Effective</span> Gene Expression Prediction from Sequence by Integrating Long-Range Interactions.”</span> <em>Nature Methods</em> 18 (October): 1196–1203. <a href="https://doi.org/10.1038/s41592-021-01252-x">https://doi.org/10.1038/s41592-021-01252-x</a>.
</div>
<div id="ref-ben-david_theory_2010" class="csl-entry" role="listitem">
Ben-David, Shai, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2010. <span>“A Theory of Learning from Different Domains.”</span> <em>Machine Learning</em> 79 (1): 151–75. <a href="https://doi.org/10.1007/s10994-009-5152-4">https://doi.org/10.1007/s10994-009-5152-4</a>.
</div>
<div id="ref-brown_language_2020" class="csl-entry" role="listitem">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language <span>Models</span> Are <span>Few</span>-<span>Shot</span> <span>Learners</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 33 (December): 1877–1901.
</div>
<div id="ref-chawla_smote_2002" class="csl-entry" role="listitem">
Chawla, Nitesh V., Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. <span>“<span>SMOTE</span>: Synthetic Minority over-Sampling Technique.”</span> <em>J. Artif. Int. Res.</em> 16 (1): 321–57.
</div>
<div id="ref-cui_class-balanced_2019" class="csl-entry" role="listitem">
Cui, Yin, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. 2019. <span>“Class-<span>Balanced</span> <span>Loss</span> <span>Based</span> on <span>Effective</span> <span>Number</span> of <span>Samples</span>.”</span> In <em>2019 <span>IEEE</span>/<span>CVF</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span> (<span>CVPR</span>)</em>, 9260–69. <a href="https://doi.org/10.1109/CVPR.2019.00949">https://doi.org/10.1109/CVPR.2019.00949</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-devlin_bert_2019" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <span>“<span>BERT</span>: <span>Pre</span>-Training of <span>Deep</span> <span>Bidirectional</span> <span>Transformers</span> for <span>Language</span> <span>Understanding</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1810.04805">https://doi.org/10.48550/arXiv.1810.04805</a>.
</div>
<div id="ref-dockes_preventing_2021" class="csl-entry" role="listitem">
Dockès, Jérôme, Gaël Varoquaux, and Jean-Baptiste Poline. 2021. <span>“Preventing Dataset Shift from Breaking Machine-Learning Biomarkers.”</span> <em>GigaScience</em> 10 (9): giab055. <a href="https://doi.org/10.1093/gigascience/giab055">https://doi.org/10.1093/gigascience/giab055</a>.
</div>
<div id="ref-finn_model-agnostic_2017" class="csl-entry" role="listitem">
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. 2017. <span>“Model-<span>Agnostic</span> <span>Meta</span>-<span>Learning</span> for <span>Fast</span> <span>Adaptation</span> of <span>Deep</span> <span>Networks</span>.”</span> In <em>Proceedings of the 34th <span>International</span> <span>Conference</span> on <span>Machine</span> <span>Learning</span></em>, 1126–35. PMLR.
</div>
<div id="ref-gaedigk_pharmacogene_2017" class="csl-entry" role="listitem">
Gaedigk, Andrea, Magnus Ingelman-Sundberg, Neil A. Miller, J. Steven Leeder, Michelle Whirl-Carrillo, Teri E. Klein, and the PharmVar Steering Committee. 2017. <span>“The <span>Pharmacogene</span> <span>Variation</span> (<span>PharmVar</span>) <span>Consortium</span>: <span>Incorporation</span> of the <span>Human</span> <span>Cytochrome</span> <span>P450</span> (<span>CYP</span>) <span>Allele</span> <span>Nomenclature</span> <span>Database</span>.”</span> <em>Clinical Pharmacology &amp; Therapeutics</em> 103 (3): 399–401. <a href="https://doi.org/10.1002/cpt.910">https://doi.org/10.1002/cpt.910</a>.
</div>
<div id="ref-ganin_domain-adversarial_2016" class="csl-entry" role="listitem">
Ganin, Yaroslav, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. 2016. <span>“Domain-<span>Adversarial</span> <span>Training</span> of <span>Neural</span> <span>Networks</span>.”</span> <em>Journal of Machine Learning Research</em> 17 (59): 1–35.
</div>
<div id="ref-hoang_locality-aware_2025" class="csl-entry" role="listitem">
Hoang, Minh, and Mona Singh. 2025. <span>“Locality-Aware Pooling Enhances Protein Language Model Performance Across Varied Applications.”</span> <em>Bioinformatics</em> 41 (Supplement_1): i217–26. <a href="https://doi.org/10.1093/bioinformatics/btaf178">https://doi.org/10.1093/bioinformatics/btaf178</a>.
</div>
<div id="ref-howard_universal_2018" class="csl-entry" role="listitem">
Howard, Jeremy, and Sebastian Ruder. 2018. <span>“Universal <span>Language</span> <span>Model</span> <span>Fine</span>-Tuning for <span>Text</span> <span>Classification</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1801.06146">https://doi.org/10.48550/arXiv.1801.06146</a>.
</div>
<div id="ref-hu_lora_2021" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. <span>“<span>LoRA</span>: <span>Low</span>-<span>Rank</span> <span>Adaptation</span> of <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a>.
</div>
<div id="ref-jawahar_what_2019" class="csl-entry" role="listitem">
Jawahar, Ganesh, Benoît Sagot, and Djamé Seddah. 2019. <span>“What Does <span>BERT</span> Learn about the Structure of Language?”</span> In <em><span>ACL</span> 2019 - 57th <span>Annual</span> <span>Meeting</span> of the <span>Association</span> for <span>Computational</span> <span>Linguistics</span></em>. Florence, Italy.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-kelley_basenji2_2020" class="csl-entry" role="listitem">
Kelley, David R. 2020. <span>“[<span>Basenji2</span>] <span>Cross</span>-Species Regulatory Sequence Activity Prediction.”</span> <em>PLOS Computational Biology</em> 16 (7): e1008050. <a href="https://doi.org/10.1371/journal.pcbi.1008050">https://doi.org/10.1371/journal.pcbi.1008050</a>.
</div>
<div id="ref-landrum_clinvar_2018" class="csl-entry" role="listitem">
Landrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. <span>“<span>ClinVar</span>: Improving Access to Variant Interpretations and Supporting Evidence.”</span> <em>Nucleic Acids Research</em> 46 (D1): D1062–67. <a href="https://doi.org/10.1093/nar/gkx1153">https://doi.org/10.1093/nar/gkx1153</a>.
</div>
<div id="ref-lin_focal_2020" class="csl-entry" role="listitem">
Lin, Tsung-Yi, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2020. <span>“Focal <span>Loss</span> for <span>Dense</span> <span>Object</span> <span>Detection</span>.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 42 (02): 318–27. <a href="https://doi.org/10.1109/TPAMI.2018.2858826">https://doi.org/10.1109/TPAMI.2018.2858826</a>.
</div>
<div id="ref-martin_clinical_2019" class="csl-entry" role="listitem">
Martin, Alicia R., Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M. Neale, and Mark J. Daly. 2019. <span>“Clinical Use of Current Polygenic Risk Scores May Exacerbate Health Disparities.”</span> <em>Nature Genetics</em> 51 (4): 584–91. <a href="https://doi.org/10.1038/s41588-019-0379-x">https://doi.org/10.1038/s41588-019-0379-x</a>.
</div>
<div id="ref-mccloskey_catastrophic_1989" class="csl-entry" role="listitem">
McCloskey, Michael, and Neal Cohen. 1989. <span>“Catastrophic <span>Interference</span> in <span>Connectionist</span> <span>Networks</span>: <span>The</span> <span>Sequential</span> <span>Learning</span> <span>Problem</span>.”</span> <em>Psychology of Learning and Motivation</em> 24 (January): 109–65. <a href="https://doi.org/10.1016/S0079-7421(08)60536-8">https://doi.org/10.1016/S0079-7421(08)60536-8</a>.
</div>
<div id="ref-meier_esm-1v_2021" class="csl-entry" role="listitem">
Meier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alexander Rives. 2021. <span>“[<span>ESM</span>-1v] <span>Language</span> Models Enable Zero-Shot Prediction of the Effects of Mutations on Protein Function.”</span> bioRxiv. <a href="https://doi.org/10.1101/2021.07.09.450648">https://doi.org/10.1101/2021.07.09.450648</a>.
</div>
<div id="ref-naderializadeh_aggregating_2025" class="csl-entry" role="listitem">
NaderiAlizadeh, Navid, and Rohit Singh. 2025. <span>“Aggregating Residue-Level Protein Language Model Embeddings with Optimal Transport.”</span> <em>Bioinformatics Advances</em> 5 (1): vbaf060. <a href="https://doi.org/10.1093/bioadv/vbaf060">https://doi.org/10.1093/bioadv/vbaf060</a>.
</div>
<div id="ref-peters_deep_2018" class="csl-entry" role="listitem">
Peters, Matthew E., Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. <span>“Deep <span>Contextualized</span> <span>Word</span> <span>Representations</span>.”</span> In <em>Proceedings of the 2018 <span>Conference</span> of the <span>North</span> <span>American</span> <span>Chapter</span> of the <span>Association</span> for <span>Computational</span> <span>Linguistics</span>: <span>Human</span> <span>Language</span> <span>Technologies</span>, <span>Volume</span> 1 (<span>Long</span> <span>Papers</span>)</em>, edited by Marilyn Walker, Heng Ji, and Amanda Stent, 2227–37. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-1202">https://doi.org/10.18653/v1/N18-1202</a>.
</div>
<div id="ref-rieke_future_2020" class="csl-entry" role="listitem">
Rieke, Nicola, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger R. Roth, Shadi Albarqouni, Spyridon Bakas, et al. 2020. <span>“The Future of Digital Health with Federated Learning.”</span> <em>Npj Digital Medicine</em> 3 (1): 119. <a href="https://doi.org/10.1038/s41746-020-00323-1">https://doi.org/10.1038/s41746-020-00323-1</a>.
</div>
<div id="ref-rives_esm-1b_2021" class="csl-entry" role="listitem">
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, et al. 2021. <span>“[<span>ESM</span>-1b] <span>Biological</span> Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.”</span> <em>Proceedings of the National Academy of Sciences of the United States of America</em> 118 (15): e2016239118. <a href="https://doi.org/10.1073/pnas.2016239118">https://doi.org/10.1073/pnas.2016239118</a>.
</div>
<div id="ref-sainz_nlp_2023" class="csl-entry" role="listitem">
Sainz, Oscar, Jon Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. <span>“<span>NLP</span> <span>Evaluation</span> in Trouble: <span>On</span> the <span>Need</span> to <span>Measure</span> <span>LLM</span> <span>Data</span> <span>Contamination</span> for Each <span>Benchmark</span>.”</span> In <em>Findings of the <span>Association</span> for <span>Computational</span> <span>Linguistics</span>: <span>EMNLP</span> 2023</em>, edited by Houda Bouamor, Juan Pino, and Kalika Bali, 10776–87. Singapore: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2023.findings-emnlp.722">https://doi.org/10.18653/v1/2023.findings-emnlp.722</a>.
</div>
<div id="ref-snell_prototypical_2017" class="csl-entry" role="listitem">
Snell, Jake, Kevin Swersky, and Richard Zemel. 2017. <span>“Prototypical <span>Networks</span> for <span>Few</span>-Shot <span>Learning</span>.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>. Vol. 30. Curran Associates, Inc.
</div>
<div id="ref-wang_tent_2021" class="csl-entry" role="listitem">
Wang, Dequan, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. 2021. <span>“Tent: <span>Fully</span> <span>Test</span>-Time <span>Adaptation</span> by <span>Entropy</span> <span>Minimization</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2006.10726">https://doi.org/10.48550/arXiv.2006.10726</a>.
</div>
<div id="ref-wang_characterizing_2018" class="csl-entry" role="listitem">
Wang, Zirui, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. 2018. <span>“Characterizing and <span>Avoiding</span> <span>Negative</span> <span>Transfer</span>.”</span> In, 11293–302.
</div>
<div id="ref-yu_assessing_2024" class="csl-entry" role="listitem">
Yu, Ying, Yuanbang Mai, Yuanting Zheng, and Leming Shi. 2024. <span>“Assessing and Mitigating Batch Effects in Large-Scale Omics Studies.”</span> <em>Genome Biology</em> 25 (1): 254. <a href="https://doi.org/10.1186/s13059-024-03401-9">https://doi.org/10.1186/s13059-024-03401-9</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../part_3/p3-ch09-transfer.html" class="pagination-link" aria-label="Transfer Learning Foundations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning Foundations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../part_3/p3-ch11-benchmarks.html" class="pagination-link" aria-label="Benchmark Landscape">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Benchmark Landscape</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025-2026, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>