<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Sequence Representation &amp; Tokens – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p2-ch06-transformers.html" rel="next">
<link href="./p2--principles.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part II: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch05-tokens.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-pgs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS &amp; Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-cadd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deleteriousness Scores</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-tokens.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-foundation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Genomic Foundation Models: Concepts &amp; Taxonomy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-pretrain.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Pretraining Objectives &amp; Strategies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-transfer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Transfer Learning &amp; Deployment</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CNN Sequence-to-Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-dna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch13-plm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch14-hybrid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">p4–multi-modal_multi-scale.qmd</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-sc-epi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Single-Cell &amp; Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch16-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Graphs, Networks, and Biology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch17-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Multi-Omics &amp; Systems Biology</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch18-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch19-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch20-vep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch21-confound.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch22-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Interpretability &amp; Mechanisms</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch23-clinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch24-variants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Pathogenic Variant Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch25-drugs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Drug Discovery &amp; Biotech</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch26-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch27-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Future Work &amp; Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-model-list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-sequence-to-model-the-representation-problem" id="toc-from-sequence-to-model-the-representation-problem" class="nav-link active" data-scroll-target="#from-sequence-to-model-the-representation-problem"><span class="header-section-number">5.1</span> From Sequence to Model: The Representation Problem</a></li>
  <li><a href="#gene-level-embeddings-a-road-not-taken" id="toc-gene-level-embeddings-a-road-not-taken" class="nav-link" data-scroll-target="#gene-level-embeddings-a-road-not-taken"><span class="header-section-number">5.2</span> Gene-Level Embeddings: A Road Not Taken</a></li>
  <li><a href="#one-hot-encoding-the-cnn-foundation" id="toc-one-hot-encoding-the-cnn-foundation" class="nav-link" data-scroll-target="#one-hot-encoding-the-cnn-foundation"><span class="header-section-number">5.3</span> One-Hot Encoding: The CNN Foundation</a></li>
  <li><a href="#k-mer-tokenization-the-dnabert-approach" id="toc-k-mer-tokenization-the-dnabert-approach" class="nav-link" data-scroll-target="#k-mer-tokenization-the-dnabert-approach"><span class="header-section-number">5.4</span> K-mer Tokenization: The DNABERT Approach</a></li>
  <li><a href="#byte-pair-encoding-learning-the-vocabulary" id="toc-byte-pair-encoding-learning-the-vocabulary" class="nav-link" data-scroll-target="#byte-pair-encoding-learning-the-vocabulary"><span class="header-section-number">5.5</span> Byte Pair Encoding: Learning the Vocabulary</a></li>
  <li><a href="#single-nucleotide-tokenization-the-hyenadna-approach" id="toc-single-nucleotide-tokenization-the-hyenadna-approach" class="nav-link" data-scroll-target="#single-nucleotide-tokenization-the-hyenadna-approach"><span class="header-section-number">5.6</span> Single-Nucleotide Tokenization: The HyenaDNA Approach</a></li>
  <li><a href="#biologically-informed-tokenization" id="toc-biologically-informed-tokenization" class="nav-link" data-scroll-target="#biologically-informed-tokenization"><span class="header-section-number">5.7</span> Biologically-Informed Tokenization</a>
  <ul class="collapse">
  <li><a href="#codon-aware-tokenization" id="toc-codon-aware-tokenization" class="nav-link" data-scroll-target="#codon-aware-tokenization"><span class="header-section-number">5.7.1</span> Codon-Aware Tokenization</a></li>
  <li><a href="#variant-tokens" id="toc-variant-tokens" class="nav-link" data-scroll-target="#variant-tokens"><span class="header-section-number">5.7.2</span> Variant Tokens</a></li>
  </ul></li>
  <li><a href="#the-context-length-evolution" id="toc-the-context-length-evolution" class="nav-link" data-scroll-target="#the-context-length-evolution"><span class="header-section-number">5.8</span> The Context Length Evolution</a></li>
  <li><a href="#trade-offs-and-practical-considerations" id="toc-trade-offs-and-practical-considerations" class="nav-link" data-scroll-target="#trade-offs-and-practical-considerations"><span class="header-section-number">5.9</span> Trade-offs and Practical Considerations</a></li>
  <li><a href="#the-emerging-consensus" id="toc-the-emerging-consensus" class="nav-link" data-scroll-target="#the-emerging-consensus"><span class="header-section-number">5.10</span> The Emerging Consensus</a></li>
  <li><a href="#implications-for-subsequent-chapters" id="toc-implications-for-subsequent-chapters" class="nav-link" data-scroll-target="#implications-for-subsequent-chapters"><span class="header-section-number">5.11</span> Implications for Subsequent Chapters</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--principles.html">Part II: Core Principles</a></li><li class="breadcrumb-item"><a href="./p2-ch05-tokens.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-token" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>TODO:</strong></p>
<ul>
<li>Verify tradeoffs and general consensus discussion is sufficient</li>
<li>Double-check that all model names (DNABERT, DNABERT-2, Nucleotide Transformer, HyenaDNA, LifeCode, GenSLMs, BioToken, etc.) align with the details and citations in the architecture chapters</li>
<li>Confirm that the high-level summary of LifeCode, GenSLMs, and BioToken matches the primary sources</li>
<li>Validate that cross-references to other chapters (<a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a>, <a href="p3-ch14-hybrid.html" class="quarto-xref"><span>Chapter 14</span></a>, <a href="p2-ch07-foundation.html" class="quarto-xref"><span>Chapter 7</span></a>, <a href="p3-ch10-cnn.html#sec-reg" class="quarto-xref"><span>Section 10.1</span></a>, <a href="p3-ch10-cnn.html#sec-splice" class="quarto-xref"><span>Section 10.3</span></a>) still point to the right sections after final re-organization</li>
<li>Revisit the numerical examples for context lengths (e.g., 1 kb, 6 kb, 1 Mb) to ensure internal consistency with Chapters 7–9 and with the latest model details</li>
</ul>
</div>
</div>
<section id="from-sequence-to-model-the-representation-problem" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="from-sequence-to-model-the-representation-problem"><span class="header-section-number">5.1</span> From Sequence to Model: The Representation Problem</h2>
<p>Every genomic deep learning model must answer a fundamental question before learning can begin: how should DNA sequence be represented as numerical input? This question might seem purely technical, a preprocessing detail to be settled and forgotten. Yet the choice of representation profoundly shapes what a model can learn, how efficiently it trains, and what biological phenomena it can capture. The previous chapters employed one-hot encoding without much discussion, treating it as the obvious default for CNN-based architectures like DeepSEA (<a href="p3-ch10-cnn.html#sec-reg" class="quarto-xref"><span>Section 10.1</span></a>) and SpliceAI (<a href="p3-ch10-cnn.html#sec-splice" class="quarto-xref"><span>Section 10.3</span></a>). This approach worked remarkably well for those models, but the emergence of transformer-based language models introduced new considerations around tokenization, vocabulary design, and the fundamental trade-offs between sequence compression and resolution.</p>
<p>The challenge can be understood through an analogy to natural language processing. When training a language model on English text, researchers must decide how to segment the continuous stream of characters into discrete tokens. One could treat each character as a token, preserving maximum resolution but creating very long sequences. Alternatively, one could use words as tokens, compressing the sequence but potentially losing information about word structure. Or one could learn a vocabulary of subword units that balances these concerns. Each choice affects what patterns the model can discover and how efficiently it can process long documents.</p>
<p>DNA presents similar choices but with important differences. The genome has only four letters rather than dozens, no natural word boundaries, and biological structure that operates at multiple scales simultaneously. A transcription factor binding site might span 6-12 nucleotides, but the regulatory grammar linking multiple binding sites can extend over hundreds of base pairs. Coding sequences follow a strict three-nucleotide codon structure, while noncoding regions have no such constraint. Any representation scheme must navigate these biological realities while remaining computationally tractable.</p>
<p>This chapter examines the evolution of sequence representation strategies in genomic deep learning. We trace the progression from one-hot encoding through k-mer tokenization to modern approaches including Byte Pair Encoding, single-nucleotide tokens, and biologically-informed tokenization schemes. Understanding these choices clarifies design decisions in models throughout Parts III and IV, and illuminates why seemingly minor representation choices can dramatically affect model capabilities.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.1 – Representation trade-off comparison</strong></p>
<p>A single genomic sequence drawn once and tokenized four ways: one-hot bases, k-mers, BPE tokens, and single-nucleotide tokens. Under each, show: - Approximate sequence length (number of tokens) - “Resolution” (e.g., whether variants map cleanly to a single token) - Typical models that use the representation</p>
</div>
</div>
</section>
<section id="gene-level-embeddings-a-road-not-taken" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="gene-level-embeddings-a-road-not-taken"><span class="header-section-number">5.2</span> Gene-Level Embeddings: A Road Not Taken</h2>
<p>Before examining sequence tokenization strategies, it is worth asking whether sequence representation is necessary at all. When Word2Vec demonstrated that meaningful word embeddings could be learned from co-occurrence patterns in text corpora, an analogous approach for biology seemed natural: learn gene embeddings from co-expression patterns across experiments.</p>
<p>Gene2Vec (2019) pursued exactly this strategy, training Word2Vec-style embeddings on gene co-expression data from thousands of experiments <span class="citation" data-cites="du_gene2vec_2019">(<a href="references.html#ref-du_gene2vec_2019" role="doc-biblioref">Du et al. 2019</a>)</span>. Genes appearing in similar expression contexts received similar vector representations, capturing functional relationships without any sequence information. The resulting embeddings improved performance on gene function prediction and could identify functionally related genes that shared no obvious sequence similarity. Similar approaches emerged for other biological entities, embedding proteins based on interaction networks or pathways based on shared gene membership.</p>
<p>Yet gene-level embeddings did not become the dominant paradigm for genomic deep learning, and the reasons illuminate why sequence-based representations proved more powerful. Gene embeddings treat each gene as an atomic unit with no internal structure. They cannot distinguish between synonymous and nonsynonymous variants within a gene, cannot predict effects of novel mutations never seen during training, and cannot generalize to newly discovered genes absent from the original embedding corpus. The representations are static rather than compositional: a gene’s embedding captures its average behavior across training contexts but cannot adapt to the specific sequence context of a particular variant or regulatory configuration.</p>
<p>The field’s commitment to sequence-based representations reflects a fundamental insight: the information determining gene function is encoded in nucleotide sequence, and models that operate directly on sequence can learn this encoding rather than relying on pre-computed summaries. A sequence model can predict how a single nucleotide change alters splicing, expression, or protein function because it has learned the underlying sequence grammar. A gene embedding cannot. This distinction becomes critical for clinical applications, where the variants of greatest interest are often novel mutations never previously observed. The remainder of this chapter examines the various strategies for representing sequence while preserving this compositional, generalizable structure.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.2 – Gene2vec vs sequence models</strong></p>
<p>Schematic comparing: - A “gene graph” with gene2vec-style embeddings (nodes with vector representations) - A sequence model taking raw DNA as input Highlight how gene-level embeddings lack positional and base-resolution information, while sequence tokens preserve genomic structure.</p>
</div>
</div>
</section>
<section id="one-hot-encoding-the-cnn-foundation" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="one-hot-encoding-the-cnn-foundation"><span class="header-section-number">5.3</span> One-Hot Encoding: The CNN Foundation</h2>
<p>One-hot encoding represents the simplest possible approach to sequence representation: each nucleotide becomes a sparse binary vector with a single active element indicating its identity. Adenine is encoded as [1, 0, 0, 0], cytosine as [0, 1, 0, 0], guanine as [0, 0, 1, 0], and thymine as [0, 0, 0, 1]. A sequence of length <span class="math inline">\(L\)</span> thus becomes a matrix of dimensions <span class="math inline">\(4 \times L\)</span>, interpretable as four channels analogous to the RGB channels of an image plus one additional channel.</p>
<p>This representation dominated the CNN era of genomic deep learning for good reason. One-hot encoding is lossless, preserving every nucleotide explicitly without any information compression. It maintains single-nucleotide resolution, enabling detection of effects from individual SNPs, which is critical for variant interpretation. The representation exhibits translation equivariance, meaning that convolutional filters learn position-invariant motifs that can be recognized anywhere in the sequence. And it requires no preprocessing, vocabulary construction, or tokenizer training, making implementation straightforward.</p>
<p>DeepSEA, ExPecto, and SpliceAI all employed one-hot encoding without modification. The convolutional layers in these models learned to detect sequence patterns directly from the binary representation, with first-layer filters discovering motifs corresponding to transcription factor binding sites and deeper layers capturing combinations and spatial arrangements. The representation worked because CNNs process sequences through local operations, with each convolutional filter examining only a small window of positions at a time. The sparse, orthogonal nature of one-hot vectors posed no obstacle to this local processing.</p>
<p>Yet for transformer architectures, one-hot encoding presents significant challenges. Transformers compute attention between all pairs of positions in a sequence, with computational cost scaling as <span class="math inline">\(O(L^2)\)</span> where <span class="math inline">\(L\)</span> is the sequence length. A 10 kb sequence requires 10,000 tokens, meaning 100 million pairwise attention computations per layer. This quickly becomes prohibitive for the long sequences that genomic applications require. Furthermore, transformers typically learn dense embeddings for each token, but with only four possible nucleotides, there is little opportunity for the model to discover rich representations through the embedding layer. The sparse one-hot vectors provide minimal information for the embedding to transform. Most critically, practical transformer context windows of 512 to 4,096 tokens translate to only 512 to 4,096 base pairs when using one-hot encoding, a tiny fraction of genes or regulatory regions and far less than the context that proved valuable for models like Enformer and SpliceAI.</p>
<p>These limitations motivated the search for alternative representations that could compress genomic sequences into fewer tokens while preserving the information needed for biological prediction.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.3 – One-hot vs CNN filters</strong></p>
<p>A 1D DNA sequence mapped to a 4-channel one-hot matrix. Convolutional filters sliding across the matrix, with motif-like filters highlighted. Optional inset: “effective receptive field” of a deep CNN stack vs sequence length.</p>
</div>
</div>
</section>
<section id="k-mer-tokenization-the-dnabert-approach" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="k-mer-tokenization-the-dnabert-approach"><span class="header-section-number">5.4</span> K-mer Tokenization: The DNABERT Approach</h2>
<p>K-mer tokenization treats overlapping subsequences of length <span class="math inline">\(k\)</span> as tokens, drawing an analogy between k-mers and words in natural language. Just as sentences are composed of words that carry meaning through their sequence and combination, genomic sequences might be understood as composed of k-mer “words” that encode biological function through their arrangement. DNABERT (2021) pioneered this approach for genomic transformers, using 6-mers as tokens and training a BERT-style masked language model on human reference sequences <span class="citation" data-cites="ji_dnabert_2021">(<a href="references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>.</p>
<p>The k-mer vocabulary has a fixed size of <span class="math inline">\(4^k\)</span> possible tokens. For 6-mers, this yields 4,096 distinct tokens, comparable to the vocabulary sizes used in some natural language models. Each token represents six consecutive nucleotides, creating a direct correspondence between subsequence and token identity. The tokenization proceeds by sliding a window across the sequence and recording each k-mer encountered.</p>
<p>DNABERT used overlapping k-mers, meaning that for a sequence like ACGTACGT, the 6-mer tokens would share five nucleotides with their neighbors. The sequence position advances by one nucleotide at a time, generating one token per position (minus the k-1 positions at the end where a complete k-mer cannot be formed). This overlapping design preserves positional information and ensures that every nucleotide contributes to multiple tokens, potentially providing redundancy that helps the model learn robust representations.</p>
<p>The DNABERT approach provided valuable proof of concept. It demonstrated that self-supervised pretraining on raw DNA sequences could improve performance over training from scratch, that learned embeddings could capture biologically meaningful regularities even when trained only on the reference genome, and that BERT-style architectures could be reused across multiple downstream tasks. DNABERT achieved state-of-the-art performance on prediction of promoters, splice sites, and transcription factor binding sites after fine-tuning with relatively small amounts of task-specific labeled data.</p>
<p>However, subsequent analysis revealed fundamental limitations of k-mer tokenization that stemmed from the overlapping design. DNABERT-2 (2024) articulated these problems clearly <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. First, overlapping k-mers provide no sequence compression. The number of tokens equals the number of nucleotides (minus a small constant), so context window limitations persist unchanged. A 10 kb sequence still requires approximately 10,000 tokens, and the quadratic attention complexity remains prohibitive for long sequences.</p>
<p>Second, overlapping tokenization creates ambiguity in how sequence positions map to tokens. A single nucleotide contributes to <span class="math inline">\(k\)</span> different tokens, complicating interpretation of which token is responsible for any given prediction. This ambiguity becomes particularly problematic for variant effect interpretation, where one wants to understand how changing a specific nucleotide alters model predictions. The effect of a single nucleotide substitution propagates through <span class="math inline">\(k\)</span> different tokens in ways that can be difficult to disentangle.</p>
<p>Third, the overlapping design introduces sample inefficiency. The model must learn that overlapping tokens share nucleotides, a relationship that is obvious from the tokenization scheme but must be discovered through training. This redundancy consumes model capacity that could otherwise be devoted to learning more complex biological patterns.</p>
<p>Fourth, the fixed <span class="math inline">\(4^k\)</span> vocabulary does not adapt to corpus statistics. Frequent and rare k-mers receive equal representation capacity in the embedding table, even though their importance for prediction may differ substantially. Common motifs that appear throughout the genome receive no more parameters than rare sequences that might represent sequencing errors or unique regulatory elements.</p>
<p>These limitations motivated exploration of alternative tokenization strategies that could achieve genuine sequence compression while preserving the information needed for biological prediction.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.4 – K-mer sliding window</strong></p>
<p>A short DNA sequence with overlapping 6-mers highlighted. Show how a single SNV changes several k-mer tokens. Side panel: token count vs <em>k</em> (e.g., bases vs 3-mer vs 6-mer) for a fixed window length.</p>
</div>
</div>
</section>
<section id="byte-pair-encoding-learning-the-vocabulary" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="byte-pair-encoding-learning-the-vocabulary"><span class="header-section-number">5.5</span> Byte Pair Encoding: Learning the Vocabulary</h2>
<p>Byte Pair Encoding offers a fundamentally different approach to tokenization. Rather than defining tokens through a fixed rule (every k consecutive nucleotides), BPE constructs a vocabulary by learning which subsequences appear frequently in the training corpus. The algorithm, originally developed for data compression, iteratively merges the most frequent adjacent token pairs until reaching a desired vocabulary size.</p>
<p>The BPE algorithm begins by initializing the vocabulary with single nucleotides: {A, C, G, T}. It then scans the training corpus to count all adjacent token pairs and identifies the most frequent pair. This pair is merged into a new token, added to the vocabulary, and all instances in the corpus are replaced with the new token. The process repeats, counting pairs again (now including the newly created token) and merging the next most frequent pair. Through many iterations, BPE builds a vocabulary of variable-length tokens that capture frequently occurring sequence patterns.</p>
<p>The key insight is that BPE produces genuine sequence compression. Unlike overlapping k-mers where each nucleotide generates its own token, BPE creates non-overlapping tokens that can span multiple nucleotides. A 10 kb sequence might compress to 2,000 or 3,000 tokens depending on its repetitive structure, enabling transformers to process much longer sequences within the same context window.</p>
<p>DNABERT-2 replaced 6-mer tokenization with BPE and demonstrated dramatic improvements <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. The new model achieved comparable performance to state-of-the-art approaches while using 21 times fewer parameters and requiring approximately 92 times less GPU time in pretraining. The efficiency gains stem directly from non-overlapping tokenization: actual sequence compression enables processing longer sequences with the same computational budget, and eliminating the redundancy of overlapping tokens allows the model to focus capacity on learning biological patterns rather than token relationships.</p>
<p>The BPE vocabulary learns corpus statistics through its construction process. Repetitive elements that appear frequently throughout the genome, such as Alu sequences or common regulatory motifs, receive dedicated tokens that span many nucleotides. These long tokens enable efficient representation of repetitive regions while preserving single-nucleotide resolution for unique sequences. Rare sequences that BPE never encountered during vocabulary construction are represented as concatenations of shorter subunits, maintaining the ability to encode any sequence while allocating more representation capacity to common patterns.</p>
<p>GROVER (Genome Rules Obtained Via Extracted Representations) extended this approach by training BPE specifically on the human genome and selecting vocabulary using a custom next-k-mer prediction task <span class="citation" data-cites="sanabria_grover_2024">(<a href="references.html#ref-sanabria_grover_2024" role="doc-biblioref">Sanabria et al. 2024</a>)</span>. Analysis of the resulting token embeddings revealed that the learned vocabulary encodes biologically meaningful structure. Common tokens cluster separately from rare ones in embedding space. GC-rich tokens segregate from AT-rich tokens, reflecting the different properties of these sequence compositions. Token length correlates with specific embedding dimensions, allowing the model to represent both the content and extent of each token. Some tokens appear primarily in repetitive regions while others distribute broadly across the genome, and this localization pattern is captured in the learned representations.</p>
<p>Yet BPE introduces its own complications. The variable-length tokens mean that variant positions fall at different locations relative to token boundaries depending on the local sequence context. A SNP might fall in the middle of a long token in one context but at a token boundary in another, potentially affecting how the model represents and processes the variant. This context-dependence can complicate variant effect interpretation, as the same nucleotide change may alter different numbers of tokens depending on surrounding sequence.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.5 – BPE merges on DNA</strong></p>
<p>Start with a short sequence tokenized as individual bases. Show successive BPE merges building longer tokens. Overlay frequency counts or “motif-like” tokens to suggest that the learned vocabulary captures biologically relevant subsequences.</p>
</div>
</div>
</section>
<section id="single-nucleotide-tokenization-the-hyenadna-approach" class="level2" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="single-nucleotide-tokenization-the-hyenadna-approach"><span class="header-section-number">5.6</span> Single-Nucleotide Tokenization: The HyenaDNA Approach</h2>
<p>While k-mer and BPE tokenization compress sequences to enable longer context windows, they sacrifice single-nucleotide resolution in doing so. This trade-off becomes problematic for variant effect prediction, where the precise position and identity of mutations is paramount. A single nucleotide polymorphism can completely alter protein function through mechanisms ranging from amino acid substitution to splice site disruption to regulatory element ablation. Multi-nucleotide tokens obscure exactly where variants fall and how they relate to the boundaries of biological features.</p>
<p>HyenaDNA (2023) took the opposite approach, using single-nucleotide tokens with no compression whatsoever <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>. Each nucleotide (A, C, G, T) is a separate token, maintaining the maximum possible resolution. Every nucleotide is independently represented in the token sequence, SNP effects can be isolated to specific token positions without ambiguity, and there are no tokenization artifacts that depend on surrounding sequence context.</p>
<p>The challenge with single-nucleotide tokens is sequence length. A 1 Mb region requires 1 million tokens, far beyond the capacity of any standard transformer. The quadratic attention complexity would require a trillion pairwise computations per layer, rendering the approach computationally infeasible with conventional architectures.</p>
<p>HyenaDNA addressed this challenge through a fundamental architectural innovation rather than a tokenization compromise. The Hyena architecture replaces the attention mechanism with implicit convolutions that scale sub-quadratically with sequence length. Where attention computes explicit pairwise interactions between all positions, Hyena uses long convolutions parameterized by a small neural network, achieving similar representational power with <span class="math inline">\(O(L \log L)\)</span> complexity rather than <span class="math inline">\(O(L^2)\)</span>. This enables processing of sequences hundreds of times longer than attention-based transformers within the same computational budget.</p>
<p>The result was a 500-fold increase in context length over dense attention models while maintaining single-nucleotide resolution. HyenaDNA could process 1 Mb sequences where DNABERT was limited to approximately 500 bp and the Nucleotide Transformer to approximately 6 kb. On the Nucleotide Transformer benchmarks, HyenaDNA reached state-of-the-art performance on 12 of 18 datasets with orders of magnitude fewer parameters and less pretraining data. On GenomicBenchmarks, it surpassed prior state-of-the-art on 7 of 8 datasets by an average of 10 accuracy points.</p>
<p>Perhaps most notably, HyenaDNA demonstrated the first use of in-context learning in genomics. The model could perform tasks based on examples provided in the context window without any fine-tuning, simply by conditioning on demonstration sequences. This capability, familiar from large language models, had not previously been shown for genomic sequences and suggests that very long context combined with high resolution enables qualitatively new forms of biological reasoning.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.6 – Context length vs resolution plot</strong></p>
<p>Scatter or line plot showing different models on a 2D plane: - x-axis: context length (bp) - y-axis: token resolution (bases per token) Place: One-hot CNNs (short context, 1 bp/token), DNABERT (medium context, 6 bp/token), DNABERT-2 / BPE models (longer context, variable bp/token), HyenaDNA (very long context, 1 bp/token).</p>
</div>
</div>
</section>
<section id="biologically-informed-tokenization" class="level2" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="biologically-informed-tokenization"><span class="header-section-number">5.7</span> Biologically-Informed Tokenization</h2>
<p>Standard tokenization schemes treat DNA as a homogeneous string of characters, ignoring the biological reality that different genomic regions serve fundamentally different functions and follow different structural rules. Coding sequences obey a strict codon structure where every three nucleotides encode an amino acid, while noncoding regions have no such constraint. Treating these regions identically wastes an opportunity to build biological knowledge directly into the representation.</p>
<section id="codon-aware-tokenization" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="codon-aware-tokenization"><span class="header-section-number">5.7.1</span> Codon-Aware Tokenization</h3>
<p>For protein-coding regions, the natural unit of sequence is the codon, not the individual nucleotide. Several models exploit this structure by treating codons as tokens. GenSLMs (2022) pioneered codon-level tokenization for genomic foundation models, treating each three-nucleotide codon as a single token and exploiting the fact that codons are the biologically meaningful units of protein-coding sequence <span class="citation" data-cites="zvyagin_genslms_2022">(<a href="references.html#ref-zvyagin_genslms_2022" role="doc-biblioref">Zvyagin et al. 2022</a>)</span>. The 64-codon vocabulary captures the complete space of possible genetic code words, with each token corresponding to either an amino acid or a stop signal. This alignment with translation semantics means that mutations affecting amino acid identity (nonsynonymous changes) alter the token sequence, while synonymous mutations within a codon alter the specific token used but maintain the broader codon-family structure.</p>
<p>Life-Code (2025) extended codon-aware tokenization to broader genomic contexts, encoding coding and noncoding regions in a way that respects reading frame and local biological function <span class="citation" data-cites="liu_life-code_2025">(<a href="references.html#ref-liu_life-code_2025" role="doc-biblioref">Liu et al. 2025</a>)</span>. The approach uses different tokenization strategies for different genomic regions based on their biological function. Coding regions are tokenized by codons, with each three-nucleotide unit encoding an amino acid becoming a single token. This aligns the token boundaries with the fundamental unit of protein translation, enabling the model to learn directly about amino acid sequences and protein structure. Noncoding regions, lacking codon structure, are tokenized by learned patterns that capture regulatory motifs and other functional elements.</p>
<p>Codon tokenization offers several conceptual advantages for modeling coding regions. The approach provides natural compression, with a protein-coding sequence of 300 amino acids spanning 900 nucleotides but compressing to 300 codon tokens. This compression enables whole-gene or even whole-genome modeling within standard transformer context windows, as GenSLMs demonstrated by processing entire viral genomes as coherent sequences. The model can potentially learn codon usage biases, translational efficiency patterns, and other codon-level phenomena that would be obscured by arbitrary k-mer boundaries.</p>
<p>This biologically-informed design enables models like Life-Code to learn protein structure through knowledge distillation from protein language models, capture interactions between coding and noncoding regions within a unified framework, and achieve state-of-the-art results across tasks involving DNA, RNA, and protein. The approach demonstrates that tokenization need not be uniform across the genome, and that encoding biological knowledge in the representation itself can improve model capabilities.</p>
<p>However, codon tokenization has significant limitations for general genomic modeling. The approach assumes a defined reading frame, which works for known coding sequences but fails for noncoding regions where no frame exists. Frame shifts, common in viral genomes and relevant for certain human genetic contexts, disrupt the codon structure and require special handling. Regulatory regions, introns, and intergenic sequences have no codon organization to leverage. These limitations restrict codon tokenization to applications where coding sequence dominates, such as protein-centric modeling or viral genomics where the majority of the genome encodes proteins.</p>
<p>The broader lesson is that tokenization can and perhaps should be informed by biological structure when that structure is known and relevant. BPE learns statistical patterns from the corpus, but those patterns need not correspond to biological units. Codon tokenization imposes biological semantics directly, at the cost of applicability to noncoding regions. Future approaches might combine these strategies, using codon-aware tokenization for coding regions and BPE or single-nucleotide tokens for noncoding sequence, though such hybrid schemes introduce complexity in handling transitions between regions.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.7 – Codon-aware vs base-level tokens</strong></p>
<p>Show a coding exon segmented into codons vs bases. Highlight a synonymous and a non-synonymous SNV at the codon level. Optional: link codon tokens to amino acid tokens to illustrate the sequence–protein bridge.</p>
</div>
</div>
</section>
<section id="variant-tokens" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="variant-tokens"><span class="header-section-number">5.7.2</span> Variant Tokens</h3>
<p>BioToken (2025) extends tokenization even further beyond sequence content to include explicit genomic structural annotations <span class="citation" data-cites="medvedev_biotoken_2025">(<a href="references.html#ref-medvedev_biotoken_2025" role="doc-biblioref">Medvedev et al. 2025</a>)</span>. Rather than treating variants as implicit changes in the sequence string, BioToken creates tokens that explicitly represent SNPs, insertions, and deletions. Known regulatory elements receive dedicated tokens encoding their presence and type. Gene structure, chromatin state, and other functional annotations are integrated directly into the token representation. This approach treats tokens as rich entities that bundle nucleotides with positional, functional, or experimental context.</p>
<p>Such variant-aware representations are especially attractive for variant effect prediction and clinical interpretation, where the input is often “reference plus variant” rather than a generic sequence. Embedding variants directly can enable the model to share information across genomic positions and contexts for similar variants, make it easier to integrate non-sequence features (such as constraint scores or population frequency) into a unified representation, and facilitate pretraining objectives that operate on variant events.</p>
<p>By incorporating biological inductive biases directly into tokenization, BioToken’s associated model (BioFM) achieves competitive or superior performance to specialized models like Enformer and SpliceAI with significantly fewer parameters, approximately 265 million compared to the billions in some contemporary models. This efficiency suggests that appropriate representation can substitute for model scale, at least partially, by making the learning problem easier through informed structure.</p>
<p>The cost is increased complexity in data preprocessing and model design. Variant tokens require careful handling of how reference and alternate alleles are represented, how indels that span multiple bases are encoded, and how the model learns to reason about variant-specific features versus general sequence context. But for translational applications, the payoff can be substantial: variant-aware tokens bring the model’s input space closer to the actual objects of biomedical interest.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.8 – Variant-aware tokenization</strong></p>
<p>Panel A: classic “reference sequence only” tokenization. Panel B: sequence plus explicit variant tokens (e.g., SNV markers or composite variant tokens). Emphasize how variant tokens can span multiple neighboring bases (e.g., indels) but still map to a conceptually single event.</p>
</div>
</div>
</section>
</section>
<section id="the-context-length-evolution" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="the-context-length-evolution"><span class="header-section-number">5.8</span> The Context Length Evolution</h2>
<p>Examining the history of genomic deep learning reveals a consistent trend toward longer sequence context, reflecting growing appreciation for the importance of distal regulatory interactions. The earliest CNN models from 2015 to 2017, including DeepSEA and DeepBind, operated on sequences of approximately 1 kb, sufficient to capture local motifs and their immediate context. The next generation of models from 2018 to 2020, including ExPecto and SpliceAI, expanded to 10-40 kb windows, enabling capture of promoter-proximal regulatory elements and the extended context needed for accurate splice site prediction.</p>
<p>The transformer era beginning in 2021 brought divergent approaches. DNABERT with its overlapping k-mers was limited to approximately 512 bp of effective context, while Enformer combined CNN preprocessing with attention to achieve 200 kb contexts. The Nucleotide Transformer (2022-2023) pushed transformer-based models to 6 kb using k-mer tokenization <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>. Then HyenaDNA and Caduceus (2023-2024) demonstrated that sub-quadratic architectures could reach 1 Mb while maintaining single-nucleotide resolution through character-level tokenization. Most recently, Evo 2 (2025) has achieved similar million-base-pair contexts using single-nucleotide tokens with BPE-style learned embeddings.</p>
<p>This progression reflects biological reality. Enhancers can regulate genes from hundreds of kilobases away. TAD boundaries and loop anchors create long-range dependencies in chromatin organization. Understanding genome function requires integrating information across these distances, and representation schemes must enable architectures capable of capturing such interactions. From a design perspective, context length is now less a fixed constraint and more a tunable design dimension, mediated jointly by tokenization, architecture, and hardware budget.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.9 – Historical timeline</strong></p>
<p>Timeline of models with approximate context lengths and tokenization strategies: DeepSEA / ExPecto / SpliceAI (one-hot), DNABERT (k-mers), DNABERT-2 / genome BPE models, Nucleotide Transformer, HyenaDNA. Optionally color-code by architecture family (CNN, transformer, long-range hybrid).</p>
</div>
</div>
</section>
<section id="trade-offs-and-practical-considerations" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="trade-offs-and-practical-considerations"><span class="header-section-number">5.9</span> Trade-offs and Practical Considerations</h2>
<p>The choice between tokenization strategies involves multiple competing considerations that depend on the intended application. Compression and resolution exist in fundamental tension. Higher compression enables longer context windows within fixed computational budgets, but loses precision for identifying exactly where variants fall and how they relate to biological features. One-hot encoding and single-nucleotide tokenization provide no compression but maintain full resolution. Non-overlapping k-mers achieve approximately k-fold compression at the cost of k-nucleotide resolution. BPE provides variable compression depending on sequence repetitiveness, with corresponding variable resolution. For variant effect prediction, where single nucleotide changes can have dramatic phenotypic consequences, resolution is paramount and the computational costs of long single-nucleotide sequences are often justified.</p>
<p>Vocabulary size affects both model capacity and efficiency. Larger vocabularies require bigger embedding tables but may capture more complex patterns directly. Smaller vocabularies are parameter-efficient but require the model to learn compositional structure through multiple layers. The vocabulary size of one-hot encoding (4 tokens plus special tokens) minimizes embedding parameters but maximizes the compositional learning burden. K-mer vocabularies scale exponentially with k, reaching 4,096 for 6-mers. BPE vocabularies are tunable, typically ranging from 4,096 to 32,000 tokens for genomic applications. Codon-aware approaches use approximately 64 codons plus additional tokens for noncoding regions.</p>
<p>Computational efficiency depends on both tokenization and architecture. For standard attention with <span class="math inline">\(O(L^2)\)</span> complexity, any compression directly reduces cost: non-overlapping k-mers reduce attention cost by a factor of <span class="math inline">\(k^2\)</span>, and BPE with average compression <span class="math inline">\(c\)</span> reduces cost by <span class="math inline">\(c^2\)</span>. But sub-quadratic architectures like Hyena change this calculus, making single-nucleotide tokenization computationally feasible at long contexts and eliminating the need to trade resolution for efficiency.</p>
<p>For variant effect prediction specifically, tokenization choice has direct implications. Single-nucleotide tokens (as in HyenaDNA) enable clean comparison of reference and alternate alleles at the same token position with no ambiguity about effect localization. K-mer tokens complicate matters because a single SNP changes <span class="math inline">\(k\)</span> overlapping tokens, requiring aggregation across affected tokens and introducing potential boundary effects. BPE tokens create context-dependent effects where the same variant may fall at different positions relative to token boundaries depending on surrounding sequence, and where re-tokenization may be needed to properly represent the alternate allele.</p>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Visual suggestion 10.10 – Design table</strong></p>
<p>A table summarizing tokenization strategies (rows) vs design axes (columns): context length, variant resolution, biological alignment, implementation complexity. Highlight recommended use cases (e.g., “best for VEP”, “best for genome-scale pretraining”).</p>
</div>
</div>
</section>
<section id="the-emerging-consensus" class="level2" data-number="5.10">
<h2 data-number="5.10" class="anchored" data-anchor-id="the-emerging-consensus"><span class="header-section-number">5.10</span> The Emerging Consensus</h2>
<p>Recent developments in the field suggest convergence toward several principles, though the optimal approach continues to evolve. First, single-nucleotide resolution has become the preferred choice for applications requiring precise variant interpretation. The development of sub-quadratic architectures like Hyena, Mamba, and state space models has eliminated the computational barriers that previously forced researchers to accept resolution trade-offs. When long context and high resolution can both be achieved, there is little reason to sacrifice resolution through compression.</p>
<p>Second, learned embeddings rather than fixed representations have become standard. Even single-nucleotide tokenization now typically involves trainable embeddings that transform the four nucleotide identities into dense vectors. This allows the model to discover meaningful representations of nucleotide properties rather than treating all positions equivalently.</p>
<p>Third, biologically-informed augmentation has emerged as a promising direction for incorporating domain knowledge. Encoding codons in coding regions, incorporating functional annotations, or using species-specific vocabularies can provide useful inductive biases that improve learning efficiency and model interpretability.</p>
<p>Fourth, hybrid approaches that combine multiple representation strategies show promise for different genomic contexts. A model might use codon-level tokenization within genes while employing single-nucleotide tokens in regulatory regions, adapting the representation to the structure of each region.</p>
<p>The choice ultimately depends on the task at hand. Variant effect prediction demands high resolution and benefits most from single-nucleotide approaches. Species classification or repeat annotation may benefit from compression that enables comparison across longer regions. Expression prediction requires sufficient context to capture distal enhancers while maintaining resolution to identify causal variants. Understanding these trade-offs is essential for selecting or designing appropriate representations for specific applications.</p>
<p>In other words, there is no single “correct” tokenization. Instead, we see a toolbox of representations, each suited to particular regimes of context length, resolution, and task demands. For many practitioners, a reasonable heuristic has emerged: use single-nucleotide tokens when variant-level reasoning or high-resolution interpretability is central, use k-mers or BPE when context length is the primary bottleneck and tasks do not require base-level precision, and consider biologically-informed tokens when integrating multi-modal or annotation-rich data.</p>
</section>
<section id="implications-for-subsequent-chapters" class="level2" data-number="5.11">
<h2 data-number="5.11" class="anchored" data-anchor-id="implications-for-subsequent-chapters"><span class="header-section-number">5.11</span> Implications for Subsequent Chapters</h2>
<p>The tokenization choices examined in this chapter set the stage for the design decisions discussed throughout the rest of the book. In <a href="p3-ch11-dna.html" class="quarto-xref"><span>Chapter 11</span></a>, we will revisit why many DNA transformers still use 6-mers, and how models like the Nucleotide Transformer balance context length with biological resolution. In <a href="p3-ch12-rna.html" class="quarto-xref"><span>Chapter 12</span></a> and <a href="p3-ch14-hybrid.html" class="quarto-xref"><span>Chapter 14</span></a>, we examine how long-range models like Enformer and Borzoi largely retained one-hot encoding or single-nucleotide input encodings for precision in variant effect prediction, leaning on architectural innovations rather than token compression to scale context. In <a href="p2-ch07-foundation.html" class="quarto-xref"><span>Chapter 7</span></a>, we explore how pretraining objectives interact with tokenization choices, including masked language modeling over k-mers versus bases, or variant-aware pretraining objectives over BioToken-style representations.</p>
<p>The representation problem remains an active area of research. As models grow larger and contexts extend further, new tokenization strategies may emerge that better balance compression, resolution, and biological structure. The field has moved from treating tokenization as a fixed preprocessing step to recognizing it as a fundamental design decision that shapes what models can learn and how they can be applied. Understanding sequence representation is therefore not a technical footnote but a core element of genomic foundation model design, with implications that ripple through every subsequent chapter of this book.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-du_gene2vec_2019" class="csl-entry" role="listitem">
Du, Jingcheng, Peilin Jia, Yulin Dai, Cui Tao, Zhongming Zhao, and Degui Zhi. 2019. <span>“Gene2vec: Distributed Representation of Genes Based on Co-Expression.”</span> <em>BMC Genomics</em> 20 (1): 82. <a href="https://doi.org/10.1186/s12864-018-5370-x">https://doi.org/10.1186/s12864-018-5370-x</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-liu_life-code_2025" class="csl-entry" role="listitem">
Liu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. <span>“Life-<span>Code</span>: <span>Central</span> <span>Dogma</span> <span>Modeling</span> with <span>Multi</span>-<span>Omics</span> <span>Sequence</span> <span>Unification</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.07299">https://doi.org/10.48550/arXiv.2502.07299</a>.
</div>
<div id="ref-medvedev_biotoken_2025" class="csl-entry" role="listitem">
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. <span>“<span>BioToken</span> and <span>BioFM</span> – <span>Biologically</span>-<span>Informed</span> <span>Tokenization</span> <span>Enables</span> <span>Accurate</span> and <span>Efficient</span> <span>Genomic</span> <span>Foundation</span> <span>Models</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.03.27.645711">https://doi.org/10.1101/2025.03.27.645711</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-sanabria_grover_2024" class="csl-entry" role="listitem">
Sanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. <span>“[<span>GROVER</span>] <span>DNA</span> Language Model <span>GROVER</span> Learns Sequence Context in the Human Genome.”</span> <em>Nature Machine Intelligence</em> 6 (8): 911–23. <a href="https://doi.org/10.1038/s42256-024-00872-0">https://doi.org/10.1038/s42256-024-00872-0</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
<div id="ref-zvyagin_genslms_2022" class="csl-entry" role="listitem">
Zvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. <span>“<span>GenSLMs</span>: <span>Genome</span>-Scale Language Models Reveal <span>SARS</span>-<span>CoV</span>-2 Evolutionary Dynamics.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.10.10.511571">https://doi.org/10.1101/2022.10.10.511571</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p2--principles.html" class="pagination-link" aria-label="Part II: Core Principles">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Part II: Core Principles</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p2-ch06-transformers.html" class="pagination-link" aria-label="Transformer Architecture for Genomics">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Transformer Architecture for Genomics</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>