<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; DNA and Genomic Models – Genomic Foundation Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./p2-ch08-rna.html" rel="next">
<link href="./p2-ch06-plm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d1e9b63c6b6094879b9f94a7628e3370.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-a1553387a0f784068632030e9fbb8a3c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--architectures.html">Part II: Deep Learning Architectures</a></li><li class="breadcrumb-item"><a href="./p2-ch07-dna.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Genomic Foundation Models</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p1--foundations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part I: Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch01-ngs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Sequencing: From Reads to Variants</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch02-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The Genomic Data Landscape</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch03-pgs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">GWAS &amp; Polygenic Scores</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p1-ch04-cadd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Deleteriousness Scores</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p2--architectures.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part II: Deep Learning Architectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch05-cnn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">CNN Sequence-to-Function Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch06-plm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch07-dna.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch08-rna.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p2-ch09-hybrid.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Long-range Hybrid Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p3--principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part III: Core Principles</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch10-tokens.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Sequence Representation &amp; Tokens</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch11-principles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Genomic FMs: Principles &amp; Practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p3-ch12-vep.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Variant Effect Prediction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p4--multi-scale.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part IV: Multi-Scale Modeling</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch13-sc-epi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Single-Cell &amp; Epigenomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch14-networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Graphs, Networks, and Biology</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p4-ch15-systems.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multi-Omics &amp; Systems Biology</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p5--eval-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part V: Evaluation and Reliability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch16-benchmarks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Benchmarks for Genomic Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch17-eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Evaluation of Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch18-confound.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Confounders in Model Training</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p5-ch19-interp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Interpretability &amp; Mechanisms</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./p6--translation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Part VI — Translation and Application</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch20-clinical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Clinical Risk Prediction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch21-variants.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Pathogenic Variant Discovery</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch22-drugs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Drug Discovery &amp; Biotech</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch23-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Sequence Design</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./p6-ch24-future.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Future Work &amp; Ethics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-a-dl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Deep Learning Primer</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-b-deployment.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Model Deployment</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-c-model-list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Referenced Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-d-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Additional Resources</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./app-e-glossary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Glossary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#from-supervised-cnns-to-self-supervised-genomic-language-models" id="toc-from-supervised-cnns-to-self-supervised-genomic-language-models" class="nav-link active" data-scroll-target="#from-supervised-cnns-to-self-supervised-genomic-language-models"><span class="header-section-number">7.1</span> From Supervised CNNs to Self-Supervised Genomic Language Models</a></li>
  <li><a href="#dnabert-bert-for-k-merized-dna" id="toc-dnabert-bert-for-k-merized-dna" class="nav-link" data-scroll-target="#dnabert-bert-for-k-merized-dna"><span class="header-section-number">7.2</span> DNABERT: BERT for K-merized DNA</a></li>
  <li><a href="#dnabert-2-improved-tokenization-and-efficiency" id="toc-dnabert-2-improved-tokenization-and-efficiency" class="nav-link" data-scroll-target="#dnabert-2-improved-tokenization-and-efficiency"><span class="header-section-number">7.3</span> DNABERT-2: Improved Tokenization and Efficiency</a></li>
  <li><a href="#nucleotide-transformer-scaling-context-and-diversity" id="toc-nucleotide-transformer-scaling-context-and-diversity" class="nav-link" data-scroll-target="#nucleotide-transformer-scaling-context-and-diversity"><span class="header-section-number">7.4</span> Nucleotide Transformer: Scaling Context and Diversity</a></li>
  <li><a href="#gpn-cross-species-pretraining-for-variant-effect-prediction" id="toc-gpn-cross-species-pretraining-for-variant-effect-prediction" class="nav-link" data-scroll-target="#gpn-cross-species-pretraining-for-variant-effect-prediction"><span class="header-section-number">7.5</span> GPN: Cross-Species Pretraining for Variant Effect Prediction</a></li>
  <li><a href="#hyenadna-megabase-context-at-single-nucleotide-resolution" id="toc-hyenadna-megabase-context-at-single-nucleotide-resolution" class="nav-link" data-scroll-target="#hyenadna-megabase-context-at-single-nucleotide-resolution"><span class="header-section-number">7.6</span> HyenaDNA: Megabase Context at Single-Nucleotide Resolution</a></li>
  <li><a href="#caduceus-bidirectional-modeling-with-reverse-complement-equivariance" id="toc-caduceus-bidirectional-modeling-with-reverse-complement-equivariance" class="nav-link" data-scroll-target="#caduceus-bidirectional-modeling-with-reverse-complement-equivariance"><span class="header-section-number">7.7</span> Caduceus: Bidirectional Modeling with Reverse-Complement Equivariance</a></li>
  <li><a href="#grover-generative-regulatory-foundation-models" id="toc-grover-generative-regulatory-foundation-models" class="nav-link" data-scroll-target="#grover-generative-regulatory-foundation-models"><span class="header-section-number">7.8</span> GROVER: Generative Regulatory Foundation Models</a></li>
  <li><a href="#genslms-codon-tokenization-and-whole-genome-modeling" id="toc-genslms-codon-tokenization-and-whole-genome-modeling" class="nav-link" data-scroll-target="#genslms-codon-tokenization-and-whole-genome-modeling"><span class="header-section-number">7.9</span> GenSLMs: Codon Tokenization and Whole-Genome Modeling</a></li>
  <li><a href="#evo-2-genome-scale-language-modeling-across-all-life" id="toc-evo-2-genome-scale-language-modeling-across-all-life" class="nav-link" data-scroll-target="#evo-2-genome-scale-language-modeling-across-all-life"><span class="header-section-number">7.10</span> Evo 2: Genome-Scale Language Modeling Across All Life</a>
  <ul class="collapse">
  <li><a href="#scale-and-training-corpus" id="toc-scale-and-training-corpus" class="nav-link" data-scroll-target="#scale-and-training-corpus"><span class="header-section-number">7.10.1</span> Scale and Training Corpus</a></li>
  <li><a href="#architecture-stripedhyena-2" id="toc-architecture-stripedhyena-2" class="nav-link" data-scroll-target="#architecture-stripedhyena-2"><span class="header-section-number">7.10.2</span> Architecture: StripedHyena 2</a></li>
  <li><a href="#emergent-biological-knowledge" id="toc-emergent-biological-knowledge" class="nav-link" data-scroll-target="#emergent-biological-knowledge"><span class="header-section-number">7.10.3</span> Emergent Biological Knowledge</a></li>
  <li><a href="#zero-shot-variant-effect-prediction" id="toc-zero-shot-variant-effect-prediction" class="nav-link" data-scroll-target="#zero-shot-variant-effect-prediction"><span class="header-section-number">7.10.4</span> Zero-Shot Variant Effect Prediction</a></li>
  <li><a href="#cross-species-applications" id="toc-cross-species-applications" class="nav-link" data-scroll-target="#cross-species-applications"><span class="header-section-number">7.10.5</span> Cross-Species Applications</a></li>
  <li><a href="#generative-capabilities" id="toc-generative-capabilities" class="nav-link" data-scroll-target="#generative-capabilities"><span class="header-section-number">7.10.6</span> Generative Capabilities</a></li>
  <li><a href="#limitations-and-trade-offs" id="toc-limitations-and-trade-offs" class="nav-link" data-scroll-target="#limitations-and-trade-offs"><span class="header-section-number">7.10.7</span> Limitations and Trade-offs</a></li>
  </ul></li>
  <li><a href="#central-dogma-aware-and-annotation-enriched-models" id="toc-central-dogma-aware-and-annotation-enriched-models" class="nav-link" data-scroll-target="#central-dogma-aware-and-annotation-enriched-models"><span class="header-section-number">7.11</span> Central-Dogma-Aware and Annotation-Enriched Models</a>
  <ul class="collapse">
  <li><a href="#life-code-the-central-dogma-as-inductive-bias" id="toc-life-code-the-central-dogma-as-inductive-bias" class="nav-link" data-scroll-target="#life-code-the-central-dogma-as-inductive-bias"><span class="header-section-number">7.11.1</span> Life-Code: The Central Dogma as Inductive Bias</a></li>
  <li><a href="#biotoken-encoding-variants-and-structure" id="toc-biotoken-encoding-variants-and-structure" class="nav-link" data-scroll-target="#biotoken-encoding-variants-and-structure"><span class="header-section-number">7.11.2</span> BioToken: Encoding Variants and Structure</a></li>
  </ul></li>
  <li><a href="#using-genomic-language-models-in-practice" id="toc-using-genomic-language-models-in-practice" class="nav-link" data-scroll-target="#using-genomic-language-models-in-practice"><span class="header-section-number">7.12</span> Using Genomic Language Models in Practice</a>
  <ul class="collapse">
  <li><a href="#embeddings-as-universal-features" id="toc-embeddings-as-universal-features" class="nav-link" data-scroll-target="#embeddings-as-universal-features"><span class="header-section-number">7.12.1</span> Embeddings as Universal Features</a></li>
  <li><a href="#fine-tuning-and-task-specific-heads" id="toc-fine-tuning-and-task-specific-heads" class="nav-link" data-scroll-target="#fine-tuning-and-task-specific-heads"><span class="header-section-number">7.12.2</span> Fine-Tuning and Task-Specific Heads</a></li>
  <li><a href="#zero-shot-and-few-shot-scoring" id="toc-zero-shot-and-few-shot-scoring" class="nav-link" data-scroll-target="#zero-shot-and-few-shot-scoring"><span class="header-section-number">7.12.3</span> Zero-Shot and Few-Shot Scoring</a></li>
  </ul></li>
  <li><a href="#emerging-themes-and-current-limitations" id="toc-emerging-themes-and-current-limitations" class="nav-link" data-scroll-target="#emerging-themes-and-current-limitations"><span class="header-section-number">7.13</span> Emerging Themes and Current Limitations</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">7.14</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./p2--architectures.html">Part II: Deep Learning Architectures</a></li><li class="breadcrumb-item"><a href="./p2-ch07-dna.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-dna" class="quarto-section-identifier"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">DNA and Genomic Models</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>TODO:</strong> - Flesh-out Evo-2 section - Add figure: timeline of genomic language model development (DNABERT → Nucleotide Transformer → HyenaDNA → Caduceus → GROVER) - Add figure: architecture comparison diagram showing transformer vs Hyena vs Mamba approaches - Add figure: context length evolution visualization showing the dramatic expansion from 512 bp to 1 Mb - Add visualization: benchmark performance comparison across Nucleotide Transformer tasks - Add figure: conceptual diagram of in-context learning in genomics (HyenaDNA) - Add table: comprehensive model comparison with parameters, training data, context length, and key innovations - Citations: verify all citations are in bibliography</p>
</div>
</div>
<p>Genomic language models extend the ideas of protein language models (<a href="p2-ch06-plm.html" class="quarto-xref"><span>Chapter 6</span></a>) to the DNA level. They treat genomes themselves as a corpus, learn statistical regularities through self-supervision, and reuse those representations for many downstream tasks. Where Chapters 5–7 focused on supervised sequence-to-function CNNs and specialized architectures, and <a href="p3-ch10-tokens.html" class="quarto-xref"><span>Chapter 10</span></a> focused on representation and tokenization, this chapter turns to genomic foundation models: large, often transformer-based or hybrid architectures trained on unlabeled genomic sequence at scale.</p>
<p>These models aim to provide a single, reusable backbone for tasks ranging from regulatory annotation and variant effect prediction to cross-species transfer and clinical prioritization. They mark the transition from building one model per dataset to constructing general-purpose genomic backbones analogous to BERT, GPT, and ESM in natural and protein language modeling.</p>
<section id="from-supervised-cnns-to-self-supervised-genomic-language-models" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="from-supervised-cnns-to-self-supervised-genomic-language-models"><span class="header-section-number">7.1</span> From Supervised CNNs to Self-Supervised Genomic Language Models</h2>
<p>The CNN era represented by DeepSEA, ExPecto, and SpliceAI (Chapters 5–7) shared a common pattern. Models took one-hot encoded DNA sequence around a locus as input, predicted task-specific labels such as chromatin marks, expression levels, or splice junctions, and optimized supervised loss functions against those labels. This approach achieved remarkable performance but suffered from three fundamental constraints.</p>
<p>The first constraint was label dependence. Every new assay, cell type, or phenotype required new labeled data to train a model. A chromatin accessibility model trained on ENCODE data could not predict histone modifications without additional labeled examples for those marks. This created substantial overhead for each new application.</p>
<p>The second constraint was task coupling. Model design became tightly coupled to the specific task. SpliceAI’s architecture was specialized for splice junction prediction, with convolutions designed to capture the relevant spatial patterns. ExPecto’s spatial feature transformation was engineered specifically for the distance-dependent relationship between regulatory elements and transcription start sites. These architectural choices, while effective for their intended purposes, did not transfer naturally to other problems.</p>
<p>The third constraint was limited reuse. Features learned for one problem did not automatically transfer to others. A model trained to predict chromatin accessibility might learn representations of regulatory motifs, but those representations were not directly accessible for other tasks like variant effect prediction or gene expression modeling without substantial re-engineering.</p>
<p>Protein language models showed a different route: self-supervised learning on unlabeled sequences, with downstream tasks solved by probing or fine-tuning. Genomic language models import this recipe to DNA. The training data comprises large collections of genomic sequences across species, individuals, or functional regions. The training objectives include masked language modeling, where the model predicts masked bases or tokens from surrounding context, and next-token or sequence modeling, where the model predicts the next token in a sequence. Some models combine these self-supervised objectives with auxiliary tasks such as predicting known annotations.</p>
<p>These pretrained models can be used in multiple ways. The simplest approach freezes the model and trains lightweight probes for specific tasks. Fine-tuning updates the entire model or uses adapter modules for specialized downstream applications. Zero-shot or few-shot scoring compares log-likelihoods of alternative sequences or alleles without any task-specific training. The promise is that once a sufficiently powerful backbone is trained, it becomes the default starting point for nearly any DNA-level prediction problem.</p>
</section>
<section id="dnabert-bert-for-k-merized-dna" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="dnabert-bert-for-k-merized-dna"><span class="header-section-number">7.2</span> DNABERT: BERT for K-merized DNA</h2>
<p>DNABERT applied the BERT masked language modeling framework to genomic sequences, using overlapping k-mers (typically 6-mers) as tokens and training on human reference sequences <span class="citation" data-cites="ji_dnabert_2021">(<a href="references.html#ref-ji_dnabert_2021" role="doc-biblioref">Ji et al. 2021</a>)</span>. As discussed in <a href="p3-ch10-tokens.html" class="quarto-xref"><span>Chapter 10</span></a>, this design had several defining characteristics.</p>
<p>The tokenization scheme converted DNA sequences into overlapping k-mers, creating a discrete vocabulary of size <span class="math inline">\(4^k\)</span>. For 6-mers, this yields a vocabulary of 4,096 tokens. The model used the standard BERT architecture with masked token prediction as its training objective. Context windows were relatively modest, spanning a few hundred base pairs (typically 512 tokens). The model was then fine-tuned on downstream tasks including promoter classification, splice site prediction, and transcription factor binding site identification.</p>
<p>DNABERT provided proof of concept for several important ideas. Self-supervised pretraining on raw DNA can improve performance over training from scratch. Learned embeddings capture biologically meaningful regularities, even when trained only on the reference genome. BERT-style architectures can be re-used across multiple downstream tasks with modest fine-tuning.</p>
<p>However, the k-mer design introduced significant limitations detailed in <a href="p3-ch10-tokens.html" class="quarto-xref"><span>Chapter 10</span></a>. The overlapping k-mer tokenization provided no true sequence compression, as each nucleotide participated in multiple adjacent tokens. This created ambiguity in positional interpretation, since the precise position of a variant within the k-mer vocabulary was unclear. The quadratic attention complexity of transformers combined with redundant overlapping tokens severely limited the effective context length.</p>
</section>
<section id="dnabert-2-improved-tokenization-and-efficiency" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="dnabert-2-improved-tokenization-and-efficiency"><span class="header-section-number">7.3</span> DNABERT-2: Improved Tokenization and Efficiency</h2>
<p>DNABERT-2 revisited both tokenization and architecture, demonstrating how much representation choices matter for genomic language models <span class="citation" data-cites="zhou_dnabert-2_2024">(<a href="references.html#ref-zhou_dnabert-2_2024" role="doc-biblioref">Zhou et al. 2024</a>)</span>. The key differences relative to the original DNABERT addressed its core limitations.</p>
<p>The tokenization scheme adopted improved approaches such as BPE-style merges that better compress redundancies and reduce effective sequence length. This allowed the model to represent longer genomic contexts within the same number of tokens. Architectural refinements improved efficiency, enabling scaling to larger contexts and training corpora without prohibitive memory costs.</p>
<p>On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring, DNABERT-2 achieved consistent gains over both the original DNABERT and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications.</p>
<p>The DNABERT family collectively established three important principles. Self-supervision on DNA works and is competitive with hand-engineered pipelines for many sequence annotation tasks. Tokenization choices have large practical consequences, as the seemingly minor decision of how to convert nucleotides into tokens substantially affects both computational efficiency and downstream performance. Masked language model training can produce reusable representations for diverse sequence tasks, suggesting that the foundation model paradigm transfers effectively from natural language to genomic sequence.</p>
</section>
<section id="nucleotide-transformer-scaling-context-and-diversity" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="nucleotide-transformer-scaling-context-and-diversity"><span class="header-section-number">7.4</span> Nucleotide Transformer: Scaling Context and Diversity</h2>
<p>DNABERT demonstrated feasibility, but its context windows and training data were modest relative to the scale of genomes. The Nucleotide Transformer pushed much further, emphasizing scale and diversity in both model size and training corpus <span class="citation" data-cites="dalla-torre_nucleotide_2023">(<a href="references.html#ref-dalla-torre_nucleotide_2023" role="doc-biblioref">Dalla-Torre et al. 2023</a>)</span>.</p>
<p>The training corpus spanned genomic data from multiple species and populations, exposing the model to diverse sequence patterns. The architecture comprised transformer encoders of various sizes, from moderate to very large parameter counts. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over DNABERT while still using dense attention. The training objective remained masked language modeling on subsequences sampled from genomes.</p>
<p>The Nucleotide Transformer work contributed several important ideas to the field. Cross-species pretraining, where training spans many genomes rather than a single reference, exposes the model to diverse sequence patterns, different regulatory architectures, and evolutionary constraints that recur across lineages. This mirrors the use of large multi-species multiple sequence alignments in protein language models but operates at the raw DNA level.</p>
<p>To quantify representation quality, the Nucleotide Transformer introduced a benchmark panel of genomic tasks that has become a standard yardstick for subsequent DNA language models. Typical tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, variant and pathogenicity proxies, and regulatory element type classification. Models are evaluated via linear probes, shallow classifiers, or light fine-tuning.</p>
<p>As with protein and natural-language models, performance improved predictably with larger models, more pretraining data, and longer context windows. These scaling trends help forecast the returns from investing in even larger genomic language models.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Architecture</th>
<th>Max Context</th>
<th>Complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DNABERT</td>
<td>Transformer</td>
<td>512 bp</td>
<td><span class="math inline">\(O(L^2)\)</span></td>
</tr>
<tr class="even">
<td>Nucleotide Transformer</td>
<td>Transformer</td>
<td>6 kb</td>
<td><span class="math inline">\(O(L^2)\)</span></td>
</tr>
<tr class="odd">
<td>HyenaDNA</td>
<td>Hyena</td>
<td>1 Mb</td>
<td><span class="math inline">\(O(L \log L)\)</span></td>
</tr>
<tr class="even">
<td>Caduceus</td>
<td>Mamba</td>
<td>1 Mb</td>
<td><span class="math inline">\(O(L)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="gpn-cross-species-pretraining-for-variant-effect-prediction" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="gpn-cross-species-pretraining-for-variant-effect-prediction"><span class="header-section-number">7.5</span> GPN: Cross-Species Pretraining for Variant Effect Prediction</h2>
<p>While the Nucleotide Transformer demonstrated the value of scaling, the Genomic Pre-trained Network (GPN) explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes <span class="citation" data-cites="benegas_gpn_2023">(<a href="references.html#ref-benegas_gpn_2023" role="doc-biblioref">Benegas, Batra, and Song 2023</a>)</span>. Rather than scaling to the largest possible model and training corpus, GPN asked whether the self-supervised paradigm could yield useful variant effect predictors even in a more constrained setting.</p>
<p>GPN was trained on unaligned reference genomes from <em>Arabidopsis thaliana</em> and seven related species within the Brassicales order. The training objective was masked language modeling on DNA sequences, predicting masked nucleotides from surrounding context. This is the same fundamental objective used by DNABERT and the Nucleotide Transformer, but applied to a much smaller and more phylogenetically focused corpus.</p>
<p>The key insight from GPN was that self-supervised DNA language models learn biologically meaningful representations without explicit supervision on functional annotations. Analysis of the trained model revealed emergent encoding of gene structure, including exon-intron boundaries and splice sites, as well as DNA sequence motifs associated with transcription factor binding and other regulatory functions. The model discovered these patterns purely from the statistical regularities of genomic sequence across related species.</p>
<p>For variant effect prediction, GPN used a likelihood ratio approach. Given a reference and alternate allele at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood, relative to the reference, are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.</p>
<p>Evaluated on <em>A. thaliana</em> variants using allele frequencies from the 1001 Genomes Project and a comprehensive database of GWAS associations, GPN outperformed traditional conservation scores including phyloP and phastCons. This was notable because phyloP and phastCons are computed from explicit multiple sequence alignments and evolutionary models, while GPN learned its representations from unaligned sequences through self-supervision alone.</p>
<p>GPN established several principles that would influence subsequent work. First, cross-species pretraining captures evolutionary constraints that transfer to variant effect prediction, even without alignment-based conservation calculations. Second, relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures for species within that group. Third, the masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.</p>
<p>The limitation of the original GPN was its scope. Training on plant genomes did not directly produce a human variant effect predictor. The later GPN-MSA addressed this gap by incorporating multi-species alignments and training on mammalian genomes, achieving strong performance on human variant benchmarks as discussed in <a href="p3-ch12-vep.html" class="quarto-xref"><span>Chapter 12</span></a>. However, the original GPN remains important as a demonstration that the DNA language model paradigm extends beyond model organisms and can discover biologically meaningful patterns through self-supervision on comparatively modest training data.</p>
</section>
<section id="hyenadna-megabase-context-at-single-nucleotide-resolution" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="hyenadna-megabase-context-at-single-nucleotide-resolution"><span class="header-section-number">7.6</span> HyenaDNA: Megabase Context at Single-Nucleotide Resolution</h2>
<p>Quadratic attention limits transformer context length to tens of kilobases at best, even with aggressive engineering. This is a fundamental architectural constraint: processing a 100 kb sequence with dense attention requires on the order of <span class="math inline">\(10^{10}\)</span> attention computations per layer. HyenaDNA addressed this limitation by replacing attention with a Hyena long-convolution architecture that scales sub-quadratically, enabling processing of sequences up to 1 Mb in length <span class="citation" data-cites="nguyen_hyenadna_2023">(<a href="references.html#ref-nguyen_hyenadna_2023" role="doc-biblioref">Nguyen et al. 2023</a>)</span>.</p>
<p>The Hyena architecture uses implicit convolutions, parameterizing long convolutional filters through neural networks rather than storing explicit filter weights. This approach achieves <span class="math inline">\(O(L \log L)\)</span> complexity through efficient FFT-based convolution, compared to the <span class="math inline">\(O(L^2)\)</span> complexity of standard attention. The result is a 500-fold increase in context length over previous dense attention models while maintaining single-nucleotide resolution.</p>
<p>HyenaDNA introduced several qualitative advances that matter for biological applications. Processing megabase-scale windows allows the model to see entire gene bodies plus their flanking regulatory regions, long-range enhancer-promoter interactions spanning tens to hundreds of kilobases, and topologically associating domain (TAD) scale structure. This aligns better with biological reality, where regulatory interactions often span substantial genomic distances.</p>
<p>Despite its long context, HyenaDNA maintains base-level resolution by using single-nucleotide tokens. This means single-nucleotide variants can be evaluated in the context of megabases of surrounding sequence without the ambiguity introduced by k-mer tokenization.</p>
<p>On Nucleotide Transformer benchmarks and additional tasks, HyenaDNA demonstrated in-context learning behaviors that had not previously been observed in genomic models. Performance improved when examples were included in the input context without updating model weights, suggesting that at sufficient scale, DNA models can adapt to new tasks or distributions via prompts rather than fine-tuning. This mirrors phenomena observed in large natural language models.</p>
<p>On GenomicBenchmarks and related evaluations, HyenaDNA achieved state-of-the-art results on the majority of tasks, often by substantial margins. These results illustrated that architectural innovations enabling longer context can simultaneously provide both extended range and improved predictive accuracy.</p>
</section>
<section id="caduceus-bidirectional-modeling-with-reverse-complement-equivariance" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="caduceus-bidirectional-modeling-with-reverse-complement-equivariance"><span class="header-section-number">7.7</span> Caduceus: Bidirectional Modeling with Reverse-Complement Equivariance</h2>
<p>A unique challenge in genomic sequence modeling is the double-stranded nature of DNA. Any sequence can be read from either strand, and the reverse complement of a sequence encodes the same information from the opposite strand’s perspective. For many biological processes, predictions should be identical or related in a consistent way regardless of which strand is presented to the model.</p>
<p>Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even when training data is augmented with both orientations. This inconsistency is problematic for applications like regulatory element prediction, where the functional element exists on one physical stretch of DNA regardless of how we choose to represent it computationally.</p>
<p>Caduceus addressed this challenge by building reverse-complement equivariance directly into the architecture <span class="citation" data-cites="schiff_caduceus_2024">(<a href="references.html#ref-schiff_caduceus_2024" role="doc-biblioref">Schiff et al. 2024</a>)</span>. The model extends the Mamba architecture, a state-space model with linear complexity in sequence length, to support both bidirectionality and reverse-complement equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures that predictions for a sequence and its reverse complement are mathematically related in the expected way.</p>
<p>The architectural innovations in Caduceus serve distinct purposes. Bidirectionality allows each position to incorporate information from both upstream and downstream context, which matters for tasks where the relevant context is not directionally asymmetric. Reverse-complement equivariance ensures consistent predictions across strand orientations, reducing spurious variability and improving calibration.</p>
<p>On downstream benchmarks, Caduceus outperformed previous long-range models. On challenging long-range variant effect prediction tasks, Caduceus exceeded the performance of models with ten times as many parameters that did not leverage bidirectionality or equivariance. This suggests that incorporating appropriate biological inductive biases can be as valuable as scaling model size.</p>
</section>
<section id="grover-generative-regulatory-foundation-models" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="grover-generative-regulatory-foundation-models"><span class="header-section-number">7.8</span> GROVER: Generative Regulatory Foundation Models</h2>
<p>Most genomic language models focus on modeling raw DNA sequence. GROVER takes a complementary approach, shifting attention from sequence to regulatory tracks <span class="citation" data-cites="sanabria_grover_2024">(<a href="references.html#ref-sanabria_grover_2024" role="doc-biblioref">Sanabria et al. 2024</a>)</span>. Rather than treating DNA as the primary input, GROVER is trained on multi-track functional genomics signals including ATAC-seq, histone modifications, and other epigenomic assays across many cell types and tissues.</p>
<p>The training objective predicts masked or held-out regulatory profiles conditioned on neighboring tracks, cell-type embeddings, or limited sequence context. The architecture uses a transformer-style backbone tailored to spatiotemporal grids of genomic positions crossed with assays and cell types.</p>
<p>GROVER occupies a role analogous to self-supervised vision models for images. It treats regulatory profiles as a high-dimensional signal over the genome and learns rich representations of regulatory states at each position. This supports tasks like imputation of missing assays, denoising of noisy experimental data, and cell-type-specific activity prediction.</p>
<p>While not a pure DNA language model, GROVER-style systems complement sequence-based models in important ways. DNA language models capture what the genome can do, encoding the potential regulatory activities specified by the sequence. Regulatory foundation models like GROVER capture what the genome is actually doing in specific contexts, representing the realized regulatory state in particular cell types and conditions. Later chapters explore how sequence-based and regulatory foundation models can be combined, using DNA language models to parameterize sequence priors and regulatory models for context-specific readouts.</p>
</section>
<section id="genslms-codon-tokenization-and-whole-genome-modeling" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="genslms-codon-tokenization-and-whole-genome-modeling"><span class="header-section-number">7.9</span> GenSLMs: Codon Tokenization and Whole-Genome Modeling</h2>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>MAKE MUCH MORE CONSISE</strong></p>
</div>
</div>
<p>The DNA language models discussed thus far, including DNABERT, Nucleotide Transformer, HyenaDNA, and Caduceus, were designed primarily for human and multi-species genomics applications such as regulatory prediction and variant effect estimation. GenSLMs (Genome-scale Language Models) represents a distinct design point that targets pathogen surveillance and viral evolution rather than human regulatory genomics <span class="citation" data-cites="zvyagin_genslms_2022">(<a href="references.html#ref-zvyagin_genslms_2022" role="doc-biblioref">Zvyagin et al. 2022</a>)</span>. The model illustrates how different biological applications motivate different architectural and tokenization choices, even within the broader family of genomic foundation models.</p>
<p>GenSLMs operates at the codon level rather than the nucleotide level. Where most DNA language models tokenize sequences into nucleotides, k-mers, or BPE-derived subwords, GenSLMs treats each three-nucleotide codon as a single token. This choice reflects an explicit alignment with the central dogma of molecular biology: codons are the fundamental units of translation from nucleic acid to protein, and mutations at the codon level determine amino acid changes that drive phenotypic evolution. The approach yields a vocabulary of 64 codon tokens (plus special tokens for noncoding regions and frame shifts), intermediate in size between character-level tokenization (4 tokens) and typical BPE vocabularies (4,096 to 32,000 tokens).</p>
<p>The codon-level tokenization enables GenSLMs to model entire viral genomes as sequences of manageable length. A SARS-CoV-2 genome spans approximately 30,000 nucleotides, which translates to roughly 10,000 codons. This places whole-genome modeling within reach of standard transformer context windows, avoiding the architectural innovations required to handle megabase-scale human sequences. The approach trades single-nucleotide resolution for the ability to capture genome-wide patterns and cross-protein dependencies that would be difficult to model at nucleotide resolution.</p>
<p>The training strategy follows a two-stage paradigm. The foundation model was pretrained on over 110 million prokaryotic gene sequences from the BV-BRC database, exposing the model to broad codon usage patterns and gene-level structure across bacterial diversity. This pretraining corpus provided the model with general understanding of coding sequence organization without being specific to any particular pathogen. The model was then fine-tuned on 1.5 million SARS-CoV-2 genome sequences to learn the specific evolutionary landscape of the virus.</p>
<p>Remarkably, a GenSLM model trained only on sequences from the first year of the pandemic, before Delta or Omicron variants emerged, could subsequently distinguish between variants of concern in its learned embedding space. When new variant sequences were projected into the model’s latent space, they clustered according to lineage identity despite the model never having observed these variants during training. This generalization suggests that the model learned structural features of SARS-CoV-2 evolution that transferred to novel variants, providing a foundation for real-time surveillance applications.</p>
<p>GenSLMs also revealed interpretable attention patterns across the viral genome. Cross-protein attention showed coupling between the Spike protein and nonstructural proteins (nsp3, nsp5) that differed between Delta and Omicron lineages. These patterns suggest that the model captured biologically meaningful co-evolutionary relationships rather than arbitrary sequence statistics, though the mechanistic interpretation of such patterns remains an area for further investigation.</p>
<p>From a computational perspective, GenSLMs demonstrated that genomic foundation models can be trained at unprecedented scale. The project achieved 1.63 Zettaflops of total computation across training runs, with sustained performance of 121 PFLOPS in mixed precision on GPU-based supercomputers. Training on specialized AI hardware accelerators (Cerebras CS-2 clusters) reduced convergence time from over a week to less than a day. This work received the 2022 Gordon Bell Special Prize for High Performance Computing-Based COVID-19 Research, establishing a benchmark for large-scale genomic model training.</p>
<p>The GenSLMs approach highlights several broader lessons for the field. First, tokenization should be aligned with biological semantics when possible. Codon-level tokenization makes sense for coding sequences where the codon-to-amino-acid mapping is the relevant biological transformation. Second, pretraining corpora can be chosen strategically to provide useful inductive biases. Prokaryotic gene pretraining exposed the model to diverse codon usage and gene organization before specializing to a particular virus. Third, different applications motivate different scales. Pathogen surveillance requires whole-genome context but benefits from relatively compact genomes, while human regulatory genomics requires kilobase to megabase context around specific loci.</p>
<p>The model’s application to emerging pathogen detection illustrates a use case distinct from the variant effect prediction focus of most human-centric genomic foundation models. Rather than scoring individual mutations for pathogenicity, GenSLMs aims to characterize entire genomes and identify novel variants that may represent public health concerns. This surveillance application places different demands on the model: speed of inference across many sequences, robust generalization to novel variants, and interpretable representations that can guide downstream analysis.</p>
</section>
<section id="evo-2-genome-scale-language-modeling-across-all-life" class="level2" data-number="7.10">
<h2 data-number="7.10" class="anchored" data-anchor-id="evo-2-genome-scale-language-modeling-across-all-life"><span class="header-section-number">7.10</span> Evo 2: Genome-Scale Language Modeling Across All Life</h2>
<div class="content-visible callout callout-style-default callout-warning callout-titled" data-when-profile="draft">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>UPDATE THIS SECTION</strong></p>
</div>
</div>
<section id="scale-and-training-corpus" class="level3" data-number="7.10.1">
<h3 data-number="7.10.1" class="anchored" data-anchor-id="scale-and-training-corpus"><span class="header-section-number">7.10.1</span> Scale and Training Corpus</h3>
<ul>
<li>9.3 trillion DNA tokens from OpenGenome2 dataset</li>
<li>Pan-species: bacteria, archaea, eukaryotes, phages, organelles</li>
<li>7B and 40B parameter variants</li>
<li>Fully open (weights, code, training data)</li>
</ul>
</section>
<section id="architecture-stripedhyena-2" class="level3" data-number="7.10.2">
<h3 data-number="7.10.2" class="anchored" data-anchor-id="architecture-stripedhyena-2"><span class="header-section-number">7.10.2</span> Architecture: StripedHyena 2</h3>
<ul>
<li>Hybrid convolutional-attention design</li>
<li>1 million token context window at single-nucleotide resolution</li>
<li>Subquadratic scaling enabling megabase processing</li>
<li>Autoregressive training objective (next-base prediction)</li>
</ul>
</section>
<section id="emergent-biological-knowledge" class="level3" data-number="7.10.3">
<h3 data-number="7.10.3" class="anchored" data-anchor-id="emergent-biological-knowledge"><span class="header-section-number">7.10.3</span> Emergent Biological Knowledge</h3>
<ul>
<li>Learns exon-intron boundaries without supervision</li>
<li>Discovers TF binding site patterns</li>
<li>Captures protein structural elements</li>
<li>Identifies prophage genomic regions</li>
<li>Mechanistic interpretability analyses reveal breadth of learned features</li>
</ul>
</section>
<section id="zero-shot-variant-effect-prediction" class="level3" data-number="7.10.4">
<h3 data-number="7.10.4" class="anchored" data-anchor-id="zero-shot-variant-effect-prediction"><span class="header-section-number">7.10.4</span> Zero-Shot Variant Effect Prediction</h3>
<ul>
<li>Likelihood-based scoring: compare P(ref) vs P(alt)</li>
<li>Competitive pathogenic/benign classification without variant-specific training</li>
<li>BRCA1 VUS classification via simple classifiers on embeddings</li>
<li>Noncoding and coding variants in a single framework</li>
</ul>
</section>
<section id="cross-species-applications" class="level3" data-number="7.10.5">
<h3 data-number="7.10.5" class="anchored" data-anchor-id="cross-species-applications"><span class="header-section-number">7.10.5</span> Cross-Species Applications</h3>
<ul>
<li>Variant interpretation in non-model organisms (livestock, crops)</li>
<li>Mutation load quantification for breeding programs</li>
<li>Genome editing design across species</li>
<li>Trade-off: generality vs human-specific optimization</li>
</ul>
</section>
<section id="generative-capabilities" class="level3" data-number="7.10.6">
<h3 data-number="7.10.6" class="anchored" data-anchor-id="generative-capabilities"><span class="header-section-number">7.10.6</span> Generative Capabilities</h3>
<ul>
<li>Mitochondrial, prokaryotic, eukaryotic sequence generation</li>
<li>Genome-scale coherence exceeding prior methods</li>
<li>Inference-time search for controllable generation</li>
<li>First inference-time scaling results in biology (epigenomic structure)</li>
</ul>
</section>
<section id="limitations-and-trade-offs" class="level3" data-number="7.10.7">
<h3 data-number="7.10.7" class="anchored" data-anchor-id="limitations-and-trade-offs"><span class="header-section-number">7.10.7</span> Limitations and Trade-offs</h3>
<ul>
<li>Generalist vs specialist: AlphaMissense/AlphaGenome may outperform on human-specific tasks</li>
<li>Cross-ref to VEP chapter (<a href="p3-ch12-vep.html" class="quarto-xref"><span>Chapter 12</span></a>) for benchmarking details</li>
<li>Calibration requirements before clinical use</li>
</ul>
</section>
</section>
<section id="central-dogma-aware-and-annotation-enriched-models" class="level2" data-number="7.11">
<h2 data-number="7.11" class="anchored" data-anchor-id="central-dogma-aware-and-annotation-enriched-models"><span class="header-section-number">7.11</span> Central-Dogma-Aware and Annotation-Enriched Models</h2>
<p>The tokenization discussion in <a href="p3-ch10-tokens.html" class="quarto-xref"><span>Chapter 10</span></a> described how biological structure can be encoded directly into the input representation. Recent models push this idea further by integrating central dogma knowledge and genomic annotations into the modeling framework itself.</p>
<section id="life-code-the-central-dogma-as-inductive-bias" class="level3" data-number="7.11.1">
<h3 data-number="7.11.1" class="anchored" data-anchor-id="life-code-the-central-dogma-as-inductive-bias"><span class="header-section-number">7.11.1</span> Life-Code: The Central Dogma as Inductive Bias</h3>
<p>Life-Code proposes codon-aware, central-dogma-informed tokenization to bridge DNA, RNA, and protein within a single language-modeling framework <span class="citation" data-cites="liu_life-code_2025">(<a href="references.html#ref-liu_life-code_2025" role="doc-biblioref">Liu et al. 2025</a>)</span>. The key insight is that different genomic regions should be tokenized differently based on their biological function.</p>
<p>Coding regions are tokenized as codons, the three-nucleotide units that specify amino acids during translation. This respects the genetic code’s fundamental structure and enables the model to learn patterns at the level of the biological unit of selection for protein-coding sequences. Noncoding regions, which lack this inherent three-nucleotide structure, are tokenized via learned subword units optimized during training. The resulting unified representations span DNA, RNA, and protein, enabling knowledge sharing across modalities.</p>
<p>Life-Code uses knowledge distillation from protein language models to import protein-level structural knowledge into DNA and RNA sequence representations. This improves performance on tasks involving coding sequence, such as predicting the effects of missense mutations or expression changes, and achieves competitive or state-of-the-art results on tasks across all three omic modalities.</p>
</section>
<section id="biotoken-encoding-variants-and-structure" class="level3" data-number="7.11.2">
<h3 data-number="7.11.2" class="anchored" data-anchor-id="biotoken-encoding-variants-and-structure"><span class="header-section-number">7.11.2</span> BioToken: Encoding Variants and Structure</h3>
<p>BioToken extends tokenization beyond nucleotide content to include explicit genomic annotations <span class="citation" data-cites="medvedev_biotoken_2025">(<a href="references.html#ref-medvedev_biotoken_2025" role="doc-biblioref">Medvedev et al. 2025</a>)</span>. Rather than representing a genomic region purely as a string of nucleotides, BioToken creates tokens that encode additional biological context.</p>
<p>Variant-aware tokens explicitly represent SNPs, insertions, and deletions as distinct tokens rather than as implicit changes in the underlying sequence. Structural annotations encode information about exons, introns, UTRs, promoters, enhancers, and other regulatory elements. Functional context tokens include signals such as conservation scores, chromatin state, or known regulatory motifs.</p>
<p>This design moves toward fully structured genomic language models where the input is not only DNA bases but also position-specific metadata. The resulting representations can directly integrate sequence, structure, and functional annotations in a unified framework.</p>
<p>The associated model BioFM, built on BioToken, achieves competitive or superior results relative to specialized models like Enformer and SpliceAI across genomic benchmarks including noncoding pathogenicity prediction, expression modulation, sQTL prediction, and long-range genomic interactions. Notably, BioFM achieves state-of-the-art performance with significantly fewer parameters (265M), substantially reducing training costs and computational requirements compared to larger models.</p>
<p>Life-Code and BioToken foreshadow the multi-modal, multi-omic foundation models discussed in Part IV, where sequence is only one of many integrated information streams.</p>
</section>
</section>
<section id="using-genomic-language-models-in-practice" class="level2" data-number="7.12">
<h2 data-number="7.12" class="anchored" data-anchor-id="using-genomic-language-models-in-practice"><span class="header-section-number">7.12</span> Using Genomic Language Models in Practice</h2>
<p>Genomic language models support multiple usage patterns analogous to those established for protein language models. Understanding these patterns is essential for applying the models effectively.</p>
<section id="embeddings-as-universal-features" class="level3" data-number="7.12.1">
<h3 data-number="7.12.1" class="anchored" data-anchor-id="embeddings-as-universal-features"><span class="header-section-number">7.12.1</span> Embeddings as Universal Features</h3>
<p>The simplest approach extracts embeddings from a pretrained model and uses them as features for downstream tasks. The workflow involves several steps: extract embeddings for windows around loci of interest, pool or select positions relevant to the task (such as promoters, candidate enhancers, or variant sites), and train a lightweight downstream model such as a linear layer, small MLP, or logistic regression.</p>
<p>This approach supports diverse applications. Regulatory element classification can distinguish promoters, enhancers, silencers, and insulators based on their learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence as an alternative to supervised models like DeepSEA. Variant effect scoring replaces or augments hand-crafted features in frameworks like CADD with language model derived features, analogous to CADD v1.7’s incorporation of protein language model features. Splicing and transcript modeling combines language model embeddings with specialized architectures like SpliceAI.</p>
<p>Because the language model remains frozen in this approach, it is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor whose representations support many downstream applications.</p>
</section>
<section id="fine-tuning-and-task-specific-heads" class="level3" data-number="7.12.2">
<h3 data-number="7.12.2" class="anchored" data-anchor-id="fine-tuning-and-task-specific-heads"><span class="header-section-number">7.12.2</span> Fine-Tuning and Task-Specific Heads</h3>
<p>When more labeled data is available, fine-tuning can significantly improve performance beyond what frozen embeddings provide. Full fine-tuning updates all language model parameters for a specific task, allowing the model to specialize its representations. Adapter-based tuning inserts small bottleneck modules into each layer and updates only those, keeping the backbone mostly frozen while still allowing task-specific adaptation.</p>
<p>Full fine-tuning tends to achieve the highest performance when sufficient labeled data is available, but it requires more compute and risks catastrophic forgetting of general knowledge. Adapter-based approaches provide a middle ground, achieving most of the performance gains while maintaining computational efficiency and preserving the backbone’s general capabilities.</p>
</section>
<section id="zero-shot-and-few-shot-scoring" class="level3" data-number="7.12.3">
<h3 data-number="7.12.3" class="anchored" data-anchor-id="zero-shot-and-few-shot-scoring"><span class="header-section-number">7.12.3</span> Zero-Shot and Few-Shot Scoring</h3>
<p>For variant interpretation, genomic language models enable zero-shot scoring based on sequence likelihood. The approach computes the model’s probability for a sequence containing the reference allele and compares it to the probability for the sequence containing the alternative allele. Variants that substantially reduce sequence probability are inferred to be more disruptive.</p>
<p>This approach requires no variant-specific training data and can score any single-nucleotide variant in any genomic context the model has learned to represent. The quality of zero-shot scoring depends on how well the model’s learned probability distribution captures biological constraints, which tends to improve with model scale and training data diversity.</p>
<p>Few-shot approaches include task examples in the input context, allowing the model to adapt its behavior based on demonstrations without parameter updates. HyenaDNA demonstrated that genomic models at sufficient scale exhibit this in-context learning capability, opening new possibilities for rapid task adaptation.</p>
</section>
</section>
<section id="emerging-themes-and-current-limitations" class="level2" data-number="7.13">
<h2 data-number="7.13" class="anchored" data-anchor-id="emerging-themes-and-current-limitations"><span class="header-section-number">7.13</span> Emerging Themes and Current Limitations</h2>
<p>The development of genomic language models over the past several years has established several important themes while also revealing significant limitations.</p>
<p>Self-supervision provides a viable path to general genomic representations. Models trained purely on the statistical structure of DNA sequence, without any functional labels, learn representations that transfer to diverse downstream tasks. This validates the foundation model paradigm for genomics and suggests continued scaling will yield further improvements.</p>
<p>Scale and diversity matter substantially for model quality. Performance improves predictably with model size, training data volume, and training data diversity. Including multiple species, populations, and genomic contexts yields more robust representations than training on a single reference genome.</p>
<p>Long-range context is biologically necessary for many applications. Regulatory phenomena operate at tens to hundreds of kilobases, and the development of efficient architectures like HyenaDNA and Caduceus finally allows modeling these interactions at single-base resolution. The progression from 512 bp to 1 Mb context lengths represents a fundamental capability improvement.</p>
<p>Self-supervision and supervision are complementary rather than competing approaches. Self-supervised language models excel at learning broad, reusable features, but they do not automatically solve every downstream problem. Specialized architectures and supervised objectives, such as Enformer and related models discussed in <a href="p2-ch09-hybrid.html" class="quarto-xref"><span>Chapter 9</span></a>, remain crucial for accurate quantitative prediction of complex genomic readouts.</p>
<p>Several important limitations remain. Current models struggle with complex variant patterns beyond single-nucleotide changes, including indels, structural variants, and epistatic interactions across distant loci. Training data and labels remain skewed toward certain ancestries, raising concerns about performance and calibration in underrepresented populations. Interpretability is limited, as it remains difficult to explain why a model assigns a particular score to a variant in terms that connect to biological mechanism. Integration with other data modalities (chromatin, expression, 3D genome structure, clinical phenotypes) is still in its early stages.</p>
</section>
<section id="summary" class="level2" data-number="7.14">
<h2 data-number="7.14" class="anchored" data-anchor-id="summary"><span class="header-section-number">7.14</span> Summary</h2>
<p>This chapter surveyed the landscape of genomic language models, from early proof-of-concept systems like DNABERT through scaled models like Nucleotide Transformer to architectural innovations enabling megabase context in HyenaDNA and Caduceus. We examined how models like GROVER complement sequence-based approaches by learning from regulatory tracks, and how annotation-enriched architectures like Life-Code and BioToken incorporate biological structure directly into the modeling framework.</p>
<p>The key lessons are that self-supervised pretraining transfers effectively to genomics, that architectural choices enabling long-range context provide both efficiency and accuracy improvements, and that biological inductive biases (reverse-complement equivariance, central dogma awareness, variant encoding) can substitute for raw scale in some applications.</p>
<p>In <a href="p2-ch09-hybrid.html" class="quarto-xref"><span>Chapter 9</span></a>, we turn to Enformer and related long-range sequence-to-function models that explicitly predict molecular readouts from sequence. These models close the loop between self-supervised sequence understanding and supervised functional prediction, addressing a key limitation of pure language models: their indirect relationship to quantitative molecular phenotypes.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-benegas_gpn_2023" class="csl-entry" role="listitem">
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023. <span>“[<span>GPN</span>] <span>DNA</span> Language Models Are Powerful Predictors of Genome-Wide Variant Effects.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (44): e2311219120. <a href="https://doi.org/10.1073/pnas.2311219120">https://doi.org/10.1073/pnas.2311219120</a>.
</div>
<div id="ref-dalla-torre_nucleotide_2023" class="csl-entry" role="listitem">
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, et al. 2023. <span>“Nucleotide <span>Transformer</span>: Building and Evaluating Robust Foundation Models for Human Genomics.”</span> <em>Nature Methods</em> 22 (2): 287–97. <a href="https://doi.org/10.1038/s41592-024-02523-z">https://doi.org/10.1038/s41592-024-02523-z</a>.
</div>
<div id="ref-ji_dnabert_2021" class="csl-entry" role="listitem">
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021. <span>“<span>DNABERT</span>: Pre-Trained <span>Bidirectional</span> <span>Encoder</span> <span>Representations</span> from <span>Transformers</span> Model for <span>DNA</span>-Language in Genome.”</span> <em>Bioinformatics</em> 37 (15): 2112–20. <a href="https://doi.org/10.1093/bioinformatics/btab083">https://doi.org/10.1093/bioinformatics/btab083</a>.
</div>
<div id="ref-liu_life-code_2025" class="csl-entry" role="listitem">
Liu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025. <span>“Life-<span>Code</span>: <span>Central</span> <span>Dogma</span> <span>Modeling</span> with <span>Multi</span>-<span>Omics</span> <span>Sequence</span> <span>Unification</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2502.07299">https://doi.org/10.48550/arXiv.2502.07299</a>.
</div>
<div id="ref-medvedev_biotoken_2025" class="csl-entry" role="listitem">
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, and Shadab Khan. 2025. <span>“<span>BioToken</span> and <span>BioFM</span> – <span>Biologically</span>-<span>Informed</span> <span>Tokenization</span> <span>Enables</span> <span>Accurate</span> and <span>Efficient</span> <span>Genomic</span> <span>Foundation</span> <span>Models</span>.”</span> bioRxiv. <a href="https://doi.org/10.1101/2025.03.27.645711">https://doi.org/10.1101/2025.03.27.645711</a>.
</div>
<div id="ref-nguyen_hyenadna_2023" class="csl-entry" role="listitem">
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. <span>“<span>HyenaDNA</span>: <span>Long</span>-<span>Range</span> <span>Genomic</span> <span>Sequence</span> <span>Modeling</span> at <span>Single</span> <span>Nucleotide</span> <span>Resolution</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15794">https://doi.org/10.48550/arXiv.2306.15794</a>.
</div>
<div id="ref-sanabria_grover_2024" class="csl-entry" role="listitem">
Sanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch. 2024. <span>“[<span>GROVER</span>] <span>DNA</span> Language Model <span>GROVER</span> Learns Sequence Context in the Human Genome.”</span> <em>Nature Machine Intelligence</em> 6 (8): 911–23. <a href="https://doi.org/10.1038/s42256-024-00872-0">https://doi.org/10.1038/s42256-024-00872-0</a>.
</div>
<div id="ref-schiff_caduceus_2024" class="csl-entry" role="listitem">
Schiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. 2024. <span>“Caduceus: <span>Bi</span>-<span>Directional</span> <span>Equivariant</span> <span>Long</span>-<span>Range</span> <span>DNA</span> <span>Sequence</span> <span>Modeling</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2403.03234">https://doi.org/10.48550/arXiv.2403.03234</a>.
</div>
<div id="ref-zhou_dnabert-2_2024" class="csl-entry" role="listitem">
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and Han Liu. 2024. <span>“<span>DNABERT</span>-2: <span>Efficient</span> <span>Foundation</span> <span>Model</span> and <span>Benchmark</span> <span>For</span> <span>Multi</span>-<span>Species</span> <span>Genome</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2306.15006">https://doi.org/10.48550/arXiv.2306.15006</a>.
</div>
<div id="ref-zvyagin_genslms_2022" class="csl-entry" role="listitem">
Zvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. <span>“<span>GenSLMs</span>: <span>Genome</span>-Scale Language Models Reveal <span>SARS</span>-<span>CoV</span>-2 Evolutionary Dynamics.”</span> bioRxiv. <a href="https://doi.org/10.1101/2022.10.10.511571">https://doi.org/10.1101/2022.10.10.511571</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./p2-ch06-plm.html" class="pagination-link" aria-label="Protein Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Protein Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./p2-ch08-rna.html" class="pagination-link" aria-label="RNA &amp; Transcript-Level Models">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">RNA &amp; Transcript-Level Models</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Josh Meehl</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>