# Part III: Foundation Model Families {#sec-part3-intro .unnumbered}

Each architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information; they learn to recognize transcription factor binding sites, splice signals, and chromatin accessibility patterns from the sequence grammar immediately surrounding each position. Protein language models treat amino acid sequences as structured compositions whose meaning emerges from evolutionary context; they learn what substitutions are tolerated by observing which sequences survived natural selection. DNA language models extend this paradigm to nucleotides, learning regulatory grammar through self-supervised objectives that predict masked or next tokens. Hybrid architectures attempt to reconcile local and global perspectives, using convolutions to extract features efficiently while deploying attention to model interactions spanning tens or hundreds of kilobases. Understanding these assumptions clarifies what each model family can capture and where each will fail.

Foundation models in genomic deep learning span distinct architectural families, each with characteristic strengths. Foundational principles and taxonomy (@sec-fm-principles) establish what defines a foundation model and provide a framework for navigating the rapidly expanding ecosystem. DNA language models (@sec-dna-lm), including DNABERT, Nucleotide Transformer, and HyenaDNA, apply self-supervised pretraining to genomic sequence, learning representations that transfer across diverse downstream tasks. Protein language models (@sec-protein-lm) achieved the earliest and most dramatic foundation model successes; ESM, ProtTrans, and their descendants emerged alongside AlphaFold2 in 2020, collectively demonstrating that deep learning could capture protein structure and function from sequence alone. AlphaFold2 revolutionized structure prediction through its Evoformer architecture, and AlphaMissense subsequently adapted that architecture for proteome-wide variant effect prediction. Hybrid architectures (@sec-regulatory), including Enformer, Borzoi, and AlphaGenome, combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases, enabling direct prediction of gene expression from sequence. Variant effect prediction (@sec-vep-fm) synthesizes these approaches, translating foundation model representations into pathogenicity scores across variant types and genomic contexts.