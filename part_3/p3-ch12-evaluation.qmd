# Evaluation Methods {#sec-ch12-eval}

> The difference between valid and misleading evaluation often lies in methodological details that standard reporting obscures.

::: {.callout-note}
## Chapter Overview

**Estimated reading time:** 35-40 minutes

**Prerequisites:** This chapter assumes familiarity with the benchmark landscape surveyed in @sec-ch11-benchmarks. Understanding of basic machine learning evaluation (train/test splits, auROC) and genomic data characteristics (@sec-ch02-data) is essential.

**Learning Objectives:** After completing this chapter, you should be able to:

1. Identify why random train-test splits fail for genomic data and select appropriate splitting strategies
2. Detect and prevent the four major types of data leakage (label, feature, temporal, benchmark)
3. Choose metrics aligned with deployment objectives, distinguishing discrimination, calibration, and clinical utility
4. Design ablation studies and baseline comparisons that isolate genuine model contributions
5. Apply statistical rigor including significance testing, effect sizes, and confidence intervals

**Chapter Structure:** This chapter examines *how* to evaluate models properly, covering splitting strategies, leakage detection, metric selection, baseline comparison, and statistical rigor. The companion chapter on the Benchmark Landscape (@sec-ch11-benchmarks) surveys *what* benchmarks exist.
:::

Genomic data makes it exceptionally easy to fool yourself. Sequences share evolutionary history, so a model that memorizes training sequences may appear to generalize when tested on homologs. Variants cluster in families and populations, so ancestry-stratified performance can masquerade as genuine prediction. Experimental measurements carry **batch effects** invisible to the untrained eye, so a model can learn to distinguish sequencing centers rather than biological states. Training labels often derive from the very databases used for evaluation, creating circular validations that inflate performance without testing genuine predictive power. Every shortcut that simplifies evaluation in other machine learning domains becomes an opportunity for false confidence in genomics.

Random data splits that work perfectly well for natural images become actively misleading when applied to biological sequences. A protein held out for testing may share 90% sequence identity with a training protein, allowing the model to succeed through memorization rather than generalization. A variant classified as pathogenic in the test set may come from the same gene family as training variants, letting the model exploit gene-level signals rather than learning variant-specific effects. A cell line in the test set may have been processed at the same sequencing center as training samples, enabling the model to recognize batch signatures rather than biological patterns. These leakages are not hypothetical; they have inflated reported performance across the genomic machine learning literature.

The difference between valid and misleading evaluation often lies not in benchmark choice but in methodological details: data splitting strategies, metric selection, baseline comparisons, ablation designs, and statistical testing. @sec-ch11-benchmarks catalogs what benchmark tasks exist, how they are constructed, and what capabilities they probe. This chapter addresses the complementary question: given a benchmark, how do we apply it to produce trustworthy results? These principles apply across all benchmark categories, from chromatin state prediction to clinical variant classification. By mastering evaluation methodology, practitioners can distinguish genuine advances from artifacts that will not survive deployment.

::: {.callout-warning}
## Difficulty Warning: Methodological Rigor

The concepts of leakage, confounding, and proper experimental design are subtle but essential. A model developer who masters these principles will avoid the common pitfalls that produce misleading benchmark results. Take time with each section; the investment will pay dividends in every evaluation you conduct.
:::


## Why Random Splits Fail {#sec-ch12-random-splits-fail}

The standard machine learning recipe calls for randomly partitioning data into training, validation, and test sets. For image classification or sentiment analysis, this approach works well because individual examples are approximately independent. A photograph of a cat shares no special relationship with another photograph of a different cat beyond their common label. Random assignment ensures that training and test distributions match, and performance on the test set provides an unbiased estimate of performance on new examples from the same distribution.

Genomic data violates these assumptions at every level. Consider a protein dataset where the goal is to predict stability from sequence. Proteins in the same family share evolutionary history and often similar structures. If a training set includes beta-lactamase variants from *E. coli* and the test set includes beta-lactamase variants from *Klebsiella*, the model may appear to generalize to "new" proteins while actually recognizing sequence patterns it saw during training. The test performance reflects memorization of family-specific features rather than general principles of protein stability.

::: {#fig-random-splits-fail layout-ncol=3}
![Images: Independent samples](../figs/part_3/ch11/06-A-fig-random-splits-fail.svg)

![Proteins: Related by evolution](../figs/part_3/ch11/06-B-fig-random-splits-fail.svg)

![Variants: Multiple dependencies](../figs/part_3/ch11/06-C-fig-random-splits-fail.svg)

Why random data splits fail for genomic machine learning. (A) Image classification: samples are truly independent, so random assignment provides valid performance estimates. (B) Protein classification: evolutionary relationships create dependencies; random splits place homologs (>80% identity) across train/test, enabling memorization of shared sequences rather than learning generalizable features. (C) Variant prediction: multiple dependencies compound—same genes across splits, related individuals sharing rare variants, and population structure confounding features and labels. These dependencies require structured splitting strategies that explicitly account for sequence homology, family relatedness, and population structure.
:::

The problem compounds when sequence identity is high. Two proteins sharing 80% sequence identity will typically have similar structures and functions. A model trained on one and tested on the other is not really being tested on a novel example; it is being asked to interpolate within a region of sequence space it has already explored. Even at 30% sequence identity, the so-called "twilight zone" of homology detection [@rost_twilight_1999], proteins often share structural and functional similarities that can be exploited by sufficiently powerful models.

Variant-level data presents analogous challenges. Variants within the same gene share genomic context, and variants affecting the same protein domain share structural environment. Variants from the same individual share haplotype background. Variants from the same population share allele frequency distributions shaped by demographic history. Each of these relationships creates opportunities for models to learn shortcuts that generalize within the training distribution but fail on genuinely novel examples.

::: {.callout-tip}
## Key Insight: The Independence Assumption

The fundamental issue is that genomic data points are not independent. Random splits assume independence; when this assumption is violated, the test set no longer provides an unbiased estimate of generalization. The consequence is systematic overestimation of performance. A model that achieves 0.90 auROC with random splitting might achieve only 0.75 auROC when evaluated on truly held-out examples, with the gap reflecting how much the model learned about biology versus how much it learned about the structure of the training data.
:::


## Homology-Aware Splitting {#sec-ch12-homology-aware-splitting}

The solution to homology-driven leakage is to explicitly account for sequence similarity when constructing data splits. Rather than random assignment, examples are clustered by sequence identity, and entire clusters are assigned to training, validation, or test sets. This ensures that no test example is "too similar" to any training example, forcing the model to demonstrate genuine generalization.

::: {#fig-homology-splitting}
![Homology-aware splitting workflow](../figs/part_3/ch11/07-fig-homology-splitting.svg)

Homology-aware splitting prevents sequence similarity leakage. Step 1: Start with all sequences in the dataset. Step 2: Cluster sequences by similarity using tools like CD-HIT or MMseqs2 at an appropriate threshold (30% identity typical for diverse protein tasks). Step 3: Visualize clusters showing which sequences are related. Step 4: Assign entire clusters to single splits—no cluster is divided across train/validation/test. Step 5: Validate that no test sequence exceeds the identity threshold with any training sequence. The threshold determines evaluation stringency: 30% identity tests distant generalization; 50% tests moderate generalization; higher thresholds permit more similarity but provide weaker generalization evidence.
:::

### Clustering Tools and Workflows {#sec-ch12-clustering-tools}

Two tools dominate homology-aware splitting in practice. CD-HIT clusters sequences by greedy incremental clustering, assigning each sequence to an existing cluster if it exceeds a similarity threshold to the cluster representative, or creating a new cluster otherwise [@li_cd-hit_2006]. Why greedy incremental rather than optimal clustering? Computing all-versus-all pairwise similarities for millions of sequences would require billions of comparisons, making optimal clustering computationally infeasible. The greedy approach processes sequences one at a time, comparing each new sequence only to existing cluster representatives rather than to all sequences. This reduces complexity from quadratic to roughly linear in the number of sequences, enabling practical application to genomic-scale datasets. The tradeoff is that cluster assignments depend on input order and may not be globally optimal, but for splitting purposes this approximation suffices. The algorithm is fast and scales to millions of sequences. For proteins, a typical workflow clusters at 40% sequence identity for stringent splitting or 70% for moderate splitting. For nucleotide sequences, thresholds are typically higher (80-95%) due to different evolutionary rates.

MMseqs2 offers faster clustering with similar sensitivity, becoming essential for large-scale analyses [@steinegger_mmseqs2_2017]. The tool supports multiple clustering modes and can handle databases with hundreds of millions of sequences. For foundation model pretraining where deduplication affects billions of sequences, MMseqs2 is often the only practical option.

::: {.callout-note}
## Practical Guidance: Choosing Identity Thresholds

The choice of identity threshold involves trade-offs:

| Threshold | Proteins | Nucleotides | Trade-off |
|-----------|----------|-------------|-----------|
| **Stringent** | 30-40% | 80-85% | Hardest test; may lack training data |
| **Moderate** | 50-70% | 85-90% | Balanced; typical benchmark choice |
| **Permissive** | 80-90% | 95%+ | Retains data but allows some leakage |

**Rule of thumb for proteins:** Use 40% identity for variant effect prediction, 30% for structure prediction (where even distant homologs share structure).

**Rule of thumb for DNA:** Use 80% for regulatory prediction, but consider gene-family splits instead of sequence identity alone.
:::

### Practical Considerations {#sec-ch12-homology-practical}

Several subtleties affect the quality of homology-aware splits. When one cluster contains half the data and is assigned to training, the remaining clusters may be too small or too biased to serve as representative test sets. This cluster size distribution problem can be mitigated through stratified sampling within clusters or careful balancing across splits, ensuring that test sets contain sufficient examples across the label distribution.

Pairwise clustering can miss hidden relationships that arise through transitive homology. Protein A may share 35% identity with protein B, and protein B may share 35% identity with protein C, yet A and C share only 20% identity. If A is in training and C is in testing, B serves as an indirect bridge that allows information to leak between splits despite no direct high-identity pair spanning them. Why does this transitive leakage matter when A and C are dissimilar? The model does not need to memorize specific sequences; it needs only to learn patterns. Information about C-like sequences can flow through B: patterns learned from B (which is similar enough to A to share features) may transfer to C (which is similar enough to B to benefit from those features). This chain of similarity creates a gradient of information flow even when the endpoints share little direct similarity. Connected component analysis or multi-step clustering can address these transitive relationships, though at increased computational cost.

Multi-domain proteins complicate whole-protein clustering because different domains may have different evolutionary histories. A protein may share one domain with training proteins and another domain with test proteins. Whether this represents leakage depends on the prediction task: if predicting whole-protein function, shared domains matter; if predicting domain-specific properties, they matter more acutely. Domain-aware splitting assigns domains rather than whole proteins to clusters, though this requires domain annotation that may not always be available.

For genomic (non-protein) sequences, repeat elements and transposable elements create analogous challenges. A model trained to predict chromatin state may learn features of LINE elements that recur throughout the genome. Excluding repetitive regions from evaluation or explicitly accounting for repeat content can clarify what the model has actually learned about regulatory sequences versus repetitive element patterns.


## Splitting by Biological Axis {#sec-ch12-splitting-biological-axis}

Beyond sequence homology, genomic data admits multiple axes along which splits can be constructed. The choice of axis determines what kind of generalization is being tested.

::: {.callout-note title="Stop and Think"}
You're building a variant pathogenicity predictor for clinical use. You could split your data by: (A) random 80/20, (B) by chromosome, (C) by gene family, or (D) by patient ancestry. Each tests different generalization. Which splitting strategy would best simulate real clinical deployment? Why might you want to try multiple strategies?

*Consider: In the clinic, which of these splits most closely resembles encountering a truly novel variant?*

:::{.callout-note collapse="true" title="Check Your Answer"}
**Best for clinical deployment simulation: (C) Gene family split**, because clinical variants often occur in genes for which you have no training data. A new disease gene identified next year won't share sequence with your training genes, so testing generalization across gene families reveals whether your model learns transferable principles rather than gene-specific patterns.

**Why multiple strategies?** Each split tests a different failure mode:
- **(A) Random** overestimates performance by allowing related variants to leak across splits
- **(B) Chromosome** tests genomic region transfer but doesn't address gene homology
- **(C) Gene family** tests whether learned features transfer to novel genes
- **(D) Ancestry** reveals whether your model works equitably across populations

A model that passes all four strategies is more likely to succeed in deployment than one that only passes random splits. The stricter the split, the lower but more realistic the performance estimate.
:::
:::

::: {#fig-splitting-strategies}
![Splitting strategies test different aspects of generalization](../figs/part_3/ch11/08-fig-splitting-strategies.svg)

Splitting strategies test different aspects of generalization. Each strategy (rows) addresses specific dependencies (columns): random splits address none; individual-aware prevents sample-level leakage; family-aware accounts for relatedness; chromosome holdout tests cross-genome transfer; gene/protein family prevents homology leakage; cohort/site holdout tests deployment robustness; temporal splits simulate prospective use; ancestry stratification reveals population biases. No single strategy addresses all dependencies; rigorous evaluation combines multiple strategies. Stricter splits produce lower but more realistic performance estimates that better predict clinical deployment success.
:::

### Splitting by Individual {#sec-ch12-splitting-individual}

For tasks involving human genetic variation, ensuring that data from the same individual (or related individuals) does not appear in both training and test sets is essential. A variant effect predictor trained on variants from person A and tested on other variants from person A may learn individual-specific patterns, such as haplotype structure or ancestry-correlated allele frequencies, that do not generalize to new individuals.

Family structure creates subtler leakage. First-degree relatives share approximately 50% of their genomes identical by descent. Even distant relatives share genomic segments that can be exploited by sufficiently powerful models. Best practice involves computing kinship coefficients across all individuals and either excluding one member of each related pair or assigning entire family clusters to the same split. The UK Biobank provides pre-computed relatedness estimates; other cohorts may require explicit calculation using tools like KING or `PLINK`. *[Citation Needed]*

### Splitting by Genomic Region {#sec-ch12-splitting-genomic-region}

Chromosome-based splits assign entire chromosomes to training or testing. This approach is common in regulatory genomics, where models trained on chromosomes 1-16 are tested on chromosomes 17-22 (or similar partitions). The advantage is simplicity and reproducibility; the disadvantage is that chromosomes are not independent. Chromosome 6 contains the HLA region with its unusual patterns of variation and selection; chromosome 21 is small and gene-poor; sex chromosomes have distinct biology. Results may vary substantially depending on which chromosomes are held out.

Region-based splits hold out contiguous segments (e.g., 1 Mb windows) distributed across the genome. This provides more uniform coverage than chromosome splits but requires careful attention to boundary effects. If a regulatory element spans the boundary between training and test regions, parts of its context may leak into training.

### Splitting by Gene or Protein Family {#sec-ch12-splitting-gene-family}

For variant effect prediction, holding out entire genes or protein families tests whether models learn general principles versus gene-specific patterns. A model that achieves high accuracy by memorizing that *TP53* variants are often pathogenic has not demonstrated understanding of mutational mechanisms. Gene-level splits force models to generalize to genes they have never seen, providing stronger evidence of biological insight.

Family-level splits extend this logic to groups of related genes. Holding out all kinases or all GPCRs tests whether models can generalize across evolutionary families. This is particularly stringent for protein structure and function prediction, where family membership strongly predicts properties.

::: {.callout-note}
## Stop and Think: Choosing the Right Split

Consider a project to predict whether coding variants cause loss of protein function. You have variants from 1000 genes, with 50-100 variants per gene. Which splitting strategy would you choose, and why?

A. Random split (80/10/10)
B. Chromosome-based (train on chr1-18, test on chr19-22)
C. Gene-based (train on 800 genes, test on 200 held-out genes)
D. Individual-based (split by patient ID)

*Consider: What would each split actually test? Which shortcuts could models exploit?*
:::

### Splitting by Experimental Context {#sec-ch12-splitting-experimental-context}

Multi-task models that predict chromatin marks across cell types can be split by cell type rather than genomic position. Training on liver, lung, and brain while testing on heart and kidney assesses whether learned regulatory logic transfers across tissues. This matters because cell-type-specific factors drive much of regulatory variation; a model that has simply learned which regions are accessible in the training cell types may fail on novel cell types even when sequence features should transfer.

Similarly, models can be split by assay type (e.g., training on ATAC-seq, testing on DNase-seq), laboratory (to assess batch effects), or time point (for longitudinal data). Each split tests a different axis of generalization.

### Splitting by Ancestry {#sec-ch12-splitting-ancestry}

For human genomic applications, ancestry-stratified evaluation has become essential. Models trained predominantly on European ancestry cohorts often show degraded performance in African, East Asian, South Asian, and admixed populations. This degradation reflects both differences in allele frequency spectra and differences in **linkage disequilibrium** patterns that affect which variants are informative.

Best practice reports performance separately for each major ancestry group represented in the data. When held-out ancestry groups are available (e.g., training on Europeans and testing on Africans), this provides the strongest test of cross-population generalization. When only European data are available, this limitation should be explicitly acknowledged, and claims about generalization should be appropriately modest. The confounding effects of ancestry on genomic predictions are detailed in @sec-ch13-confounding.

### Splitting by Time {#sec-ch12-splitting-time}

Temporal splits assign data to training and test sets based on when observations were collected, annotations were created, or variants were classified. This strategy tests whether models generalize forward in time, the actual deployment scenario for any predictive system.

For variant pathogenicity prediction, temporal splits are particularly revealing. ClinVar (@sec-ch02-clinvar) provides submission dates enabling clean temporal partitioning. Training on ClinVar annotations from 2018 and testing on variants first classified in 2022 asks whether the model can predict labels that did not yet exist during training. This avoids the circularity that arises when training and test labels were assigned by similar processes at similar times. Variants classified more recently may reflect updated curation standards, new functional evidence, or reclassifications of previously uncertain variants; a model that performs well on these genuinely new classifications demonstrates predictive validity rather than recapitulation of historical curation patterns.

Implementing temporal splits requires metadata that many datasets lack. ClinVar provides submission dates, enabling clean temporal partitioning. UniProt tracks annotation dates for functional assignments. Clinical cohorts with longitudinal follow-up naturally admit temporal splits based on diagnosis dates. When temporal metadata is unavailable, publication dates of source literature can serve as proxies, though these may not perfectly reflect when information became available to model developers.

The key limitation of temporal splits is non-stationarity. The distribution of variants classified in 2022 may differ systematically from those classified in 2018, not because biology changed but because research priorities, sequencing technologies, and ascertainment patterns evolved. Performance degradation on temporally held-out data may reflect distribution shift rather than genuine failure to generalize. Combining temporal splits with stratified analysis (performance by variant type, gene category, or evidence strength) helps disentangle these factors.


## Leakage Taxonomy and Detection {#sec-ch12-leakage-detection}

Even with careful splitting, leakage can enter evaluations through multiple pathways. A variant effect predictor that achieves 0.95 auROC on held-out test data may be exploiting information that would never exist for truly novel variants, rendering the performance estimate meaningless for clinical deployment. Understanding common leakage patterns helps practitioners design cleaner evaluations and critically assess published results.

Genomic machine learning faces four distinct leakage types, each creating different pathways for inflated performance: label leakage, feature leakage, temporal leakage, and benchmark leakage. These categories are not mutually exclusive; a single evaluation may suffer from multiple forms simultaneously, with compounding effects on apparent performance.

| Leakage Type | Definition | Example | Detection Strategy |
|--------------|------------|---------|-------------------|
| **Label leakage** | Target labels derived from features the model can access | ClinVar classifications informed by SIFT/PolyPhen scores | Compare to baseline using only those features |
| **Feature leakage** | Input features encode future or target information | Conservation scores for pathogenicity prediction | Ablate suspicious features; measure degradation |
| **Temporal leakage** | Using future information to predict past | Training on 2023 labels to predict 2020 classifications | Strict temporal splits with date metadata |
| **Benchmark leakage** | Test set construction influenced by evaluated methods | Benchmark selected proteins with good sequence coverage | Check benchmark construction procedure |

: The four major leakage types in genomic machine learning, with detection strategies for each. {#tbl-leakage-types}

### Label Leakage {#sec-ch12-label-leakage}

Label leakage occurs when target labels are derived from information that the model can access through its features. The classic example is training pathogenicity predictors on ClinVar annotations while using sequence features that contributed to those annotations. If ClinVar curators used *SIFT* and *PolyPhen* scores when classifying variants, and the new model uses similar sequence features, high performance may reflect recapitulation of curation criteria rather than independent predictive power.

The ClinVar circularity problem represents a particularly insidious form of label leakage. When computational predictions contributed to the pathogenicity classifications that later become training labels, new models learn to replicate their predecessors rather than discover independent signal. This circularity propagates through generations of models, each inheriting and reinforcing the biases of earlier predictors. The circularity problem for classical variant effect predictors is examined in @sec-ch04-circularity, with broader treatment of how such label contamination creates confounded evaluations in @sec-ch13-label-bias.

Expression models face analogous challenges when trained on features derived from the same samples used to define expression labels. The information flow becomes circular: labels inform features, which predict labels, creating apparent performance that would not generalize to independent samples.

### Feature Leakage {#sec-ch12-feature-leakage}

Feature leakage occurs when input features encode information about the target that would not be available at prediction time. In genomics, conservation scores are a common source. If a model uses PhyloP scores as features and the target is pathogenicity, the model may learn that conserved positions are more likely pathogenic without learning anything about variant-specific biology. This would be appropriate if conservation scores are intended to be part of the prediction pipeline, but problematic if the goal is to develop a model that predicts pathogenicity from sequence alone.

Similarly, population allele frequency encodes selection pressure. A model that learns "rare variants are more likely pathogenic" has discovered a useful heuristic but not necessarily mechanistic understanding. Whether this counts as leakage depends on the intended use case. For clinical variant interpretation where allele frequency is always available, exploiting this feature is appropriate. For understanding variant biology, it may mask whether the model has learned anything beyond frequency-based priors.

Feature leakage also arises when features encode information about data partitions or batch structure rather than biology. Coverage patterns that differ systematically between cases and controls, quality metrics that correlate with sequencing center, or variant density profiles that reflect caller-specific behavior all constitute feature leakage of this form.

### Temporal Leakage {#sec-ch12-temporal-leakage}

Temporal leakage violates the causal structure of prediction by using future information to predict past events. A model trained on ClinVar annotations from 2023 and tested on annotations that were uncertain in 2020 may perform well because new annotations were informed by model-like predictions. The apparent validation is circular: the model predicts labels that were partially derived from model-like reasoning applied after the prediction timepoint.

Clinical outcome prediction faces similar risks when laboratory values, imaging results, or clinical notes recorded after the prediction timepoint enter the feature set. A model predicting 30-day mortality that includes vital signs from day 15 has trivial access to outcome-correlated information. Proper temporal splits must respect not only when samples were collected but when each feature became available.

Training on variants classified in 2023 to predict classifications that were uncertain in 2020 allows models to learn from reclassification patterns rather than intrinsic variant properties. The model exploits the trajectory of scientific knowledge rather than the underlying biology.

### Benchmark Leakage {#sec-ch12-benchmark-leakage}

Benchmark leakage occurs when test set construction was influenced by methods similar to those being evaluated. If a protein function benchmark was created by selecting proteins with high-confidence annotations, and those annotations were partly derived from sequence similarity searches, sequence-based models may perform well by exploiting the same similarity that guided benchmark construction.

Foundation models face particular challenges with benchmark leakage. If a DNA language model is pretrained on all publicly available genomic sequence including ENCODE data, and then evaluated on ENCODE-derived benchmarks, the pretraining has exposed the model to information about the test distribution even if specific test examples were held out. The model may have learned statistical patterns in ENCODE data that transfer to ENCODE benchmarks without reflecting genuine biological understanding.

This form of leakage is especially difficult to detect because it operates at the level of distributional overlap rather than specific example memorization. A model that has never seen a particular test sequence may still have learned the statistical regularities that make that sequence predictable within the benchmark distribution.

### Detecting Leakage {#sec-ch12-detecting-leakage}

Several strategies help detect leakage, though none provides definitive proof of its absence. These approaches complement each other; rigorous evaluation employs multiple strategies, recognizing that each catches different leakage pathways while remaining blind to others.

::: {.callout-note}
## Practical Guidance: Leakage Detection Checklist

When evaluating your own model or reviewing published results, work through these detection strategies:

1. **Baseline analysis:** Does a simple model using only potentially leaky features (allele frequency, conservation) achieve similar performance?

2. **Feature ablation:** Does removing suspicious features cause dramatic performance drops?

3. **Confounder analysis:** Does performance remain after conditioning on potential confounders (gene, ancestry, batch)?

4. **Temporal validation:** Does performance hold on prospectively collected data?

5. **Overlap audit:** Has the overlap between pretraining data and benchmark test sets been documented and checked?
:::

Simple models that could not plausibly have learned biology provide an essential baseline analysis. If a linear model using only allele frequency achieves 0.80 auROC on a pathogenicity benchmark, and a sophisticated deep model achieves 0.82, the marginal improvement may not justify claims of biological insight. The deep model's performance is bounded by what simple confounders already explain.

Systematic feature ablation removes potentially leaky features and measures performance degradation. If removing conservation scores causes a 20-point drop in auROC, the model was heavily dependent on conservation rather than learning independent predictors. This approach identifies which features drive performance but cannot distinguish legitimate signal from leakage without domain knowledge about what information should be available at prediction time.

Explicit confounder analysis models potential confounders and tests whether model predictions remain informative after conditioning. If a variant effect predictor's scores become non-predictive after controlling for gene length and expression level, the model may have learned gene-level confounders rather than variant-level effects. @sec-ch13-confounding examines how leakage relates to these broader confounding structures.

Temporal validation evaluates models on data collected after the training data was frozen. If performance degrades substantially on newer data, the model may have been fitted to temporal artifacts in the original dataset. This approach is particularly valuable for detecting temporal leakage but requires access to prospectively collected data.

Finally, overlap auditing explicitly checks for sequence or sample overlap between pretraining corpora and evaluation benchmarks. For foundation models, this requires documenting pretraining data composition and comparing against benchmark construction procedures. The audit may reveal that apparent generalization is actually interpolation within seen distributions.


## Metrics for Genomic Tasks {#sec-ch12-metrics-genomic-tasks}

Metrics quantify model performance but different metrics answer different questions. Choosing appropriate metrics requires clarity about what aspect of performance matters for the intended application.

::: {.callout-note title="Stop and Think"}
Before reading about specific metrics, consider: You're evaluating a variant pathogenicity predictor where only 0.5% of variants are truly pathogenic. Your model achieves auROC = 0.92. Is this good? What other information do you need to decide whether to use this model in practice?

*Hint: Think about what happens when you apply a threshold to actually flag variants for follow-up.*
:::

::: {#fig-metric-selection}
![Metric selection flowchart](../figs/part_3/ch11/10-fig-metric-selection.svg)

Metric selection depends on task characteristics and deployment requirements. Binary classification requires different metrics depending on class balance: auROC provides stable ranking assessment for balanced data, while auPRC is essential for imbalanced settings where auROC can mislead. Probability calibration matters for deployment: a model may rank correctly while producing systematically overconfident probabilities. Clinical decision-making requires metrics like net benefit that account for the costs of different error types. Continuous predictions use correlation and error metrics, but high correlation does not guarantee clinical utility—a model may track relative differences while failing to capture absolute effect sizes. Selecting appropriate metrics prevents optimizing for misleading targets.
:::

### Discrimination Metrics {#sec-ch12-discrimination-metrics}

For binary outcomes (pathogenic versus benign, bound versus unbound, accessible versus closed), discrimination metrics assess how well the model separates classes. The auROC measures the probability that a randomly selected positive example is ranked above a randomly selected negative example. auROC is threshold-independent and widely reported but can be misleading when classes are highly imbalanced.

The auPRC better reflects performance when positives are rare. For variant pathogenicity prediction, where perhaps 1% of variants are truly pathogenic, a model achieving 0.95 auROC might still have poor precision at useful recall levels. auPRC directly captures the **precision**-**recall** trade-off that matters for applications requiring both high sensitivity and manageable false positive rates.

::: {.callout-tip}
## Key Insight: Why auROC Can Mislead

The distinction between auROC and auPRC reflects a mathematical property with practical consequences:

- **auROC is invariant to class imbalance**: A model's auROC remains identical whether 1% or 50% of examples are positive, because it measures pairwise ranking between one positive and one negative.
- **This invariance becomes a liability in deployment**: A model with 0.95 auROC applied to a dataset where 0.1% of variants are pathogenic might flag 100 false positives for every true positive at a threshold capturing 80% of positives.

**Rule of thumb:** Report both auROC (for comparison across datasets) and auPRC (for realistic assessment of deployment utility). When in doubt, auPRC is the more honest metric for imbalanced problems.
:::

This same invariance becomes a liability when evaluating for deployment. A model with 0.95 auROC applied to a dataset where 0.1% of variants are pathogenic might flag 100 false positives for every true pathogenic variant at a threshold capturing 80% of positives. The auROC provides no warning of this behavior because it treats a positive-to-negative pair the same regardless of how many negatives exist. For any application where false positives carry real costs (manual curation, clinical follow-up, unnecessary patient anxiety), auROC presents an optimistic picture that collapses upon deployment.

auPRC explicitly accounts for the negative class size. When positives are rare, achieving high precision requires a model that scores the vast majority of negatives lower than positives, not just a typical negative. This makes auPRC sensitive to class imbalance in exactly the way deployment is sensitive to class imbalance. A model moving from a balanced benchmark to a 1000:1 imbalanced application will show stable auROC but declining auPRC, mirroring the actual increase in false discovery rate users will experience. For this reason, auPRC (or equivalently, average precision) should be the primary metric when the deployment class distribution is known and imbalanced.

Threshold-dependent metrics including **sensitivity**, **specificity**, **positive predictive value**, and **negative predictive value** require specifying a decision threshold. These metrics are more interpretable for specific use cases (e.g., "the model identifies 90% of pathogenic variants while flagging only 5% of benign variants as false positives") but require choosing thresholds that may not generalize across settings.

### Regression and Correlation Metrics {#sec-ch12-regression-correlation-metrics}

For continuous predictions (expression levels, effect sizes, binding affinities), correlation metrics assess agreement between predicted and observed values. **Pearson correlation** measures linear association; **Spearman correlation** measures rank association and is robust to nonlinear relationships. The **coefficient of determination ($R^2$)** measures variance explained, though interpretation requires care when baseline performance is near zero.

For predictions at genomic scale (e.g., predicted versus observed expression across thousands of genes), these metrics may obscure important patterns. A model might achieve high genome-wide correlation by correctly predicting which genes are highly expressed while failing on the genes where predictions matter most. Task-specific stratification, such as correlation within expression quantiles or among disease-relevant genes, provides more actionable information.

### Ranking and Prioritization Metrics {#sec-ch12-ranking-prioritization-metrics}

Many genomic workflows care about ranking rather than absolute prediction. Variant prioritization pipelines rank candidates for follow-up; gene prioritization ranks targets for experimental validation. **Top-k recall** measures the fraction of true positives captured in the top $k$ predictions. **Enrichment at k** compares the true positive rate in the top $k$ to the background rate. **Normalized discounted cumulative gain (NDCG)** weights ranking quality by position, penalizing relevant items placed lower in the list more than those placed near the top. Why penalize by position rather than treating all rankings equally? The cost of ranking a true positive lower depends on where it falls in the list. A pathogenic variant ranked 5th instead of 1st will still be found quickly; the same variant ranked 500th instead of 495th has already been effectively lost in the noise. NDCG captures this intuition through logarithmic discounting: moving an item from position 10 to position 2 improves the score more than moving it from position 100 to position 92, because early ranks matter more for workflows with finite follow-up capacity.

These metrics align with how predictions are actually used. If experimental capacity permits validating only 20 variants per locus, top-20 recall matters more than global auROC. Reporting both global metrics and rank-aware metrics at relevant cutoffs provides a complete picture.

### Clinical Utility Metrics {#sec-ch12-clinical-utility-metrics}

For clinical applications, discrimination and calibration are necessary but not sufficient. **Decision curves** plot net benefit across decision thresholds, where net benefit weighs the value of true positives against the cost of false positives at each threshold. A model may achieve high auROC but offer no net benefit at clinically relevant thresholds if it fails to discriminate in the region where decisions are actually made.

**Net reclassification improvement (NRI)** measures how often adding genomic features to a clinical model changes risk classifications in the correct direction. This directly addresses whether genomics adds clinical value beyond existing predictors. @sec-ch28-clinical-risk provides detailed treatment of clinical evaluation frameworks.


## Baseline Selection {#sec-ch12-baseline-selection}

Baseline comparisons determine the meaning of reported performance. A model achieving 0.85 auROC might represent a major advance if the best prior method achieved 0.70, or a trivial improvement if simple heuristics achieve 0.83. Choosing appropriate baselines is as important as choosing appropriate metrics.

### Strong Baselines, Not Straw Men {#sec-ch12-strong-baselines}

The temptation to compare against weak baselines inflates apparent contributions. A deep learning model compared against a naive prior or a deliberately crippled baseline will appear impressive regardless of whether it offers genuine value. Strong baselines force honest assessment of improvement.

For sequence-based predictions, **position weight matrices (PWMs)** and k-mer logistic regression provide classical baselines that capture sequence composition without deep learning. If a convolutional model barely outperforms logistic regression on k-mer counts, the convolutional architecture may not be contributing as much as claimed.

For variant effect prediction, simple features like allele frequency, conservation scores, and amino acid properties provide baselines that any sophisticated model should substantially exceed. *CADD* (@sec-ch04-cadd) serves as a well-calibrated baseline that combines many hand-crafted features; outperforming *CADD* demonstrates that learning provides value beyond feature engineering.

For foundation models, comparisons should include both randomly initialized models of similar architecture (to isolate the value of pretraining) and simpler pretrained models (to isolate the value of scale or architectural innovations). Claiming that pretraining helps requires demonstrating improvement over training from scratch on the same downstream data.

::: {.callout-tip title="Knowledge Check"}
For each scenario, identify the appropriate baseline:

1. A new DNA language model claims to predict TF binding sites better than previous approaches. What baselines should it beat?
2. A variant pathogenicity predictor claims state-of-the-art performance. What would a "straw man" comparison look like, and what would a rigorous comparison include?
3. A foundation model claims that pretraining improves downstream performance. What comparison demonstrates the value of pretraining specifically?

:::{.callout-note collapse="true" title="Check Your Answer"}
1. It should beat: PWM/k-mer baselines, the best current deep learning model (like Enformer or DNABERT-2), and a randomly initialized model of the same architecture to show pretraining value.

2. A straw man would compare only to outdated methods or use different data; rigorous comparison includes current best tools (CADD, REVEL, AlphaMissense), simple baselines (conservation scores, allele frequency), and ablations testing each component's contribution.

3. Compare against a randomly initialized model with identical architecture trained from scratch on the same downstream task data - this isolates whether gains come from pretraining rather than just model size or architecture.
:::
:::

### Historical Baselines and Progress Tracking {#sec-ch12-historical-baselines}

Comparing to methods from five years ago may demonstrate progress but overstates the contribution of any single method. Comparisons should include the best currently available alternatives, not just historically important ones. When prior work is not directly comparable (different data, different splits, different metrics), reimplementing baselines on common benchmarks provides fairer comparison.

Field-wide progress tracking benefits from persistent benchmarks with frozen test sets. Once test set results for a benchmark are published, that benchmark becomes less useful for future model development because the test set is no longer truly held out. Periodic benchmark refresh with new held-out data helps maintain evaluation integrity.

### Non-Deep-Learning Baselines {#sec-ch12-non-dl-baselines}

Deep learning models should be compared against strong non-deep alternatives. Gradient-boosted trees, random forests, and regularized linear models often achieve competitive performance with far less computation. If a 100-million-parameter transformer barely outperforms XGBoost on tabular features, the complexity may not be justified.

This comparison is especially important for clinical deployment, where simpler models may be preferred for interpretability, computational efficiency, or regulatory approval. Demonstrating that deep learning provides substantial gains over strong non-deep baselines strengthens the case for adoption.


## Ablation Studies {#sec-ch12-ablation-studies}

Ablation studies systematically remove or modify model components to understand their contributions. Where baselines compare across methods, ablations investigate within a method, revealing which design choices actually matter.

### Component Isolation {#sec-ch12-component-isolation}

Standard ablations remove individual components: attention layers, skip connections, normalization schemes, specific input features. If removing attention heads causes minimal performance degradation, the model may not be exploiting long-range dependencies as claimed. If removing a particular input modality has no effect, that modality may not be contributing useful information.

Ablations should be designed to test specific hypotheses. If the claim is that a foundation model learns biologically meaningful representations, ablating pretraining (comparing to random initialization) directly tests this claim. If the claim is that cross-attention between modalities enables integration, ablating cross-attention while retaining separate encoders tests whether integration provides value.

### Hyperparameter Sensitivity {#sec-ch12-hyperparameter-sensitivity}

Reporting performance across hyperparameter ranges reveals robustness. A model that achieves state-of-the-art performance only at a narrow learning rate range with specific regularization may be overfit to the evaluation setup. Consistent performance across reasonable hyperparameter variations provides stronger evidence of genuine capability.

### Architecture Search Confounds {#sec-ch12-architecture-search-confounds}

When model development involves extensive architecture search, reported performance conflates the value of the final architecture with the value of search on the validation set. The validation set is no longer truly held out; it has been used to select among architectures. Final evaluation on a completely untouched test set, with the architecture fixed before test set examination, provides cleaner assessment.

### Reporting Standards {#sec-ch12-reporting-standards}

Ablation tables should clearly indicate what was changed in each condition, the number of random seeds or runs, and measures of variance. Single-run ablations can produce misleading results due to training stochasticity. Reporting means and standard deviations across multiple runs reveals whether observed differences exceed random variation.


## Statistical Rigor {#sec-ch12-statistical-rigor}

Performance differences between models may reflect genuine capability differences or random variation in training and evaluation. Statistical analysis distinguishes signal from noise.

::: {.callout-note}
## Two Cultures: Inference vs. Prediction

Readers from biostatistics may find this chapter's evaluation paradigm unfamiliar. The distinction traces to what Leo Breiman called the "two cultures" of statistical modeling [@breiman_statistical_2001].

| Aspect | Inferential (Classical Statistics) | Predictive (Machine Learning) |
|--------|-----------------------------------|------------------------------|
| **Primary goal** | Estimate parameters, test hypotheses | Generalize to new data |
| **Key question** | "Is this effect significant?" | "How well does this predict?" |
| **Validation** | p-values, confidence intervals | Held-out test sets, cross-validation |
| **Model preference** | Interpretable (linear, logistic) | Whatever predicts best |
| **Typical application** | GWAS effect sizes, clinical trials | Foundation models, risk prediction |

: The two cultures of statistical modeling applied to genomics. {#tbl-two-cultures}

**Inferential modeling** asks whether an observed relationship is "real" (unlikely under the null hypothesis). A GWAS reports effect sizes with p-values because the goal is understanding which variants associate with disease and estimating their effects. Model complexity is constrained to enable interpretation: a coefficient in logistic regression has meaning; a weight in a neural network does not.

**Predictive modeling** asks whether a model generalizes beyond training data. A variant effect predictor reports auROC on held-out variants because the goal is accurate classification, not mechanistic understanding. Model complexity is constrained only by overfitting: if a billion-parameter transformer predicts better, use it.

Foundation models are fundamentally predictive. They optimize for generalization, not inference. This explains why this chapter emphasizes held-out test performance rather than hypothesis testing---and why readers trained in classical biostatistics may need to shift their evaluation intuitions.

The cultures are not opposed. Mendelian randomization (@sec-ch26-mendelian-randomization) uses predictive models (genetic instruments) to answer inferential questions (causal effects). Polygenic risk scores use inferential results (GWAS effect sizes) for predictive applications. But understanding which culture a method belongs to clarifies what its evaluation metrics actually measure.
:::

### Significance Testing {#sec-ch12-significance-testing}

For classification metrics, significance tests ask whether observed differences exceed what would be expected from sampling variation. **Bootstrap** confidence intervals resample the test set with replacement, recompute metrics on each resample, and report the distribution of metric values. Non-overlapping 95% confidence intervals suggest significant differences. **Permutation tests** shuffle predictions between models and measure how often shuffled differences exceed observed differences.

For comparing multiple models across multiple benchmarks, correction for multiple testing becomes important. Without correction, 20 pairwise comparisons will produce an expected one false positive at the 0.05 level even when all models perform equally. The **Bonferroni correction** divides the significance threshold by the number of tests; the **Benjamini-Hochberg procedure** controls false discovery rate with more power than Bonferroni [@benjamini_controlling_1995].

### Effect Sizes {#sec-ch12-effect-sizes}

Statistical significance does not imply practical significance. A difference of 0.001 auROC might be statistically significant with millions of test examples while being practically meaningless. **Effect sizes** quantify the magnitude of differences independent of sample size. Cohen's *d* for continuous outcomes and odds ratios for binary outcomes provide standardized measures of effect magnitude.

Reporting both significance tests and effect sizes provides complete information. A result that is statistically significant with a tiny effect size warrants different interpretation than one that is significant with a large effect size.

### Confidence Intervals on Metrics {#sec-ch12-confidence-intervals}

Point estimates of auROC or correlation should be accompanied by confidence intervals. DeLong's method provides analytical confidence intervals for auROC [@delong_comparing_1988]; bootstrap methods provide distribution-free intervals for any metric. Reporting "auROC = $0.85$ (95% CI: $0.82$--$0.88$)" is more informative than "auROC = $0.85$" alone.

### Variance Across Random Seeds {#sec-ch12-variance-random-seeds}

Deep learning models are sensitive to initialization and optimization stochasticity. Training the same architecture with different random seeds can produce substantially different results. Best practice trains multiple runs and reports means and standard deviations. If the standard deviation across runs exceeds the difference between methods, claimed improvements may not be reproducible.


## Evaluating Foundation Models {#sec-ch12-evaluating-fm}

Genomic foundation models (@sec-ch14-fm-principles) admit multiple evaluation paradigms, each testing different aspects of learned representations.

::: {#fig-fm-evaluation-paradigms layout-ncol=3}
![Zero-shot evaluation](../figs/part_3/ch11/09-A-fig-fm-evaluation-paradigms.svg)

![Linear probing](../figs/part_3/ch11/09-B-fig-fm-evaluation-paradigms.svg)

![Fine-tuning](../figs/part_3/ch11/09-C-fig-fm-evaluation-paradigms.svg)

Foundation model evaluation paradigms test different capabilities. (A) Zero-shot: frozen model predictions test whether pretraining captured task-relevant patterns; requires no task data but limited to aligned tasks. (B) Linear probing: frozen embeddings with learned linear classifier test whether task features are linearly accessible in representations; requires minimal data and isolates representation quality. (C) Fine-tuning: full gradient updates test total potential but conflate representation and adaptation; achieves best performance but requires substantial data. The value of pretraining is best measured by the gap between few-shot and from-scratch performance—large gaps indicate that pretraining provides transferable features beyond what task data alone would support.
:::

### Zero-Shot Evaluation {#sec-ch12-zero-shot-eval}

In **zero-shot evaluation**, the pretrained model is applied without any task-specific training. For masked language models, this typically means using token probabilities to score variants or classify sequences. A variant that disrupts a position the model predicts with high confidence may indicate functional importance.

Zero-shot performance tests whether pretraining captures task-relevant structure without explicit supervision. Strong zero-shot performance suggests the pretraining objective aligned with the evaluation task; weak zero-shot performance suggests misalignment. Comparing zero-shot performance to simple baselines (e.g., conservation scores for variant effects) calibrates whether the foundation model provides value beyond what simpler approaches achieve.

### Linear Probing {#sec-ch12-linear-probing}

**Linear probing** freezes the foundation model and trains only a linear classifier on extracted embeddings. This isolates representation quality from fine-tuning capacity. If a linear probe on foundation model embeddings substantially outperforms a linear probe on random embeddings, the foundation model has learned useful features.

Layer-wise probing reveals where information is encoded. Early layers may capture local sequence features while later layers capture more abstract patterns. If the information needed for a task is extractable from early layers, the model may not require the full depth of the architecture for that application.

### Fine-Tuning Evaluation {#sec-ch12-fine-tuning-eval}

Full **fine-tuning** adapts all model parameters to the downstream task. This provides the best performance but conflates representation quality with adaptation capacity. A foundation model might achieve high fine-tuned performance through the capacity of its architecture rather than the quality of its pretrained representations.

Comparing fine-tuned foundation models to equivalently architected models trained from scratch isolates the value of pretraining. If both approaches converge to similar performance given sufficient downstream data, pretraining provides label efficiency (less data needed to reach a given performance level) rather than improved final performance. Data efficiency curves, plotting performance against downstream training set size, reveal this trade-off.

### Transfer Across Tasks {#sec-ch12-transfer-tasks}

Foundation models justify their "foundation" designation by transferring to diverse downstream tasks. Evaluating on a single task, however well-designed, cannot assess breadth of transfer. Multi-task evaluation across regulatory prediction, variant effects, protein properties, and other applications reveals whether foundation models provide general-purpose representations or excel only on tasks similar to their pretraining objective.

Transfer across species, tissues, and experimental modalities provides additional evidence of generalization. A DNA language model that transfers from human to mouse, or from blood cells to neurons, demonstrates that its representations capture biological principles rather than species-specific or tissue-specific patterns.


## Calibration Essentials {#sec-ch12-calibration}

Strong discrimination does not guarantee useful probability estimates. A model achieving 0.95 auROC might assign probability 0.99 to all positive examples and 0.98 to all negatives, ranking perfectly while providing meaningless confidence values. Clinical decision-making requires both: accurate ranking to identify high-risk variants and accurate probabilities to inform the weight of computational evidence. Calibration assesses whether predicted probabilities match observed frequencies, a property essential for rational integration of model outputs into diagnostic workflows.

### Assessing Calibration {#sec-ch12-assessing-calibration}

The most intuitive assessment comes from **reliability diagrams**, which plot predicted probabilities against observed frequencies. The construction bins predictions into intervals (commonly ten bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computes the mean predicted probability within each bin, computes the fraction of positive examples within each bin, and plots these quantities against each other. Perfect calibration produces points along the diagonal; systematic deviations reveal overconfidence (points below the diagonal) or underconfidence (points above).

A single summary statistic, the **expected calibration error (ECE)**, captures miscalibration as the weighted average absolute difference between predicted and observed probabilities across bins. Lower ECE indicates better calibration. The metric depends on binning choices; equal-width bins may place most examples in a few bins for models with concentrated predictions, while equal-mass bins ensure each bin contains the same number of examples but may span wide probability ranges. ECE should be reported alongside reliability diagrams for interpretability.

Aggregate calibration metrics can mask important heterogeneity. A model might achieve low aggregate ECE while being systematically overconfident for rare variant classes and underconfident for common ones, with opposite errors canceling in the aggregate statistic. Stratified calibration analysis across ancestry groups, variant classes, and gene categories identifies these disparities. For genomic models intended for diverse populations, subgroup-stratified calibration is not optional; aggregate metrics can mask clinically significant differential performance.

### Recalibration Methods {#sec-ch12-recalibration-methods}

Post-hoc recalibration adjusts predicted probabilities without retraining the underlying model. Methods range from single-parameter approaches like **temperature scaling** [@guo_calibration_2017], which divides logits by a learned constant to compress overconfident distributions, to non-parametric transformations like **isotonic regression**, which fits a monotonic function mapping raw scores to calibrated probabilities. **Platt scaling** [@platt_probabilistic_1999] fits a logistic regression from model outputs to true labels, providing intermediate flexibility. Each method makes different assumptions about the structure of miscalibration and requires different amounts of calibration data. The mathematical details, theoretical foundations, and guidance for method selection are developed in @sec-ch24-post-hoc-calibration.

All recalibration methods require held-out calibration data distinct from both training and test sets. Calibrating on test data and then evaluating calibration on the same test data produces overoptimistic estimates. For foundation models, the calibration set should be drawn from the deployment distribution; calibrating on ClinVar expert-reviewed variants may not transfer to variants in less-studied genes or underrepresented populations.

### Calibration in Model Comparison {#sec-ch12-calibration-comparison}

When comparing models, calibration metrics complement discrimination metrics. Two models with identical auROC may have dramatically different calibration, and the better-calibrated model will produce more reliable clinical evidence even though its ranking performance is equivalent. Reporting both discrimination (auROC, auPRC) and calibration (ECE, reliability diagrams) provides a complete picture of model performance.

Calibration can often be improved post-hoc without sacrificing discrimination. Temperature scaling preserves ranking while adjusting probability magnitudes, meaning a model can be recalibrated to improve ECE without changing auROC. This observation suggests that raw discrimination metrics may be more fundamental indicators of model quality, with calibration treated as an adjustable property. The comprehensive treatment of calibration theory is developed in @sec-ch24-calibration, including its relationship to uncertainty quantification (@sec-ch24-types) and methods for quantifying different sources of prediction uncertainty. Clinical deployment requires additional calibration considerations examined in @sec-ch28-calibration.


## Putting It All Together {#sec-ch12-putting-together}

When designing or evaluating a genomic model assessment, working through a systematic checklist helps identify gaps and potential problems. The following questions organize this review, though the specific considerations will vary by application.

::: {.callout-note}
## Practical Guidance: Evaluation Design Checklist

Use this checklist when designing an evaluation or reviewing published work:

**Data Splitting**

- [ ] Have individuals, genomic regions, gene families, and ancestries been appropriately separated?
- [ ] Has homology-aware clustering been applied with appropriate identity thresholds?
- [ ] Is there any plausible pathway for leakage or circularity?

**Baselines**

- [ ] Are comparisons made against the best available alternatives?
- [ ] Do non-deep-learning baselines establish floors that justify complexity?
- [ ] Does improvement over baselines warrant additional computational costs?

**Metrics**

- [ ] Are multiple metrics reported (discrimination, calibration, ranking)?
- [ ] Are confidence intervals provided?
- [ ] Are subgroup-stratified metrics reported for clinically relevant populations?

**Ablations**

- [ ] Have systematic ablations demonstrated which design choices drive performance?
- [ ] Is performance robust across hyperparameter ranges and random seeds?

**Statistical Rigor**

- [ ] Are significance tests applied with appropriate correction?
- [ ] Are effect sizes reported alongside p-values?

**For Foundation Models**

- [ ] Is performance reported across zero-shot, probing, and fine-tuning regimes?
- [ ] Do data efficiency curves reveal where pretraining provides value?
- [ ] Has transfer been tested across diverse tasks?
:::

First, consider the level of decision the model is intended to support. A model intended for molecular prediction faces different evaluation requirements than one designed for variant prioritization, patient risk stratification, or clinical action. Metrics should align with the actual decision context: enrichment metrics suit variant ranking, while net benefit matters for clinical decisions.

Second, examine whether data splits adequately prevent leakage. Are individuals, genomic regions, gene families, and ancestries appropriately separated? Has homology-aware clustering been applied with appropriate identity thresholds? Is there any plausible pathway for leakage or circularity through shared labels, features, or distributional overlap?

Third, assess the baseline comparisons. Are comparisons made against the best available alternatives, not just historical or deliberately weak baselines? Do non-deep-learning baselines establish floors that justify architectural complexity? Does the improvement over baselines warrant the additional computational and interpretability costs?

Fourth, evaluate metric selection. Are multiple metrics reported to capture discrimination, calibration, and ranking quality? Are metrics computed with confidence intervals that convey uncertainty? Are subgroup-stratified metrics reported to assess whether performance varies across clinically relevant populations?

Fifth, examine whether ablation studies isolate component contributions. Have systematic ablations demonstrated which design choices drive performance? Is performance robust across hyperparameter ranges and random seeds, or does it depend on specific configurations?

Sixth, consider statistical rigor. Are significance tests applied with appropriate correction for multiple comparisons? Are effect sizes reported alongside *p*-values to distinguish statistical from practical significance? Are confidence intervals provided for key metrics?

For foundation models specifically, additional considerations apply. Is performance reported across zero-shot, probing, and fine-tuning regimes? Do data efficiency curves reveal where pretraining provides value? Has transfer been tested across diverse tasks to justify the "foundation" designation?

Finally, assess robustness to deployment conditions. How does performance vary across cohorts, platforms, and ancestries? How does the model behave under distribution shift, missing data, or label noise? Would the evaluation translate to realistic deployment scenarios?

This checklist is not exhaustive but covers the most common evaluation pitfalls. Working through it systematically at the design stage can prevent problems that are difficult to fix retrospectively. Reviewers and readers can use the same checklist to critically assess published work.


## The Question Behind the Metric {#sec-ch12-question-behind-metric}

The question is never simply "what is the auROC?" but rather "what has been demonstrated, and how much should we trust it?" A reported metric summarizes one aspect of model behavior on one dataset under one evaluation protocol. Whether that metric predicts performance in deployment depends on details that standard reporting obscures: how data were split, whether leakage occurred, which subgroups were evaluated, what baselines were compared, and whether statistical conclusions account for multiple comparisons and estimation uncertainty.

The shortcuts that accelerate research in other machine learning domains produce misleading conclusions when applied to genomic data. Random train-test splits ignore homology that creates pseudo-replication. Single-metric comparisons miss failure modes in clinically relevant subgroups. Significance tests without effect sizes conflate statistical and practical importance. Benchmark evaluation without temporal awareness allows indirect leakage through shared community resources. Homology, population structure, batch effects, and label circularity create countless opportunities for self-deception, and genomic data exhibit all of these in abundance.

Rigorous evaluation requires sustained effort at every stage, from experimental design through statistical analysis. The confounding and leakage structures examined in @sec-ch13-confounding detail how **population stratification**, batch effects, and ascertainment bias produce results that evaporate under deployment. Uncertainty quantification (@sec-ch24-uncertainty) extends calibration assessment to epistemic versus aleatoric uncertainty (@sec-ch24-types) and selective prediction (@sec-ch24-selective-prediction). Interpretability (@sec-ch25-interpretability) addresses whether models have learned genuine biology or exploited confounded patterns, with attribution methods in @sec-ch25-attribution providing specific diagnostic tools. For clinical applications specifically, risk prediction frameworks (@sec-ch28-clinical-risk) develop evaluation approaches tailored to decision-making, where net benefit and decision curves supplement discrimination metrics. Together, these perspectives provide the critical apparatus for engaging with genomic foundation model claims.

::: {.callout-tip title="Test Yourself"}
Before reviewing the summary, test your recall:

1. What is homology leakage, and why does it make random train-test splits inadequate for protein and DNA sequence benchmarks?
2. Explain the four types of data leakage (label, feature, temporal, benchmark) and give a concrete genomic example of each.
3. A model achieves 0.85 Spearman correlation on a DMS benchmark with random splits, but only 0.60 with contiguous region splits. What does this performance gap reveal?
4. Why does auROC alone fail to capture calibration quality, and when would a well-calibrated model matter more than high discrimination?
5. How does label circularity in ClinVar (where computational predictions influence annotations) compromise benchmark validity for new variant effect predictors?

:::{.callout-note collapse="true" title="Check Your Answer"}
1. **Homology leakage** occurs when evolutionarily related sequences appear in both training and test sets, allowing models to succeed through memorization of family-specific patterns rather than learning general biological principles. Random splits fail because they ignore sequence similarity—a test protein at 80% identity to a training protein provides minimal generalization evidence, yet random splits routinely create such pairs. Homology-aware splitting (using tools like CD-HIT at 30-40% identity thresholds) ensures test sequences are evolutionarily distant from training sequences.

2. **Four leakage types:**
   - **Label leakage:** Target labels derived from model features. *Example:* ClinVar pathogenicity classifications that incorporated SIFT/PolyPhen predictions, then used to evaluate new predictors using similar features—the model learns to replicate curation criteria rather than discover independent signal.
   - **Feature leakage:** Input features encode unavailable future information. *Example:* Using population allele frequency as a feature for pathogenicity prediction captures selection pressure (rare = likely pathogenic) without learning variant biology; appropriate for clinical use but problematic for mechanistic understanding.
   - **Temporal leakage:** Using future information to predict past events. *Example:* Training on 2023 ClinVar annotations to predict variants that were uncertain in 2020, when those 2023 annotations may have used model-like predictions made after 2020.
   - **Benchmark leakage:** Test set construction influenced by similar methods. *Example:* A protein benchmark selecting well-annotated proteins via sequence similarity searches, then evaluating sequence-based models that exploit the same similarity used in benchmark construction.

3. The 0.85 → 0.60 performance drop reveals the model relies heavily on **local sequence context** rather than learning positional effects or long-range constraints. Random splits allow nearby positions to appear in both training and test sets; the model learns that "positions near training variants tend to have similar effects." Contiguous splits remove this crutch by placing entire sequence regions in test, forcing genuine spatial generalization. The 0.25 correlation gap represents how much performance came from interpolating between nearby measured positions versus understanding the protein's functional landscape.

4. **auROC measures ranking** (whether positives score higher than negatives) but is **invariant to probability calibration** (whether a score of 0.8 actually means 80% probability). A model can achieve 0.95 auROC by assigning 0.99 to all pathogenic variants and 0.98 to all benign variants—perfect ranking, useless probabilities. **Well-calibrated models matter most when:**
   - Making threshold-based decisions (reporting variants above some probability cutoff)
   - Integrating model evidence with other information (Bayesian updating requires calibrated likelihoods)
   - Communicating uncertainty to clinicians (a stated "80% probability" should be reliable)
   - Comparing risks across different variant types or populations

5. **Label circularity** creates a feedback loop: if ClinVar curators used computational predictions (SIFT, PolyPhen, conservation scores) to classify variants, and those classifications become training labels for new predictors using similar features, the new model learns to **replicate curation criteria** rather than provide independent evidence. This inflates benchmark performance without improving biological insight—the model predicts what previous models predicted, not what biology determines. Detecting circularity requires: (1) comparing against baselines using only the potentially circular features, (2) checking whether performance exceeds what those features alone achieve, (3) temporal validation on prospectively classified variants, and (4) examining whether the model adds value beyond existing predictors in clinical adjudication.
:::
:::

::: {.callout-note}
## Chapter Summary

This chapter covered the methodological foundations for proper model evaluation in genomic machine learning.

**Key Takeaways:**

- **Random splits fail** for genomic data because sequences share homology, individuals share ancestry, and samples share batch effects
- **Homology-aware splitting** (CD-HIT/MMseqs2 at appropriate thresholds) prevents the most common leakage pathway
- **Four leakage types** (label, feature, temporal, benchmark) require different detection strategies
- **Metric selection** must match deployment objectives: auPRC for imbalanced data, calibration for probability estimates, ranking metrics for prioritization
- **Strong baselines** and proper ablations distinguish genuine advances from benchmark-specific tuning
- **Foundation model evaluation** requires zero-shot, probing, and fine-tuning comparisons to isolate representation quality

**Looking Ahead:** The next chapter (@sec-ch13-confounding) examines how confounding and leakage structures beyond homology create spurious performance claims, including population stratification, batch effects, and ascertainment bias.

**Connections:**

- Apply evaluation principles when assessing claims in later chapters on foundation models (@sec-ch14-fm-principles through @sec-ch18-vep-fm)
- Calibration concepts developed here connect to uncertainty quantification (@sec-ch24-uncertainty)
- Clinical utility metrics introduced here are expanded for clinical risk prediction (@sec-ch28-clinical-risk)
:::
