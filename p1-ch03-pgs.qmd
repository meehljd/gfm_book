::: {.callout-warning .content-visible when-profile="draft"}
**TODO**
- Get scientific and methodological review from Dr. Schaid’s group.
- Double-check all equation notation for consistency with later chapters.
:::

# GWAS & Polygenic Scores {#sec-pgs}

Genome-wide association studies (GWAS) and polygenic scores (PGS) form the statistical backbone of modern human genetics. GWAS connect variation in DNA to variation in traits. Polygenic scores then aggregate many small genetic effects across the genome into a single number per individual. Together, they have delivered striking insights into complex traits, but they also expose fundamental limitations that motivate the mechanistic models developed in later parts of this book.

In this chapter we:

- Introduce the GWAS framework and its standard statistical models.
- Explain how linkage disequilibrium (LD) shapes association signals.
- Describe fine-mapping and its role in identifying putative causal variants.
- Show how GWAS results are converted into polygenic scores.
- Discuss how to interpret PGS and why they often fail to transfer across ancestries.
- Highlight the limitations of GWAS and PGS, motivating the need for genomic foundation models.

Throughout, we will connect to data resources introduced in Chapter @sec-data and to confounding and fairness issues discussed in Chapter @sec-confound.

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: High-level pipeline figure showing:
1. Cohort with genotypes + phenotypes.
2. GWAS producing a Manhattan plot.
3. Fine-mapping yielding credible sets and PIPs.
4. Polygenic score construction and downstream risk stratification.
:::

## The GWAS Paradigm

A GWAS requires three ingredients:

1. **Genotypes** – typically dense SNP arrays or imputed variants across the genome, often augmented with structural variants or exome data.
2. **Phenotypes** – quantitative traits (e.g., height, LDL cholesterol) or binary outcomes (e.g., disease vs control), plus relevant covariates (age, sex, etc.).
3. **A statistical model** – usually a regression model that tests one variant at a time for association with the phenotype.

The basic GWAS workflow is:

1. Perform quality control on individuals and variants.
2. For each variant, fit a regression model of the phenotype on genotype and covariates.
3. Record the estimated effect size and an association statistic (e.g., p-value).
4. Correct for multiple testing to control false positives.
5. Summarize results across the genome (Manhattan and QQ plots) and follow up notable loci.

Practically oriented guides provide detailed recommendations on QC, covariate selection, and interpretation [@marees_gwas_2018]. Biobanks and consortia described in @sec-data provide the sample sizes that make modern GWAS possible.

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: Flow diagram of the GWAS pipeline with annotations:
- Inputs: genotype matrix, phenotype vector, covariates.
- “For each variant” loop with regression model.
- Outputs: table of effect sizes and p-values, Manhattan and QQ plots.
:::

### Continuous Phenotypes

For quantitative traits, the standard association model is linear regression. Consider a single bi-allelic variant \(j\). Let \(g_{ij}\) denote the genotype dosage for individual \(i\), encoded as 0, 1, or 2 copies of the alternative allele (or as an imputed fractional dosage). Let \(c_i\) be a vector of covariates (e.g., age, sex, ancestry principal components). The model is

$$
y_i = \alpha + \beta_j g_{ij} + \gamma^\top c_i + \varepsilon_i,
$$

where:

- \(y_i\) is the phenotype for individual \(i\).
- \(\alpha\) is the intercept (baseline phenotype when genotype and covariates are zero).
- \(\beta_j\) is the per-allele effect size for variant \(j\).
- \(\gamma\) is a vector of covariate coefficients.
- \(\varepsilon_i\) is a residual error term, typically assumed to be Gaussian with mean 0 and constant variance.

#### Effect Size

The coefficient \(\beta_j\) has a direct interpretation: it is the expected change in the phenotype associated with one additional copy of the alternative allele, holding covariates fixed. If phenotype units are meaningful (e.g., cm of height), \(\beta_j\) is interpretable on that scale. Often we standardize phenotypes to zero mean and unit variance, so \(\beta_j\) is in units of standard deviations per allele.

Because GWAS typically considers millions of variants, most individual \(|\beta_j|\) are small. Nonetheless, in aggregate, many such effects can explain a substantial fraction of trait heritability, as we will see when we discuss polygenic scores and “missing heritability” [@yang_common_2010].

#### Significance Testing and Multiple Comparisons

To test association, we compute a test statistic for the null hypothesis \(H_0: \beta_j = 0\). Under standard assumptions, the usual \(t\)-statistic for \(\beta_j\) approximately follows a normal distribution in large samples. This yields a p-value for each variant.

With \(M\) variants tested (often \(M \approx 10^6\)–\(10^7\)), we must correct for multiple comparisons. A widely used heuristic is the **genome-wide significance threshold** of \(5 \times 10^{-8}\). This value reflects a Bonferroni correction for roughly one million effectively independent tests, accounting for LD among common SNPs [@peer_estimation_2008].

Variants with p-values below this threshold are reported as genome-wide significant hits. Variants with less extreme p-values may still contribute to polygenic scores, particularly in LD-aware methods.

#### Covariates

Covariates play two roles:

- **Precision**: adjusting for variables like age and sex can reduce residual variance and increase power.
- **Control of confounding**: failing to adjust for variables related to both genotype and phenotype can produce spurious associations (e.g., if cases and controls differ systematically in age or sex).

In practice, GWAS covariate sets often include age, sex, study-specific technical covariates (batch, array type), and ancestry principal components (see below) [@marees_gwas_2018].

#### Ancestry PCs

Population structure can induce confounding: if allele frequencies differ across subgroups and the phenotype also differs across groups for non-genetic reasons, naive GWAS may detect associations that reflect group differences rather than causal effects.

Principal component analysis (PCA) on genotype data captures major axes of genetic variation [@price_pca_2006; @patterson_population_2006]. The top principal components often correspond to broad ancestry gradients (continental-level differences). Including these PCs as covariates attenuates spurious associations due to ancestry stratification in the linear regression framework.

However, ancestry adjustment in GWAS is only a partial solution to a much broader set of challenges. Ancestry is intertwined with environment, socioeconomic status, healthcare access, and clinical practice in ways that simple covariate adjustment cannot fully resolve. These issues become critical when translating GWAS results to polygenic scores and clinical applications. We return to the full complexity of ancestry as a confounder, including its impact on model performance, fairness, and generalizability, in @sec-confound.

#### Intercept and Residuals

The intercept \(\alpha\) and residuals \(\varepsilon_i\) are often overlooked. Systematic inflation in test statistics (e.g., due to cryptic relatedness or subtle structure) can manifest as a departure from the expected p-value distribution, even after including PCs. Genomic control, LD score regression, and more recent methods quantify and sometimes correct for this inflation, though we do not detail them here.

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: QQ plot and Manhattan plot for a sample GWAS:
- Show expected vs observed \(-\log_{10}(p)\) to illustrate inflation/deflation.
- Show genome-wide Manhattan plot with annotated genome-wide significant loci.
:::

### Binary Phenotypes

For case–control traits (e.g., disease status), GWAS typically uses logistic regression. Let \(y_i \in \{0,1\}\) denote case (1) or control (0). The model is

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \beta_j g_{ij} + \gamma^\top c_i.
$$

The left-hand side is the log-odds of being a case. Here:

- \(\beta_j\) represents the log-odds ratio (log-OR) per additional copy of the alternative allele.
- \(\exp(\beta_j)\) is the odds ratio (OR).

For rare diseases, odds ratios are often interpreted as approximating relative risks, but strictly speaking they differ: the OR compares odds, not probabilities. Converting ORs into absolute risk requires baseline disease prevalence, which we discuss later when interpreting polygenic scores.

Case–control sampling can distort absolute risk but preserves the ordering of risk: a variant that increases odds in the sampled data also increases risk in the source population. This is why GWAS conducted in case–control designs still provide useful effect sizes for polygenic scores, provided downstream models account for baseline incidence.

Logistic regression shares the same covariate structure as linear regression, and the same challenges with confounding, population structure, and technical artifacts apply.

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: 
- Simple 2×2 table (allele count vs case/control) leading into logistic regression.
- Plot of probability of disease vs genotype dosage for a variant with OR > 1, illustrating the logistic curve.
:::

## Linkage Disequilibrium and Association Signals

GWAS tests variants one at a time, but the genome is inherited in blocks. Nearby variants tend to be correlated because they are co-inherited on haplotypes. This **linkage disequilibrium (LD)** is central to interpreting GWAS results.

A GWAS association signal at variant \(j\) may arise because:

- Variant \(j\) is itself causal for the phenotype.
- Variant \(j\) tags a causal variant \(k\) that is correlated with it in the population.
- Multiple causal variants in a region jointly influence the phenotype, producing a complex local pattern of association.

Understanding LD is essential before moving to fine-mapping and polygenic scoring.

### Haplotype Structure and Recombination

Meiosis recombines parental chromosomes through crossover events. Over many generations, recombination breaks down long-range correlations but preserves short-range structure. The result is:

- Regions of high LD (haplotype blocks) where many variants are strongly correlated.
- Recombination hotspots where LD decays rapidly.

Patterns of LD vary across populations due to demographic history (bottlenecks, expansions, admixture). These differences play a major role in how GWAS signals and polygenic scores transfer across ancestries.

### Measuring Correlation: The \(r^2\) Statistic

The most commonly used measure of LD between two bi-allelic variants is the squared correlation \(r^2\) between their allele counts:

- \(r^2 \approx 1\) indicates nearly perfect correlation (variants are almost always observed together).
- \(r^2 \approx 0\) indicates independence.

From a GWAS perspective:

- If variant \(j\) is causal and variant \(k\) is in high LD with \(j\), both may show similar p-values.
- If multiple variants are causal, LD patterns can cause their signals to blend into a single “peak” in the Manhattan plot.

This correlation structure complicates efforts to pinpoint which variants are truly causal.

### Causal Versus Tag Variants

A **causal variant** directly influences the phenotype (e.g., by altering a protein, transcription factor binding, or splicing). A **tag variant** is merely correlated with a causal variant due to LD. Tag variants are often easier to genotype, especially on array platforms, and can stand in for causal variants in association analyses.

GWAS catalogs are therefore catalogs of **associated loci**, not necessarily of causal variants. Distinguishing causal from tag variants requires additional information (fine-mapping, functional assays, or mechanistic models), which we discuss below and in later chapters.

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: Local locus illustration:
- Top: Manhattan “zoom-in” showing a cluster of strongly associated variants.
- Middle: LD heatmap (r²) across the locus.
- Bottom: Track indicating “causal” variant(s) vs tag variants, possibly overlaying gene annotations.
:::

## From Association Signals to Fine-Mapping

Fine-mapping aims to move from “this region is associated” to “these are the most likely causal variants in this region.” Given a set of summary association statistics and an LD matrix, Bayesian fine-mapping methods estimate the probability that each variant is causal.

At a high level:

1. Define a region around an index SNP (e.g., all variants within 1 Mb).
2. Specify a prior over which variants are causal (e.g., at most \(K\) causal variants per region).
3. Use the observed association statistics and LD to compute the posterior probability of each configuration of causal variants.
4. Derive **posterior inclusion probabilities (PIPs)** for each variant and **credible sets** that contain the true causal variant(s) with a specified probability (e.g., 95%).

Comprehensive reviews describe how summary statistics and LD matrices from reference panels can be combined to perform fine-mapping at scale [@pasaniuc_dissecting_2016].

### Bayesian Fine-Mapping Framework

Bayesian fine-mapping methods differ in their priors and computational approximations, but they share common ingredients:

- A prior on the number of causal variants and their effect sizes.
- A likelihood model linking effect sizes to observed marginal associations given LD.
- Posterior inference to compute PIPs and credible sets.

Variants with high PIPs are strong candidates for causal involvement. Credible sets reflect the uncertainty due to LD and limited sample size. Narrow credible sets (few variants) are more actionable for functional follow-up.

### Applications and Multi-Ancestry Leverage

Fine-mapping serves several downstream purposes:

- **Variant prioritization**: variants with high PIPs are prime targets for mechanistic follow-up (e.g., regulatory assays).
- **Gene prioritization**: mapping credible-set variants to genes (via proximity, eQTLs, or chromatin interactions) helps identify likely effector genes.
- **Polygenic score construction**: PIPs can be used to weight variants or to select variants with high posterior probability of causality.

Multi-ancestry data often sharpen fine-mapping. Because LD patterns differ across populations, a variant that is highly correlated with many others in one population may have fewer strong proxies in another. Joint fine-mapping across ancestries can therefore reduce credible set sizes and refine causal inferences [@pasaniuc_dissecting_2016].

Integration of fine-mapping with functional annotations and multi-omics data at scale—such as in large open resources linking GWAS, fine-mapping, and functional genomics—further improves prioritization of likely causal variants [@mountjoy_open_2021].

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: Fine-mapping cartoon:
- Panel A: Manhattan peak highlighting a broad associated region.
- Panel B: Bar plot of PIPs for variants in the region.
- Panel C: Example of a 95% credible set, indicating how many variants remain plausible.
- Optional overlay: effect of adding a second ancestry on credible set narrowing.
:::

## Constructing Polygenic Scores

Polygenic scores (PGS) aggregate estimated variant effects across the genome into a single scalar per individual:

$$
\text{PGS}_i = \sum_{j} w_j g_{ij},
$$

where \(w_j\) is a weight assigned to variant \(j\) based on GWAS or fine-mapping, and \(g_{ij}\) is the genotype dosage for individual \(i\).

There are many design choices:

- Which variants to include.
- How to set the weights \(w_j\).
- How to account for LD.
- How to validate and calibrate the resulting score.

### Terminology: PGS vs PRS

::: {.callout-note}
### Terminology: PGS vs PRS

The literature uses overlapping terminology:

- **Polygenic risk score (PRS)** – historically common, especially for disease endpoints.
- **Polygenic score (PGS)** – more general, used for both diseases and quantitative traits.
- **Genomic risk score** and related terms also appear, often interchangeably.

In this book we use **polygenic score (PGS)** as the default, with “risk” added when we specifically discuss disease risks. Methodological overviews of PGS construction and evaluation are available in [@choi_prs_2020].
:::

We now describe three families of approaches: clumping & thresholding, LD-aware Bayesian methods, and fine-mapping-informed scores.

### Clumping and Thresholding

The simplest and still widely used approach is **clumping and thresholding** (“C+T”):

1. **Clumping**:
   - Rank variants by their GWAS significance (e.g., p-value).
   - Starting from the most significant variant, remove nearby variants within a specified window that are in high LD (e.g., \(r^2 > 0.1\)–0.2).
   - This yields a set of approximately independent variants.

2. **Thresholding**:
   - Apply a p-value threshold (e.g., \(5 \times 10^{-8}\), \(10^{-5}\), or even 1).
   - Retain only variants with p-values below this threshold.

3. **Weighting**:
   - Set weights \(w_j\) equal to the GWAS effect size estimates (e.g., \(\hat\beta_j\)) for the retained variants, and zero otherwise.

Hyperparameters (LD window, \(r^2\) threshold, p-value threshold) are often chosen by grid search to maximize predictive performance in a validation set. This tuning can introduce overfitting, especially in small samples or when the validation set is not representative of future deployment populations.

C+T scores are easy to compute and interpret, but they ignore much of the information in GWAS summary statistics:

- Most variants are discarded.
- LD is only handled via coarse pruning.
- Variants with modest p-values that collectively explain substantial variance may be underweight or excluded.

### LD-Aware Bayesian Methods

A more principled approach is to model the joint distribution of effect sizes explicitly, accounting for LD among variants. In these methods, we treat the true effect sizes \(\beta = (\beta_1, \dots, \beta_M)\) as random variables drawn from a prior distribution, and use the GWAS summary statistics and LD matrix to infer posterior mean effect sizes \(\mathbb{E}[\beta_j \mid \text{data}]\). The posterior means (or related shrinkage estimates) become the PGS weights \(w_j\).

LDpred, for example, assumes that a fraction \(p\) of variants have nonzero effects drawn from a Gaussian distribution, while the rest have zero effect [@vilhjalmsson_modeling_2015]. Given GWAS summary statistics and an LD reference panel, LDpred computes approximate posterior effect sizes that:

- Shrink noisy estimates toward zero.
- Borrow strength across correlated variants.
- Reduce overfitting compared with C+T.

Related methods (e.g., lassosum and others) use different priors or optimization strategies but share the general idea: jointly model effect sizes under LD rather than pruning it away. These methods generally yield better predictive performance than C+T when properly tuned, especially as sample sizes increase.

However, they still face challenges:

- They typically rely on an LD reference panel that may not match the target population.
- They are sensitive to model misspecification (e.g., wrong prior on effect sizes).
- They often assume a single ancestry or treat ancestries separately.

### Fine-Mapping-Informed Polygenic Scores

Fine-mapping outputs—particularly PIPs—provide an appealing basis for polygenic scores:

- Variants with high PIPs are more likely to be causal.
- Fine-mapping can incorporate functional annotations and multi-ancestry information.
- PIPs can be used to select variants or to reweight effect sizes.

Two broad strategies are common:

1. **Selection**: include only variants with PIP above a threshold (e.g., 0.1 or 0.5) and set \(w_j\) proportional to their estimated effect sizes.
2. **Weighting**: use PIPs directly as weights (or as multiplicative factors), so that variants with higher PIPs contribute more to the score even if effect size estimates are similar.

Fine-mapping-informed PGS aim to concentrate weight on variants that are more likely to be biologically meaningful and robust across ancestries. Large-scale resources that jointly analyze GWAS, fine-mapping, and functional genomics provide datasets from which such scores can be derived [@mountjoy_open_2021].

These approaches sit conceptually between purely statistical shrinkage (e.g., LDpred) and mechanistic models. They still rely on GWAS summary statistics but use additional structure—LD, functional annotations, and multi-ancestry patterns—to enrich the score.

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: Three-panel PGS construction schematic:
- Panel A: C+T pipeline (Manhattan plot → pruning → few variants with raw betas).
- Panel B: LD-aware Bayesian pipeline (summary stats + LD matrix → posterior effect sizes).
- Panel C: Fine-mapping-informed pipeline (credible sets + PIPs → weighted score).
:::

## Interpreting Polygenic Scores

Once a polygenic score has been computed for an individual, how should it be interpreted?

At a minimum, a PGS can be used to:

- Rank individuals by genetic susceptibility (relative risk).
- Stratify populations into risk quantiles (e.g., top 1% vs average).
- Estimate the proportion of phenotypic variance explained by common variants (on the liability scale for diseases).

### Relative versus Absolute Risk

Polygenic scores are most naturally interpreted in **relative** terms. For a disease outcome, we might fit a logistic regression model in a target cohort:

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \theta \, \text{PGS}_i + \eta^\top z_i,
$$

where \(z_i\) collects covariates (age, sex, etc.) and \(\theta\) is the effect of a one-unit increase in the PGS. We can rescale the PGS (e.g., to have unit standard deviation) so that \(\exp(\theta)\) is the **odds ratio per SD of PGS**.

Relative risk communication often focuses on percentiles:

- Individuals in the top 1% of the PGS distribution may have several-fold higher disease odds than those near the median.
- Risk gradients can be displayed as odds ratios or hazard ratios by PGS decile or percentile.

Absolute risk requires combining the PGS with baseline incidence rates (e.g., age-specific disease rates in the population). Clinical guidelines increasingly emphasize absolute risk (e.g., 10-year risk of coronary artery disease) for decision-making, and using PGS in this context requires careful calibration and validation in representative populations (see Chapter @sec-clinical).

### Ancestry, Linkage Disequilibrium, and Transferability

A major challenge for PGS is **transferability** across populations:

- GWAS in large European-ancestry cohorts identify many variants and yield precise effect size estimates.
- The same PGS often explains substantially less variance and exhibits poorer risk discrimination in non-European ancestries.

Several factors contribute:

- **LD differences**: variants tagging causal alleles in one ancestry may not be in LD with them in another.
- **Allele frequency differences**: rare variants in one population may be common in another, affecting power and effect size estimates.
- **Gene–environment interactions** and environment-specific exposures.
- **Statistical artifacts** due to mismatched LD reference panels and QC pipelines.

Empirical studies document pronounced drops in PGS performance when applied across ancestries and emphasize the importance of diverse training data and careful evaluation [@verma_diversity_2024]. Equity concerns are central: deploying PGS trained primarily in one ancestry can exacerbate health disparities if scores are systematically less accurate for underrepresented groups (see also Chapter @sec-confound).

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: 
- Panel A: Overlaid histograms of PGS for cases vs controls in a single ancestry, with risk-by-percentile curve.
- Panel B: Bar plot or line plot comparing variance explained (R² or liability-scale \(R^2\)) for the same PGS across multiple ancestry groups, highlighting decreased performance.
:::

## Limitations of GWAS and PGS, and the Case for Mechanistic Models

GWAS and PGS have transformed human genetics, but they have also revealed persistent gaps between statistical association and biological understanding.

### Achievements and the Clinical Adoption Gap

GWAS have:

- Identified thousands of loci associated with hundreds of traits.
- Demonstrated that common variants contribute meaningfully to disease risk.
- Enabled construction of PGS that rival or exceed traditional risk factors for some diseases in specific settings.

Yet clinical adoption remains limited:

- Many PGS offer modest incremental predictive power over standard risk factors.
- Performance often varies sharply across ancestries.
- Mechanistic interpretation is often shallow: we know *where* in the genome association lies but not exactly *how* it influences disease.

### Association Without Mechanism

Most GWAS hits lie in noncoding regions. They often reside in enhancers, promoters, or other regulatory elements, but the causal chain from variant to molecular effect to disease is typically unknown. Even when credible sets are small, they frequently contain multiple plausible regulatory variants.

This “association without mechanism” problem limits:

- Our ability to identify drug targets.
- Translation into therapeutic interventions.
- Interpretability and trust in genetic risk predictions.

### Population Transferability

As discussed above, PGS constructed from predominantly European-ancestry GWAS often perform poorly in other ancestries [@verma_diversity_2024]. This is not just a technical nuisance; it raises fundamental questions about what PGS capture:

- Are they measuring biology that is genuinely universal but poorly estimated in underrepresented groups?
- Or are they partly capturing ancestry-specific LD patterns, environmental context, and even subtle confounders?

These issues complicate the use of PGS in diverse clinical populations and underscore the importance of diverse cohorts and careful causal reasoning (Chapters @sec-confound and @sec-systems).

### The Noncoding Variant Challenge

Many trait-associated variants are noncoding and likely act through gene regulation, chromatin structure, or RNA processing rather than protein sequence. Without a mechanistic model of how sequence specifies regulatory activity, GWAS can tell us *where* something is happening but not *what* or *why*.

Deep learning models trained on regulatory genomics data, such as those that predict chromatin accessibility, transcription factor binding, and gene expression from sequence [@zhou_deepsea_2015; @zhou_expecto_2018], begin to address this gap. They map sequence changes directly to predicted molecular consequences, providing hypotheses about regulatory mechanisms that underlie GWAS signals.

### Static Scores in a Dynamic Context

PGS are typically static:

- Computed once from inherited genetic variants.
- Interpreted as a lifetime, context-independent risk modifier.

In reality, disease risk unfolds over time in a dynamic environment:

- Exposures (diet, smoking, pollution) change across the life course.
- Physiological states (e.g., inflammation, hormone levels) fluctuate.
- Gene regulation and cellular states respond to these changing conditions.

Static PGS cannot capture temporal trajectories or gene–environment interactions. They are a coarse summary of inherited liability, not a full model of disease dynamics.

### Missing Heritability

Classical quantitative genetics connects family resemblance to **narrow-sense heritability** \(h^2\): the proportion of phenotypic variance attributable to additive genetic effects. Early GWAS explained only a small fraction of this heritability, leading to the “missing heritability” puzzle [@manolio_finding_2009].

Subsequent work showed that much of the “missing” component could be accounted for by many common variants of small effect, even if individually nonsignificant, when modeled jointly [@yang_common_2010]. Nonetheless, there remain gaps:

- Rare variants with large effects.
- Structural variants, copy-number variation, and other classes not well captured by standard GWAS.
- Epistasis and gene–environment interactions.

Pooled across traits, GWAS and PGS suggest that complex traits are highly polygenic, but they do not fully close the gap between observed heritability, biological mechanism, and actionable clinical information.

### Ancestry, Portability, and Fairness

Polygenic scores derived from predominantly European ancestry GWAS show substantially reduced performance when applied to other populations. African ancestry individuals typically see 40-75% reductions in prediction accuracy compared to European ancestry individuals, even for the same trait in the same study [@duncan_analysis_2019]. This performance gap stems from multiple factors: different LD structures across populations, population-specific causal variants not captured in European-trained scores, environmental and gene-by-environment differences, and ascertainment biases in training data.

These portability failures are not merely technical limitations but raise fundamental questions about equity and fairness in precision medicine. If genomic models work primarily for individuals of European ancestry, they risk exacerbating existing health disparities. Multi-ancestry GWAS, diverse training cohorts, and methods that account for population structure help but do not fully solve the problem [@marquez-luna_incorporating_2021].

The challenges of ancestry in genomic prediction extend far beyond PGS to every model in this book. Foundation models can easily learn to exploit ancestry signals as shortcuts, benchmark evaluations mask poor cross-ancestry performance by reporting only aggregate metrics, and deployment in diverse clinical populations requires explicit validation across ancestry groups. These issues are addressed comprehensively in @sec-confound, which provides frameworks for detecting, mitigating, and reporting ancestry-related biases in genomic models.

### Toward Mechanistic Models

These limitations collectively motivate the mechanistic approaches that form the core of this book. We would like models that:

- Connect sequence variation directly to molecular phenotypes (e.g., regulatory activity, splicing, chromatin structure).
- Integrate multi-omics data across scales (Chapters @sec-splice and @sec-systems).
- Provide causal hypotheses about how variants influence cellular and organismal phenotypes.
- Generalize across populations and contexts by modeling underlying biology rather than solely statistical association patterns.

Deep learning models trained on large regulatory and transcriptomic datasets, such as those predicting chromatin accessibility, transcription factor binding, and expression from DNA sequence, are one step in this direction [@zhou_deepsea_2015; @zhou_expecto_2018]. When combined with GWAS and fine-mapping results, these models can prioritize variants that are both statistically associated and predicted to be functionally impactful [@mountjoy_open_2021].

Downstream chapters will show how:

- Multi-omics integration (Chapter @sec-systems) connects molecular and clinical data.
- Variant effect prediction methods (Chapter @sec-splice and Chapter @sec-vep) refine our understanding of GWAS loci.
- Genomic foundation models move beyond association to provide mechanistic insight and improved, equitable clinical translation (Part VI).

The goal is not to discard GWAS and PGS, but to build on them—replacing “association without mechanism” with models that make testable, mechanistic claims and support robust, fair deployment in real-world settings.

::: {.callout-warning .content-visible when-profile="draft"}
*Suggested visual*: Conceptual “bridge” figure:
- Left: GWAS & PGS (Manhattan plot, PGS distribution) with labels “statistical association.”
- Right: Mechanistic models (sequence → regulatory predictions → cellular phenotypes) with labels “biological mechanism.”
- Middle: Fine-mapping and integration steps (PIPs, functional annotations, multi-omics).
This figure should foreshadow Parts III–VI of the book.
:::
