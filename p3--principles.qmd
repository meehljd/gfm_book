# Part III: Core Principles {.unnumbered}

The previous part surveyed the architectural landscape of deep learning for genomics, from convolutional sequence-to-function models through protein and DNA language models to hybrid architectures that combine CNNs with transformers. These chapters focused on individual model families and their capabilities. Part III now steps back to examine the conceptual foundations that unite these approaches and distinguish foundation models from their task-specific predecessors.

Three questions organize this part. First, how should genomic sequences be represented as input to neural networks? The choice of tokenization scheme, whether single nucleotides, k-mers, learned subword vocabularies, or biologically informed tokens, profoundly shapes what patterns a model can discover and how efficiently it processes long sequences. Second, what training objectives and adaptation strategies enable models to learn reusable representations from unlabeled data? The shift from supervised learning on narrow tasks to self-supervised pretraining on broad sequence data defines the foundation model paradigm. Third, how can these representations be applied to the clinically crucial problem of predicting variant effects? Variant effect prediction serves as both a unifying application that draws on all the ideas developed earlier and a window into how foundation models perform on tasks that matter for patient care.

These three chapters build on each other. @sec-token examines tokenization strategies from one-hot encoding through BPE to recent approaches that incorporate genomic annotations directly into the vocabulary. @sec-princ develops a practical framework for understanding pretraining objectives, fine-tuning strategies, and the trade-offs involved in adapting foundation models to new tasks. @sec-vep then applies these principles to variant interpretation, surveying how models like AlphaMissense, GPN-MSA, Evo 2, and AlphaGenome translate learned representations into pathogenicity predictions. Together, they provide the conceptual vocabulary needed to understand the multi-scale and systems-level approaches covered in Part IV.