# DNA and Genomic Models  {#sec-dna}

Genomic language models extend the ideas of protein language models (@sec-prot) to the DNA level. They treat genomes themselves as a corpus, learn statistical regularities through self-supervision, and reuse those representations for many downstream tasks. Where Chapters 5–7 focused on supervised sequence-to-function CNNs and specialized architectures, and @sec-token focused on representation and tokenization, this chapter turns to genomic foundation models: large, often transformer-based or hybrid architectures trained on unlabeled genomic sequence at scale.

These models aim to provide a single, reusable backbone for tasks ranging from regulatory annotation and variant effect prediction to cross-species transfer and clinical prioritization. They mark the transition from building one model per dataset to constructing general-purpose genomic backbones analogous to BERT, GPT, and ESM in natural and protein language modeling.

::: {.callout-warning .content-visible when-profile="draft"}
**TODO:**
- Flesh-out Evo-2 section
- Add figure: timeline of genomic language model development (DNABERT → Nucleotide Transformer → HyenaDNA → Caduceus → GROVER)
- Add figure: architecture comparison diagram showing transformer vs Hyena vs Mamba approaches
- Add figure: context length evolution visualization showing the dramatic expansion from 512 bp to 1 Mb
- Add visualization: benchmark performance comparison across Nucleotide Transformer tasks
- Add figure: conceptual diagram of in-context learning in genomics (HyenaDNA)
- Add table: comprehensive model comparison with parameters, training data, context length, and key innovations
- Citations: verify all citations are in bibliography
:::

## From Supervised CNNs to Self-Supervised Genomic Language Models

The CNN era represented by DeepSEA, ExPecto, and SpliceAI (Chapters 5–7) shared a common pattern. Models took one-hot encoded DNA sequence around a locus as input, predicted task-specific labels such as chromatin marks, expression levels, or splice junctions, and optimized supervised loss functions against those labels. This approach achieved remarkable performance but suffered from three fundamental constraints.

The first constraint was label dependence. Every new assay, cell type, or phenotype required new labeled data to train a model. A chromatin accessibility model trained on ENCODE data could not predict histone modifications without additional labeled examples for those marks. This created substantial overhead for each new application.

The second constraint was task coupling. Model design became tightly coupled to the specific task. SpliceAI's architecture was specialized for splice junction prediction, with convolutions designed to capture the relevant spatial patterns. ExPecto's spatial feature transformation was engineered specifically for the distance-dependent relationship between regulatory elements and transcription start sites. These architectural choices, while effective for their intended purposes, did not transfer naturally to other problems.

The third constraint was limited reuse. Features learned for one problem did not automatically transfer to others. A model trained to predict chromatin accessibility might learn representations of regulatory motifs, but those representations were not directly accessible for other tasks like variant effect prediction or gene expression modeling without substantial re-engineering.

Protein language models showed a different route: self-supervised learning on unlabeled sequences, with downstream tasks solved by probing or fine-tuning. Genomic language models import this recipe to DNA. The training data comprises large collections of genomic sequences across species, individuals, or functional regions. The training objectives include masked language modeling, where the model predicts masked bases or tokens from surrounding context, and next-token or sequence modeling, where the model predicts the next token in a sequence. Some models combine these self-supervised objectives with auxiliary tasks such as predicting known annotations.

These pretrained models can be used in multiple ways. The simplest approach freezes the model and trains lightweight probes for specific tasks. Fine-tuning updates the entire model or uses adapter modules for specialized downstream applications. Zero-shot or few-shot scoring compares log-likelihoods of alternative sequences or alleles without any task-specific training. The promise is that once a sufficiently powerful backbone is trained, it becomes the default starting point for nearly any DNA-level prediction problem.

::: {.callout-note}
**Figure suggestion:** Side-by-side comparison showing the supervised CNN paradigm (left panel: fixed architecture trained for specific chromatin prediction task) versus the self-supervised language model paradigm (right panel: general backbone pretrained on unlabeled sequence, then adapted to multiple downstream tasks with lightweight heads).
:::

## DNABERT: BERT for K-merized DNA

DNABERT applied the BERT masked language modeling framework to genomic sequences, using overlapping k-mers (typically 6-mers) as tokens and training on human reference sequences [@ji_dnabert_2021]. As discussed in @sec-token, this design had several defining characteristics.

The tokenization scheme converted DNA sequences into overlapping k-mers, creating a discrete vocabulary of size $4^k$. For 6-mers, this yields a vocabulary of 4,096 tokens. The model used the standard BERT architecture with masked token prediction as its training objective. Context windows were relatively modest, spanning a few hundred base pairs (typically 512 tokens). The model was then fine-tuned on downstream tasks including promoter classification, splice site prediction, and transcription factor binding site identification.

DNABERT provided proof of concept for several important ideas. Self-supervised pretraining on raw DNA can improve performance over training from scratch. Learned embeddings capture biologically meaningful regularities, even when trained only on the reference genome. BERT-style architectures can be re-used across multiple downstream tasks with modest fine-tuning.

However, the k-mer design introduced significant limitations detailed in @sec-token. The overlapping k-mer tokenization provided no true sequence compression, as each nucleotide participated in multiple adjacent tokens. This created ambiguity in positional interpretation, since the precise position of a variant within the k-mer vocabulary was unclear. The quadratic attention complexity of transformers combined with redundant overlapping tokens severely limited the effective context length.

## DNABERT-2: Improved Tokenization and Efficiency

DNABERT-2 revisited both tokenization and architecture, demonstrating how much representation choices matter for genomic language models [@zhou_dnabert-2_2024]. The key differences relative to the original DNABERT addressed its core limitations.

The tokenization scheme adopted improved approaches such as BPE-style merges that better compress redundancies and reduce effective sequence length. This allowed the model to represent longer genomic contexts within the same number of tokens. Architectural refinements improved efficiency, enabling scaling to larger contexts and training corpora without prohibitive memory costs.

On standardized benchmarks spanning sequence classification, regulatory element prediction, and variant effect scoring, DNABERT-2 achieved consistent gains over both the original DNABERT and non-pretrained baselines. These improvements validated the importance of thoughtful tokenization design for genomic applications.

The DNABERT family collectively established three important principles. Self-supervision on DNA works and is competitive with hand-engineered pipelines for many sequence annotation tasks. Tokenization choices have large practical consequences, as the seemingly minor decision of how to convert nucleotides into tokens substantially affects both computational efficiency and downstream performance. Masked language model training can produce reusable representations for diverse sequence tasks, suggesting that the foundation model paradigm transfers effectively from natural language to genomic sequence.

## Nucleotide Transformer: Scaling Context and Diversity

DNABERT demonstrated feasibility, but its context windows and training data were modest relative to the scale of genomes. The Nucleotide Transformer pushed much further, emphasizing scale and diversity in both model size and training corpus [@dalla-torre_nucleotide_2023].

The training corpus spanned genomic data from multiple species and populations, exposing the model to diverse sequence patterns. The architecture comprised transformer encoders of various sizes, from moderate to very large parameter counts. Context length expanded to approximately 6 kb per input sequence, representing an order-of-magnitude increase over DNABERT while still using dense attention. The training objective remained masked language modeling on subsequences sampled from genomes.

The Nucleotide Transformer work contributed several important ideas to the field. Cross-species pretraining, where training spans many genomes rather than a single reference, exposes the model to diverse sequence patterns, different regulatory architectures, and evolutionary constraints that recur across lineages. This mirrors the use of large multi-species multiple sequence alignments in protein language models but operates at the raw DNA level.

To quantify representation quality, the Nucleotide Transformer introduced a benchmark panel of genomic tasks that has become a standard yardstick for subsequent DNA language models. Typical tasks include promoter and enhancer classification, histone mark and chromatin accessibility prediction, variant and pathogenicity proxies, and regulatory element type classification. Models are evaluated via linear probes, shallow classifiers, or light fine-tuning.

As with protein and natural-language models, performance improved predictably with larger models, more pretraining data, and longer context windows. These scaling trends help forecast the returns from investing in even larger genomic language models.

::: {.callout-note}
**Table suggestion:** Comparison table showing key model characteristics:

| Model | Architecture | Max Context | Complexity |
|-------|--------------|-------------|------------|
| DNABERT | Transformer | 512 bp | $O(L^2)$ |
| Nucleotide Transformer | Transformer | 6 kb | $O(L^2)$ |
| HyenaDNA | Hyena | 1 Mb | $O(L \log L)$ |
| Caduceus | Mamba | 1 Mb | $O(L)$ |
:::

## GPN: Cross-Species Pretraining for Variant Effect Prediction

While the Nucleotide Transformer demonstrated the value of scaling, the Genomic Pre-trained Network (GPN) explored a complementary direction: what can be learned from cross-species pretraining on relatively small, well-annotated genomes [@benegas_gpn_2023]. Rather than scaling to the largest possible model and training corpus, GPN asked whether the self-supervised paradigm could yield useful variant effect predictors even in a more constrained setting.

GPN was trained on unaligned reference genomes from *Arabidopsis thaliana* and seven related species within the Brassicales order. The training objective was masked language modeling on DNA sequences, predicting masked nucleotides from surrounding context. This is the same fundamental objective used by DNABERT and the Nucleotide Transformer, but applied to a much smaller and more phylogenetically focused corpus.

The key insight from GPN was that self-supervised DNA language models learn biologically meaningful representations without explicit supervision on functional annotations. Analysis of the trained model revealed emergent encoding of gene structure, including exon-intron boundaries and splice sites, as well as DNA sequence motifs associated with transcription factor binding and other regulatory functions. The model discovered these patterns purely from the statistical regularities of genomic sequence across related species.

For variant effect prediction, GPN used a likelihood ratio approach. Given a reference and alternate allele at a position, the model computes the log-likelihood of each under the learned sequence distribution. Variants that substantially reduce sequence likelihood, relative to the reference, are inferred to be more disruptive. This scoring strategy exploits the fact that constrained positions should have confident predictions for the reference allele, while unconstrained positions allow more flexibility.

Evaluated on *A. thaliana* variants using allele frequencies from the 1001 Genomes Project and a comprehensive database of GWAS associations, GPN outperformed traditional conservation scores including phyloP and phastCons. This was notable because phyloP and phastCons are computed from explicit multiple sequence alignments and evolutionary models, while GPN learned its representations from unaligned sequences through self-supervision alone.

GPN established several principles that would influence subsequent work. First, cross-species pretraining captures evolutionary constraints that transfer to variant effect prediction, even without alignment-based conservation calculations. Second, relatively small models trained on focused phylogenetic groups can outperform larger generic conservation measures for species within that group. Third, the masked language modeling objective naturally produces representations suitable for variant scoring via likelihood comparisons.

The limitation of the original GPN was its scope. Training on plant genomes did not directly produce a human variant effect predictor. The later GPN-MSA addressed this gap by incorporating multi-species alignments and training on mammalian genomes, achieving strong performance on human variant benchmarks as discussed in @sec-vep. However, the original GPN remains important as a demonstration that the DNA language model paradigm extends beyond model organisms and can discover biologically meaningful patterns through self-supervision on comparatively modest training data.

## HyenaDNA: Megabase Context at Single-Nucleotide Resolution

Quadratic attention limits transformer context length to tens of kilobases at best, even with aggressive engineering. This is a fundamental architectural constraint: processing a 100 kb sequence with dense attention requires on the order of $10^{10}$ attention computations per layer. HyenaDNA addressed this limitation by replacing attention with a Hyena long-convolution architecture that scales sub-quadratically, enabling processing of sequences up to 1 Mb in length [@nguyen_hyenadna_2023].

The Hyena architecture uses implicit convolutions, parameterizing long convolutional filters through neural networks rather than storing explicit filter weights. This approach achieves $O(L \log L)$ complexity through efficient FFT-based convolution, compared to the $O(L^2)$ complexity of standard attention. The result is a 500-fold increase in context length over previous dense attention models while maintaining single-nucleotide resolution.

HyenaDNA introduced several qualitative advances that matter for biological applications. Processing megabase-scale windows allows the model to see entire gene bodies plus their flanking regulatory regions, long-range enhancer-promoter interactions spanning tens to hundreds of kilobases, and topologically associating domain (TAD) scale structure. This aligns better with biological reality, where regulatory interactions often span substantial genomic distances.

Despite its long context, HyenaDNA maintains base-level resolution by using single-nucleotide tokens. This means single-nucleotide variants can be evaluated in the context of megabases of surrounding sequence without the ambiguity introduced by k-mer tokenization.

On Nucleotide Transformer benchmarks and additional tasks, HyenaDNA demonstrated in-context learning behaviors that had not previously been observed in genomic models. Performance improved when examples were included in the input context without updating model weights, suggesting that at sufficient scale, DNA models can adapt to new tasks or distributions via prompts rather than fine-tuning. This mirrors phenomena observed in large natural language models.

On GenomicBenchmarks and related evaluations, HyenaDNA achieved state-of-the-art results on the majority of tasks, often by substantial margins. These results illustrated that architectural innovations enabling longer context can simultaneously provide both extended range and improved predictive accuracy.

::: {.callout-note}
**Figure suggestion:** Visualization showing the dramatic expansion of context length across genomic language models, with representative regulatory phenomena (TF binding sites, enhancer-promoter loops, TAD boundaries) marked at their characteristic length scales. Models plotted chronologically from DNABERT (512 bp) through Nucleotide Transformer (6 kb) to HyenaDNA (1 Mb).
:::

## Caduceus: Bidirectional Modeling with Reverse-Complement Equivariance

A unique challenge in genomic sequence modeling is the double-stranded nature of DNA. Any sequence can be read from either strand, and the reverse complement of a sequence encodes the same information from the opposite strand's perspective. For many biological processes, predictions should be identical or related in a consistent way regardless of which strand is presented to the model.

Standard neural networks can produce divergent predictions for a sequence and its reverse complement, even when training data is augmented with both orientations. This inconsistency is problematic for applications like regulatory element prediction, where the functional element exists on one physical stretch of DNA regardless of how we choose to represent it computationally.

Caduceus addressed this challenge by building reverse-complement equivariance directly into the architecture [@schiff_caduceus_2024]. The model extends the Mamba architecture, a state-space model with linear complexity in sequence length, to support both bidirectionality and reverse-complement equivariance. The BiMamba component enables information flow in both directions along the sequence, while the MambaDNA block ensures that predictions for a sequence and its reverse complement are mathematically related in the expected way.

The architectural innovations in Caduceus serve distinct purposes. Bidirectionality allows each position to incorporate information from both upstream and downstream context, which matters for tasks where the relevant context is not directionally asymmetric. Reverse-complement equivariance ensures consistent predictions across strand orientations, reducing spurious variability and improving calibration.

On downstream benchmarks, Caduceus outperformed previous long-range models. On challenging long-range variant effect prediction tasks, Caduceus exceeded the performance of models with ten times as many parameters that did not leverage bidirectionality or equivariance. This suggests that incorporating appropriate biological inductive biases can be as valuable as scaling model size.

## GROVER: Generative Regulatory Foundation Models

Most genomic language models focus on modeling raw DNA sequence. GROVER takes a complementary approach, shifting attention from sequence to regulatory tracks [@sanabria_grover_2024]. Rather than treating DNA as the primary input, GROVER is trained on multi-track functional genomics signals including ATAC-seq, histone modifications, and other epigenomic assays across many cell types and tissues.

The training objective predicts masked or held-out regulatory profiles conditioned on neighboring tracks, cell-type embeddings, or limited sequence context. The architecture uses a transformer-style backbone tailored to spatiotemporal grids of genomic positions crossed with assays and cell types.

GROVER occupies a role analogous to self-supervised vision models for images. It treats regulatory profiles as a high-dimensional signal over the genome and learns rich representations of regulatory states at each position. This supports tasks like imputation of missing assays, denoising of noisy experimental data, and cell-type-specific activity prediction.

While not a pure DNA language model, GROVER-style systems complement sequence-based models in important ways. DNA language models capture what the genome can do, encoding the potential regulatory activities specified by the sequence. Regulatory foundation models like GROVER capture what the genome is actually doing in specific contexts, representing the realized regulatory state in particular cell types and conditions. Later chapters explore how sequence-based and regulatory foundation models can be combined, using DNA language models to parameterize sequence priors and regulatory models for context-specific readouts.

## GenSLMs: Codon Tokenization and Whole-Genome Modeling

The DNA language models discussed thus far, including DNABERT, Nucleotide Transformer, HyenaDNA, and Caduceus, were designed primarily for human and multi-species genomics applications such as regulatory prediction and variant effect estimation. GenSLMs (Genome-scale Language Models) represents a distinct design point that targets pathogen surveillance and viral evolution rather than human regulatory genomics [@zvyagin_genslms_2022]. The model illustrates how different biological applications motivate different architectural and tokenization choices, even within the broader family of genomic foundation models.

GenSLMs operates at the codon level rather than the nucleotide level. Where most DNA language models tokenize sequences into nucleotides, k-mers, or BPE-derived subwords, GenSLMs treats each three-nucleotide codon as a single token. This choice reflects an explicit alignment with the central dogma of molecular biology: codons are the fundamental units of translation from nucleic acid to protein, and mutations at the codon level determine amino acid changes that drive phenotypic evolution. The approach yields a vocabulary of 64 codon tokens (plus special tokens for noncoding regions and frame shifts), intermediate in size between character-level tokenization (4 tokens) and typical BPE vocabularies (4,096 to 32,000 tokens).

The codon-level tokenization enables GenSLMs to model entire viral genomes as sequences of manageable length. A SARS-CoV-2 genome spans approximately 30,000 nucleotides, which translates to roughly 10,000 codons. This places whole-genome modeling within reach of standard transformer context windows, avoiding the architectural innovations required to handle megabase-scale human sequences. The approach trades single-nucleotide resolution for the ability to capture genome-wide patterns and cross-protein dependencies that would be difficult to model at nucleotide resolution.

The training strategy follows a two-stage paradigm. The foundation model was pretrained on over 110 million prokaryotic gene sequences from the BV-BRC database, exposing the model to broad codon usage patterns and gene-level structure across bacterial diversity. This pretraining corpus provided the model with general understanding of coding sequence organization without being specific to any particular pathogen. The model was then fine-tuned on 1.5 million SARS-CoV-2 genome sequences to learn the specific evolutionary landscape of the virus.

Remarkably, a GenSLM model trained only on sequences from the first year of the pandemic, before Delta or Omicron variants emerged, could subsequently distinguish between variants of concern in its learned embedding space. When new variant sequences were projected into the model's latent space, they clustered according to lineage identity despite the model never having observed these variants during training. This generalization suggests that the model learned structural features of SARS-CoV-2 evolution that transferred to novel variants, providing a foundation for real-time surveillance applications.

GenSLMs also revealed interpretable attention patterns across the viral genome. Cross-protein attention showed coupling between the Spike protein and nonstructural proteins (nsp3, nsp5) that differed between Delta and Omicron lineages. These patterns suggest that the model captured biologically meaningful co-evolutionary relationships rather than arbitrary sequence statistics, though the mechanistic interpretation of such patterns remains an area for further investigation.

From a computational perspective, GenSLMs demonstrated that genomic foundation models can be trained at unprecedented scale. The project achieved 1.63 Zettaflops of total computation across training runs, with sustained performance of 121 PFLOPS in mixed precision on GPU-based supercomputers. Training on specialized AI hardware accelerators (Cerebras CS-2 clusters) reduced convergence time from over a week to less than a day. This work received the 2022 Gordon Bell Special Prize for High Performance Computing-Based COVID-19 Research, establishing a benchmark for large-scale genomic model training.

The GenSLMs approach highlights several broader lessons for the field. First, tokenization should be aligned with biological semantics when possible. Codon-level tokenization makes sense for coding sequences where the codon-to-amino-acid mapping is the relevant biological transformation. Second, pretraining corpora can be chosen strategically to provide useful inductive biases. Prokaryotic gene pretraining exposed the model to diverse codon usage and gene organization before specializing to a particular virus. Third, different applications motivate different scales. Pathogen surveillance requires whole-genome context but benefits from relatively compact genomes, while human regulatory genomics requires kilobase to megabase context around specific loci.

The model's application to emerging pathogen detection illustrates a use case distinct from the variant effect prediction focus of most human-centric genomic foundation models. Rather than scoring individual mutations for pathogenicity, GenSLMs aims to characterize entire genomes and identify novel variants that may represent public health concerns. This surveillance application places different demands on the model: speed of inference across many sequences, robust generalization to novel variants, and interpretable representations that can guide downstream analysis.

## Evo 2: Genome-Scale Language Modeling Across All Life

::: {.callout-warning .content-visible when-profile="draft"}
**TODO:** Flesh out this section with more detail on architecture, training corpus composition, benchmark performance, and generative capabilities. Add discussion of zero-shot variant scoring methodology and cross-species transfer results.
:::

Evo 2 represents the next frontier in genomic foundation models: training at truly genome-scale across the full diversity of life. While previous models either focused on specific organisms (DNABERT on human, GPN on plants, GenSLMs on viruses) or trained on multi-species corpora at more limited scale (Nucleotide Transformer), Evo 2 aims to learn universal genomic patterns that span bacteria, archaea, eukaryotes, and phages.

The training corpus for Evo 2 draws from the OpenGenome2 dataset, comprising 9.3 trillion DNA tokens across all domains of life. This massive scale exposes the model to the full spectrum of genomic organization, from compact prokaryotic gene arrangements to sprawling eukaryotic regulatory landscapes with extensive noncoding sequence. The model comes in two sizes: a 7 billion parameter variant suitable for most applications and a 40 billion parameter version for maximum capacity.

The architecture builds on StripedHyena 2, a hybrid design that combines convolutional operations with selective attention mechanisms. This enables the model to process sequences up to 1 million tokens (nucleotides) in length while maintaining computational tractability. The autoregressive training objective, predicting the next base given all previous bases, differs from the masked language modeling used in DNABERT and related models, potentially providing complementary strengths for sequence generation and likelihood-based scoring.

Evo 2 exhibits several forms of emergent biological knowledge despite training only on raw sequence. The model learns to identify exon-intron boundaries without explicit annotation, discovers transcription factor binding site patterns that match known motifs, captures aspects of protein secondary and tertiary structure when processing coding sequences, and even identifies prophage insertion regions in bacterial genomes. These capabilities emerge from pure sequence statistics, demonstrating that genome-scale pretraining captures fundamental biological organization.

For variant effect prediction, Evo 2 enables zero-shot scoring via likelihood ratios. By comparing the model's probability for sequences containing reference versus alternative alleles, variants can be scored for their consistency with learned genomic patterns. On benchmarks of pathogenic versus benign variants, Evo 2's zero-shot scores achieve competitive performance with specialized supervised methods, though careful calibration remains necessary before clinical application. The model also supports classification of variants of uncertain significance in genes like BRCA1 through simple classifiers trained on its embeddings.

The pan-species training enables several cross-species applications. Variant interpretation extends naturally to non-model organisms, supporting conservation genomics and agricultural breeding programs where labeled training data is scarce. The model's learned representations cluster sequences by phylogenetic relationships even without explicit evolutionary modeling. This makes Evo 2 particularly valuable for applications in livestock genomics, crop improvement, and wildlife conservation where human-trained models provide limited guidance.

Beyond discriminative tasks, Evo 2 demonstrates generative capabilities. The model can synthesize plausible mitochondrial genomes, prokaryotic operons, and eukaryotic regulatory regions with coherence across kilobase to megabase scales. Inference-time search procedures enable controllable generation with specified properties, such as desired GC content or regulatory motif composition. Recent work has shown that inference-time scaling, where additional computation at test time improves generation quality, applies to genomic generation tasks including prediction of epigenomic structure.

The trade-off inherent in Evo 2's design is generality versus specialization. As a generalist model spanning all life, it may be outperformed by human-specific models like AlphaMissense or tissue-specific models on narrow benchmarks. However, its breadth enables applications beyond the scope of specialized models, particularly in organisms and genomic contexts where training data is limited. The model represents a bet that the shared principles of genome organization across life provide sufficient signal for a single foundation to support diverse applications.

::: {.callout-note}
**Figure suggestion:** Multi-panel figure showing (A) training corpus composition across the tree of life, (B) StripedHyena 2 architecture schematic highlighting hybrid attention-convolution blocks, (C) t-SNE projection of sequence embeddings colored by taxonomic group, and (D) zero-shot variant effect scores compared to experimental pathogenicity labels.
:::

## Central-Dogma-Aware and Annotation-Enriched Models

The tokenization discussion in @sec-token described how biological structure can be encoded directly into the input representation. Recent models push this idea further by integrating central dogma knowledge and genomic annotations into the modeling framework itself.

### Life-Code: The Central Dogma as Inductive Bias

Life-Code proposes codon-aware, central-dogma-informed tokenization to bridge DNA, RNA, and protein within a single language-modeling framework [@liu_life-code_2025]. The key insight is that different genomic regions should be tokenized differently based on their biological function.

Coding regions are tokenized as codons, the three-nucleotide units that specify amino acids during translation. This respects the genetic code's fundamental structure and enables the model to learn patterns at the level of the biological unit of selection for protein-coding sequences. Noncoding regions, which lack this inherent three-nucleotide structure, are tokenized via learned subword units optimized during training. The resulting unified representations span DNA, RNA, and protein, enabling knowledge sharing across modalities.

Life-Code uses knowledge distillation from protein language models to import protein-level structural knowledge into DNA and RNA sequence representations. This improves performance on tasks involving coding sequence, such as predicting the effects of missense mutations or expression changes, and achieves competitive or state-of-the-art results on tasks across all three omic modalities.

### BioToken: Encoding Variants and Structure

BioToken extends tokenization beyond nucleotide content to include explicit genomic annotations [@medvedev_biotoken_2025]. Rather than representing a genomic region purely as a string of nucleotides, BioToken creates tokens that encode additional biological context.

Variant-aware tokens explicitly represent SNPs, insertions, and deletions as distinct tokens rather than as implicit changes in the underlying sequence. Structural annotations encode information about exons, introns, UTRs, promoters, enhancers, and other regulatory elements. Functional context tokens include signals such as conservation scores, chromatin state, or known regulatory motifs.

This design moves toward fully structured genomic language models where the input is not only DNA bases but also position-specific metadata. The resulting representations can directly integrate sequence, structure, and functional annotations in a unified framework.

The associated model BioFM, built on BioToken, achieves competitive or superior results relative to specialized models like Enformer and SpliceAI across genomic benchmarks including noncoding pathogenicity prediction, expression modulation, sQTL prediction, and long-range genomic interactions. Notably, BioFM achieves state-of-the-art performance with significantly fewer parameters (265M), substantially reducing training costs and computational requirements compared to larger models.

Life-Code and BioToken foreshadow the multi-modal, multi-omic foundation models discussed in Part IV, where sequence is only one of many integrated information streams.

::: {.callout-note}
**Figure suggestion:** Comparative diagram showing three tokenization approaches for the same genomic locus: (top) standard nucleotide tokenization, (middle) Life-Code with region-specific codon and subword tokens, (bottom) BioToken composite tokens encoding sequence, variants, and functional annotations.
:::

## Using Genomic Language Models in Practice

Genomic language models support multiple usage patterns analogous to those established for protein language models. Understanding these patterns is essential for applying the models effectively.

### Embeddings as Universal Features

The simplest approach extracts embeddings from a pretrained model and uses them as features for downstream tasks. The workflow involves several steps: extract embeddings for windows around loci of interest, pool or select positions relevant to the task (such as promoters, candidate enhancers, or variant sites), and train a lightweight downstream model such as a linear layer, small MLP, or logistic regression.

This approach supports diverse applications. Regulatory element classification can distinguish promoters, enhancers, silencers, and insulators based on their learned representations. Chromatin state prediction uses sequence embeddings to predict ATAC-seq or histone mark presence as an alternative to supervised models like DeepSEA. Variant effect scoring replaces or augments hand-crafted features in frameworks like CADD with language model derived features, analogous to CADD v1.7's incorporation of protein language model features. Splicing and transcript modeling combines language model embeddings with specialized architectures like SpliceAI.

Because the language model remains frozen in this approach, it is computationally efficient and avoids catastrophic forgetting when new tasks are added. The pretrained model serves as a general-purpose feature extractor whose representations support many downstream applications.

### Fine-Tuning and Task-Specific Heads

When more labeled data is available, fine-tuning can significantly improve performance beyond what frozen embeddings provide. Full fine-tuning updates all language model parameters for a specific task, allowing the model to specialize its representations. Adapter-based tuning inserts small bottleneck modules into each layer and updates only those, keeping the backbone mostly frozen while still allowing task-specific adaptation.

Full fine-tuning tends to achieve the highest performance when sufficient labeled data is available, but it requires more compute and risks catastrophic forgetting of general knowledge. Adapter-based approaches provide a middle ground, achieving most of the performance gains while maintaining computational efficiency and preserving the backbone's general capabilities.

### Zero-Shot and Few-Shot Scoring

For variant interpretation, genomic language models enable zero-shot scoring based on sequence likelihood. The approach computes the model's probability for a sequence containing the reference allele and compares it to the probability for the sequence containing the alternative allele. Variants that substantially reduce sequence probability are inferred to be more disruptive.

This approach requires no variant-specific training data and can score any single-nucleotide variant in any genomic context the model has learned to represent. The quality of zero-shot scoring depends on how well the model's learned probability distribution captures biological constraints, which tends to improve with model scale and training data diversity.

Few-shot approaches include task examples in the input context, allowing the model to adapt its behavior based on demonstrations without parameter updates. HyenaDNA demonstrated that genomic models at sufficient scale exhibit this in-context learning capability, opening new possibilities for rapid task adaptation.

## Emerging Themes and Current Limitations

The development of genomic language models over the past several years has established several important themes while also revealing significant limitations.

Self-supervision provides a viable path to general genomic representations. Models trained purely on the statistical structure of DNA sequence, without any functional labels, learn representations that transfer to diverse downstream tasks. This validates the foundation model paradigm for genomics and suggests continued scaling will yield further improvements.

Scale and diversity matter substantially for model quality. Performance improves predictably with model size, training data volume, and training data diversity. Including multiple species, populations, and genomic contexts yields more robust representations than training on a single reference genome.

Long-range context is biologically necessary for many applications. Regulatory phenomena operate at tens to hundreds of kilobases, and the development of efficient architectures like HyenaDNA and Caduceus finally allows modeling these interactions at single-base resolution. The progression from 512 bp to 1 Mb context lengths represents a fundamental capability improvement.

Biological inductive biases can substitute for scale in some applications. Reverse-complement equivariance in Caduceus, central dogma awareness in Life-Code, and variant encoding in BioToken all demonstrate that incorporating domain knowledge into model architecture improves data efficiency and often matches or exceeds the performance of larger generic models.

Self-supervision and supervision are complementary rather than competing approaches. Self-supervised language models excel at learning broad, reusable features, but they do not automatically solve every downstream problem. Specialized architectures and supervised objectives, such as Enformer and related models discussed in @sec-hybrid, remain crucial for accurate quantitative prediction of complex genomic readouts.

Several important limitations remain. Current models struggle with complex variant patterns beyond single-nucleotide changes, including indels, structural variants, and epistatic interactions across distant loci. Training data and labels remain skewed toward certain ancestries, raising concerns about performance and calibration in underrepresented populations. Interpretability is limited, as it remains difficult to explain why a model assigns a particular score to a variant in terms that connect to biological mechanism. Integration with other data modalities (chromatin, expression, 3D genome structure, clinical phenotypes) is still in its early stages.

The evaluation of these models also presents challenges. Benchmarks often focus on classification tasks with clear labels (promoter versus nonpromoter, pathogenic versus benign), but many real applications involve continuous regulatory effects or context-dependent outcomes that are harder to assess. The field would benefit from standardized evaluation protocols that better capture the diversity of genomic applications and the nuances of regulatory prediction. These evaluation challenges are explored in depth in @sec-benchmarks and @sec-eval.

## Summary

This chapter surveyed the landscape of genomic language models, from early proof-of-concept systems like DNABERT through scaled models like Nucleotide Transformer to architectural innovations enabling megabase context in HyenaDNA and Caduceus. We examined how models like GROVER complement sequence-based approaches by learning from regulatory tracks, how GenSLMs demonstrated domain-specific tokenization for viral surveillance, and how Evo 2 pushed toward universal genome-scale modeling across all life. Finally, we explored annotation-enriched architectures like Life-Code and BioToken that incorporate biological structure directly into the modeling framework.

The key lessons are that self-supervised pretraining transfers effectively to genomics, that architectural choices enabling long-range context provide both efficiency and accuracy improvements, and that biological inductive biases (reverse-complement equivariance, central dogma awareness, variant encoding) can substitute for raw scale in some applications. These models establish a foundation for the field but leave important gaps in our ability to predict quantitative molecular readouts, integrate multi-omic context, and provide mechanistic explanations of their predictions.

In @sec-hybrid, we turn to Enformer and related long-range sequence-to-function models that explicitly predict molecular readouts from sequence. These models close the loop between self-supervised sequence understanding and supervised functional prediction, addressing a key limitation of pure language models: their indirect relationship to quantitative molecular phenotypes.