# Sequence Representation and Tokenization {#sec-representations}

## From Sequence to Model: The Representation Problem

Genomic foundation models can process context windows spanning hundreds of thousands of nucleotides, yet a single-nucleotide variant at a splice junction can render all that context meaningless if the tokenization scheme obscures precisely where the mutation falls. Three billion nucleotides encode the human genome; a clinician needs to know whether one of them causes disease. This asymmetry between scale and precision defines the central challenge of genomic sequence representation. Before a model learns any parameters, representation choices have already determined what biological phenomena it can detect, what resolution it can achieve, and ultimately which clinical questions it can answer.

The clinical stakes emerge immediately when considering a splice-disrupting variant in *BRCA1*. The canonical splice acceptor consists of just two nucleotides (the AG dinucleotide marking exon boundaries), and a single substitution at this site can cause exon skipping, producing a truncated protein that loses tumor suppression function. If a tokenization scheme merges those two nucleotides with surrounding sequence into a coarse multi-nucleotide token, the model cannot distinguish pathogenic splice mutations from benign synonymous changes nearby. The variant's precise position relative to token boundaries becomes invisible. Conversely, a regulatory variant's effect might depend on an enhancer fifty kilobases upstream; if the representation limits context to a few hundred nucleotides, that distal information never reaches the model. **A model cannot learn patterns it cannot see, and representation choices determine visibility.**

An analogy to natural language processing illuminates the fundamental trade-offs. Training a language model on English text requires deciding how to segment the continuous character stream into discrete tokens. Character-level tokenization preserves maximum resolution but creates very long sequences. Word-level tokenization compresses the sequence but potentially loses information about morphology and subword structure. Learned subword vocabularies balance these concerns by letting corpus statistics guide segmentation. Each choice affects what patterns the model can discover and how efficiently it can process long documents.

DNA presents similar choices but with critical differences. The genome has only four letters rather than dozens, no natural word boundaries, and biological structure operating at multiple scales simultaneously. A transcription factor binding site might span 6 to 12 nucleotides, but the regulatory grammar linking multiple binding sites can extend over hundreds of base pairs. Coding sequences follow a strict three-nucleotide codon structure; noncoding regions have no such constraint. Any representation scheme must navigate these biological realities while remaining computationally tractable for sequences that dwarf typical language model inputs by orders of magnitude.

The tension between resolution and computational tractability forces uncomfortable choices. Single-nucleotide resolution is essential for variant interpretation, where changing one base pair can cause disease. Long context is essential for regulatory modeling, where distal elements control gene expression across tens of kilobases. Standard transformer attention scales quadratically with sequence length, creating a direct conflict: preserve resolution and sacrifice context, or compress sequence and lose the precision needed for clinical applications. Understanding this tension clarifies design decisions across every model examined in subsequent chapters.

## One-Hot Encoding: The CNN Foundation

A child inherits a *DMD* variant from her mother. Whether this variant causes Duchenne muscular dystrophy or remains clinically silent depends on its exact position relative to the exon-intron boundary: one nucleotide can determine whether the splicing machinery recognizes the junction. This is why single-nucleotide resolution is not a technical nicety but a clinical necessity. The earliest deep learning approaches to genomic sequence modeling recognized this requirement and adopted the simplest representation capable of preserving it: one-hot encoding, where each nucleotide becomes a sparse binary vector with a single active element indicating its identity. Adenine is encoded as [1, 0, 0, 0], cytosine as [0, 1, 0, 0], guanine as [0, 0, 1, 0], and thymine as [0, 0, 0, 1]. A sequence of length $L$ thus becomes a matrix of dimensions $4 \times L$, interpretable as four channels analogous to the RGB channels of an image plus one.

The properties that made one-hot encoding dominant in the CNN era stem from this simple design. The representation is lossless, preserving every nucleotide explicitly without information compression. It maintains single-nucleotide resolution, enabling detection of effects from individual SNPs. The encoding exhibits translation equivariance, meaning convolutional filters learn position-invariant motifs recognizable anywhere in the sequence. And it requires no preprocessing, vocabulary construction, or tokenizer training, making implementation straightforward. *DeepSEA*, *ExPecto*, and *SpliceAI* all employed one-hot encoding without modification, with convolutional layers learning to detect sequence patterns directly from the binary representation.

The key insight underlying CNN success with one-hot encoding is that convolutions process sequences through local operations. Each filter examines only a small window of positions at a time, and the sparse, orthogonal nature of one-hot vectors poses no obstacle to this local processing. First-layer filters effectively learn position weight matrices that score short k-mer patterns, while deeper layers capture combinations and spatial arrangements of these primitive motifs. The representation worked because it aligned with the architectural inductive bias of convolutions: local pattern detection does not require global sequence compression.

For transformer architectures, one-hot encoding creates a fundamental mismatch. Transformers compute attention between all pairs of positions in a sequence, with computational cost scaling as $O(L^2)$ where $L$ is sequence length. A 10 kb sequence requires 10,000 tokens, demanding 100 million pairwise attention computations per layer. This quickly becomes prohibitive for the long sequences genomic applications require. The problem compounds because transformers typically learn dense embeddings for each token, but with only four possible nucleotides, the embedding layer has minimal opportunity for rich representation learning. The sparse one-hot vectors provide too little structure for the embedding to transform meaningfully.

This mismatch produces severe context limitation. Transformer context windows of 512 to 4,096 tokens translate to only 512 to 4,096 base pairs when using one-hot encoding, representing a tiny fraction of genes or regulatory regions. Compare this to *Enformer*'s 200 kb receptive field or *SpliceAI*'s 10 kb context, both achieved through architectural innovations operating on one-hot encoded sequence. **For transformer-based foundation models, one-hot encoding forces an impossible choice between the long contexts needed for regulatory modeling and computational tractability.** This tension motivated the search for alternative representations that could compress genomic sequences into fewer tokens while preserving information needed for biological prediction.

## K-mer Tokenization: The DNABERT Approach

The computational constraints of one-hot encoding for transformers led researchers to explore sequence compression through k-mer tokenization. This approach treats overlapping subsequences of length $k$ as tokens, drawing an analogy between k-mers and words in natural language. Just as sentences compose words carrying meaning through sequence and combination, genomic sequences might be understood as k-mer "words" encoding biological function through their arrangement. *DNABERT* pioneered this approach for genomic transformers in 2021, using 6-mers as tokens and training a BERT-style masked language model on human reference sequences [@ji_dnabert_2021].

The k-mer vocabulary has a fixed size of $4^k$ possible tokens. For 6-mers, this yields 4,096 distinct tokens, comparable to vocabulary sizes in some natural language models. Each token represents six consecutive nucleotides, creating direct correspondence between subsequence and token identity. *DNABERT* used overlapping k-mers: for a sequence like ACGTACGT, successive 6-mer tokens share five nucleotides with their neighbors. The sequence position advances by one nucleotide at a time, generating one token per position (minus the $k-1$ positions at the sequence end where a complete k-mer cannot form).

*DNABERT* provided valuable proof of concept for genomic transformers. It demonstrated that self-supervised pretraining on raw DNA sequences could improve performance over training from scratch, that learned embeddings could capture biologically meaningful regularities even when trained only on the reference genome, and that BERT-style architectures could transfer across multiple downstream tasks. *DNABERT* achieved strong performance on promoter prediction, splice site identification, and transcription factor binding site recognition after fine-tuning with relatively small amounts of task-specific labeled data.

Subsequent analysis revealed fundamental limitations rooted in the overlapping design. *DNABERT-2* articulated these problems clearly in 2024 [@zhou_dnabert-2_2024]. Overlapping k-mers provide no sequence compression: the number of tokens equals the number of nucleotides (minus a small constant), so context window limitations persist unchanged. A 10 kb sequence still requires approximately 10,000 tokens, and the quadratic attention complexity remains prohibitive for long sequences. **The very design that seemed to add biological meaning through k-mer structure failed to address the computational bottleneck motivating the approach.**

The overlapping design creates additional complications beyond computational cost. A single nucleotide contributes to $k$ different tokens (each k-mer containing that position), complicating interpretation of which token drives any given prediction. This ambiguity becomes particularly problematic for variant effect interpretation, where understanding how a specific nucleotide change alters model predictions is essential. The effect of a single substitution propagates through $k$ different tokens in ways that can be difficult to disentangle. The model must also learn that overlapping tokens share nucleotides, a relationship obvious from the tokenization scheme but requiring discovery through training. This redundancy consumes model capacity that could otherwise capture more complex biological patterns. The fixed $4^k$ vocabulary does not adapt to corpus statistics; frequent and rare k-mers receive equal representation capacity in the embedding table despite potentially differing importance for prediction.

## Byte Pair Encoding: Learning the Vocabulary

The limitations of k-mer tokenization raise a question: what if the vocabulary itself could be learned from data? Byte Pair Encoding addresses this by constructing vocabulary through iterative discovery of frequent subsequences rather than defining tokens through a fixed rule. The algorithm, originally developed for data compression, builds vocabulary through a simple procedure. BPE initializes the vocabulary with single nucleotides: {A, C, G, T}. It then scans the training corpus to count all adjacent token pairs, identifies the most frequent pair, merges this pair into a new token added to the vocabulary, and replaces all instances in the corpus with the merged token. The process repeats through many iterations (typically thousands), building a vocabulary of variable-length tokens capturing frequently occurring sequence patterns.

The critical difference from k-mer tokenization is that BPE produces genuine sequence compression through non-overlapping tokens. Unlike overlapping k-mers where each nucleotide generates its own token, BPE creates tokens spanning multiple nucleotides without overlap. A 10 kb sequence might compress to 2,000 or 3,000 tokens depending on its repetitive structure, enabling transformers to process substantially longer sequences within the same context window.

*DNABERT-2* replaced 6-mer tokenization with BPE and demonstrated dramatic improvements [@zhou_dnabert-2_2024]. The new model achieved comparable performance to state-of-the-art approaches while using 21 times fewer parameters and requiring approximately 92 times less GPU time in pretraining. These efficiency gains stem directly from non-overlapping tokenization: actual sequence compression enables processing longer sequences with the same computational budget, and eliminating overlapping token redundancy allows the model to focus capacity on learning biological patterns rather than token relationships.

The BPE vocabulary learns corpus statistics through its construction process. Repetitive elements appearing frequently throughout the genome (such as Alu sequences or common regulatory motifs) receive dedicated tokens spanning many nucleotides. These long tokens enable efficient representation of repetitive regions while preserving single-nucleotide resolution for unique sequences. Rare sequences that BPE never encountered during vocabulary construction are represented as concatenations of shorter subunits, maintaining the ability to encode any sequence while allocating more representation capacity to common patterns.

*GROVER* (Genome Rules Obtained Via Extracted Representations) extended this approach by training BPE specifically on the human genome and selecting vocabulary using a custom next-k-mer prediction task [@sanabria_grover_2024]. Analysis of the resulting token embeddings revealed that the learned vocabulary encodes biologically meaningful structure without explicit supervision. Common tokens cluster separately from rare ones in embedding space. GC-rich tokens segregate from AT-rich tokens, reflecting the different properties of these sequence compositions. Token length correlates with specific embedding dimensions, allowing the model to represent both the content and extent of each token. Some tokens appear primarily in repetitive regions while others distribute broadly across the genome, and this localization pattern emerges in the learned representations.

BPE introduces complications of its own that matter for clinical applications. Variable-length tokens mean that variant positions fall at different locations relative to token boundaries depending on local sequence context. A SNP might fall in the middle of a long token in one sequence context but at a token boundary in another, potentially affecting how the model represents and processes the variant. **The same nucleotide change may alter different numbers of tokens depending on surrounding sequence, creating inconsistent input representations for what should be comparable biological events.** The trade-off between compression and interpretability becomes a design choice depending on intended application.

## Single-Nucleotide Tokenization: Maximum Resolution

While k-mer and BPE tokenization compress sequences to enable longer context windows, they sacrifice the single-nucleotide resolution essential for variant effect prediction. A single nucleotide polymorphism can completely alter protein function through mechanisms ranging from amino acid substitution to splice site disruption to regulatory element ablation. When a pathogenic variant and a benign variant differ by one nucleotide position, multi-nucleotide tokens obscure exactly where variants fall and how they relate to the boundaries of biological features.

*HyenaDNA* took the opposite approach in 2023, using single-nucleotide tokens with no compression whatsoever [@nguyen_hyenadna_2023]. Each nucleotide (A, C, G, T) becomes a separate token, maintaining maximum possible resolution. Every nucleotide is independently represented, SNP effects can be isolated to specific token positions without ambiguity, and no tokenization artifacts depend on surrounding sequence context.

The challenge is sequence length. A 1 Mb region requires 1 million tokens, far beyond the capacity of any standard transformer. The quadratic attention complexity would demand a trillion pairwise computations per layer, rendering the approach computationally infeasible with conventional architectures.

*HyenaDNA* addressed this challenge through architectural innovation rather than tokenization compromise. The Hyena architecture replaces the attention mechanism with implicit convolutions (long convolutions parameterized by a small neural network) that scale sub-quadratically with sequence length. Where attention computes explicit pairwise interactions between all positions, Hyena achieves similar representational power with $O(L \log L)$ complexity rather than $O(L^2)$. This enables processing sequences hundreds of times longer than attention-based transformers within the same computational budget.

The practical impact was substantial: a 500-fold increase in context length over dense attention models while maintaining single-nucleotide resolution. *HyenaDNA* could process 1 Mb sequences where *DNABERT* was limited to approximately 500 bp and the *Nucleotide Transformer* to approximately 6 kb. On the Nucleotide Transformer benchmarks, *HyenaDNA* reached state-of-the-art performance on 12 of 18 datasets with orders of magnitude fewer parameters and less pretraining data. On GenomicBenchmarks, it surpassed prior state-of-the-art on 7 of 8 datasets by an average of 10 accuracy points.

*HyenaDNA* also demonstrated the first use of in-context learning in genomics. The model could perform tasks based on examples provided in the context window without any fine-tuning (conditioning on demonstration sequences rather than updating parameters). This capability, familiar from large language models, had not previously been shown for genomic sequences and suggests that very long context combined with high resolution enables qualitatively new forms of biological reasoning.

The development of sub-quadratic architectures including Hyena, Mamba, and state space models has fundamentally changed the tokenization calculus. **When computational constraints no longer force a choice between resolution and context length, single-nucleotide tokenization becomes the natural choice for applications requiring precise variant interpretation.** The architectural innovations examined in @sec-attention effectively decouple the resolution decision from the context length decision, eliminating what had seemed like an inherent trade-off.

## Biologically-Informed Tokenization

Standard tokenization schemes treat DNA as a homogeneous string of characters, ignoring the biological reality that different genomic regions serve fundamentally different functions and follow different structural rules. Coding sequences obey a strict codon structure where every three nucleotides encode an amino acid; noncoding regions have no such constraint. Treating these regions identically wastes an opportunity to build biological knowledge directly into the representation.

For protein-coding regions, the natural unit of sequence is the codon rather than the individual nucleotide. *GenSLMs* pioneered codon-level tokenization for genomic foundation models in 2022, treating each three-nucleotide codon as a single token and exploiting the fact that codons are the biologically meaningful units of protein-coding sequence [@zvyagin_genslms_2022]. The 64-codon vocabulary captures the complete space of possible genetic code words, with each token corresponding to either an amino acid or a stop signal. This alignment with translation semantics means that mutations affecting amino acid identity (nonsynonymous changes) alter the token sequence, while synonymous mutations within a codon alter the specific token used but maintain broader codon-family structure.

*Life-Code* extended codon-aware tokenization to broader genomic contexts in 2025, encoding coding and noncoding regions in a way that respects reading frame and local biological function [@liu_life-code_2025]. Coding regions are tokenized by codons, aligning token boundaries with the fundamental unit of protein translation. Noncoding regions, lacking codon structure, are tokenized by learned patterns capturing regulatory motifs and other functional elements. This biologically-informed design enables *Life-Code* to learn protein structure through knowledge distillation from protein language models, capture interactions between coding and noncoding regions within a unified framework, and achieve state-of-the-art results across tasks involving DNA, RNA, and protein.

*BioToken* extends tokenization further to include explicit genomic structural annotations [@medvedev_biotoken_2025]. Rather than treating variants as implicit changes in the sequence string, *BioToken* creates tokens explicitly representing SNPs, insertions, and deletions. Known regulatory elements receive dedicated tokens encoding their presence and type. Gene structure, chromatin state, and other functional annotations integrate directly into the token representation. This approach treats tokens as rich entities bundling nucleotides with positional, functional, or experimental context.

Variant-aware representations hold particular promise for clinical applications, where the input is often "reference plus variant" rather than a generic sequence. By incorporating biological inductive biases directly into tokenization, *BioToken*'s associated model achieves competitive or superior performance to specialized models like *Enformer* and *SpliceAI* with significantly fewer parameters. **This efficiency suggests that appropriate representation can partially substitute for model scale by making the learning problem easier through informed structure.**

The broader principle is that tokenization can and should incorporate biological structure when that structure is known and relevant. BPE learns statistical patterns from the corpus, but those patterns need not correspond to biological units. Codon tokenization imposes biological semantics directly, at the cost of applicability to noncoding regions. Future approaches might combine these strategies: codon-aware tokenization for coding regions, BPE or single-nucleotide tokens for noncoding sequence, and explicit variant tokens for clinical interpretation tasks.

## From Tokens to Embeddings: Learning Representations

A patient's genome contains a variant of uncertain significance in *SCN5A*, a cardiac ion channel gene. Whether this variant affects protein function depends on subtle sequence features that determine how the protein folds, where it localizes, and how it interacts with other cellular components. The clinical question is binary (pathogenic or benign), but the biological answer emerges from continuous biophysical properties. This gap between discrete genetic variation and continuous biological effect is precisely what embedding layers must bridge: transforming discrete tokens into dense numerical representations that neural networks can process and from which they can learn.

The operation itself is simple: a lookup table assigns each token to a learned vector. The embedding layer maintains a matrix $E$ of dimensions $V \times d$, where $V$ is vocabulary size and $d$ is embedding dimension. Each token maps to a row of this matrix, and during training, backpropagation adjusts the embedding vectors to support downstream prediction. This simplicity belies its importance; the distinction between discrete tokens and their dense representations shapes what models can learn.

Consider the difference between one-hot encoding and learned embeddings. A one-hot representation treats each nucleotide as maximally distinct from every other: the dot product between any two different nucleotides is zero, providing no information about their relationships. Adenine and thymine are equally different from each other as adenine and guanine, despite the biological reality that purines (A, G) share structural properties distinct from pyrimidines (C, T), and that complementary base pairs (A-T, G-C) have special significance for DNA structure and function.

Learned embeddings allow the model to discover such relationships from data. If distinguishing purines from pyrimidines helps the model predict regulatory function, the embedding space will organize to reflect this distinction. If complementary relationships matter, they will emerge in the geometry of the learned space.

The embedding dimension $d$ controls representational capacity. Small embeddings of 32 to 64 dimensions suffice for simple tokenization schemes like single nucleotides, where only four vectors must be distinguished. Larger vocabularies require larger embeddings: *DNABERT-2*'s BPE tokens use 768-dimensional embeddings, comparable to natural language models. The choice involves a trade-off between expressiveness and efficiency, as larger embeddings increase both model capacity and computational cost.

Analysis of trained DNA language models reveals that embedding spaces organize around biologically meaningful properties even without explicit supervision. GC content, often considered a nuisance variable in genomics, emerges as a major axis of variation in embedding space because it correlates with many functional properties including gene density, chromatin accessibility, and mutation rate. Repetitive elements cluster together in embedding space. Coding sequence embeddings differ systematically from noncoding embeddings, even when the tokenization scheme makes no explicit distinction between these region types.

This emergent organization has practical implications. The structure learned in the embedding layer propagates through all subsequent computations. If embeddings fail to capture relevant distinctions, later layers must learn them from scratch. If embeddings encode spurious correlations, the model may exploit them inappropriately. Understanding what embeddings learn, and whether that learning aligns with biological reality, becomes an important diagnostic for model behavior (@sec-interpretability).

The relationship between tokenization and embedding deserves emphasis. Coarse tokenization through large k-mers or aggressive BPE creates more token types, each with room for rich embedding representations but requiring the model to learn more parameters. Fine tokenization through single nucleotides creates fewer token types with simpler embeddings but forces the model to build complex representations through composition across layers. Neither approach is uniformly superior; the optimal choice depends on available training data, model scale, and task requirements.

## Position Encodings: Where Tokens Live

A regulatory variant 50 kilobases upstream of *MYC* can drive oncogenesis by disrupting an enhancer; the identical sequence change at a different genomic location might have no phenotypic consequence. Position is not merely metadata but fundamental biological information. Yet transformers process tokens as sets rather than sequences. The attention mechanism computes interactions between all pairs of tokens regardless of their positions, treating a sequence as a bag of elements with no inherent order. For language, this creates a problem: "dog bites man" and "man bites dog" contain identical tokens but mean very different things. In language, this positional dependency operates at the scale of sentences or paragraphs; word order determines syntax and meaning, but the relevant context rarely extends beyond a few hundred tokens. Genomic position dependencies operate across fundamentally different scales. A transcription factor binding site functions differently depending on whether it sits in a promoter, an intron, or an intergenic desert, and these distinctions can span tens of kilobases. Enhancer-promoter interactions routinely bridge 50 to 500 kilobases of intervening sequence. Moreover, absolute genomic coordinates carry accumulated knowledge with no linguistic analog: the position chr17:41,276,045 indexes decades of clinical observations, population frequencies, and functional studies that inform variant interpretation before a model processes a single nucleotide.  For genomics, the problem is more severe: a transcription factor binding site has entirely different effects depending on whether it appears in a promoter, an enhancer, or a gene body. **Position must somehow be encoded, and the encoding strategy determines what spatial relationships the model can learn.**

The standard solution adds positional information to token embeddings before attention computation. The combined representation carries both content (what nucleotide or k-mer) and position (where in the sequence). Several strategies have emerged, each with distinct properties that matter for genomic applications.

### Absolute Positional Embeddings

Absolute positional embeddings assign a learnable vector to each position in the sequence. Position 1 receives embedding $p_1$, position 2 receives $p_2$, and so forth. These embeddings add to the token embeddings, creating combined representations carrying both identity and location. BERT and early genomic transformers like *DNABERT* used this approach. The limitation is that the model can only handle sequences up to the maximum position seen during training. A model trained with 512-position embeddings cannot process position 513; no embedding exists for it. This fixed maximum context proves particularly restrictive for genomics, where biological phenomena span scales from individual binding sites to megabase regulatory domains.

### Sinusoidal Positional Encodings

Sinusoidal positional encodings address the fixed-length limitation by computing position embeddings from mathematical functions rather than learning them. The original Transformer paper used sinusoids of different frequencies: $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$ and $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$. Each position receives a unique pattern of sines and cosines, with lower-frequency components capturing coarse position and higher-frequency components capturing fine position. The mathematical structure means any position, even one never seen during training, can receive a well-defined encoding. Sinusoidal encodings also have the property that relative positions are represented consistently: the relationship between positions 10 and 20 mirrors that between positions 110 and 120.

### Relative Positional Encodings

Relative positional encodings directly represent the distance between tokens rather than their absolute locations. When computing attention between positions $i$ and $j$, the model incorporates information about $(j - i)$, the relative offset. This approach recognizes that for many biological phenomena, relative positioning matters more than absolute coordinates. A transcription factor binding site 50 bases upstream of a transcription start site has similar effects whether the TSS is at genomic position 1,000 or 1,000,000. Relative encodings also generalize naturally to sequences longer than those seen during training, since relative distances remain bounded even as absolute positions grow.

### Rotary Position Embeddings

Rotary position embeddings (RoPE) encode position by rotating token embeddings in the complex plane (multiplying embeddings by a rotation matrix whose angle depends on position) rather than adding a position vector. This approach preserves relative distance information in the dot product used for attention: the attention score between two positions depends on their relative separation regardless of absolute location. RoPE has become popular in recent large language models and has been adopted by several genomic foundation models including variants of the *Nucleotide Transformer*.

### Attention with Linear Biases

ALiBi (Attention with Linear Biases) takes a different approach entirely, adding position-dependent biases directly to attention scores rather than modifying embeddings. The bias penalizes attention between distant positions, with the penalty increasing linearly with distance. ALiBi requires no learned position parameters and generalizes straightforwardly to longer sequences. The linear distance penalty may not perfectly capture biological relationships (where some regulatory interactions span consistent long distances while others operate locally), but the simplicity and extrapolation properties have proven valuable.

### Genomic-Specific Considerations

For genomic applications, the choice of position encoding has implications beyond sequence length. Biological coordinates matter: a variant at chr17:41,276,045 has specific meaning that should be preserved, and knowing the genomic coordinate enables lookup of prior knowledge from population databases and clinical repositories. Cross-strand relationships exist: the reverse complement of a sequence carries related but distinct information. Circular genomes like mitochondrial DNA and bacterial chromosomes have no beginning or end, creating wraparound relationships that linear position encodings cannot naturally represent.

Several recent models have explored genomic-specific position encoding strategies. Some incorporate absolute genomic coordinates, allowing models to learn position-specific patterns like centromeric sequences or telomeric regions. Others encode strand explicitly, representing Watson and Crick strands as distinct position modalities. Models for bacterial or viral genomes sometimes use circular position encodings that respect the topology of circular chromosomes. These adaptations illustrate that position encoding is not merely a technical detail but a design choice shaping what biological patterns a model can capture.

## Special Considerations for Biological Sequences

The double-stranded nature of DNA creates an ambiguity that has no parallel in natural language: should a model treat the forward and reverse complement strands as the same sequence, different sequences, or related-but-distinct entities? A transcription factor binding site for p53 functions when bound to either strand, yet the gene it regulates is transcribed from only one. This strand ambiguity ripples through every aspect of model design, from data augmentation to architectural constraints to output interpretation.

A sequence ACGT on the forward strand corresponds to ACGT read 5' to 3', but also implies the reverse complement TGCA on the opposite strand read in the opposite direction. Some biological features are strand-specific: a gene on the forward strand is transcribed from that strand only. Other features are strand-agnostic: many transcription factor binding sites function identically on either strand. Representation schemes must decide whether to treat strands as equivalent through data augmentation with reverse complements, as distinct through explicit strand encoding, or as related-but-different through equivariant architectures processing both strands jointly.

The *Nucleotide Transformer* addressed strand by including both orientations during training, using data augmentation to ensure the model sees sequences from both directions. *Caduceus* introduced a more elegant solution in 2024: a bidirectional architecture processing forward and reverse complement strands simultaneously through shared computation [@schiff_caduceus_2024]. The model outputs are equivariant to reverse complementation (reversing and complementing the input produces correspondingly transformed outputs). This inductive bias ensures consistent treatment of strand without requiring augmentation or doubling computational cost.

Circular genomes present another topological consideration. Bacterial chromosomes and plasmids, mitochondrial DNA, and many viral genomes are circular, with no natural start or end position. Linear position encodings impose arbitrary boundaries on these sequences. Some models address this through circular position encodings that wrap around at sequence boundaries, while others process circular genomes as linear sequences with the understanding that boundary effects may introduce artifacts.

Genomic coordinates carry information absent from raw sequence. The position chr17:41,276,045 refers to a specific location in the *BRCA1* gene, and variants at this position have been extensively studied. Knowing the genomic coordinate enables lookup of prior knowledge: population frequencies from gnomAD, clinical interpretations from ClinVar, functional annotations from ENCODE. Some representation schemes incorporate coordinate information explicitly, enabling models to learn position-specific patterns and integrate with external databases. Others deliberately exclude coordinates to force models to learn purely from sequence, trading prior knowledge for generalization to novel sequences or other species.

Multiple sequence inputs arise frequently in genomic applications. Variant effect prediction requires comparing reference and alternate alleles. Comparative genomics involves aligned sequences from multiple species. Some regulatory predictions require input from multiple genomic regions, such as promoter plus enhancer. Representation schemes must accommodate these multi-sequence inputs through concatenation, paired encoding, or specialized architectures processing multiple sequences jointly.

## Trade-offs and Practical Guidance

The choice between tokenization strategies involves multiple competing considerations depending on the intended application. Understanding these trade-offs enables informed design decisions rather than arbitrary choices.

### Resolution Versus Compression

The tension between compression and resolution represents the fundamental trade-off. Higher compression enables longer context windows within fixed computational budgets but loses precision for identifying exactly where variants fall and how they relate to biological features. One-hot encoding and single-nucleotide tokenization provide no compression but maintain full resolution. Non-overlapping k-mers achieve approximately k-fold compression at the cost of k-nucleotide resolution. BPE provides variable compression depending on sequence repetitiveness, with correspondingly variable resolution. For variant effect prediction, where single nucleotide changes can have dramatic phenotypic consequences, resolution is paramount and the computational costs of long single-nucleotide sequences are often justified.

### Vocabulary Size and Model Capacity

Vocabulary size affects both model capacity and efficiency in ways that interact with embedding design. Larger vocabularies require bigger embedding tables but may capture more complex patterns directly in the token representation. Smaller vocabularies are parameter-efficient but require the model to learn compositional structure through multiple layers. One-hot encoding's vocabulary of four tokens (plus special tokens) minimizes embedding parameters but maximizes the compositional learning burden. K-mer vocabularies scale exponentially with $k$, reaching 4,096 for 6-mers. BPE vocabularies are tunable, typically ranging from 4,096 to 32,000 tokens for genomic applications.

### Computational Efficiency

Computational efficiency depends on both tokenization and architecture in ways that have shifted as new architectures have emerged. For standard attention with $O(L^2)$ complexity, any compression directly reduces cost: non-overlapping k-mers reduce attention cost by a factor of $k^2$, and BPE with average compression $c$ reduces cost by $c^2$. Sub-quadratic architectures like Hyena and Mamba change this calculus entirely, making single-nucleotide tokenization computationally feasible at long contexts and eliminating the need to trade resolution for efficiency.

### Variant Interpretation Requirements

Variant interpretation has specific requirements favoring certain representation choices. Single-nucleotide tokens enable clean comparison of reference and alternate alleles at the same token position with no ambiguity about effect localization. K-mer tokens complicate matters because a single SNP changes $k$ overlapping tokens, requiring aggregation across affected tokens and introducing potential boundary effects. BPE tokens create context-dependent effects where the same variant may fall at different positions relative to token boundaries depending on surrounding sequence.

### Practical Heuristics

Several heuristics have emerged from practical experience. Single-nucleotide tokens work best when variant-level reasoning or high-resolution interpretability is central to the application. K-mers or BPE provide advantages when context length is the primary bottleneck and tasks do not require base-level precision. Biologically-informed tokens merit consideration when integrating multi-modal or annotation-rich data. Position encoding should match task requirements: relative encodings for tasks where absolute position is arbitrary, coordinate-aware encodings for clinical applications requiring integration with external databases.

## Implications for Subsequent Chapters

The representation choices examined here set the stage for design decisions discussed throughout the rest of the book. In @sec-cnn, convolutional filters operating on one-hot encoded sequence implicitly learn k-mer-like patterns, with first-layer filters resembling known transcription factor binding motifs. The CNN approach can be understood as learning the tokenization jointly with the prediction task rather than fixing it in advance.

In @sec-attention, transformer architectures bring position encoding to the foreground. The choice between absolute, relative, and rotary position encodings affects whether models can generalize to sequence lengths beyond training and whether they capture the relative spatial relationships central to regulatory grammar. The quadratic complexity of attention creates strong pressure for sequence compression, motivating the k-mer and BPE approaches discussed here.

In @sec-dna-lm, DNA transformers balance context length with biological resolution through various tokenization strategies. The pretraining objectives for DNA language models interact with tokenization choices: masked language modeling over k-mers versus individual bases produces different learned representations with different downstream transfer properties. In @sec-protein-lm, protein language models use amino acids as natural tokens, sidestepping many tokenization debates but raising new questions about how to represent insertions, deletions, and post-translational modifications.

In @sec-regulatory, long-range models like *Enformer* and *Borzoi* largely retained one-hot encoding or single-nucleotide input encodings for precision in variant effect prediction, leaning on architectural innovations (hybrid CNN-transformer designs) rather than token compression to scale context. The choice reflects the primacy of variant interpretation in regulatory genomics applications.

The representation problem remains an active area of research. As models grow larger and contexts extend further, new tokenization strategies may emerge that better balance compression, resolution, and biological structure. The field has moved from treating tokenization as a fixed preprocessing step to recognizing it as a fundamental design decision shaping what models can learn and how they can be applied. Understanding sequence representation is therefore not a technical footnote but a core element of genomic foundation model design, with implications rippling through every subsequent chapter of this book.