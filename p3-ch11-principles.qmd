::: {.callout-warning .content-visible when-profile="draft"}
**TODO:**

- Add figure: taxonomy of genomic foundation models (DNA LM, seqâ†’function, variant-centric, multi-omic) showing the four quadrants with representative models in each
- Add figure: design dimensions diagram showing data, architecture, objectives, and tokenization as orthogonal axes
- Add table: comparison of GFM families (context length, parameter count, pretraining objective, key applications)
- Add figure: evaluation pyramid from molecular readouts to clinical decisions
- Add decision tree flowchart for practitioners choosing appropriate GFM for their task
- Add figure: adapter strategies (linear probe, LoRA, full fine-tune) with computational cost comparison
:::


Pretraining objectives (MLM, autoregressive, contrastive)
Generative objectives (diffusion, flow matching, VAEs)
Fine-tuning strategies
Transfer and adaptation
Scaling laws

# Genomic FMs: Principles & Practice {#sec-princ}

Genomic foundation models represent the culmination of several threads developed across the earlier parts of this book: high-fidelity variant calling, regulatory sequence-to-function prediction, protein language models, and long-context transformers for DNA. These models extend the ideas presented in previous chapters into systems that are general-purpose, pretrained at scale, and reusable across a wide range of genomic and genetic tasks.

This chapter steps back from individual architectures to address a more fundamental question: what does it mean for a model to be a genomic foundation model? We organize the emerging ecosystem into a practical taxonomy, distill design principles that guide model selection and development, and provide guidance for practitioners seeking to integrate these models into their workflows. The conceptual framework established here will guide the remaining chapters of Part IV as we examine specific application domains.

## From Task-Specific Models to Genomic Foundation Models

The earlier chapters traced a fairly linear progression through the history of computational approaches to genomic prediction. Hand-crafted scores and shallow models such as CADD and early pathogenicity predictors established the value of integrating diverse annotations for variant interpretation [@rentzsch_cadd_2019; @schubach_cadd_2024]. Task-specific deep models such as DeepSEA, ExPecto, Sei, Enformer, and SpliceAI demonstrated that neural networks could learn regulatory and splicing effects directly from sequence, often surpassing the performance of feature-engineered approaches [@zhou_deepsea_2015; @zhou_expecto_2018; @chen_deepsea_2022; @avsec_enformer_2021; @jaganathan_spliceai_2019]. Sequence language models over proteins and DNA, including ESM, DNABERT, Nucleotide Transformer, HyenaDNA, and GROVER, showed that general sequence representations could be learned via self-supervision and then transferred to diverse downstream tasks [@rives_esm_2021; @lin_esm-2_2022; @brandes_genome-wide_2023; @ji_dnabert_2021; @dalla-torre_nucleotide_2023; @nguyen_hyenadna_2023; @sanabria_grover_2024].

Foundation models build on these ingredients but fundamentally change the contract between model and user. The primary product of a genomic foundation model is not a task-specific prediction head but rather a reusable representation, and sometimes a general interface, that can be adapted to many downstream tasks with modest additional supervision. HyenaDNA exemplifies this paradigm: a genomic foundation model pretrained on the human reference genome with context lengths up to one million tokens at single-nucleotide resolution using a Hyena-based long-range architecture. DNABERT-2, Nucleotide Transformer V2, Caduceus-Ph, GROVER, and related models form a parallel family of transformer-style DNA foundation models. A recent benchmark comparing these five models across diverse tasks including classification, gene expression prediction, variant effect quantification, and TAD recognition illustrates both the promise and the limitations of current DNA foundation models [@manzo_comparative_2025].

At a high level, genomic foundation models extend the pretrain-then-finetune paradigm from natural language processing and protein modeling into genomics, but with domain-specific constraints that distinguish them from their counterparts in other fields. These constraints include extreme context lengths necessary to capture distal regulatory interactions, single-nucleotide sensitivity required for variant effect prediction, and strong mechanistic priors that arise from decades of molecular biology research.

## What Makes a Model a Genomic Foundation Model?

The term "foundation model" is sometimes used loosely in the genomics literature, applied to any large neural network trained on genomic data. For practical purposes, it is useful to establish working criteria that separate true genomic foundation models from ordinary deep models that happen to operate on biological sequences.

### Working Definition

A genomic foundation model is a pretrained model that satisfies several key properties. First, it learns from large-scale genomic data with minimal task-specific supervision, typically through pretraining on entire genomes or large portions thereof across species or populations. The objectives employed during pretraining include masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction.

Second, a genomic foundation model produces general-purpose representations. These take the form of embeddings of sequences, variants, loci, or genes that prove useful across many downstream tasks. Critically, these representations can be extracted and reused with light adapters or linear probes rather than requiring full model retraining.

Third, genomic foundation models are designed for broad transfer. They support many downstream tasks without retraining the full model, enabling transfer across assays (from chromatin marks to gene expression), across tissues, across species, and across variant types.

Fourth, these models scale along at least one dimension. Some scale context length, as in HyenaDNA's million-token window. Others scale parameter count, as in the ESM and Nucleotide Transformer families. Still others scale data diversity through pan-genomic pretraining or cross-species corpora.

Fifth, genomic foundation models typically expose a relatively standardized interface. This includes a common API for embeddings, sequence scoring, and mask-based perturbation, and models are often distributed via model hubs such as Hugging Face with documented recipes for downstream applications.

Many excellent deep models for genomics fail one or more of these criteria. Early versions of DeepSEA or SpliceAI, for instance, were trained for specific assays or tasks, used narrowly scoped inputs and outputs, and were not designed for broad reuse beyond their original application domains.

### Foundation Models Versus Large Models

Scale alone does not make a model a foundation model. A very large Enformer-like model trained solely on human chromatin tracks is powerful but remains strongly bound to a specific prediction interface that maps sequence to a fixed set of chromatin tracks. By contrast, a DNA language model like HyenaDNA or DNABERT-2 is explicitly trained to model raw sequence using a general objective and is naturally repurposed as an embedding engine for diverse downstream applications.

This distinction matters because it affects how models should be evaluated. Foundation models must be assessed across families of tasks rather than single benchmarks, using resources like TraitGym for trait-level performance and ProteinGym for variant effect prediction [@benegas_traitgym_2025; @notin_proteingym_2023]. The distinction also affects how models should be integrated into existing pipelines, since foundation models serve as feature extractors while task-specific models typically serve as end-to-end predictors.

## A Taxonomy of Genomic Foundation Models

The landscape of genomic foundation models can be organized into four broad families, each with distinct characteristics, strengths, and typical applications. Understanding this taxonomy helps practitioners select appropriate models for their specific needs and helps researchers position new contributions within the broader field.

### DNA Language Models

The first family comprises DNA language models that learn sequence representations from raw nucleotide strings. Representative examples include DNABERT and DNABERT-2, which apply BERT-style masked language modeling to DNA sequences [@ji_dnabert_2021; @zhou_dnabert-2_2024]. The Nucleotide Transformer family scales this approach to larger models and cross-species training corpora [@dalla-torre_nucleotide_2023]. HyenaDNA uses implicit convolutions rather than attention to achieve subquadratic complexity, enabling context lengths up to one million nucleotides [@nguyen_hyenadna_2023]. Caduceus incorporates bidirectional processing and reverse-complement equivariance as architectural inductive biases. GROVER combines BPE-style tokenization with training on regulatory tracks rather than raw sequence alone [@sanabria_grover_2024].

These models share several characteristics. They are typically trained on reference genomes with self-supervised objectives, they produce embeddings that can be probed or fine-tuned for diverse tasks, and they vary primarily in context length, architectural family (transformer versus state space model versus hybrid), and tokenization strategy.

### Sequence-to-Function Genomic Foundation Models

The second family comprises sequence-to-function models that predict molecular readouts directly from sequence. These models blur into foundation model territory when their output space is sufficiently broad and their internal representations are reused for tasks beyond the original assay set. Examples include Enformer, which predicts thousands of chromatin and expression tracks from 200 kb sequence windows [@avsec_enformer_2021], and Sei, which organizes predictions into interpretable sequence classes that capture regulatory grammar [@chen_deepsea_2022]. These models typically operate over longer context windows of 100 kb or more and provide variant effect scores by computing delta-predictions between reference and alternative alleles.

Enformer serves as a prototypical example of a sequence-to-function model that has been widely reused as a feature extractor for downstream tasks including gene expression prediction and fine-mapping of regulatory variants. While these models were originally trained for specific assays, they approximate foundation models when the output space spans many cell types and assays and when their internal representations prove useful for tasks beyond the original prediction targets.

### Variant-Centric Genomic Foundation Models

A third class of foundation models focuses not on raw sequence but on genetic variants as the fundamental unit. These models embed variants using contextual information from local sequence, gene structure, and external annotations, and they predict variant pathogenicity, molecular consequences, or trait-level effect sizes.

Examples in this space include CADD and its deep-learning-enhanced successor models, which integrate annotations and sequence features for broad variant pathogenicity scoring [@rentzsch_cadd_2019; @schubach_cadd_2024]. AlphaMissense repurposes ESM-style protein language models to predict missense pathogenicity at scale [@cheng_alphamissense_2023]. Delphi, MIFM, and related models couple genomic foundation model embeddings with polygenic score estimation for complex traits [@georgantas_delphi_2024; @rakowski_mifm_2025; @wu_genome-wide_2024]. Emerging variant representation learning datasets and benchmarks such as GV-Rep explicitly probe how well foundation models represent genetic variants and clinical annotations.

Variant-centric foundation models blur the line between feature extractors and trait models. Their predictions can be plugged directly into polygenic score pipelines, risk stratification tools, or rare disease interpretation workflows, making them particularly relevant for clinical applications.

### Multi-omic and Cross-Modal Foundation Models

Finally, a growing set of models aim to natively integrate multiple modalities. These include models that jointly process DNA sequence, chromatin state, and gene expression; models that incorporate sequence and 3D genome structure from Hi-C or Micro-C experiments; and models that combine DNA with non-sequence modalities such as images or free text descriptions of function.

Recent work on architectures like Omni-DNA explores transformer-based auto-regressive models that jointly handle DNA and task-specific tokens, enabling multi-task learning over sequence, epigenetic marks, and even textual descriptions of function. These models move genomic foundation models closer to a unified interface for genome biology, though at the cost of more complex training objectives and data engineering requirements.

## Design Dimensions of Genomic Foundation Models

When designing or selecting a genomic foundation model, it is helpful to think in terms of several orthogonal design dimensions. Each dimension involves trade-offs that affect model performance, computational requirements, and suitability for specific applications.

### Data: What Does the Model See?

The choice of training data fundamentally shapes what a model can learn. Key decisions include species coverage, assay diversity, population diversity, and sampling strategies.

Regarding species coverage, models may be trained on human genomes only, focusing on clinical and human genetics applications, or they may incorporate cross-species pretraining on dozens or hundreds of species. Cross-species training, as employed by Nucleotide Transformer and many protein language models, encourages discovery of conserved regulatory code and can improve out-of-domain generalization [@dalla-torre_nucleotide_2023; @rives_esm_2021].

Assay diversity matters for sequence-to-function models. The choice of which epigenomic assays, cell types, and perturbation datasets to include during training determines what molecular readouts the model can predict and, more subtly, what regulatory patterns it learns to recognize. Collections like Cistrome provide rich training data spanning transcription factor binding, histone modifications, and chromatin accessibility across many cell types [@zheng_cistrome_2019].

Population diversity is crucial for avoiding biased models. Inclusion of genomes from diverse ancestries is necessary to prevent embedding population-specific biases into foundation models and downstream risk scores. Early deep learning approaches to polygenic score estimation, including Delphi and MIFM, explicitly tackle ancestry-aware evaluation to quantify and mitigate these biases [@georgantas_delphi_2024; @rakowski_mifm_2025; @wu_genome-wide_2024].

Context length and sampling strategies also play important roles. Some models randomly slice long chromosomes into training windows, as in HyenaDNA. Others use targeted sampling around genes, enhancers, or known variants. Warm-up schedules that gradually increase context length can stabilize training for long-context models.

### Architecture: How Does the Model Process Sequence?

Architectural choices determine the computational properties of a model, including maximum practical context length, memory and compute requirements, and ease of adaptation for different tasks.

Transformer architectures dominate current genomic foundation models and come in several flavors. Encoder-only models following the BERT design, such as DNABERT and Nucleotide Transformer, are well-suited for classification and embedding tasks. Decoder-only models following the GPT design, such as GROVER and some Omni-DNA variants, align naturally with generative tasks. Encoder-decoder hybrids support tasks requiring explicit outputs such as sequence-to-text explanations.

Attention-free long-range models address the quadratic complexity of standard attention. Hyena-based models like HyenaDNA use implicit convolutions to achieve subquadratic complexity, enabling million-token contexts. State space models and related architectures trade exact attention for scalable long-range interactions while maintaining competitive performance on many tasks.

Dense-attention long-range transformers demonstrate that with careful engineering and context extension schedules, dense-attention transformers can also reach approximately 200 kb contexts at single-nucleotide resolution. Models like Gene42 show that the attention-versus-efficiency trade-off is not absolute.

Hybrid architectures combine multiple approaches. CNN-plus-transformer stacks use local convolutions followed by global attention, as seen in Enformer-like models [@avsec_enformer_2021]. Cross-attention mechanisms can integrate DNA with auxiliary modalities such as chromatin state or 3D contact maps.

### Objectives: What Does the Model Learn to Predict?

The training objective shapes the representations a model learns and its suitability for different downstream applications.

Masked token prediction randomly masks nucleotides or k-mers and trains the model to predict them given surrounding context. This approach, used by DNABERT, DNABERT-2, and many transformer-based models, encourages learning of local and medium-range dependencies [@ji_dnabert_2021; @zhou_dnabert-2_2024].

Next-token prediction uses an autoregressive language model objective, as in GROVER and HyenaDNA. This approach naturally aligns with generative tasks and in-context learning, and it leverages techniques developed for large language models in natural language processing.

Denoising and span corruption objectives replace or permute spans of sequence and train the model to reconstruct them. These approaches encourage robustness to small perturbations and attention to long-range structure.

Multi-task sequence-to-function prediction directly predicts chromatin profiles, transcription factor binding, accessibility, expression, and other molecular readouts from sequence. Models like DeepSEA, Enformer, and Sei use this approach, which functions as a powerful regularizer and provides a direct bridge between sequence patterns and molecular phenotypes [@zhou_deepsea_2015; @avsec_enformer_2021; @chen_deepsea_2022].

Cross-modal objectives jointly predict sequence features and other modalities. Examples include contrastive alignment between DNA slices and 3D contacts or histone marks, and joint prediction of sequence, epigenetic tracks, and textual function labels in Omni-DNA-like architectures.

### Tokenization and Representations

Tokenization presents a non-trivial design choice for DNA models, with different approaches offering distinct trade-offs.

Character-level tokenization treats each nucleotide as a separate token. This is the simplest approach and maintains single-nucleotide resolution, making it compatible with precise variant effect prediction. HyenaDNA and many sequence-to-function models use this approach [@nguyen_hyenadna_2023].

K-mer tokenization groups nucleotides into overlapping or non-overlapping k-mers, creating vocabularies of size $4^k$. For 6-mers, this yields 4,096 tokens. K-mer tokenization reduces sequence length and helps transformers reach longer effective contexts, but at the cost of positional ambiguity and reduced resolution [@ji_dnabert_2021].

Learned tokenization approaches, such as BPE-style methods used in BioToken, discover subsequence units optimized for downstream performance rather than using fixed vocabularies [@medvedev_biotoken_2025]. These approaches can allocate vocabulary capacity efficiently, representing common patterns compactly while maintaining the ability to encode rare sequences.

Internally, genomic foundation models typically produce per-position embeddings $h_i \in \mathbb{R}^d$ for each nucleotide or token, pooled sequence embeddings that summarize an entire region through mean pooling, CLS tokens, or learned pooling operations, and variant embeddings constructed by contrasting reference versus alternative alleles, sometimes augmented with structural context. The choice of pooling strategy can significantly influence downstream performance, and benchmarking studies have found that simple mean pooling of per-token embeddings often outperforms more elaborate strategies across many tasks [@manzo_comparative_2025].

## Evaluating Genomic Foundation Models

Because genomic foundation models are intended to serve as foundations for many applications, their evaluation must be broader than single-task metrics. A model that excels at one benchmark may fail on others, and performance on standard benchmarks may not predict utility for real-world applications.

### Downstream Task Suites and Benchmarks

Emerging benchmark suites provide structured evaluations across diverse tasks. ProteinGym evaluates variant effect prediction across many proteins for protein language models [@notin_proteingym_2023]. TraitGym assesses trait-level performance of regulatory and genomic models across complex trait prediction tasks [@benegas_traitgym_2025]. Comparative evaluations of DNA language models and regulatory models, such as the work by Manzo and colleagues, compare models across regulatory genomics tasks [@manzo_comparative_2025]. DNA foundation model benchmarks systematically compare models like DNABERT-2, Nucleotide Transformer V2, HyenaDNA, Caduceus-Ph, and GROVER across classification, variant effect, and TAD recognition tasks. Variant-centric benchmarks like GV-Rep probe models' ability to represent clinical variants and their genomic contexts.

A key lesson from these benchmarks is that no single model dominates all tasks. General-purpose DNA foundation models often perform well overall but may lag specialized architectures for gene expression and eQTL prediction, while excelling for variant prioritization and regulatory element annotation.

### Evaluation Modes

Genomic foundation models can be evaluated in several regimes that test different aspects of their utility.

Zero-shot evaluation uses frozen embeddings with simple operations such as similarity computations or clustering, or with predefined scoring rules. This tests whether useful information is accessible without any task-specific training. An example would be using HyenaDNA embeddings directly for in-context learning on simple motif tasks.

Linear probes train shallow linear or logistic regression heads on top of frozen embeddings. This provides a quick measure of how easily information is linearly decodable from the model's representations and is often used as a diagnostic for representation quality.

Lightweight adaptation includes approaches like low-rank adaptation (LoRA), prompt tuning, or small MLP heads fine-tuned on specific tasks. These methods balance performance with computational cost and stability, enabling adaptation without the full expense of end-to-end fine-tuning.

Full fine-tuning updates all model parameters on a downstream task. This typically yields the best task-specific performance but requires more data and computation, and risks overfitting to the specific task distribution.

The choice among these evaluation modes depends on the amount of labeled data available, computational constraints, and whether the goal is to assess representation quality or to achieve maximum task performance.

## Practical Integration of Genomic Foundation Models

For practitioners seeking to use genomic foundation models in their work, several questions guide the choice of model and integration strategy.

### Selecting a Model for Your Task

The appropriate model depends on the specific application. For missense variant interpretation, protein language models like ESM-2 or AlphaMissense provide strong baselines with well-characterized performance [@cheng_alphamissense_2023]. For non-coding variant interpretation, sequence-to-function models like Enformer or DNA language models fine-tuned on regulatory tasks are more appropriate [@avsec_enformer_2021]. For tasks requiring very long genomic context, such as enhancer-promoter linking or structural variant interpretation, models like HyenaDNA or long-context dense-attention models like Gene42 should be considered. For regulatory variant interpretation near genes, Enformer-like or DeepSEA-like models can be compared against DNA language models working via embeddings [@zhou_deepsea_2015; @chen_deepsea_2022; @ji_dnabert_2021]. For trait-level prediction with large cohorts, polygenic score pipelines incorporating GFM-based variant priors, such as Delphi or MIFM, offer promising approaches [@georgantas_delphi_2024; @rakowski_mifm_2025; @wu_genome-wide_2024]. For method development and benchmarking, standardized benchmark suites like TraitGym, ProteinGym, GV-Rep, and DNA foundation model comparison studies ensure that comparisons are meaningful [@benegas_traitgym_2025; @notin_proteingym_2023; @manzo_comparative_2025].

### Integration Strategies

Once a model is selected, several integration strategies are available. The simplest approach uses the model as a feature extractor, computing embeddings or predictions for variants or sequences of interest and then feeding these features into downstream models or pipelines. This approach is computationally efficient and compatible with existing infrastructure.

Adapter-based fine-tuning keeps the foundation model frozen while training small adapter modules on task-specific data. This preserves the general knowledge in the foundation model while adapting its representations to the specific task.

End-to-end fine-tuning updates the entire model on task-specific data. This can achieve the best performance but requires more data and computation and may sacrifice generality.

Ensemble approaches combine predictions from multiple models, often achieving better performance and calibration than any single model. This is particularly valuable when different models have complementary strengths.

## Safety, Robustness, and Responsible Use

As genomic foundation models become infrastructure for clinical and research pipelines, considerations of safety and robustness move from optional extras to essential requirements.

### Robustness and Adversarial Sensitivity

Recent work on genomic foundation model robustness highlights that these models can be surprisingly sensitive to adversarial perturbations at both the input sequence level and through soft prompts in embedding space. Even when perturbations are hardly biologically plausible, they reveal fragility of decision boundaries in high-dimensional representation space and potential failure modes where small spurious changes strongly impact pathogenicity or variant effect predictions.

These findings suggest that adversarial testing should become part of genomic foundation model validation, especially for clinical use cases. Robust training approaches, including data augmentation, adversarial objectives, or distributionally robust optimization, may be needed for high-stakes applications.

### Bias, Fairness, and Ancestry

Genomic foundation models trained predominantly on reference genomes or Euro-centric cohorts risk encoding biased priors. These biases can manifest as underestimation of risk in underrepresented ancestries and misclassification of benign variants that are common in certain populations but rare in training data.

Deep polygenic score and variant interpretation pipelines that incorporate genomic foundation models should perform ancestry-stratified evaluation and consider explicit debiasing through reweighting and careful calibration [@georgantas_delphi_2024; @rakowski_mifm_2025; @wu_genome-wide_2024].

### Data Governance and Privacy

Because genomic foundation models are often trained on large collections of genomic sequences, data use agreements and privacy protections must be respected. Some cohort-level datasets cannot be used for unrestricted pretraining due to consent restrictions. Even when training on reference genomes, leakage from labeled clinical datasets into training may complicate downstream evaluation.

To date, most published genomic foundation models emphasize training on public reference genomes or synthetic benchmarks, but clinical deployment will require stronger guarantees about data provenance and privacy protection.

## Open Challenges and Future Directions

Genomic foundation models are still in their early days, and several open challenges stand out as important directions for future work.

### Toward Unified Multi-omic Foundation Models

Current genomic foundation models remain fragmented across DNA-only language models, sequence-to-function models tied to specific assays, variant-centric pathogenicity models, and protein and RNA language models. A major frontier is the development of unified multi-omic foundation models that jointly model DNA, RNA, protein, chromatin, and 3D genome structure. Such models would support cross-modal queries, enabling questions like "given this variant, what is the likely impact on transcription factor binding, chromatin accessibility, and gene expression in a specific cell type?" They would also provide interpretable pathways connecting sequence variation to phenotypes. Models like Omni-DNA represent first steps in this direction, demonstrating that multi-task, cross-modal training is feasible at scale.

### Integrating Causal and Mechanistic Structure

Most genomic foundation models are trained with purely predictive objectives. Incorporating more causal structure could improve robustness to distribution shift between cell types or interventions and enable counterfactual reasoning about hypothetical perturbations like enhancer knockouts.

Potential routes toward more causal models include causal representation learning on top of foundation model embeddings, mechanistic constraints derived from gene regulatory networks or biochemical kinetics, and joint modeling of perturbation data from CRISPR screens or gene knockouts with observational genomics.

### Efficient and Accessible Deployment

Even if genomic foundation models train on large clusters, their deployment should be feasible in typical research labs and clinical environments. Approaches to improve accessibility include distillation into smaller student models, efficient inference via sparsity, quantization, and hardware-aware architectures, and task-specific adapters that keep the frozen backbone small enough for on-premise use.

The long-range efficiency of architectures like HyenaDNA and the emergence of dense-attention models like Gene42 suggest multiple viable paths to deployable genomic foundation models.

## Summary

This chapter has provided a framework for understanding genomic foundation models as a distinct class of computational tools for genome biology. We defined what it means for a model to be a genomic foundation model, emphasizing the properties of scale, generality, and reusability that distinguish foundation models from task-specific deep models. We proposed a practical taxonomy organizing the field into DNA language models, sequence-to-function genomic foundation models, variant-centric genomic foundation models, and emerging multi-omic models.

We surveyed the core design dimensions along which models differ, including data composition, architecture, training objectives, and tokenization strategies. We discussed evaluation regimes and benchmark suites that assess genomic foundation models across diverse tasks and outlined how practitioners can integrate these models into variant interpretation, regulatory genomics, and trait prediction pipelines. Finally, we highlighted emerging concerns around robustness, bias, and responsible deployment that must be addressed as these models move toward clinical applications.

The remaining chapters of Part IV will dive deeper into specific application domains. @sec-vep recasts variant effect prediction in the foundation model era, examining how protein and DNA-based approaches can be combined and calibrated. @sec-systems broadens the view from isolated sequences to multi-omic and systems-level representations, exploring models that integrate genomic, transcriptomic, proteomic, and phenotype data. Throughout, the conceptual framework established here will help organize a rapidly evolving ecosystem of genomic foundation models.