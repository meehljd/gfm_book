# Part II: Deep Learning Architectures {.unnumbered}

Part I established the data resources, statistical foundations, and pre-deep learning variant scoring methods that preceded the current era of genomic modeling. This part turns to the architectural innovations that transformed what is computationally possible. The chapters that follow trace an arc from early convolutional neural networks through the explosion of transformer-based approaches to hybrid designs that attempt to combine the strengths of both paradigms.

The progression is not merely chronological. Each architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information. Transformer-based language models treat sequences as structured compositions of tokens whose meaning emerges from context, leveraging self-attention to capture dependencies across arbitrary distances. Hybrid architectures attempt to reconcile these perspectives, using convolutions to extract local features efficiently while deploying attention mechanisms to model long-range interactions that span tens or hundreds of kilobases.

@sec-cnn begins with the CNN-based models that first demonstrated deep learning could outperform handcrafted features for regulatory genomics. DeepSEA, ExPecto, and SpliceAI established the paradigm of training deep networks on functional genomics data to predict chromatin accessibility, transcription factor binding, gene expression, and splicing from sequence alone. These models remain widely used and provide the conceptual foundation for everything that follows. @sec-prot then examines protein language models, where the success of masked language modeling on natural language translated directly to amino acid sequences. Models like ESM and ProtTrans learn rich representations of protein structure and function without explicit supervision, and AlphaFold demonstrated that these representations could revolutionize structure prediction. @sec-dna applies analogous language modeling strategies to DNA, surveying DNABERT, Nucleotide Transformer, HyenaDNA, and related approaches that treat genomic sequence as text to be understood through self-supervised pretraining. @sec-rna extends this treatment to RNA, covering models that predict secondary structure, capture splicing regulation beyond what CNN-based methods achieve, and represent the emerging frontier of RNA foundation models. Finally, @sec-hybrid examines hybrid architectures like Enformer and Borzoi that combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases, enabling direct modeling of enhancer-promoter interactions and long-range chromatin effects.

By the end of this part, readers will have a working understanding of the major architectural paradigms in genomic deep learning: what each assumes, what each can and cannot capture, and how these design choices translate to practical performance on regulatory prediction tasks.