::: {.callout-warning .content-visible when-profile="draft"}
**TODO:**

- Add figure: Schematic showing how ancestry structure creates spurious correlations between genotypes and phenotypes
- Add figure: Example PCA/UMAP visualization showing batch clustering vs. label clustering
- Add figure: Illustration of different data splitting strategies (individual-level, locus-level, chromosome-level, time-based)
- Add table: Summary of common confounders with detection methods and mitigation strategies
- Add case study box: Real-world example of ancestry confounding inflating model performance
- Add case study box: Example of benchmark leakage in variant effect prediction
- Consider adding discussion of case-control matching strategies
- Consider adding example of domain adaptation for batch correction
:::


# Confounders in Model Training {#sec-confound}

In previous chapters, we treated model performance curves and ROC–AUC numbers as if they transparently reflected how well a model learns biology. In practice, genomic data is riddled with structure that makes it dangerously easy for models, especially large, overparameterized ones, to exploit shortcuts.

Population structure, technical batch effects, benchmark leakage, and label noise can all inflate headline metrics while leaving real-world performance and clinical reliability largely unchanged. These issues are not unique to deep learning; they affect traditional statistics and GWAS as well. But the scale, flexibility, and opacity of modern genomic foundation models (GFMs) make them particularly susceptible.

This chapter surveys the main confounders that arise when training and evaluating genomic models, and outlines practical strategies to detect, mitigate, and transparently report them. Five recurring themes structure the discussion: ancestry stratification and population bias, benchmark leakage and train/test overlap, technical artifacts and batch effects, label noise and ground-truth uncertainty, and cross-ancestry transferability of polygenic scores and other models. Throughout, the key message is simple: architecture advances are only as meaningful as the datasets and evaluation protocols that support them.


## Why Confounders Are Ubiquitous in Genomic ML

A confounder is a variable that influences both the features (such as genotypes or functional readouts) and the labels (such as case/control status or functional effect), creating spurious associations. In genomics, confounders abound for several interconnected reasons.

Data are observational rather than randomized. Disease labels, population sampling, and technical pipelines are all determined by real-world constraints and historical biases rather than experimental design. This stands in contrast to domains where randomized experiments can isolate causal effects.

Population structure is strong and multi-layered. Ancestry, relatedness, and local adaptation affect allele frequencies throughout the genome. These patterns create correlations between genetic variants and both phenotypic outcomes and geographical, environmental, and socioeconomic factors.

Technical pipelines are complex. Each step from sample collection through library preparation, sequencing, alignment, variant calling, and quality control can introduce systematic differences between cohorts. When these differences align with labels, they become exploitable shortcuts.

Labels are noisy. Clinical databases such as ClinVar and high-throughput functional assays contain uncertain and sometimes incorrect annotations. Models trained on noisy labels may learn to predict the noise rather than the underlying biology.

Deep models are powerful pattern detectors. If confounders produce consistent patterns that correlate with labels, models will happily learn those shortcuts instead of the causal biology we care about. The result is impressive performance on held-out data that share the same hidden structure, but brittle behavior as soon as we change ancestry, institution, assay, or time period.


## Ancestry Stratification and Population Bias

### How Ancestry Becomes a Shortcut

Human genetic variation is structured by ancestry: allele frequencies and haplotype patterns differ across populations due to demographic history, drift, and selection. Disease prevalence, environmental exposures, and health-care access are also ancestry- and region-dependent.

This creates a classic confounding scenario. Features, in the form of genotypes or sequence variants, reflect ancestry. Labels, whether case/control status, disease subtype, or even pathogenic versus benign annotations, can vary with ancestry. If a case cohort is primarily of one ancestry and controls are primarily of another, a model can achieve high predictive performance by acting as an ancestry classifier rather than a disease predictor. The same issue arises for variant effect prediction: variants common in one ancestry but rare in another can be spuriously tagged as pathogenic or benign based on how databases were curated rather than on their biological effects.

### Manifestations in Genomic Models

Ancestry confounding manifests in several common patterns. Case/control imbalance across ancestries occurs when cases over-represent individuals of one ancestry while controls over-represent another. Reference database bias arises when variant annotations derive mostly from European-ancestry cohorts, making "benign" often synonymous with "common in Europeans." Implicit ancestry markers allow high-capacity models to recover ancestry even when explicit labels are removed, through cryptic relatedness, shared haplotypes, and local LD patterns that differentiate populations.

For transformer-based GFMs trained on large genomic corpora, even subtle ancestry differences are enough to support a shortcut. These models can pick up on patterns invisible to simpler methods, which is both their strength and, in the presence of confounders, their vulnerability.

### Detecting Ancestry Confounding

Several practical diagnostics can reveal ancestry confounding. PCA or UMAP visualization of genotypes or embeddings allows inspection of whether cases and controls cluster by ancestry. If they do, that is a red flag requiring further investigation. Stratified performance evaluation examines metrics separately within each ancestry group; large performance drops or reversals across groups suggest the model relies on cross-ancestry differences rather than disease biology. Ancestry-only baselines fit a simple classifier on ancestry principal components or self-identified ancestry alone. If this baseline approaches the full model's performance, the model is likely exploiting similar information. Permutation tests within ancestry strata shuffle labels within ancestry groups. This should destroy performance for a truly disease-specific signal, but not for models relying on cross-ancestry differences.

### Mitigating Ancestry Bias

Mitigation is imperfect, but several strategies help reduce the impact of ancestry confounding. Balanced study design recruits cases and controls with similar ancestry distributions wherever possible, or matches controls to cases on ancestry. Within-ancestry evaluation reports metrics for each ancestry separately and uses training–validation splits that preserve within-group structure. Covariate adjustment includes ancestry PCs, kinship matrices, or mixed-model random effects in simpler models; for deep models, conditioning on or adversarially removing ancestry signals from learned embeddings can help. Multi-ancestry training trains on diverse populations rather than restricting to a single ancestry, and explicitly models ancestry as a domain variable. Fairness-aware objectives introduce regularizers or constraints that penalize performance disparities across ancestry groups, which becomes especially important in clinical deployment contexts.

The broader implications of ancestry for model development and clinical deployment are discussed in @sec-clinical and @sec-pgs. Here, the key point is that ancestry confounding can inflate apparent model performance while undermining the very generalization we care about.


## Benchmark Leakage and Train/Test Overlap

Even with perfectly balanced ancestries, evaluation can be misleading if information leaks from training to test sets. Leakage is especially insidious in genomics because the genome is highly structured and redundant, public datasets and benchmarks are heavily reused, and many papers do not fully specify how splits were constructed.

### Forms of Leakage

Common leakage patterns include individual overlap, where the same person or close relative appears in both train and test sets, directly or via related cohorts. Variant overlap occurs when exact variants, or near-identical ones at the same locus, appear in both splits, which happens when different datasets are merged without careful deduplication. Locus-level overlap splits variants in the same gene, regulatory element, or LD block between train and test. A model may learn locus-specific idiosyncrasies instead of general rules, achieving strong apparent performance without learning transferable patterns. Database reuse leakage occurs when benchmarks constructed from ClinVar, gnomAD, or other public databases overlap with external sets used for evaluation, whether through direct inclusion or through shared curation of the same underlying variants. Time-based leakage trains models on data that include later submissions of the same variants or patients that are used as "future" test examples.

For large models, even very small overlaps can inflate metrics, particularly when test sets are small. A model that memorizes a few hundred variants in a test set of a few thousand can achieve substantial apparent performance gains without learning anything general.

### Safer Splitting Strategies

To reduce leakage, individual-level splits ensure that no individual, or closely related individuals if kinship is known, appears in both train and test sets. Locus- or gene-level splits hold out entire genes, enhancers, or genomic regions for variant effect prediction, so that test loci are truly unseen. Chromosome-based splits hold out entire chromosomes or chromosome arms for genome-wide tasks. This is not perfect, since genes on different chromosomes may share regulatory logic, but it greatly reduces local dependency leakage. Time-based splits train on data up to a cutoff date and test on later data, mimicking realistic deployment where models must predict on data that did not exist during training. Transparent data provenance tracks the origin of each sample and variant, including database version and submission ID, to avoid accidental reuse.

### Evaluation Design and Reporting

Beyond the split itself, evaluation design matters. Reporting both in-distribution performance on the same cohort and out-of-distribution performance on new cohorts, ancestries, or technical pipelines reveals how much of apparent performance reflects genuine generalization. Cross-cohort benchmarks that train on one cohort and test on another with different recruitment or sequencing characteristics provide stronger evidence of robustness. Sharing code and detailed recipes for dataset construction allows others to reproduce and critique splitting choices.


## Technical Artifacts: Batch Effects and Platform Differences

While ancestry and population structure reflect biological reality, batch effects are artifacts of the measurement process. Differences in sample collection protocols, library preparation kits, sequencing platforms and chemistry versions, read length, depth, and coverage, and alignment and variant calling pipelines can all introduce systematic shifts in feature distributions.

### How Batch Effects Confound Models

Technical batches often correlate with labels. A case cohort may be sequenced at one institution on one platform, while controls are sequenced elsewhere with different protocols. A longitudinal study might switch from one capture kit or sequencer to another halfway through, coinciding with changes in enrollment criteria. Public datasets may aggregate studies with very different technical characteristics.

In such settings, a model can achieve high accuracy by recognizing batch signatures, such as patterns of missingness, depth, or noise spectra, rather than bona fide biological signals. The model learns to distinguish batches, not biology, and evaluation within the same batch structure produces misleadingly optimistic results.

### Diagnosing Technical Confounders

Common diagnostics include embedding visualization by batch, which projects learned embeddings or expression/coverage profiles via PCA or UMAP, then colors points by batch, platform, or institution. Strong clustering by these variables suggests technical structure that the model might exploit. Batch-only baselines train a classifier using only batch labels or simple technical covariates such as read depth or platform indicators. High baseline performance is a warning sign that batch information predicts labels. Negative controls evaluate models on samples where labels should be uncorrelated with batch, such as technical replicates or randomized subsets. Replicate consistency examines whether predictions are consistent across technical replicates processed in different batches.

### Mitigating Batch Effects

Mitigation is an active research area, with common approaches including careful study design that randomizes cases and controls across batches whenever possible, avoiding systematic alignment between batch and outcome. Preprocessing harmonization uses standardized pipelines for alignment and variant calling, reprocessing raw data when feasible to reduce inter-study differences. Statistical batch correction methods such as ComBat, Harmony, and related approaches can reduce batch effects in expression or chromatin data; similar ideas can be applied to embeddings from GFMs. Domain adaptation and adversarial training train representations that are predictive of labels while being invariant to batch or platform, using techniques like gradient reversal layers or distribution matching objectives. Explicit multi-domain modeling treats each batch or platform as a domain and learns domain-conditional parameters or mixture-of-experts models.

Even with aggressive correction, residual batch structure typically remains. Transparent reporting and robustness checks are essential for understanding how much residual confounding might remain.


## Label Noise and Ground-Truth Uncertainty

Large-scale genomic models rely on labels from clinical variant interpretation databases, GWAS-derived case/control status, high-throughput functional screens such as MPRA, saturation mutagenesis, and CRISPR screens, and curated gold-standard sets for variant effect prediction, splicing predictions, or PGS. These labels are not error-free.

### Sources of Label Noise

Conflicting annotations arise because ClinVar often contains variants with conflicting interpretations or uncertain significance, and criteria for pathogenicity change over time as knowledge advances. A variant classified as pathogenic five years ago may be reclassified as benign today, or vice versa. Ascertainment bias means that variants labeled as benign may simply be common in some populations, while variants labeled as pathogenic may be enriched in clinically ascertained cohorts that over-represent certain ancestries or disease presentations. Measurement noise in functional assays reflects the variable reproducibility of high-throughput experiments across labs, conditions, and replicates. Thresholding continuous scores into discrete classes compounds the issue by introducing arbitrary boundaries. Phenotyping noise arises because clinical case/control labels may be inaccurate due to misdiagnosis, incomplete records, or heterogeneous disease definitions across recruitment sites.

### Consequences for Models

Label noise can limit achievable performance, especially for tasks with overlapping phenotype definitions. It can encourage models to learn spurious proxies that correlate with annotation errors rather than biology. It can bias calibration and decision thresholds, particularly in imbalanced settings where a small fraction of mislabeled examples in the minority class has disproportionate impact.

In some scenarios, training on noisy labels still improves performance if noise is roughly symmetric or if the dataset is very large. However, for rare disease variants and high-stakes predictions, even small fractions of mislabeled examples can be problematic.

### Strategies for Robust Learning with Noisy Labels

Several approaches address label noise. Curated subsets restrict training and evaluation to high-confidence annotations, such as ClinVar pathogenic and benign classifications with multiple submitters and no conflicts, even at the cost of reduced size. Soft labels and uncertainty modeling use probabilistic labels derived from inter-rater disagreement, confidence scores, or continuous assay measurements rather than hard binary labels. Robust losses employ loss functions less sensitive to mislabeled points, such as label smoothing, margin-based losses, or methods that down-weight high-loss outliers. Noise-aware training explicitly models label noise, for example via a noise transition matrix or latent variable models, and jointly infers true labels alongside model parameters. Consensus across modalities combines evidence from protein structure, evolutionary conservation, regulatory context, and clinical data, treating disagreements as signals of uncertainty.

Mechanistic interpretability can also help flag model predictions that disagree with known biology, potentially identifying mislabeled examples in training data.


## Cross-Ancestry PGS Transferability and Model Fairness

Polygenic scores and other genome-wide predictors have gained traction as potential tools for early disease risk stratification. However, many PGS have been developed primarily in individuals of European ancestry, raising concerns about reduced predictive accuracy in underrepresented ancestries, biased calibration where risk is systematically over- or under-estimated in certain groups, and downstream disparities if PGS-informed clinical decisions are applied uniformly.

### Why Transferability Fails

Reasons for poor cross-ancestry transfer include allele frequency differences, where effect estimates calibrated in one population may not generalize when allele frequencies change. LD pattern differences mean that tagging SNPs used in PGS may capture causal variants in one ancestry but not another. Gene–environment interaction occurs because environmental exposures and lifestyle factors that interact with genetic risk differ across populations. Ascertainment and recruitment biases arise because early GWAS datasets often oversampled certain ancestries, clinical populations, or socioeconomic strata.

These issues carry over to deep learning-based PGS and GFMs fine-tuned for disease prediction. Even if the underlying model is trained on diverse genomes in a self-supervised fashion, the supervised fine-tuning and evaluation data can reintroduce bias.

### Towards More Equitable Models

Approaches to improve cross-ancestry performance and fairness include multi-ancestry GWAS and training data that include diverse cohorts at the design stage rather than as an afterthought. Ancestry-aware modeling conditions effect sizes or model parameters on ancestry, or learns ancestry-invariant representations coupled with ancestry-specific calibration. Transfer learning and fine-tuning adapt models from ancestries with large datasets to those with smaller datasets using domain adaptation techniques. Fairness metrics report group-wise calibration, sensitivity, specificity, and decision-curve analyses rather than just overall AUC. Stakeholder engagement works with clinicians, ethicists, and affected communities to decide when and how PGS should be used, and what constitutes acceptable performance gaps.


## From Cautionary Tales to Best Practices

Modern genomic foundation models promise impressive capabilities: genome-scale variant effect prediction, cross-species transfer, multi-omics integration, and clinically actionable risk scores. Yet without rigorous attention to confounders, these capabilities can be overstated or misapplied.

Emerging work on genomic evaluation frameworks emphasizes several principles. Data documentation provides detailed datasheets for datasets and benchmarks, including recruitment, ancestry composition, technical pipelines, and label provenance. Robust evaluation protocols include cross-cohort, cross-ancestry, and time-split evaluations that stress-test models beyond their training distribution. Confounder-aware training explicitly models ancestry, batch, and label uncertainty, and uses adversarial or domain-adaptation techniques. Transparent reporting clearly communicates limitations, potential failure modes, and groups for whom the model has not been validated.


## A Practical Checklist for Confounder-Resilient Genomic Modeling

To close, here is a concise checklist applicable when designing, training, and evaluating genomic models.

For population structure, quantify ancestry and relatedness via PCs or kinship. Ensure cases and controls are balanced within ancestry groups. Report performance stratified by ancestry.

For data splits and leakage, confine individuals, families, and closely related samples to a single split. Split at the locus, gene, or chromosome level where appropriate. Check for overlap with external databases used in evaluation.

For batch and platform effects, assess whether technical variables such as batch, platform, or institution are correlated with labels. Visualize embeddings colored by batch. Use harmonization, batch correction, or domain adaptation as needed.

For label quality, understand how labels are defined and quantify their uncertainty. Filter to high-confidence subsets for primary evaluation. Employ robust training strategies to handle label noise.

For cross-group performance and fairness, report metrics for each ancestry and relevant subgroup. Assess whether risk scores are calibrated across groups, or whether group-specific calibration is required. Consider the ethical and clinical implications of residual performance gaps.

For reproducibility and transparency, fully document dataset construction and splitting procedures and make them shareable. Ensure code and evaluation pipelines are available for independent verification.

By systematically addressing these points, we can ensure that the gains from modern architectures, whether transformers, SSMs, or GFMs, translate into trustworthy advances in genomic science and medicine, rather than brittle models that merely reflect quirks of our data and history.