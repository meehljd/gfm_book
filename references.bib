
@article{he_nucleic_2023,
	title = {Nucleic {Transformer}: {Classifying} {DNA} {Sequences} with {Self}-{Attention} and {Convolutions}},
	volume = {12},
	shorttitle = {Nucleic {Transformer}},
	url = {https://doi.org/10.1021/acssynbio.3c00154},
	doi = {10.1021/acssynbio.3c00154},
	abstract = {Much work has been done to apply machine learning and deep learning to genomics tasks, but these applications usually require extensive domain knowledge, and the resulting models provide very limited interpretability. Here, we present the Nucleic Transformer, a conceptually simple but effective and interpretable model architecture that excels in the classification of DNA sequences. The Nucleic Transformer employs self-attention and convolutions on nucleic acid sequences, leveraging two prominent deep learning strategies commonly used in computer vision and natural language analysis. We demonstrate that the Nucleic Transformer can be trained without much domain knowledge to achieve high performance in Escherichia coli promoter classification, viral genome identification, enhancer classification, and chromatin profile predictions.},
	number = {11},
	urldate = {2024-01-16},
	journal = {ACS Synthetic Biology},
	author = {He, Shujun and Gao, Baizhen and Sabnis, Rushant and Sun, Qing},
	month = nov,
	year = {2023},
	note = {12 citations (Crossref/DOI) [2025-10-22]
Publisher: American Chemical Society},
	keywords = {Printed, Read},
	pages = {3205--3214},
	file = {He et al. - 2023 - Nucleic Transformer Classifying DNA Sequences wit.pdf:/Users/meehl.joshua/Zotero/storage/NM5AH96I/2023-11-17 - He et al. - Nucleic Transformer Classifying DNA Sequences with Self-Attention and Convolutions.pdf:application/pdf},
}

@article{benegas_gpn_2023,
	title = {[{GPN}] {DNA} language models are powerful predictors of genome-wide variant effects},
	volume = {120},
	url = {https://www.pnas.org/doi/10.1073/pnas.2311219120},
	doi = {10.1073/pnas.2311219120},
	abstract = {The expanding catalog of genome-wide association studies (GWAS) provides biological insights across a variety of species, but identifying the causal variants behind these associations remains a significant challenge. Experimental validation is both labor-intensive and costly, highlighting the need for accurate, scalable computational methods to predict the effects of genetic variants across the entire genome. Inspired by recent progress in natural language processing, unsupervised pretraining on large protein sequence databases has proven successful in extracting complex information related to proteins. These models showcase their ability to learn variant effects in coding regions using an unsupervised approach. Expanding on this idea, we here introduce the Genomic Pre-trained Network (GPN), a model designed to learn genome-wide variant effects through unsupervised pretraining on genomic DNA sequences. Our model also successfully learns gene structure and DNA motifs without any supervision. To demonstrate its utility, we train GPN on unaligned reference genomes of Arabidopsis thaliana and seven related species within the Brassicales order and evaluate its ability to predict the functional impact of genetic variants in A. thaliana by utilizing allele frequencies from the 1001 Genomes Project and a comprehensive database of GWAS. Notably, GPN outperforms predictors based on popular conservation scores such as phyloP and phastCons. Our predictions for A. thaliana can be visualized as sequence logos in the UCSC Genome Browser (https://genome.ucsc.edu/s/gbenegas/gpn-arabidopsis). We provide code (https://github.com/songlab-cal/gpn) to train GPN for any given species using its DNA sequence alone, enabling unsupervised prediction of variant effects across the entire genome.},
	number = {44},
	urldate = {2023-11-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Benegas, Gonzalo and Batra, Sanjit Singh and Song, Yun S.},
	month = oct,
	year = {2023},
	note = {81 citations (Crossref/DOI) [2025-10-22]
Publisher: Proceedings of the National Academy of Sciences},
	keywords = {Printed, Read},
	pages = {e2311219120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/YLQDHAPZ/2023-10-31 - Benegas et al. - [GPN] DNA language models are powerful predictors of genome-wide variant effects.pdf:application/pdf},
}

@article{ma_bingo-large_2023,
	title = {'{Bingo}'-a large language model- and graph neural network-based workflow for the prediction of essential genes from protein data},
	volume = {25},
	issn = {1477-4054},
	doi = {10.1093/bib/bbad472},
	abstract = {The identification and characterization of essential genes are central to our understanding of the core biological functions in eukaryotic organisms, and has important implications for the treatment of diseases caused by, for example, cancers and pathogens. Given the major constraints in testing the functions of genes of many organisms in the laboratory, due to the absence of in vitro cultures and/or gene perturbation assays for most metazoan species, there has been a need to develop in silico tools for the accurate prediction or inference of essential genes to underpin systems biological investigations. Major advances in machine learning approaches provide unprecedented opportunities to overcome these limitations and accelerate the discovery of essential genes on a genome-wide scale. Here, we developed and evaluated a large language model- and graph neural network (LLM-GNN)-based approach, called 'Bingo', to predict essential protein-coding genes in the metazoan model organisms Caenorhabditis elegans and Drosophila melanogaster as well as in Mus musculus and Homo sapiens (a HepG2 cell line) by integrating LLM and GNNs with adversarial training. Bingo predicts essential genes under two 'zero-shot' scenarios with transfer learning, showing promise to compensate for a lack of high-quality genomic and proteomic data for non-model organisms. In addition, the attention mechanisms and GNNExplainer were employed to manifest the functional sites and structural domain with most contribution to essentiality. In conclusion, Bingo provides the prospect of being able to accurately infer the essential genes of little- or under-studied organisms of interest, and provides a biological explanation for gene essentiality.},
	language = {eng},
	number = {1},
	journal = {Briefings in Bioinformatics},
	author = {Ma, Jiani and Song, Jiangning and Young, Neil D. and Chang, Bill C. H. and Korhonen, Pasi K. and Campos, Tulio L. and Liu, Hui and Gasser, Robin B.},
	month = nov,
	year = {2023},
	pmid = {38152979},
	pmcid = {PMC10753293},
	note = {14 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {bbad472},
	file = {Ma et al. - 2023 - 'Bingo'-a large language model- and graph neural n.pdf:/Users/meehl.joshua/Zotero/storage/5JLFDTHW/2023-11-22 - Ma et al. - 'Bingo'-a large language model- and graph neural network-based workflow for the prediction of essent.pdf:application/pdf},
}

@article{cheng_alphamissense_2023,
	title = {[{AlphaMissense}] {Accurate} proteome-wide missense variant effect prediction with {AlphaMissense}},
	volume = {381},
	url = {https://www.science.org/doi/full/10.1126/science.adg7492},
	doi = {10.1126/science.adg7492},
	abstract = {The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89\% of missense variants as either likely benign or likely pathogenic.},
	number = {6664},
	urldate = {2023-09-21},
	journal = {Science},
	author = {Cheng, Jun and Novati, Guido and Pan, Joshua and Bycroft, Clare and Žemgulytė, Akvilė and Applebaum, Taylor and Pritzel, Alexander and Wong, Lai Hong and Zielinski, Michal and Sargeant, Tobias and Schneider, Rosalia G. and Senior, Andrew W. and Jumper, John and Hassabis, Demis and Kohli, Pushmeet and Avsec, Žiga},
	month = sep,
	year = {2023},
	note = {1325 citations (Crossref/DOI) [2025-10-22]
Publisher: American Association for the Advancement of Science},
	keywords = {Printed, Read},
	pages = {eadg7492},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/MZM35E4X/2023-09-19 - Cheng et al. - [AlphaMissense] Accurate proteome-wide missense variant effect prediction with AlphaMissense.pdf:application/pdf;science.adg7492_sm:/Users/meehl.joshua/Zotero/storage/TXHUTZAK/science.adg7492_sm.pdf:application/pdf},
}

@misc{zhang_scientific_2024,
	title = {Scientific {Large} {Language} {Models}: {A} {Survey} on {Biological} \& {Chemical} {Domains}},
	shorttitle = {Scientific {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.14656},
	doi = {10.48550/arXiv.2401.14656},
	abstract = {Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of "scientific language", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Zhang, Qiang and Ding, Keyang and Lyv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and Zhuang, Xiang and Wang, Zeyuan and Qin, Ming and Zhang, Mengyao and Zhang, Jinlu and Cui, Jiyu and Xu, Renjun and Chen, Hongyang and Fan, Xiaohui and Xing, Huabin and Chen, Huajun},
	month = jul,
	year = {2024},
	note = {arXiv:2401.14656 [cs]},
	keywords = {Printed, Podcast},
	file = {2024-07-23 - Zhang et al. - Scientific Large Language Models A Survey on Biological & Chemical Domains:/Users/meehl.joshua/Zotero/storage/VCHZLF8Z/2024-07-23 - Zhang et al. - Scientific Large Language Models A Survey on Biological & Chemical Domains.pdf:application/pdf;arXiv.org Snapshot:/Users/meehl.joshua/Zotero/storage/9TI5RRYF/2401.html:text/html},
}

@article{avsec_enformer_2021,
	title = {[{Enformer}] {Effective} gene expression prediction from sequence by integrating long-range interactions},
	volume = {18},
	url = {https://consensus.app/papers/effective-gene-expression-prediction-from-sequence-by-avsec-agarwal/6afb944129f35bad916e6f4a889c07cb/},
	doi = {10.1038/s41592-021-01252-x},
	abstract = {The next phase of genome biology research requires understanding how DNA sequence encodes phenotypes, from the molecular to organismal levels. How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequence through the use of a new deep learning architecture called Enformer that is able to integrate long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Notably, Enformer outperformed the best team on the critical assessment of genome interpretation (CAGI5) challenge for noncoding variant interpretation with no additional training. Furthermore, Enformer learned to predict promoter-enhancer interactions directly from DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of growing human disease associations to cell-type-specific gene regulatory mechanisms and provide a framework to interpret cis-regulatory evolution. To foster these downstream applications, we have made the pre-trained Enformer model openly available, and provide pre-computed effect predictions for all common variants in the 1000 Genomes dataset. One-sentence summary Improved noncoding variant effect prediction and candidate enhancer prioritization from a more accurate sequence to expression model driven by extended long-range interaction modelling.},
	urldate = {2024-12-25},
	journal = {Nature Methods},
	author = {Avsec, Žiga and Agarwal, Vikram and Visentin, D. and Ledsam, J. and Grabska-Barwinska, A. and Taylor, Kyle R. and Assael, Yannis and Jumper, J. and Kohli, Pushmeet and Kelley, David R.},
	month = oct,
	year = {2021},
	note = {846 citations (Semantic Scholar/DOI) [2025-10-22]
875 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {1196--1203},
	file = {41592_2021_1252_MOESM1_ESM:/Users/meehl.joshua/Zotero/storage/7PUC6VKT/41592_2021_1252_MOESM1_ESM.pdf:application/pdf;Full Text:/Users/meehl.joshua/Zotero/storage/WTX3AUV5/2021-10-04 - Avsec et al. - [Enformer] Effective gene expression prediction from sequence by integrating long-range interactions.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/JWYN5S8L/6afb944129f35bad916e6f4a889c07cb.html:text/html},
}

@misc{brixi_evo_2025,
	title = {[{Evo} 2] {Genome} modeling and design across all domains of life with {Evo} 2},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.02.18.638918v1},
	doi = {10.1101/2025.02.18.638918},
	abstract = {All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.},
	language = {en},
	urldate = {2025-02-26},
	publisher = {bioRxiv},
	author = {Brixi, Garyk and Durrant, Matthew G. and Ku, Jerome and Poli, Michael and Brockman, Greg and Chang, Daniel and Gonzalez, Gabriel A. and King, Samuel H. and Li, David B. and Merchant, Aditi T. and Naghipourfar, Mohsen and Nguyen, Eric and Ricci-Tam, Chiara and Romero, David W. and Sun, Gwanggyu and Taghibakshi, Ali and Vorontsov, Anton and Yang, Brandon and Deng, Myra and Gorton, Liv and Nguyen, Nam and Wang, Nicholas K. and Adams, Etowah and Baccus, Stephen A. and Dillmann, Steven and Ermon, Stefano and Guo, Daniel and Ilango, Rajesh and Janik, Ken and Lu, Amy X. and Mehta, Reshma and Mofrad, Mohammad R. K. and Ng, Madelena Y. and Pannu, Jaspreet and Ré, Christopher and Schmok, Jonathan C. and John, John St and Sullivan, Jeremy and Zhu, Kevin and Zynda, Greg and Balsam, Daniel and Collison, Patrick and Costa, Anthony B. and Hernandez-Boussard, Tina and Ho, Eric and Liu, Ming-Yu and McGrath, Thomas and Powell, Kimberly and Burke, Dave P. and Goodarzi, Hani and Hsu, Patrick D. and Hie, Brian L.},
	month = feb,
	year = {2025},
	note = {71 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.02.18.638918
Section: New Results},
	keywords = {Printed, Important, Podcast, Read},
	file = {2025-02-21 - Brixi et al. - [Evo 2] Genome modeling and design across all domains of life with Evo 2:/Users/meehl.joshua/Zotero/storage/BSMA7JT7/2025-02-21 - Brixi et al. - [Evo 2] Genome modeling and design across all domains of life with Evo 2.pdf:application/pdf},
}

@misc{zvyagin_genslms_2022,
	title = {{GenSLMs}: {Genome}-scale language models reveal {SARS}-{CoV}-2 evolutionary dynamics},
	copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{GenSLMs}},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.10.511571v2},
	doi = {10.1101/2022.10.10.511571},
	abstract = {We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.},
	language = {en},
	urldate = {2025-01-28},
	publisher = {bioRxiv},
	author = {Zvyagin, Maxim and Brace, Alexander and Hippe, Kyle and Deng, Yuntian and Zhang, Bin and Bohorquez, Cindy Orozco and Clyde, Austin and Kale, Bharat and Perez-Rivera, Danilo and Ma, Heng and Mann, Carla M. and Irvin, Michael and Pauloski, J. Gregory and Ward, Logan and Hayot-Sasson, Valerie and Emani, Murali and Foreman, Sam and Xie, Zhen and Lin, Diangen and Shukla, Maulik and Nie, Weili and Romero, Josh and Dallago, Christian and Vahdat, Arash and Xiao, Chaowei and Gibbs, Thomas and Foster, Ian and Davis, James J. and Papka, Michael E. and Brettin, Thomas and Stevens, Rick and Anandkumar, Anima and Vishwanath, Venkatram and Ramanathan, Arvind},
	month = nov,
	year = {2022},
	note = {24 citations (Crossref/DOI) [2025-10-22]
Pages: 2022.10.10.511571
Section: New Results},
	keywords = {Printed, Podcast, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/2UK2TV2N/2022-11-23 - Zvyagin et al. - GenSLMs Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics.pdf:application/pdf},
}

@misc{ayanian_strand_2025,
	title = {[{STRAND}] {Introducing} {STRAND}: {A} {Foundational} {Sequence} {Transformer} for {Range} {Adaptive} {Nucleotide} {Decoding}},
	shorttitle = {Introducing {STRAND}},
	url = {https://www.researchsquare.com/article/rs-6115078/v1},
	doi = {10.21203/rs.3.rs-6115078/v1},
	abstract = {The advent of high-throughput sequencing has led to an exponential increase in genomic data, highlighting the need for efficient and accurate models to analyze and interpret this information. Here, we introduce a novel, exomic foundational model that leverages a combination of human reference genome and multispecies data to improve variant detection and interpretation. Our model utilizes a short- range transformer architecture and is trained on a large dataset of human exomic sequences derived from the Tapestry study. Through a series of ablation studies and scaling experiments, we demonstrate the effectiveness of our model in pre- dicting next token accuracy and identifying clinically pathogenic variants. We also show that our model outperforms existing models in a range of downstream tasks, including variant effect prediction and disease state identification. In fact, our largest STRAND variant (1B parameters) surpassed previous benchmarks, demonstrating a mean accuarcy of 0.880 (8.2\% improvement over the original NT and 7\% improvement over NT-v2). Furthermore, we construct a unique exomic ClinVar dataset to evaluate the model’s performance on pathogenicity and disease states. Our results highlight the potential of this model to improve our understanding of the human exome and its role in disease. The model and its applications have significant implications for genomic based diagnosis and personalized medicine including tailored therapeutic development.},
	urldate = {2025-04-16},
	publisher = {Research Square},
	author = {Ayanian, Shant and Korfiatis, Panos and Osborne, Collin and Molnar, Carl and Blasi, Marc and Perez, Xoab and XU, Clark and Das, Pravat and Callstrom, Matthew and Shah, Vijay and Ryu, Alexander and Lazaridis, Konstantinos and Venkatesh, Ganesh and Pondenkandath, Vinay and Vassilieva, Natalia and Kanakiya, Bhargav and Levin, May and Redlon, Matthew and Myasoedova, Elena},
	month = mar,
	year = {2025},
	note = {0 citations (Crossref/DOI) [2025-10-22]
ISSN: 2693-5015},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/6VWT2TCV/2025-03-26 - Ayanian et al. - [STRAND] Introducing STRAND A Foundational Sequence Transformer for Range Adaptive Nucleotide Decod.pdf:application/pdf},
}

@article{dalla-torre_nucleotide_2023,
	title = {Nucleotide {Transformer}: building and evaluating robust foundation models for human genomics},
	volume = {22},
	copyright = {2024 The Author(s)},
	issn = {1548-7105},
	shorttitle = {Nucleotide {Transformer}},
	url = {https://www.nature.com/articles/s41592-024-02523-z},
	doi = {10.1038/s41592-024-02523-z},
	abstract = {The prediction of molecular phenotypes from DNA sequences remains a longstanding challenge in genomics, often driven by limited annotated data and the inability to transfer learnings between tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named Nucleotide Transformer, ranging from 50 million up to 2.5 billion parameters and integrating information from 3,202 human genomes and 850 genomes from diverse species. These transformer models yield context-specific representations of nucleotide sequences, which allow for accurate predictions even in low-data settings. We show that the developed models can be fine-tuned at low cost to solve a variety of genomics applications. Despite no supervision, the models learned to focus attention on key genomic elements and can be used to improve the prioritization of genetic variants. The training and application of foundational models in genomics provides a widely applicable approach for accurate molecular phenotype prediction from DNA sequence.},
	language = {en},
	number = {2},
	urldate = {2025-04-16},
	journal = {Nature Methods},
	author = {Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and Lopez Carranza, Nicolas and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and de Almeida, Bernardo P. and Sirelkhatim, Hassan and Richard, Guillaume and Skwark, Marcin and Beguir, Karim and Lopez, Marie and Pierrot, Thomas},
	month = jan,
	year = {2023},
	note = {123 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Important, Podcast, Read},
	pages = {287--297},
	file = {[Preprint] Dalla-Torre et al. - 2023 - Nucleotide Transformer building and evaluating robust foundation models for human genomics:/Users/meehl.joshua/Zotero/storage/METWGZEF/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/K28P6JFS/2023-01-15 - Dalla-Torre et al. - Nucleotide Transformer building and evaluating robust foundation models for human genomics.pdf:application/pdf},
}

@misc{benegas_traitgym_2025,
	title = {[{TraitGym}] {Benchmarking} {DNA} {Sequence} {Models} for {Causal} {Regulatory} {Variant} {Prediction} in {Human} {Genetics}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.02.11.637758v1},
	doi = {10.1101/2025.02.11.637758},
	abstract = {Machine learning holds immense promise in biology, particularly for the challenging task of identifying causal variants for Mendelian and complex traits. Two primary approaches have emerged for this task: supervised sequence-to-function models trained on functional genomics experimental data and self-supervised DNA language models that learn evolutionary constraints on sequences. However, the field currently lacks consistently curated datasets with accurate labels, especially for non-coding variants, that are necessary to comprehensively benchmark these models and advance the field. In this work, we present TraitGym, a curated dataset of regulatory genetic variants that are either known to be causal or are strong candidates across 113 Mendelian and 83 complex traits, along with carefully constructed control variants. We frame the causal variant prediction task as a binary classification problem and benchmark various models, including functional-genomics-supervised models, self-supervised models, models that combine machine learning predictions with curated annotation features, and ensembles of these. Our results provide insights into the capabilities and limitations of different approaches for predicting the functional consequences of non-coding genetic variants. We find that alignment-based models CADD and GPN-MSA compare favorably for Mendelian traits and complex disease traits, while functional-genomics-supervised models Enformer and Borzoi perform better for complex non-disease traits. The benchmark, including a Google Colab notebook to evaluate a model in a few minutes, is available at https://huggingface.co/datasets/songlab/TraitGym.},
	language = {en},
	urldate = {2025-05-15},
	publisher = {bioRxiv},
	author = {Benegas, Gonzalo and Eraslan, Gökcen and Song, Yun S.},
	month = feb,
	year = {2025},
	note = {6 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.02.11.637758
Section: New Results},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/ZTJXK2RH/2025-02-12 - Benegas et al. - [TraitGym] Benchmarking DNA Sequence Models for Causal Regulatory Variant Prediction in Human Geneti.pdf:application/pdf},
}

@article{schubach_cadd_2024,
	title = {{CADD} v1.7: using protein language models, regulatory {CNNs} and other nucleotide-level scores to improve genome-wide variant predictions},
	volume = {52},
	issn = {0305-1048},
	shorttitle = {{CADD} v1.7},
	url = {https://doi.org/10.1093/nar/gkad989},
	doi = {10.1093/nar/gkad989},
	abstract = {Machine Learning-based scoring and classification of genetic variants aids the assessment of clinical findings and is employed to prioritize variants in diverse genetic studies and analyses. Combined Annotation-Dependent Depletion (CADD) is one of the first methods for the genome-wide prioritization of variants across different molecular functions and has been continuously developed and improved since its original publication. Here, we present our most recent release, CADD v1.7. We explored and integrated new annotation features, among them state-of-the-art protein language model scores (Meta ESM-1v), regulatory variant effect predictions (from sequence-based convolutional neural networks) and sequence conservation scores (Zoonomia). We evaluated the new version on data sets derived from ClinVar, ExAC/gnomAD and 1000 Genomes variants. For coding effects, we tested CADD on 31 Deep Mutational Scanning (DMS) data sets from ProteinGym and, for regulatory effect prediction, we used saturation mutagenesis reporter assay data of promoter and enhancer sequences. The inclusion of new features further improved the overall performance of CADD. As with previous releases, all data sets, genome-wide CADD v1.7 scores, scripts for on-site scoring and an easy-to-use webserver are readily provided via https://cadd.bihealth.org/ or https://cadd.gs.washington.edu/ to the community.},
	number = {D1},
	urldate = {2025-05-23},
	journal = {Nucleic Acids Research},
	author = {Schubach, Max and Maass, Thorben and Nazaretyan, Lusiné and Röner, Sebastian and Kircher, Martin},
	month = jan,
	year = {2024},
	note = {235 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {D1143--D1154},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/55MT7UQH/2024-01-05 - Schubach et al. - CADD v1.7 using protein language models, regulatory CNNs and other nucleotide-level scores to impro.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/X6G5SM2W/7511313.html:text/html},
}

@misc{benegas_genomic_2024,
	title = {Genomic {Language} {Models}: {Opportunities} and {Challenges}},
	shorttitle = {Genomic {Language} {Models}},
	url = {http://arxiv.org/abs/2407.11435},
	doi = {10.48550/arXiv.2407.11435},
	abstract = {Large language models (LLMs) are having transformative impacts across a wide range of scientific fields, particularly in the biomedical sciences. Just as the goal of Natural Language Processing is to understand sequences of words, a major objective in biology is to understand biological sequences. Genomic Language Models (gLMs), which are LLMs trained on DNA sequences, have the potential to significantly advance our understanding of genomes and how DNA elements at various scales interact to give rise to complex functions. In this review, we showcase this potential by highlighting key applications of gLMs, including fitness prediction, sequence design, and transfer learning. Despite notable recent progress, however, developing effective and efficient gLMs presents numerous challenges, especially for species with large, complex genomes. We discuss major considerations for developing and evaluating gLMs.},
	urldate = {2025-05-23},
	publisher = {arXiv},
	author = {Benegas, Gonzalo and Ye, Chengzhong and Albors, Carlos and Li, Jianan Canal and Song, Yun S.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.11435 [q-bio]
version: 1},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/SYJK775Y/2024-07-16 - Benegas et al. - Genomic Language Models Opportunities and Challenges.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/68U6JRJV/2407.html:text/html},
}

@article{benegas_gpn-msa_2024,
	title = {{GPN}-{MSA}: an alignment-based {DNA} language model for genome-wide variant effect prediction},
	issn = {2692-8205},
	shorttitle = {{GPN}-{MSA}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10592768/},
	doi = {10.1101/2023.10.10.561776},
	abstract = {Whereas protein language models have demonstrated remarkable efficacy in predicting the effects of missense variants, DNA counterparts have not yet achieved a similar competitive edge for genome-wide variant effect predictions, especially in complex genomes such as that of humans. To address this challenge, we here introduce GPN-MSA, a novel framework for DNA language models that leverages whole-genome sequence alignments across multiple species and takes only a few hours to train. Across several benchmarks on clinical databases (ClinVar, COSMIC, OMIM), experimental functional assays (DMS, DepMap), and population genomic data (gnomAD), our model for the human genome achieves outstanding performance on deleteriousness prediction for both coding and non-coding variants.},
	urldate = {2025-05-23},
	journal = {bioRxiv},
	author = {Benegas, Gonzalo and Albors, Carlos and Aw, Alan J. and Ye, Chengzhong and Song, Yun S.},
	month = apr,
	year = {2024},
	pmid = {37873118},
	pmcid = {PMC10592768},
	note = {19 citations (Crossref/DOI) [2025-10-22]
31 citations (Semantic Scholar/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {2023.10.10.561776},
	file = {Submitted Version:/Users/meehl.joshua/Zotero/storage/VU58S8BV/2024-04-06 - Benegas et al. - GPN-MSA an alignment-based DNA language model for genome-wide variant effect prediction.pdf:application/pdf},
}

@article{linder_borzoi_2025,
	title = {[{Borzoi}] {Predicting} {RNA}-seq coverage from {DNA} sequence as a unifying model of gene regulation},
	volume = {57},
	copyright = {2025 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-024-02053-6},
	doi = {10.1038/s41588-024-02053-6},
	abstract = {Sequence-based machine-learning models trained on genomics data improve genetic variant interpretation by providing functional predictions describing their impact on the cis-regulatory code. However, current tools do not predict RNA-seq expression profiles because of modeling challenges. Here, we introduce Borzoi, a model that learns to predict cell-type-specific and tissue-specific RNA-seq coverage from DNA sequence. Using statistics derived from Borzoi’s predicted coverage, we isolate and accurately score DNA variant effects across multiple layers of regulation, including transcription, splicing and polyadenylation. Evaluated on quantitative trait loci, Borzoi is competitive with and often outperforms state-of-the-art models trained on individual regulatory functions. By applying attribution methods to the derived statistics, we extract cis-regulatory motifs driving RNA expression and post-transcriptional regulation in normal tissues. The wide availability of RNA-seq data across species, conditions and assays profiling specific aspects of regulation emphasizes the potential of this approach to decipher the mapping from DNA sequence to regulatory function.},
	language = {en},
	number = {4},
	urldate = {2025-05-24},
	journal = {Nature Genetics},
	author = {Linder, Johannes and Srivastava, Divyanshi and Yuan, Han and Agarwal, Vikram and Kelley, David R.},
	month = jan,
	year = {2025},
	note = {66 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Important, Podcast},
	pages = {949--961},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/CZUJB3B3/2025-01-08 - Linder et al. - [Borzoi] Predicting RNA-seq coverage from DNA sequence as a unifying model of gene regulation.pdf:application/pdf},
}

@misc{medvedev_biotoken_2025,
	title = {{BioToken} and {BioFM} – {Biologically}-{Informed} {Tokenization} {Enables} {Accurate} and {Efficient} {Genomic} {Foundation} {Models}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.03.27.645711v1},
	doi = {10.1101/2025.03.27.645711},
	abstract = {Genomic variation underlies human phenotypic diversity, disease susceptibility, and evolutionary adaptation. Although large-scale genomic sequencing has transformed our ability to map genetic variation, accurately modeling and interpreting this data remains challenging due to fundamental limitations in existing genomic foundation models (GFMs). Current genomic models typically treat DNA simplistically as linear textual sequences, overlooking critical biological context, such as genomic structural annotations, regulatory elements, and functional contexts central to genomic interpretation. As a result, these models are prone to positional memorization of common sequences, severely limiting their generalization to biologically meaningful tasks. Here, we introduce BioToken, a modular and extendable tokenization framework designed to encode genomic variants and biologically relevant structural annotations directly into genomic representations. By utilizing intrinsic inductive biases, BioToken facilitates meaningful representation learning and generalization across diverse molecular phenotypes, such as gene expression, alternative splicing, and variant pathogenicity prediction. Built on BioToken, our genomic foundation model, BioFM, achieves competitive or superior results relative to specialized models (e.g., Enformer, SpliceTransformer) across a comprehensive suite of genomic benchmarks, including noncoding pathogenicity, expression modulation, sQTL prediction, and long-range genomic interactions. Notably, BioFM achieves state-of-the-art performance with significantly fewer parameters (265M), substantially reducing training costs and computational requirements. Our findings high-light the substantial advantages of integrating biologically-informed inductive biases into genomic foundation modeling, providing a robust and accessible path forward in genomics. We provide our code and model checkpoints to support further research in this direction.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Medvedev, Aleksandr and Viswanathan, Karthik and Kanithi, Praveenkumar and Vishniakov, Kirill and Munjal, Prateek and Christophe, Clément and Pimentel, Marco AF and Rajan, Ronnie and Khan, Shadab},
	month = apr,
	year = {2025},
	note = {1 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.03.27.645711
Section: New Results},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/XZBQLGZE/2025-04-01 - Medvedev et al. - BioToken and BioFM – Biologically-Informed Tokenization Enables Accurate and Efficient Genomic Found.pdf:application/pdf},
}

@misc{liu_ukbiobert_2025,
	title = {[{UKBioBert}; {UKBioFormer}; {UKBioZoi}] {Pre}-training {Genomic} {Language} {Model} with {Variants} for {Better} {Modeling} {Functional} {Genomics}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.02.26.640468v2},
	doi = {10.1101/2025.02.26.640468},
	abstract = {Sequence-to-function models can predict gene expression from sequence data and be used to link genetic information with transcriptomics data to understand regulatory processes and their effects on complex phenotypes. The genomic language models are pre-trained with large-scale DNA sequences and can generate robust representations of these sequences by learning the genomic context. However, few studies can estimate the predictability of gene expression levels and bridge these two classes of models together to explore individualized gene expression prediction. In this manuscript, we propose UKBioBERT as a DNA language model pre-trained with genetic variants from UK BioBank. We demonstrate that UKBioBERT generates informative embeddings capable of identifying gene functions, and improving gene expression prediction in cell lines, thereby enhancing our understanding of gene expression predictability. Building upon these embeddings, we combine UKBioBERT with state-of-the-art sequence-to-function architectures, Enformer and Borzoi, to create UKBioFormer and UKBioZoi. These models exhibit better performance in predicting highly predictable gene expression levels and can be generalized across different cohorts. Furthermore, UKBioFormer effectively captures the relationship between genetic variants and expression variations, enabling in-silico mutation analyses. Collectively, our findings underscore the value of integrating genomic language models and sequence-to-function approaches for advancing functional genomics research.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Liu, Tianyu and Zhang, Xiangyu and Ying, Rex and Zhao, Hongyu},
	month = mar,
	year = {2025},
	note = {0 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.02.26.640468
Section: New Results},
	keywords = {Printed, Podcast, Recall},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/C7DNIDEP/2025-03-02 - Liu et al. - [UKBioBert\; UKBioFormer\; UKBioZoi] Pre-training Genomic Language Model with Variants for Better Mode.pdf:application/pdf;media-2:/Users/meehl.joshua/Zotero/storage/VG5ZEBES/2025-03-02 - Liu et al. - [UKBioBert\; UKBioFormer\; UKBioZoi] Pre-training Genomic Language Model with Variants for Better Mode.pdf:application/pdf},
}

@article{consens_transformers_2025,
	title = {Transformers and genome language models},
	volume = {7},
	copyright = {2025 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-025-01007-9},
	doi = {10.1038/s42256-025-01007-9},
	abstract = {Large language models based on the transformer deep learning architecture have revolutionized natural language processing. Motivated by the analogy between human language and the genome’s biological code, researchers have begun to develop genome language models (gLMs) based on transformers and related architectures. This Review explores the use of transformers and language models in genomics. We survey open questions in genomics amenable to the use of gLMs, and motivate the use of gLMs and the transformer architecture for these problems. We discuss the potential of gLMs for modelling the genome using unsupervised pretraining tasks, specifically focusing on the power of zero- and few-shot learning. We explore the strengths and limitations of the transformer architecture, as well as the strengths and limitations of current gLMs more broadly. Additionally, we contemplate the future of genomic modelling beyond the transformer architecture, based on current trends in research. This Review serves as a guide for computational biologists and computer scientists interested in transformers and language models for genomic data.},
	language = {en},
	number = {3},
	urldate = {2025-05-27},
	journal = {Nature Machine Intelligence},
	author = {Consens, Micaela E. and Dufault, Cameron and Wainberg, Michael and Forster, Duncan and Karimzadeh, Mehran and Goodarzi, Hani and Theis, Fabian J. and Moses, Alan and Wang, Bo},
	month = mar,
	year = {2025},
	note = {24 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast},
	pages = {346--362},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/F6KZ9ENC/2025-03-13 - Consens et al. - Transformers and genome language models.pdf:application/pdf},
}

@misc{naghipourfar_cdsfm_2024,
	title = {[{cdsFM} - {EnCodon}/{DeCodon}] {A} {Suite} of {Foundation} {Models} {Captures} the {Contextual} {Interplay} {Between} {Codons}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.10.10.617568v1},
	doi = {10.1101/2024.10.10.617568},
	abstract = {In the canonical genetic code, many amino acids are assigned more than one codon. Work by us and others has shown that the choice of these synonymous codon is not random, and carries regulatory and functional consequences. Existing protein foundation models ignore this context-dependent role of coding sequence in shaping the protein landscape of the cell. To address this gap, we introduce cdsFM, a suite of codon-resolution large language models, including both EnCodon and DeCodon models, with up to 1B parameters. Pre-trained on 60 million protein-coding sequences from more than 5,000 species, our models effectively learn the relationship between codons and amino acids, recapitualing the overall structure of the genetic code. In addition to outperforming state-of-the-art genomic foundation models in a variety of zero-shot and few-shot learning tasks, the larger pre-trained models were superior in predicting the choice of synonymous codons. To systematically assess the impact of synonymous codon choices on protein expression and our models’ ability to capture these effects, we generated a large dataset measuring overall and surface expression levels of three proteins as a function of changes in their synonymous codons. We showed that our EnCodon models could be readily fine-tuned to predict the contextual consequences of synonymous codon choices. Armed with this knowledge, we applied EnCodon to existing clinical datasets of synonymous variants, and we identified a large number of synonymous codons that are likely pathogenic, several of which we experimentally confirmed in a cellbased model. Together, our findings establish the cdsFM suite as a powerful tool for decoding the complex functional grammar underlying the choice of synonymous codons.},
	language = {en},
	urldate = {2025-05-27},
	publisher = {bioRxiv},
	author = {Naghipourfar, Mohsen and Chen, Siyu and Howard, Mathew K. and Macdonald, Christian B. and Saberi, Ali and Hagen, Timo and Mofrad, Mohammad R. K. and Coyote-Maestas, Willow and Goodarzi, Hani},
	month = oct,
	year = {2024},
	note = {6 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.10.10.617568
Section: New Results},
	keywords = {Printed},
	file = {2024-10-13 - Naghipourfar et al. - [cdsFM - EnCodonDeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Cod:/Users/meehl.joshua/Zotero/storage/PMT8CUN4/2024-10-13 - Naghipourfar et al. - [cdsFM - EnCodonDeCodon] A Suite of Foundation Models Captures the Contextual Interplay Between Cod.pdf:application/pdf},
}

@article{jurenaite_setquence_2024,
	title = {{SetQuence} \& {SetOmic}: {Deep} set transformers for whole genome and exome tumour analysis},
	volume = {235},
	issn = {0303-2647},
	shorttitle = {{SetQuence} \& {SetOmic}},
	url = {https://www.sciencedirect.com/science/article/pii/S0303264723002708},
	doi = {10.1016/j.biosystems.2023.105095},
	abstract = {In oncology, Deep Learning has shown great potential to personalise tasks such as tumour type classification, based on per-patient omics data-sets. Being high dimensional, incorporation of such data in one model is a challenge, often leading to one-dimensional studies and, therefore, information loss. Instead, we first propose relying on non-fixed sets of whole genome or whole exome variant-associated sequences, which can be used for supervised learning of oncology-relevant tasks by our Set Transformer based Deep Neural Network, SetQuence. We optimise this architecture to improve its efficiency. This allows for exploration of not just coding but also non-coding variants, from large datasets. Second, we extend the model to incorporate these representations together with multiple other sources of omics data in a flexible way with SetOmic. Evaluation, using these representations, shows improved robustness and reduced information loss compared to previous approaches, while still being computationally tractable. By means of Explainable Artificial Intelligence methods, our models are able to recapitulate the biological contribution of highly attributed features in the tumours studied. This validation opens the door to novel directions in multi-faceted genome and exome wide biomarker discovery and personalised treatment among other presently clinically relevant tasks.},
	urldate = {2025-05-28},
	journal = {BioSystems},
	author = {Jurenaite, Neringa and León-Periñán, Daniel and Donath, Veronika and Torge, Sunna and Jäkel, René},
	month = jan,
	year = {2024},
	note = {4 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {105095},
	file = {Accepted Version:/Users/meehl.joshua/Zotero/storage/FLZFTRF2/2024-01-01 - Jurenaite et al. - SetQuence & SetOmic Deep set transformers for whole genome and exome tumour analysis.pdf:application/pdf;ScienceDirect Snapshot:/Users/meehl.joshua/Zotero/storage/AW65ASE7/S0303264723002708.html:text/html},
}

@misc{lee_g2pt_2025,
	title = {[{G2PT}] {A} genotype-phenotype transformer to assess and explain polygenic risk},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2024.10.23.619940v2},
	doi = {10.1101/2024.10.23.619940},
	abstract = {Genome-wide association studies have linked millions of genetic variants to biomedical phenotypes, but their utility has been limited by lack of mechanistic understanding and widespread epistatic interactions. Recently, Transformer models have emerged as a powerful machine learning architecture with potential to address these and other challenges. Accordingly, here we introduce the Genotype-to-Phenotype Transformer (G2PT), a framework for modeling hierarchical information flow among variants, genes, multigenic systems, and phenotypes. As proof-of-concept, we use G2PT to model the genetics of TG/HDL (triglycerides to high-density lipoprotein cholesterol), an indicator of metabolic health. G2PT predicts this trait via attention to 1,395 variants underlying at least 20 systems, including immune response and cholesterol transport, with accuracy exceeding state-of-the-art. It implicates 40 epistatic interactions, including epistasis between APOA4 and CETP in phospholipid transfer, a target pathway for cholesterol modification. This work positions hierarchical graph transformers as a next-generation approach to polygenic risk.},
	language = {en},
	urldate = {2025-05-28},
	publisher = {bioRxiv},
	author = {Lee, Ingoo and Wallace, Zachary S. and Wang, Yuqi and Park, Sungjoon and Nam, Hojung and Majithia, Amit R. and Ideker, Trey},
	month = apr,
	year = {2025},
	note = {1 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.10.23.619940
Section: New Results},
	keywords = {Printed, Podcast, Recall},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/2ZPTDM6K/2025-04-11 - Lee et al. - [G2PT] A genotype-phenotype transformer to assess and explain polygenic risk.pdf:application/pdf},
}

@misc{zhou_dnabert-2_2024,
	title = {{DNABERT}-2: {Efficient} {Foundation} {Model} and {Benchmark} {For} {Multi}-{Species} {Genome}},
	shorttitle = {{DNABERT}-2},
	url = {http://arxiv.org/abs/2306.15006},
	doi = {10.48550/arXiv.2306.15006},
	abstract = {Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates \$36\$ distinct datasets across \$9\$ tasks, with input lengths ranging from \$70\$ to \$10000\$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with \$21 {\textbackslash}times\$ fewer parameters and approximately \$92 {\textbackslash}times\$ less GPU time in pre-training.},
	urldate = {2025-05-28},
	publisher = {arXiv},
	author = {Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana and Liu, Han},
	month = mar,
	year = {2024},
	note = {arXiv:2306.15006 [q-bio]},
	keywords = {Printed, Important, Podcast, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/2KZP5KPQ/2024-03-18 - Zhou et al. - DNABERT-2 Efficient Foundation Model and Benchmark For Multi-Species Genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/KUYT79IL/2306.html:text/html},
}

@article{notin_proteingym_2023,
	title = {{ProteinGym}: {Large}-{Scale} {Benchmarks} for {Protein} {Fitness} {Prediction} and {Design}},
	volume = {36},
	shorttitle = {{ProteinGym}},
	url = {https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html},
	language = {en},
	urldate = {2025-05-28},
	journal = {Advances in Neural Information Processing Systems},
	author = {Notin, Pascal and Kollasch, Aaron and Ritter, Daniel and van Niekerk, Lood and Paul, Steffanie and Spinner, Han and Rollins, Nathan and Shaw, Ada and Orenbuch, Rose and Weitzman, Ruben and Frazer, Jonathan and Dias, Mafalda and Franceschi, Dinko and Gal, Yarin and Marks, Debora},
	month = dec,
	year = {2023},
	keywords = {Printed, Read},
	pages = {64331--64379},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/G7F5ZNHE/2023-12-15 - Notin et al. - ProteinGym Large-Scale Benchmarks for Protein Fitness Prediction and Design.pdf:application/pdf},
}

@article{xu_deeprank-gnn-esm_2024,
	title = {{DeepRank}-{GNN}-esm: a graph neural network for scoring protein–protein models using protein language model},
	volume = {4},
	issn = {2635-0041},
	shorttitle = {{DeepRank}-{GNN}-esm},
	url = {https://doi.org/10.1093/bioadv/vbad191},
	doi = {10.1093/bioadv/vbad191},
	abstract = {Protein–Protein interactions (PPIs) play critical roles in numerous cellular processes. By modelling the 3D structures of the correspond protein complexes valuable insights can be obtained, providing, e.g. starting points for drug and protein design. One challenge in the modelling process is however the identification of near-native models from the large pool of generated models. To this end we have previously developed DeepRank-GNN, a graph neural network that integrates structural and sequence information to enable effective pattern learning at PPI interfaces. Its main features are related to the Position Specific Scoring Matrices (PSSMs), which are computationally expensive to generate, significantly limits the algorithm's usability.We introduce here DeepRank-GNN-esm that includes as additional features protein language model embeddings from the ESM-2 model. We show that the ESM-2 embeddings can actually replace the PSSM features at no cost in-, or even better performance on two PPI-related tasks: scoring docking poses and detecting crystal artifacts. This new DeepRank version bypasses thus the need of generating PSSM, greatly improving the usability of the software and opening new application opportunities for systems for which PSSM profiles cannot be obtained or are irrelevant (e.g. antibody-antigen complexes).DeepRank-GNN-esm is freely available from https://github.com/DeepRank/DeepRank-GNN-esm.},
	number = {1},
	urldate = {2025-06-05},
	journal = {Bioinformatics Advances},
	author = {Xu, Xiaotong and Bonvin, Alexandre M J J},
	month = jan,
	year = {2024},
	note = {21 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {vbad191},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/S3N65UGE/2024-01-05 - Xu and Bonvin - DeepRank-GNN-esm a graph neural network for scoring protein–protein models using protein language m.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/3C5TQCJX/7511844.html:text/html},
}

@article{li_cgmega_2024,
	title = {{CGMega}: explainable graph neural network framework with attention mechanisms for cancer gene module dissection},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	shorttitle = {{CGMega}},
	url = {https://www.nature.com/articles/s41467-024-50426-6},
	doi = {10.1038/s41467-024-50426-6},
	abstract = {Cancer is rarely the straightforward consequence of an abnormality in a single gene, but rather reflects a complex interplay of many genes, represented as gene modules. Here, we leverage the recent advances of model-agnostic interpretation approach and develop CGMega, an explainable and graph attention-based deep learning framework to perform cancer gene module dissection. CGMega outperforms current approaches in cancer gene prediction, and it provides a promising approach to integrate multi-omics information. We apply CGMega to breast cancer cell line and acute myeloid leukemia (AML) patients, and we uncover the high-order gene module formed by ErbB family and tumor factors NRG1, PPM1A and DLG2. We identify 396 candidate AML genes, and observe the enrichment of either known AML genes or candidate AML genes in a single gene module. We also identify patient-specific AML genes and associated gene modules. Together, these results indicate that CGMega can be used to dissect cancer gene modules, and provide high-order mechanistic insights into cancer development and heterogeneity.},
	language = {en},
	number = {1},
	urldate = {2025-06-05},
	journal = {Nature Communications},
	author = {Li, Hao and Han, Zebei and Sun, Yu and Wang, Fu and Hu, Pengzhen and Gao, Yuang and Bai, Xuemei and Peng, Shiyu and Ren, Chao and Xu, Xiang and Liu, Zeyu and Chen, Hebing and Yang, Yang and Bo, Xiaochen},
	month = jul,
	year = {2024},
	note = {33 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast},
	pages = {5997},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/NP5DWLR5/2024-07-17 - Li et al. - CGMega explainable graph neural network framework with attention mechanisms for cancer gene module.pdf:application/pdf},
}

@article{yuan_linger_2025,
	title = {[{LINGER}] {Inferring} gene regulatory networks from single-cell multiome data using atlas-scale external data},
	volume = {43},
	copyright = {2024 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-024-02182-7},
	doi = {10.1038/s41587-024-02182-7},
	abstract = {Existing methods for gene regulatory network (GRN) inference rely on gene expression data alone or on lower resolution bulk data. Despite the recent integration of chromatin accessibility and RNA sequencing data, learning complex mechanisms from limited independent data points still presents a daunting challenge. Here we present LINGER (Lifelong neural network for gene regulation), a machine-learning method to infer GRNs from single-cell paired gene expression and chromatin accessibility data. LINGER incorporates atlas-scale external bulk data across diverse cellular contexts and prior knowledge of transcription factor motifs as a manifold regularization. LINGER achieves a fourfold to sevenfold relative increase in accuracy over existing methods and reveals a complex regulatory landscape of genome-wide association studies, enabling enhanced interpretation of disease-associated variants and genes. Following the GRN inference from reference single-cell multiome data, LINGER enables the estimation of transcription factor activity solely from bulk or single-cell gene expression data, leveraging the abundance of available gene expression data to identify driver regulators from case-control studies.},
	language = {en},
	number = {2},
	urldate = {2025-06-09},
	journal = {Nature Biotechnology},
	author = {Yuan, Qiuyue and Duren, Zhana},
	month = apr,
	year = {2025},
	note = {57 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast, Recall},
	pages = {247--257},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/HPBMHBML/2025-04-12 - Yuan and Duren - [LINGER] Inferring gene regulatory networks from single-cell multiome data using atlas-scale externa.pdf:application/pdf},
}

@article{gao_high-ppi_2023,
	title = {[{HIGH}-{PPI}] {Hierarchical} graph learning for protein–protein interaction},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-36736-1},
	doi = {10.1038/s41467-023-36736-1},
	abstract = {Protein-Protein Interactions (PPIs) are fundamental means of functions and signalings in biological systems. The massive growth in demand and cost associated with experimental PPI studies calls for computational tools for automated prediction and understanding of PPIs. Despite recent progress, in silico methods remain inadequate in modeling the natural PPI hierarchy. Here we present a double-viewed hierarchical graph learning model, HIGH-PPI, to predict PPIs and extrapolate the molecular details involved. In this model, we create a hierarchical graph, in which a node in the PPI network (top outside-of-protein view) is a protein graph (bottom inside-of-protein view). In the bottom view, a group of chemically relevant descriptors, instead of the protein sequences, are used to better capture the structure-function relationship of the protein. HIGH-PPI examines both outside-of-protein and inside-of-protein of the human interactome to establish a robust machine understanding of PPIs. This model demonstrates high accuracy and robustness in predicting PPIs. Moreover, HIGH-PPI can interpret the modes of action of PPIs by identifying important binding and catalytic sites precisely. Overall, “HIGH-PPI [https://github.com/zqgao22/HIGH-PPI]” is a domain-knowledge-driven and interpretable framework for PPI prediction studies.},
	language = {en},
	number = {1},
	urldate = {2025-06-09},
	journal = {Nature Communications},
	author = {Gao, Ziqi and Jiang, Chenran and Zhang, Jiawen and Jiang, Xiaosen and Li, Lanqing and Zhao, Peilin and Yang, Huanming and Huang, Yong and Li, Jia},
	month = feb,
	year = {2023},
	note = {139 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast, Read},
	pages = {1093},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/FD89JZZA/2023-02-25 - Gao et al. - [HIGH-PPI] Hierarchical graph learning for protein–protein interaction.pdf:application/pdf},
}

@article{lin_deepvariant_2022,
	title = {[{DeepVariant}] {Comparison} of {GATK} and {DeepVariant} by trio sequencing},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-05833-4},
	doi = {10.1038/s41598-022-05833-4},
	abstract = {While next-generation sequencing (NGS) has transformed genetic testing, it generates large quantities of noisy data that require a significant amount of bioinformatics to generate useful interpretation. The accuracy of variant calling is therefore critical. Although GATK HaplotypeCaller is a widely used tool for this purpose, newer methods such as DeepVariant have shown higher accuracy in assessments of gold-standard samples for whole-genome sequencing (WGS) and whole-exome sequencing (WES), but a side-by-side comparison on clinical samples has not been performed. Trio WES was used to compare GATK (4.1.2.0) HaplotypeCaller and DeepVariant (v0.8.0). The performance of the two pipelines was evaluated according to the Mendelian error rate, transition-to-transversion (Ti/Tv) ratio, concordance rate, and pathological variant detection rate. Data from 80 trios were analyzed. The Mendelian error rate of the 77 biological trios calculated from the data by DeepVariant (3.09 ± 0.83\%) was lower than that calculated from the data by GATK (5.25 ± 0.91\%) (p {\textless} 0.001). DeepVariant also yielded a higher Ti/Tv ratio (2.38 ± 0.02) than GATK (2.04 ± 0.07) (p {\textless} 0.001), suggesting that DeepVariant proportionally called more true positives. The concordance rate between the 2 pipelines was 88.73\%. Sixty-three disease-causing variants were detected in the 80 trios. Among them, DeepVariant detected 62 variants, and GATK detected 61 variants. The one variant called by DeepVariant but not GATK HaplotypeCaller might have been missed by GATK HaplotypeCaller due to low coverage. OTC exon 2 (139 bp) deletion was not detected by either method. Mendelian error rate calculation is an effective way to evaluate variant callers. By this method, DeepVariant outperformed GATK, while the two pipelines performed equally in other parameters.},
	language = {en},
	number = {1},
	urldate = {2025-06-09},
	journal = {Scientific Reports},
	author = {Lin, Yi-Lin and Chang, Pi-Chuan and Hsu, Ching and Hung, Miao-Zi and Chien, Yin-Hsiu and Hwu, Wuh-Liang and Lai, FeiPei and Lee, Ni-Chung},
	month = feb,
	year = {2022},
	note = {48 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast, Read},
	pages = {1809},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/5U8ARZ5W/2022-02-02 - Lin et al. - [DeepVariant] Comparison of GATK and DeepVariant by trio sequencing.pdf:application/pdf},
}

@article{poplin_deepvariant_2018,
	title = {[{DeepVariant}] {A} universal {SNP} and small-indel variant caller using deep neural networks},
	volume = {36},
	copyright = {2018 Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.4235},
	doi = {10.1038/nbt.4235},
	abstract = {DeepVariant uses convolutional neural networks to improve the accuracy of variant calling.},
	language = {en},
	number = {10},
	urldate = {2025-06-09},
	journal = {Nature Biotechnology},
	author = {Poplin, Ryan and Chang, Pi-Chuan and Alexander, David and Schwartz, Scott and Colthurst, Thomas and Ku, Alexander and Newburger, Dan and Dijamco, Jojo and Nguyen, Nam and Afshar, Pegah T. and Gross, Sam S. and Dorfman, Lizzie and McLean, Cory Y. and DePristo, Mark A.},
	month = sep,
	year = {2018},
	note = {1350 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Important, Read},
	pages = {983--987},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/EDS63RVP/2018-09-24 - Poplin et al. - [DeepVariant] A universal SNP and small-indel variant caller using deep neural networks.pdf:application/pdf},
}

@article{kesimoglu_graf_2024,
	title = {{GRAF}: {Graph} {Attention}-aware {Fusion} {Networks}},
	volume = {14},
	issn = {2045-2322},
	shorttitle = {{GRAF}},
	url = {http://arxiv.org/abs/2303.16781},
	doi = {10.1038/s41598-024-78555-4},
	abstract = {A large number of real-world networks include multiple types of nodes and edges. Graph Neural Network (GNN) emerged as a deep learning framework to generate node and graph embeddings for downstream machine learning tasks. However, popular GNN-based architectures operate on single homogeneous networks. Enabling them to work on multiple networks brings additional challenges due to the heterogeneity of the networks and the multiplicity of the existing associations. In this study, we present a computational approach named GRAF (Graph Attention-aware Fusion Networks) utilizing GNN-based approaches on multiple networks with the help of attention mechanisms and network fusion. Using attention-based neighborhood aggregation, GRAF learns the importance of each neighbor per node (called node-level attention) followed by the importance of association (called association-level attention). Then, GRAF processes a network fusion step weighing each edge according to learned node- and association-level attentions. Considering that the fused network could be a highly dense network with many weak edges depending on the given input networks, we included an edge elimination step with respect to edges' weights. Finally, GRAF utilizes Graph Convolutional Network (GCN) on the fused network and incorporates node features on graph-structured data for a node classification or a similar downstream task. To demonstrate GRAF's generalizability, we applied it to four datasets from different domains and observed that GRAF outperformed or was on par with the baselines, state-of-the-art methods, and its own variations for each node classification task. Source code for our tool is publicly available at https://github.com/bozdaglab/GRAF .},
	number = {1},
	urldate = {2025-06-10},
	journal = {Scientific Reports},
	author = {Kesimoglu, Ziynet Nesibe and Bozdag, Serdar},
	month = nov,
	year = {2024},
	note = {2 citations (Crossref/DOI) [2025-10-22]
arXiv:2303.16781 [cs]},
	keywords = {Printed, Podcast},
	pages = {29119},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/9JPG92LV/2024-11-24 - Kesimoglu and Bozdag - GRAF Graph Attention-aware Fusion Networks.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/V2KMIF88/2303.html:text/html},
}

@misc{holur_rde-dna2vec_2024,
	title = {[{RDE}-{DNA2vec}] {Embed}-{Search}-{Align}: {DNA} {Sequence} {Alignment} using {Transformer} {Models}},
	shorttitle = {Embed-{Search}-{Align}},
	url = {http://arxiv.org/abs/2309.11087},
	doi = {10.48550/arXiv.2309.11087},
	abstract = {DNA sequence alignment, an important genomic task, involves assigning short DNA reads to the most probable locations on an extensive reference genome. Conventional methods tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have encoded DNA sequences into vectors using Transformers and have shown promising results in tasks involving classification of short DNA sequences. Performance at sequence classification tasks does not, however, guarantee sequence alignment, where it is necessary to conduct a genome-wide search to align every read successfully, a significantly longer-range task by comparison. We bridge this gap by developing a “Embed-Search-Align” (ESA) framework, where a novel Reference-Free DNA Embedding (RDE ) Transformer model generates vector embeddings of reads and fragments of the reference in a shared vector space; read-fragment distance metric is then used as a surrogate for sequence similarity. ESA introduces: (1) Contrastive loss for self-supervised training of DNA sequence representations, facilitating rich reference-free, sequence-level embeddings, and (2) a DNA vector store to enable search across fragments on a global scale. RDE is 99\% accurate when aligning 250-length reads onto a human reference genome of 3 gigabases (single-haploid), rivaling conventional algorithmic sequence alignment methods such as Bowtie and BWA-Mem. RDE far exceeds the performance of 6 recent DNATransformer model baselines such as Nucleotide Transformer, Hyena-DNA, and shows task transfer across chromosomes and species.},
	language = {en},
	urldate = {2025-06-17},
	publisher = {arXiv},
	author = {Holur, Pavan and Enevoldsen, K. C. and Rajesh, Shreyas and Mboning, Lajoyce and Georgiou, Thalia and Bouchard, Louis-S. and Pellegrini, Matteo and Roychowdhury, Vwani},
	month = dec,
	year = {2024},
	note = {arXiv:2309.11087 [q-bio]},
	keywords = {Printed, Podcast},
	file = {PDF:/Users/meehl.joshua/Zotero/storage/SQHD88SR/2024-12-05 - Holur et al. - [RDE-DNA2vec] Embed-Search-Align DNA Sequence Alignment using Transformer Models.pdf:application/pdf},
}

@article{bunne_how_2024,
	title = {How to build the virtual cell with artificial intelligence: {Priorities} and opportunities},
	volume = {187},
	issn = {0092-8674, 1097-4172},
	shorttitle = {How to build the virtual cell with artificial intelligence},
	url = {https://www.cell.com/cell/abstract/S0092-8674(24)01332-1},
	doi = {10.1016/j.cell.2024.11.015},
	language = {English},
	number = {25},
	urldate = {2025-06-17},
	journal = {Cell},
	author = {Bunne, Charlotte and Roohani, Yusuf and Rosen, Yanay and Gupta, Ankit and Zhang, Xikun and Roed, Marcel and Alexandrov, Theo and AlQuraishi, Mohammed and Brennan, Patricia and Burkhardt, Daniel B. and Califano, Andrea and Cool, Jonah and Dernburg, Abby F. and Ewing, Kirsty and Fox, Emily B. and Haury, Matthias and Herr, Amy E. and Horvitz, Eric and Hsu, Patrick D. and Jain, Viren and Johnson, Gregory R. and Kalil, Thomas and Kelley, David R. and Kelley, Shana O. and Kreshuk, Anna and Mitchison, Tim and Otte, Stephani and Shendure, Jay and Sofroniew, Nicholas J. and Theis, Fabian and Theodoris, Christina V. and Upadhyayula, Srigokul and Valer, Marc and Wang, Bo and Xing, Eric and Yeung-Levy, Serena and Zitnik, Marinka and Karaletsos, Theofanis and Regev, Aviv and Lundberg, Emma and Leskovec, Jure and Quake, Stephen R.},
	month = dec,
	year = {2024},
	pmid = {39672099},
	note = {100 citations (Crossref/DOI) [2025-10-22]
Publisher: Elsevier},
	keywords = {Printed, Podcast},
	pages = {7045--7063},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/7FPRS8FK/2024-12-12 - Bunne et al. - How to build the virtual cell with artificial intelligence Priorities and opportunities.pdf:application/pdf},
}

@article{du_gene2vec_2019,
	title = {Gene2vec: distributed representation of genes based on co-expression},
	volume = {20},
	issn = {1471-2164},
	shorttitle = {Gene2vec},
	url = {https://doi.org/10.1186/s12864-018-5370-x},
	doi = {10.1186/s12864-018-5370-x},
	abstract = {Existing functional description of genes are categorical, discrete, and mostly through manual process. In this work, we explore the idea of gene embedding, distributed representation of genes, in the spirit of word embedding.},
	number = {1},
	urldate = {2025-06-27},
	journal = {BMC Genomics},
	author = {Du, Jingcheng and Jia, Peilin and Dai, Yulin and Tao, Cui and Zhao, Zhongming and Zhi, Degui},
	month = feb,
	year = {2019},
	note = {153 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {82},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/K7FUULFI/2019-02-04 - Du et al. - Gene2vec distributed representation of genes based on co-expression.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/A5ECCJ6Y/s12864-018-5370-x.html:text/html},
}

@misc{avsec_alphagenome_2025,
	title = {{AlphaGenome}: {AI} for better understanding the genome},
	shorttitle = {{AlphaGenome}},
	url = {https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/},
	abstract = {Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.},
	language = {en},
	urldate = {2025-06-27},
	journal = {Google DeepMind},
	author = {Avsec, Ziga and Latysheva, Natasha and Cheng, Jun},
	month = jun,
	year = {2025},
	keywords = {Printed, Important, Podcast},
	file = {2025-06-25 - Avsec et al. - AlphaGenome AI for better understanding the genome:/Users/meehl.joshua/Zotero/storage/397U6HN6/2025-06-25 - Avsec et al. - AlphaGenome AI for better understanding the genome.pdf:application/pdf},
}

@misc{marin_bend_2024,
	title = {{BEND}: {Benchmarking} {DNA} {Language} {Models} on biologically meaningful tasks},
	shorttitle = {{BEND}},
	url = {http://arxiv.org/abs/2311.12570},
	doi = {10.48550/arXiv.2311.12570},
	abstract = {The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features. BEND is available at https://github.com/frederikkemarin/BEND.},
	language = {en},
	urldate = {2025-06-30},
	publisher = {arXiv},
	author = {Marin, Frederikke Isa and Teufel, Felix and Horlacher, Marc and Madsen, Dennis and Pultz, Dennis and Winther, Ole and Boomsma, Wouter},
	month = apr,
	year = {2024},
	note = {arXiv:2311.12570 [q-bio]},
	keywords = {Printed, Read},
	file = {PDF:/Users/meehl.joshua/Zotero/storage/PAK9JYZT/2024-04-09 - Marin et al. - BEND Benchmarking DNA Language Models on biologically meaningful tasks.pdf:application/pdf},
}

@article{cheng_dnalongbench_2024,
	title = {{DNALONGBENCH}: {A} {Benchmark} {Suite} {For} {Long}-{Range} {DNA} {Prediction} {Tasks}},
	shorttitle = {{DNALONGBENCH}},
	url = {https://openreview.net/forum?id=opv67PpqLS},
	abstract = {Modeling long-range DNA dependencies is crucial for understanding genome structure and function across a wide range of biological contexts in health and disease. However, effectively capturing the extensive long-range dependencies between DNA sequences, spanning millions of base pairs as seen in tasks such as three-dimensional (3D) chromatin folding, remains a significant challenge. Additionally, a comprehensive benchmark suite for evaluating tasks reliant on long-range dependencies is notably absent. To address this gap, we introduce DNALONGBENCH, a benchmark dataset spanning five important genomics tasks that consider long-range dependencies up to 1 million base pairs: enhancer-target gene interaction, expression quantitative trait loci, 3D genome organization, regulatory sequence activity, and transcription initiation signal. To comprehensively assess DNALONGBENCH, we evaluate the performance of five baseline methods: a task-specific expert model, a convolutional neural network (CNN)-based model, and three fine-tuned DNA foundation models -- HyenaDNA, Caduceus-Ph and Caduceus-PS. We envision DNALONGBENCH having the potential to become a standardized resource that facilitates comprehensive comparisons and rigorous evaluations of emerging DNA sequence-based deep learning models that consider long-range dependencies.},
	language = {en},
	urldate = {2025-07-01},
	author = {Cheng, Wenduo and Song, Zhenqiao and Zhang, Yang and Wang, Shike and Wang, Danqing and Yang, Muyu and Li, Lei and Ma, Jian},
	month = oct,
	year = {2024},
	keywords = {Printed},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/FBTTSJEW/2024-10-04 - Cheng et al. - DNALONGBENCH A Benchmark Suite For Long-Range DNA Prediction Tasks.pdf:application/pdf},
}

@article{trop_genomics_2024,
	title = {The {Genomics} {Long}-{Range} {Benchmark}: {Advancing} {DNA} {Language} {Models}},
	shorttitle = {The {Genomics} {Long}-{Range} {Benchmark}},
	url = {https://openreview.net/forum?id=8O9HLDrmtq},
	abstract = {The advent of language models (LMs) in genomics necessitates benchmarks that can assess models’ capabilities and limitations. In contrast to protein models, DNA LMs can be used to study non-coding regions of the genome and must account for unique challenges, especially interactions across long sequence lengths. However, existing benchmarks for DNA LMs are defined over short sequence datasets and can involve tasks that are often not considered to be biologically meaningful. Here, we present the Human Genomics Long-Range Benchmark (LRB), which focuses on biologically meaningful tasks and supports long-range contexts. We complement our benchmark with fine-tuning recipes that meaningfully improve performance and affect model evaluation. We evaluate DNA LMs across nine compiled human genome tasks and observe that DNA LMs achieve competitive performance relative to supervised baselines on several tasks (e.g., genome annotation), but there remains a significant gap in domains, such as variant effect and gene expression prediction. Additionally, we introduce a visualization tool to examine model performance split by various genomic properties. Lastly, we present methods for context-length extrapolation of transformer-based models that enable studying the effect of context length on DNA LM performance.},
	language = {en},
	urldate = {2025-07-01},
	author = {Trop, Evan and Schiff, Yair and Marroquin, Edgar Mariano and Kao, Chia Hsiang and Gokaslan, Aaron and Polen, McKinley and Shao, Mingyi and Kallala, Aymen and Almeida, Bernardo P. de and Pierrot, Thomas and Li, Yang I. and Kuleshov, Volodymyr},
	month = oct,
	year = {2024},
	keywords = {Printed},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/93EY8JX5/2024-10-04 - Trop et al. - The Genomics Long-Range Benchmark Advancing DNA Language Models.pdf:application/pdf},
}

@article{robson_guanine_2024,
	title = {{GUANinE} v1.0: {Benchmark} {Datasets} for {Genomic} {AI} {Sequence}-to-{Function} {Models}},
	issn = {2692-8205},
	shorttitle = {{GUANinE} v1.0},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10614795/},
	doi = {10.1101/2023.10.12.562113},
	abstract = {Computational genomics increasingly relies on machine learning methods for genome interpretation, and the recent adoption of neural sequence-to-function models highlights the need for rigorous model specification and controlled evaluation, problems familiar to other fields of AI. Research strategies that have greatly benefited other fields — including benchmarking, auditing, and algorithmic fairness — are also needed to advance the field of genomic AI and to facilitate model development. Here we propose a genomic AI benchmark, GUANinE, for evaluating model generalization across a number of distinct genomic tasks. Compared to existing task formulations in computational genomics, GUANinE is large-scale, de-noised, and suitable for evaluating pretrained models. GUANinE v1.0 primarily focuses on functional genomics tasks such as functional element annotation and gene expression prediction, and it also draws upon connections to evolutionary biology through sequence conservation tasks. The current GUANinE tasks provide insight into the performance of existing genomic AI models and non-neural baselines, with opportunities to be refined, revisited, and broadened as the field matures. Finally, the GUANinE benchmark allows us to evaluate new self-supervised T5 models and explore the tradeoffs between tokenization and model performance, while showcasing the potential for self-supervision to complement existing pretraining procedures.},
	urldate = {2025-07-01},
	journal = {bioRxiv},
	author = {Robson, eyes s. and Loannidis, Nilah M.},
	month = mar,
	year = {2024},
	pmid = {37904945},
	pmcid = {PMC10614795},
	note = {6 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Read},
	pages = {2023.10.12.562113},
	file = {Submitted Version:/Users/meehl.joshua/Zotero/storage/S5PNRNPZ/2024-03-07 - Robson and Loannidis - GUANinE v1.0 Benchmark Datasets for Genomic AI Sequence-to-Function Models.pdf:application/pdf},
}

@misc{li_gv-rep_2024,
	title = {{GV}-{Rep}: {A} {Large}-{Scale} {Dataset} for {Genetic} {Variant} {Representation} {Learning}},
	shorttitle = {{GV}-{Rep}},
	url = {http://arxiv.org/abs/2407.16940},
	doi = {10.48550/arXiv.2407.16940},
	abstract = {Genetic variants (GVs) are defined as differences in the DNA sequences among individuals and play a crucial role in diagnosing and treating genetic diseases. The rapid decrease in next generation sequencing cost has led to an exponential increase in patient-level GV data. This growth poses a challenge for clinicians who must efficiently prioritize patient-specific GVs and integrate them with existing genomic databases to inform patient management. To addressing the interpretation of GVs, genomic foundation models (GFMs) have emerged. However, these models lack standardized performance assessments, leading to considerable variability in model evaluations. This poses the question: How effectively do deep learning methods classify unknown GVs and align them with clinically-verified GVs? We argue that representation learning, which transforms raw data into meaningful feature spaces, is an effective approach for addressing both indexing and classification challenges. We introduce a large-scale Genetic Variant dataset, named GV-Rep, featuring variable-length contexts and detailed annotations, designed for deep learning models to learn GV representations across various traits, diseases, tissue types, and experimental contexts. Our contributions are three-fold: (i) Construction of a comprehensive dataset with 7 million records, each labeled with characteristics of the corresponding variants, alongside additional data from 17,548 gene knockout tests across 1,107 cell types, 1,808 variant combinations, and 156 unique clinically verified GVs from real-world patients. (ii) Analysis of the structure and properties of the dataset. (iii) Experimentation of the dataset with pre-trained GFMs. The results show a significant gap between GFMs current capabilities and accurate GV representation. We hope this dataset will help advance genomic deep learning to bridge this gap.},
	urldate = {2025-07-01},
	publisher = {arXiv},
	author = {Li, Zehui and Subasri, Vallijah and Stan, Guy-Bart and Zhao, Yiren and Wang, Bo},
	month = dec,
	year = {2024},
	note = {arXiv:2407.16940 [cs]
version: 2},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/NNI5QP5G/2024-12-05 - Li et al. - GV-Rep A Large-Scale Dataset for Genetic Variant Representation Learning.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/NUE39X2N/2407.html:text/html},
}

@article{monti_evaluation_2024,
	title = {Evaluation of polygenic scoring methods in five biobanks shows larger variation between biobanks than methods and finds benefits of ensemble learning},
	volume = {111},
	issn = {0002-9297, 1537-6605},
	url = {https://www.cell.com/ajhg/abstract/S0002-9297(24)00209-X},
	doi = {10.1016/j.ajhg.2024.06.003},
	language = {English},
	number = {7},
	urldate = {2025-07-03},
	journal = {The American Journal of Human Genetics},
	author = {Monti, Remo and Eick, Lisa and Hudjashov, Georgi and Läll, Kristi and Kanoni, Stavroula and Wolford, Brooke N. and Wingfield, Benjamin and Pain, Oliver and Wharrie, Sophie and Jermy, Bradley and McMahon, Aoife and Hartonen, Tuomo and Heyne, Henrike and Mars, Nina and Lambert, Samuel and Hveem, Kristian and Inouye, Michael and Heel, David A. van and Mägi, Reedik and Marttinen, Pekka and Ripatti, Samuli and Ganna, Andrea and Lippert, Christoph},
	month = jul,
	year = {2024},
	pmid = {38908374},
	note = {24 citations (Crossref/DOI) [2025-10-22]
Publisher: Elsevier},
	keywords = {Printed, Podcast},
	pages = {1431--1447},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/WNBFQ6YM/2024-07-11 - Monti et al. - Evaluation of polygenic scoring methods in five biobanks shows larger variation between biobanks tha.pdf:application/pdf},
}

@misc{camillo_cpgpt_2024,
	title = {{CpGPT}: a {Foundation} {Model} for {DNA} {Methylation}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{CpGPT}},
	url = {https://www.biorxiv.org/content/10.1101/2024.10.24.619766v1},
	doi = {10.1101/2024.10.24.619766},
	abstract = {DNA methylation is a critical epigenetic modification that regulates gene expression and plays a significant role in development and disease processes. Here, we present the Cytosine-phosphate-Guanine Pretrained Transformer (CpGPT), a novel foundation model pretrained on over 1,500 DNA methylation datasets encompassing over 100,000 samples from diverse tissues and conditions. CpGPT leverages an improved transformer architecture to learn comprehensive representations of methylation patterns, allowing it to impute and reconstruct genome-wide methylation profiles from limited input data. By capturing sequence, positional, and epigenetic contexts, CpGPT outperforms specialized models when finetuned for aging-related tasks, including chronological age prediction, mortality risk, and morbidity assessments. The model is highly adaptable across different methylation platforms and tissue types. Furthermore, analysis of sample-specific attention weights enables the identification of the most influential CpG sites for individual predictions. As a foundation model, CpGPT sets a new benchmark for DNA methylation analysis, achieving strong performance in the Biomarkers of Aging Challenge, where it placed second overall in chronological age estimation and first on the public leaderboard in methylation-based mortality prediction.
HighlightsCpGPT is a novel foundation model for DNA methylation analysis, pretrained on over 1,500 datasets encompassing 100,000+ samples.The model demonstrates strong performance in zero-shot tasks including imputation, array conversion, and reference mapping.CpGPT achieves state-of-the-art results in mortality prediction and chronological age estimation.Sample-specific interpretability is enabled through analysis of attention weights.},
	language = {en},
	urldate = {2025-07-05},
	publisher = {bioRxiv},
	author = {Camillo, Lucas Paulo de Lima and Sehgal, Raghav and Armstrong, Jenel and Higgins-Chen, Albert T. and Horvath, Steve and Wang, Bo},
	month = oct,
	year = {2024},
	note = {17 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.10.24.619766
Section: New Results},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/BD6NXKAF/2024-10-29 - Camillo et al. - CpGPT a Foundation Model for DNA Methylation.pdf:application/pdf},
}

@article{chandak_primekg_2023,
	title = {[{PrimeKG}] {Building} a knowledge graph to enable precision medicine},
	volume = {10},
	copyright = {2023 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-023-01960-3},
	doi = {10.1038/s41597-023-01960-3},
	abstract = {Developing personalized diagnostic strategies and targeted treatments requires a deep understanding of disease biology and the ability to dissect the relationship between molecular and genetic factors and their phenotypic consequences. However, such knowledge is fragmented across publications, non-standardized repositories, and evolving ontologies describing various scales of biological organization between genotypes and clinical phenotypes. Here, we present PrimeKG, a multimodal knowledge graph for precision medicine analyses. PrimeKG integrates 20 high-quality resources to describe 17,080 diseases with 4,050,249 relationships representing ten major biological scales, including disease-associated protein perturbations, biological processes and pathways, anatomical and phenotypic scales, and the entire range of approved drugs with their therapeutic action, considerably expanding previous efforts in disease-rooted knowledge graphs. PrimeKG contains an abundance of ‘indications’, ‘contradictions’, and ‘off-label use’ drug-disease edges that lack in other knowledge graphs and can support AI analyses of how drugs affect disease-associated networks. We supplement PrimeKG’s graph structure with language descriptions of clinical guidelines to enable multimodal analyses and provide instructions for continual updates of PrimeKG as new data become available.},
	language = {en},
	number = {1},
	urldate = {2025-07-08},
	journal = {Scientific Data},
	author = {Chandak, Payal and Huang, Kexin and Zitnik, Marinka},
	month = feb,
	year = {2023},
	note = {273 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast, Read},
	pages = {67},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/N8HI9QP5/2023-02-02 - Chandak et al. - [PrimeKG] Building a knowledge graph to enable precision medicine.pdf:application/pdf},
}

@article{mukherjee_embedgem_2024,
	title = {{EmbedGEM}: a framework to evaluate the utility of embeddings for genetic discovery},
	volume = {4},
	shorttitle = {{EmbedGEM}},
	url = {https://dx.doi.org/10.1093/bioadv/vbae135},
	doi = {10.1093/bioadv/vbae135},
	abstract = {AbstractSummary. Machine learning-derived embeddings are a compressed representation of high content data modalities. Embeddings can capture detailed infor},
	language = {en},
	number = {1},
	urldate = {2025-07-11},
	journal = {Bioinformatics Advances},
	author = {Mukherjee, Sumit and McCaw, Zachary R. and Pei, Jingwen and Merkoulovitch, Anna and Soare, Tom and Tandon, Raghav and Amar, David and Somineni, Hari and Klein, Christoph and Satapati, Santhosh and Lloyd, David and Probert, Christopher and Team, Insitro Research and Koller, Daphne and O’Dushlaine, Colm and Karaletsos, Theofanis},
	month = jan,
	year = {2024},
	note = {2 citations (Crossref/DOI) [2025-10-22]
Publisher: Oxford Academic},
	keywords = {Printed, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/INMU3Z95/2024-01-05 - Mukherjee et al. - EmbedGEM a framework to evaluate the utility of embeddings for genetic discovery.pdf:application/pdf},
}

@article{jaganathan_spliceai_2019,
	title = {[{SpliceAI}] {Predicting} {Splicing} from {Primary} {Sequence} with {Deep} {Learning}},
	volume = {176},
	issn = {0092-8674},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867418316295},
	doi = {10.1016/j.cell.2018.12.015},
	abstract = {The splicing of pre-mRNAs into mature transcripts is remarkable for its precision, but the mechanisms by which the cellular machinery achieves such specificity are incompletely understood. Here, we describe a deep neural network that accurately predicts splice junctions from an arbitrary pre-mRNA transcript sequence, enabling precise prediction of noncoding genetic variants that cause cryptic splicing. Synonymous and intronic mutations with predicted splice-altering consequence validate at a high rate on RNA-seq and are strongly deleterious in the human population. De novo mutations with predicted splice-altering consequence are significantly enriched in patients with autism and intellectual disability compared to healthy controls and validate against RNA-seq in 21 out of 28 of these patients. We estimate that 9\%–11\% of pathogenic mutations in patients with rare genetic disorders are caused by this previously underappreciated class of disease variation.},
	number = {3},
	urldate = {2025-07-11},
	journal = {Cell},
	author = {Jaganathan, Kishore and Kyriazopoulou Panagiotopoulou, Sofia and McRae, Jeremy F. and Darbandi, Siavash Fazel and Knowles, David and Li, Yang I. and Kosmicki, Jack A. and Arbelaez, Juan and Cui, Wenwu and Schwartz, Grace B. and Chow, Eric D. and Kanterakis, Efstathios and Gao, Hong and Kia, Amirali and Batzoglou, Serafim and Sanders, Stephan J. and Farh, Kyle Kai-How},
	month = jan,
	year = {2019},
	note = {2156 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Important, Podcast, Read},
	pages = {535--548.e24},
	file = {Full Text:/Users/meehl.joshua/Zotero/storage/RNFB2GEK/2019-01-24 - Jaganathan et al. - [SpliceAI] Predicting Splicing from Primary Sequence with Deep Learning.pdf:application/pdf;ScienceDirect Snapshot:/Users/meehl.joshua/Zotero/storage/7K7VMGER/S0092867418316295.html:text/html},
}

@article{manzo_comparative_2025,
	title = {Comparative {Analysis} of {Deep} {Learning} {Models} for {Predicting} {Causative} {Regulatory} {Variants}},
	issn = {2692-8205},
	doi = {10.1101/2025.05.19.654920},
	abstract = {MOTIVATION: Genome-wide association studies (GWAS) have identified numerous noncoding variants associated with complex human diseases, disorders, and traits. However, resolving the uncertainty between GWAS association and causality remains a significant challenge. The small subset of noncoding GWAS variants with causative effects on gene regulatory elements can only be detected through accurate methods that assess the impact of DNA sequence variation on gene regulatory activity. Deep learning models, such as those based on Convolutional Neural Networks (CNNs) and transformers, have gained prominence in predicting the regulatory effects of genetic variants, particularly in enhancers, by learning patterns from genomic and epigenomic data. Despite their potential, selecting the most suitable model is hindered by the lack of standardized benchmarks, consistent training conditions, and performance evaluation criteria in existing reviews.
RESULTS: This study evaluates state-of-the-art deep learning models for predicting the effects of genetic variants on enhancer activity using nine datasets stemming from MPRA, raQTL, and eQTL experiments, profiling the regulatory impact of 54,859 SNPs across four human cell lines. The results reveal that CNN models, such as TREDNet and SEI, consistently outperform other architectures in predicting the regulatory impact of single-nucleotide polymorphisms (SNPs). However, hybrid CNN-transformer models, such as Borzoi, display superior performance in identifying causal SNPs within a linkage disequilibrium block. While fine-tuning enhances the performance of transformer-based models, it remains insufficient to surpass CNN and hybrid models when evaluated under optimized conditions.},
	language = {eng},
	journal = {bioRxiv: The Preprint Server for Biology},
	author = {Manzo, Gaetano and Borkowski, Kathryn and Ovcharenko, Ivan},
	month = jun,
	year = {2025},
	pmid = {40568119},
	pmcid = {PMC12190767},
	note = {0 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Recall},
	pages = {2025.05.19.654920},
	file = {2025-06-11 - Manzo et al. - Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants:/Users/meehl.joshua/Zotero/storage/83FB7EZC/2025-06-11 - Manzo et al. - Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants.pdf:application/pdf},
}

@article{wu_genome-wide_2024,
	title = {Genome-wide fine-mapping improves identification of causal variants},
	issn = {2693-5015},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11326397/},
	doi = {10.21203/rs.3.rs-4759390/v1},
	abstract = {Fine-mapping refines genotype-phenotype association signals to identify causal variants underlying complex traits. However, current methods typically focus on individual genomic segments without considering the global genetic architecture. Here, we demonstrate the advantages of performing genome-wide fine-mapping (GWFM) and develop methods to facilitate GWFM. In simulations and real data analyses, GWFM outperforms current methods in error control, mapping power and precision, replication rate, and trans-ancestry phenotype prediction. For 48 well-powered traits in the UK Biobank, we identify causal variants that collectively explain 17\% of the SNP-based heritability, and predict that fine-mapping 50\% of that would require 2 million samples on average. We pinpoint a known causal variant, as proof-of-principle, at FTO for body mass index, unveil a hidden secondary variant with evolutionary conservation, and identify new missense causal variants for schizophrenia and Crohn’s disease. Overall, we analyse 600 complex traits with 13 million SNPs, highlighting the efficacy of GWFM with functional annotations.},
	urldate = {2025-07-11},
	journal = {Research Square},
	author = {Wu, Yang and Zheng, Zhili and Thibaut2, Loic and Goddard, Michael E. and Wray, Naomi R. and Visscher, Peter M. and Zeng, Jian},
	month = aug,
	year = {2024},
	pmid = {39149449},
	pmcid = {PMC11326397},
	note = {0 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast},
	pages = {rs.3.rs--4759390},
	file = {PubMed Central Full Text PDF:/Users/meehl.joshua/Zotero/storage/4NAX9JFI/2024-08-07 - Wu et al. - Genome-wide fine-mapping improves identification of causal variants.pdf:application/pdf},
}

@misc{rakowski_mifm_2025,
	title = {[{MIFM}] {Multiple} instance fine-mapping: predicting causal regulatory variants with a deep sequence model},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Multiple instance fine-mapping},
	url = {https://www.medrxiv.org/content/10.1101/2025.06.13.25329551v1},
	doi = {10.1101/2025.06.13.25329551},
	abstract = {Identifying causal genetic variants in a computational manner remains an open problem. Training end-to-end prediction models is not possible without large ground-truth datasets, while results of genome-wide association studies (GWAS) are entangled by linkage disequilibrium (LD), and gene expression datasets do not contain genetic variation at individual-level. Here, we propose Multiple Instance Fine-mapping (MIFM) – a multiple instance learning (MIL) objective to overcome the lack of strong labels by grouping putatively causal variants together based on their LD scores. Using MIFM, we trained a deep classifier on a dataset aggregating over 13, 000 GWAS to predict causal variants based on their underlying DNA sequences. We validated variants prioritized by MIFM by constructing polygenic risk scores which transferred better to different target ancestries. Furthermore, we demonstrated how MIFM can be used to disentangle effect sizes of highly-correlated variants to better fine-map GWAS results.
Author summary Genome-wide association studies have identified tens of thousands genetic variants associated with traits or diseases. However, the majority of identified variants is only spuriously correlated with the phenotype of interest, having no causal effect on it. Instead, these variants are often inherited together with nearby biologically causal variants, thus creating the spurious associations. Fine-mapping, i.e., predicting which variants are causal, is crucial for downstream tasks, such as uncovering the biological mechanisms affecting the phenotype or robustly identifying individuals with high genetic risk of a disease. While most fine-mapping methods are based on the available association statistics or functional annotations of genetic regions, it should be possible to identify causal variants based on their neighboring DNA sequences. However, training a standard machine learning classifier for that task is obstructed by the scarcity of strong, ground-truth labels. Here, we proposed a method to train sequence models predicting variant causality using weakly-labeled data. We trained a model on a large set of associated variants, and demonstrated its utility by improving cross-ancestry predictions of genetic risk, or disentangling the effect sizes of highly correlated variants.},
	language = {en},
	urldate = {2025-07-11},
	publisher = {medRxiv},
	author = {Rakowski, Alexander and Lippert, Christoph},
	month = jun,
	year = {2025},
	note = {0 citations (Crossref/DOI) [2025-10-22]
Pages: 2025.06.13.25329551},
	keywords = {Printed, Podcast, Recall},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/AI64IU6X/2025-06-14 - Rakowski and Lippert - [MIFM] Multiple instance fine-mapping predicting causal regulatory variants with a deep sequence mo.pdf:application/pdf},
}

@article{lin_cluster_2024,
	title = {Cluster effect for {SNP}–{SNP} interaction pairs for predicting complex traits},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-66311-7},
	doi = {10.1038/s41598-024-66311-7},
	abstract = {Single nucleotide polymorphism (SNP) interactions are the key to improving polygenic risk scores. Previous studies reported several significant SNP–SNP interaction pairs that shared a common SNP to form a cluster, but some identified pairs might be false positives. This study aims to identify factors associated with the cluster effect of false positivity and develop strategies to enhance the accuracy of SNP–SNP interactions. The results showed the cluster effect is a major cause of false-positive findings of SNP–SNP interactions. This cluster effect is due to high correlations between a causal pair and null pairs in a cluster. The clusters with a hub SNP with a significant main effect and a large minor allele frequency (MAF) tended to have a higher false-positive rate. In addition, peripheral null SNPs in a cluster with a small MAF tended to enhance false positivity. We also demonstrated that using the modified significance criterion based on the 3 p-value rules and the bootstrap approach (3pRule + bootstrap) can reduce false positivity and maintain high true positivity. In addition, our results also showed that a pair without a significant main effect tends to have weak or no interaction. This study identified the cluster effect and suggested using the 3pRule + bootstrap approach to enhance SNP–SNP interaction detection accuracy.},
	language = {en},
	number = {1},
	urldate = {2025-07-12},
	journal = {Scientific Reports},
	author = {Lin, Hui-Yi and Mazumder, Harun and Sarkar, Indrani and Huang, Po-Yu and Eeles, Rosalind A. and Kote-Jarai, Zsofia and Muir, Kenneth R. and Schleutker, Johanna and Pashayan, Nora and Batra, Jyotsna and Neal, David E. and Nielsen, Sune F. and Nordestgaard, Børge G. and Grönberg, Henrik and Wiklund, Fredrik and MacInnis, Robert J. and Haiman, Christopher A. and Travis, Ruth C. and Stanford, Janet L. and Kibel, Adam S. and Cybulski, Cezary and Khaw, Kay-Tee and Maier, Christiane and Thibodeau, Stephen N. and Teixeira, Manuel R. and Cannon-Albright, Lisa and Brenner, Hermann and Kaneva, Radka and Pandha, Hardev and Park, Jong Y.},
	month = aug,
	year = {2024},
	note = {0 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast},
	pages = {18677},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/I92KLVAR/2024-08-12 - Lin et al. - Cluster effect for SNP–SNP interaction pairs for predicting complex traits.pdf:application/pdf},
}

@misc{georgantas_delphi_2024,
	title = {Delphi: {A} {Deep}-learning {Method} for {Polygenic} {Risk} {Prediction}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {Delphi},
	url = {https://www.medrxiv.org/content/10.1101/2024.04.19.24306079v2},
	doi = {10.1101/2024.04.19.24306079},
	abstract = {Polygenic risk scores (PRS) are relative measures of an individual’s genetic propensity to a particular trait or disease. Most PRS methods assume that mutation effects scale linearly with the number of alleles and are constant across individuals. While these assumptions simplify computation, they increase error, particularly for less-represented racial groups. We developed and provide Delphi (deep learning for phenotype inference), a deep-learning method that relaxes these assumptions to produce more predictive PRS. In contrast to other methods, Delphi can integrate up to hundreds of thousands of SNPs as input. We compare our results to a standard, linear PRS model, lasso regression, and a gradient-boosted trees-based method. We show that deep learning can be an effective approach to genetic risk prediction. We report a relative increase in the percentage variance explained compared to the state-of-the-art by 11.4\% for body mass index, 18.9\% for systolic blood pressure, 7.5\% for LDL, 35\% for C-reactive protein, 16.2\% for height, 29.6 \% for pulse rate; in addition, Delphi provides 2\% absolute explained variance for blood glucose while other tested methods were non-predictive. Furthermore, we show that Delphi tends to increase the weight of high-effect mutations. This work demonstrates an effective deep learning method for modeling genetic risk that also showed to generalize well when evaluated on individuals from non-European ancestries.},
	language = {en},
	urldate = {2025-07-12},
	publisher = {medRxiv},
	author = {Georgantas, Costa and Kutalik, Zoltán and Richiardi, Jonas},
	month = jul,
	year = {2024},
	note = {3 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.04.19.24306079},
	keywords = {Printed, Podcast},
	file = {PDF:/Users/meehl.joshua/Zotero/storage/YHXTGSCR/2024-07-02 - Georgantas et al. - Delphi A Deep-learning Method for Polygenic Risk Prediction.pdf:application/pdf},
}

@article{an_lungenn_2024,
	title = {[{LungENN}] {Systematic} identification of pathogenic variants of non-small cell lung cancer in the promoters of {DNA}-damage repair genes},
	volume = {110},
	issn = {2352-3964},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11663791/},
	doi = {10.1016/j.ebiom.2024.105480},
	abstract = {Background
Deficiency in DNA-damage repair (DDR) genes, often due to disruptive coding variants, is linked to higher cancer risk. Our previous study has revealed the association between rare loss-of-function variants in DDR genes and the risk of lung cancer. However, it is still challenging to study the predisposing role of rare regulatory variants of these genes.

Methods
Based on whole-genome sequencing data from 2984 patients with non-small cell lung cancer (NSCLC) and 3020 controls, we performed massively parallel reporter assays on 1818 rare variants located in the promoters of DDR genes. Pathway- or gene-level burden analyses were performed using Firth’s logistic regression or generalized linear model.

Findings
We identified 750 rare functional regulatory variants (frVars) that showed allelic differences in transcriptional activity within the promoter regions of DDR genes. Interestingly, the burden of frVars was significantly elevated in cases (odds ratio [OR] = 1.17, p = 0.026), whereas the burden of variants prioritized solely based on bioinformatics annotation was comparable between cases and controls (OR = 1.04, p = 0.549). Among the frVars, 297 were down-regulated transcriptional activity (dr-frVars) and 453 were up-regulated transcriptional activity (ur-frVars); especially, dr-frVars (OR = 1.30, p = 0.008) rather than ur-frVars (OR = 1.06, p = 0.495) were significantly associated with risk of NSCLC. Individuals with NSCLC carried more dr-frVars from Fanconi anemia, homologous recombination, and nucleotide excision repair pathways. In addition, we identified seven genes (i.e., BRCA2, GTF2H1, DDB2, BLM, ALKBH2, APEX1, and RAD51B) with promoter dr-frVars that were associated with lung cancer susceptibility.

Interpretation
Our findings indicate that functional promoter variants in DDR genes, in addition to protein-truncating variants, can be pathogenic and contribute to lung cancer susceptibility.

Funding
10.13039/501100001809National Natural Science Foundation of China, Youth Foundation of Jiangsu Province, Research Unit of Prospective Cohort of Cardiovascular Diseases and Cancer of Chinese Academy of Medical Sciences, and 10.13039/501100004608Natural Science Foundation of Jiangsu Province.},
	urldate = {2025-07-12},
	journal = {eBioMedicine},
	author = {An, Mingxing and Chen, Congcong and Xiang, Jun and Li, Yang and Qiu, Pinyu and Tang, Yiru and Liu, Xinyue and Gu, Yayun and Qin, Na and He, Yuanlin and Zhu, Meng and Jiang, Yue and Dai, Juncheng and Jin, Guangfu and Ma, Hongxia and Wang, Cheng and Hu, Zhibin and Shen, Hongbing},
	month = dec,
	year = {2024},
	pmid = {39631147},
	pmcid = {PMC11663791},
	note = {2 citations (Semantic Scholar/DOI) [2025-10-22]
2 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast},
	pages = {105480},
	file = {PubMed Central Full Text PDF:/Users/meehl.joshua/Zotero/storage/GSTZTEE7/2024-12-03 - An et al. - [LungENN] Systematic identification of pathogenic variants of non-small cell lung cancer in the prom.pdf:application/pdf},
}

@article{cao_glue_2022,
	title = {[{GLUE}] {Multi}-omics single-cell data integration and regulatory inference with graph-linked embedding},
	volume = {40},
	copyright = {2022 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01284-4},
	doi = {10.1038/s41587-022-01284-4},
	abstract = {Despite the emergence of experimental methods for simultaneous measurement of multiple omics modalities in single cells, most single-cell datasets include only one modality. A major obstacle in integrating omics data from multiple modalities is that different omics layers typically have distinct feature spaces. Here, we propose a computational framework called GLUE (graph-linked unified embedding), which bridges the gap by modeling regulatory interactions across omics layers explicitly. Systematic benchmarking demonstrated that GLUE is more accurate, robust and scalable than state-of-the-art tools for heterogeneous single-cell multi-omics data. We applied GLUE to various challenging tasks, including triple-omics integration, integrative regulatory inference and multi-omics human cell atlas construction over millions of cells, where GLUE was able to correct previous annotations. GLUE features a modular design that can be flexibly extended and enhanced for new analysis tasks. The full package is available online at https://github.com/gao-lab/GLUE.},
	language = {en},
	number = {10},
	urldate = {2025-07-12},
	journal = {Nature Biotechnology},
	author = {Cao, Zhi-Jie and Gao, Ge},
	month = may,
	year = {2022},
	note = {397 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Read},
	pages = {1458--1466},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/M96CQGML/2022-05-02 - Cao and Gao - [GLUE] Multi-omics single-cell data integration and regulatory inference with graph-linked embedding.pdf:application/pdf},
}

@article{hoffmann_needl_2024,
	title = {[{NeEDL}] {Network} medicine-based epistasis detection in complex diseases: ready for quantum computing},
	volume = {52},
	issn = {0305-1048},
	shorttitle = {Network medicine-based epistasis detection in complex diseases},
	url = {https://doi.org/10.1093/nar/gkae697},
	doi = {10.1093/nar/gkae697},
	abstract = {Most heritable diseases are polygenic. To comprehend the underlying genetic architecture, it is crucial to discover the clinically relevant epistatic interactions (EIs) between genomic single nucleotide polymorphisms (SNPs) (1–3). Existing statistical computational methods for EI detection are mostly limited to pairs of SNPs due to the combinatorial explosion of higher-order EIs. With NeEDL (network-based epistasis detection via local search), we leverage network medicine to inform the selection of EIs that are an order of magnitude more statistically significant compared to existing tools and consist, on average, of five SNPs. We further show that this computationally demanding task can be substantially accelerated once quantum computing hardware becomes available. We apply NeEDL to eight different diseases and discover genes (affected by EIs of SNPs) that are partly known to affect the disease, additionally, these results are reproducible across independent cohorts. EIs for these eight diseases can be interactively explored in the Epistasis Disease Atlas (https://epistasis-disease-atlas.com). In summary, NeEDL demonstrates the potential of seamlessly integrated quantum computing techniques to accelerate biomedical research. Our network medicine approach detects higher-order EIs with unprecedented statistical and biological evidence, yielding unique insights into polygenic diseases and providing a basis for the development of improved risk scores and combination therapies.},
	number = {17},
	urldate = {2025-07-12},
	journal = {Nucleic Acids Research},
	author = {Hoffmann, Markus and Poschenrieder, Julian M and Incudini, Massimiliano and Baier, Sylvie and Fritz, Amelie and Maier, Andreas and Hartung, Michael and Hoffmann, Christian and Trummer, Nico and Adamowicz, Klaudia and Picciani, Mario and Scheibling, Evelyn and Harl, Maximilian V and Lesch, Ingmar and Frey, Hunor and Kayser, Simon and Wissenberg, Paul and Schwartz, Leon and Hafner, Leon and Acharya, Aakriti and Hackl, Lena and Grabert, Gordon and Lee, Sung-Gwon and Cho, Gyuhyeok and Cloward, Matthew E and Jankowski, Jakub and Lee, Hye Kyung and Tsoy, Olga and Wenke, Nina and Pedersen, Anders Gorm and Bønnelykke, Klaus and Mandarino, Antonio and Melograna, Federico and Schulz, Laura and Climente-González, Héctor and Wilhelm, Mathias and Iapichino, Luigi and Wienbrandt, Lars and Ellinghaus, David and Van Steen, Kristel and Grossi, Michele and Furth, Priscilla A and Hennighausen, Lothar and Di Pierro, Alessandra and Baumbach, Jan and Kacprowski, Tim and List, Markus and Blumenthal, David B},
	month = sep,
	year = {2024},
	note = {6 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast},
	pages = {10144--10160},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/XNB4W3RH/2024-09-23 - Hoffmann et al. - [NeEDL] Network medicine-based epistasis detection in complex diseases ready for quantum computing.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/59MEXEZA/gkae697.html:text/html},
}

@article{li_mogcn_2022,
	title = {{MoGCN}: {A} {Multi}-{Omics} {Integration} {Method} {Based} on {Graph} {Convolutional} {Network} for {Cancer} {Subtype} {Analysis}},
	volume = {13},
	issn = {1664-8021},
	shorttitle = {{MoGCN}},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2022.806842/full},
	doi = {10.3389/fgene.2022.806842},
	abstract = {In light of the rapid accumulation of large-scale omics datasets, numerous studies have attempted to characterize the molecular and clinical features of cancers from a multi-omics perspective. However, there are great challenges in integrating multi-omics using machine learning methods for cancer subtype classification. In this study, MoGCN, a multi-omics integration model based on graph convolutional network (GCN) was developed for cancer subtype classification and analysis. Genomics, transcriptomics and proteomics datasets for 511 breast invasive carcinoma (BRCA) samples were downloaded from the Cancer Genome Atlas (TCGA). The autoencoder (AE) and the similarity network fusion (SNF) methods were used to reduce dimensionality and construct the patient similarity network (PSN), respectively. Then the vector features and the PSN were input into the GCN for training and testing. Feature extraction and network visualization were used for further biological knowledge discovery and subtype classification. In the analysis of multi-dimensional omics data of the BRCA samples in TCGA, MoGCN achieved the highest accuracy in cancer subtype classification compared with several popular algorithms. Moreover, MoGCN can extract the most significant features of each omics layer and provide candidate functional molecules for further analysis of their biological effects. And network visualization showed that MoGCN could make clinically intuitive diagnosis. The generality of MoGCN was proven on the TCGA pan-kidney cancer datasets. MoGCN and datasets are public available at https://github.com/Lifoof/MoGCN. Our study shows that MoGCN performs well for heterogeneous data integration and the interpretability of classification results, which confers great potential for applications in biomarker identification and clinical diagnosis.},
	language = {English},
	urldate = {2025-07-12},
	journal = {Frontiers in Genetics},
	author = {Li, Xiao and Ma, Jie and Leng, Ling and Han, Mingfei and Li, Mansheng and He, Fuchu and Zhu, Yunping},
	month = feb,
	year = {2022},
	note = {90 citations (Crossref/DOI) [2025-10-22]
Publisher: Frontiers},
	keywords = {Printed, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/NTWP6PXW/2022-02-02 - Li et al. - MoGCN A Multi-Omics Integration Method Based on Graph Convolutional Network for Cancer Subtype Anal.pdf:application/pdf},
}

@article{clarke_deeprvat_2024,
	title = {[{DeepRVAT}] {Integration} of variant annotations using deep set networks boosts rare variant association testing},
	volume = {56},
	copyright = {2024 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-024-01919-z},
	doi = {10.1038/s41588-024-01919-z},
	abstract = {Rare genetic variants can have strong effects on phenotypes, yet accounting for rare variants in genetic analyses is statistically challenging due to the limited number of allele carriers and the burden of multiple testing. While rich variant annotations promise to enable well-powered rare variant association tests, methods integrating variant annotations in a data-driven manner are lacking. Here we propose deep rare variant association testing (DeepRVAT), a model based on set neural networks that learns a trait-agnostic gene impairment score from rare variant annotations and phenotypes, enabling both gene discovery and trait prediction. On 34 quantitative and 63 binary traits, using whole-exome-sequencing data from UK Biobank, we find that DeepRVAT yields substantial gains in gene discoveries and improved detection of individuals at high genetic risk. Finally, we demonstrate how DeepRVAT enables calibrated and computationally efficient rare variant tests at biobank scale, aiding the discovery of genetic risk factors for human disease traits.},
	language = {en},
	number = {10},
	urldate = {2025-07-12},
	journal = {Nature Genetics},
	author = {Clarke, Brian and Holtkamp, Eva and Öztürk, Hakime and Mück, Marcel and Wahlberg, Magnus and Meyer, Kayla and Munzlinger, Felix and Brechtmann, Felix and Hölzlwimmer, Florian R. and Lindner, Jonas and Chen, Zhifen and Gagneur, Julien and Stegle, Oliver},
	month = sep,
	year = {2024},
	note = {12 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast},
	pages = {2271--2280},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/G5IIH2WR/2024-09-25 - Clarke et al. - [DeepRVAT] Integration of variant annotations using deep set networks boosts rare variant associatio.pdf:application/pdf},
}

@misc{wu_generator_2025,
	title = {{GENERator}: {A} {Long}-{Context} {Generative} {Genomic} {Foundation} {Model}},
	shorttitle = {{GENERator}},
	url = {http://arxiv.org/abs/2502.07272},
	doi = {10.48550/arXiv.2502.07272},
	abstract = {Advancements in DNA sequencing technologies have significantly improved our ability to decode genomic sequences. However, the prediction and interpretation of these sequences remain challenging due to the intricate nature of genetic material. Large language models (LLMs) have introduced new opportunities for biological sequence analysis. Recent developments in genomic language models have underscored the potential of LLMs in deciphering DNA sequences. Nonetheless, existing models often face limitations in robustness and application scope, primarily due to constraints in model structure and training data scale. To address these limitations, we present GENERator, a generative genomic foundation model featuring a context length of 98k base pairs (bp) and 1.2B parameters. Trained on an expansive dataset comprising 386B bp of eukaryotic DNA, the GENERator demonstrates state-of-the-art performance across both established and newly proposed benchmarks. The model adheres to the central dogma of molecular biology, accurately generating protein-coding sequences that translate into proteins structurally analogous to known families. It also shows significant promise in sequence optimization, particularly through the prompt-responsive generation of enhancer sequences with specific activity profiles. These capabilities position the GENERator as a pivotal tool for genomic research and biotechnological advancement, enhancing our ability to interpret and predict complex biological systems and enabling precise genomic interventions. Implementation details and supplementary resources are available at https://github.com/GenerTeam/GENERator.},
	urldate = {2025-07-22},
	publisher = {arXiv},
	author = {Wu, Wei and Li, Qiuyi and Li, Mingyang and Fu, Kun and Feng, Fuli and Ye, Jieping and Xiong, Hui and Wang, Zheng},
	month = apr,
	year = {2025},
	note = {arXiv:2502.07272 [cs]},
	keywords = {Printed, Podcast},
	file = {2025-04-01 - Wu et al. - GENERator A Long-Context Generative Genomic Foundation Model:/Users/meehl.joshua/Zotero/storage/45JLEH46/2025-04-01 - Wu et al. - GENERator A Long-Context Generative Genomic Foundation Model.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/QJ4JL7T9/2502.html:text/html},
}

@article{fishman_gena-lm_2025,
	title = {{GENA}-{LM}: a family of open-source foundational {DNA} language models for long sequences},
	volume = {53},
	issn = {1362-4962},
	shorttitle = {{GENA}-{LM}},
	url = {https://doi.org/10.1093/nar/gkae1310},
	doi = {10.1093/nar/gkae1310},
	abstract = {Recent advancements in genomics, propelled by artificial intelligence, have unlocked unprecedented capabilities in interpreting genomic sequences, mitigating the need for exhaustive experimental analysis of complex, intertwined molecular processes inherent in DNA function. A significant challenge, however, resides in accurately decoding genomic sequences, which inherently involves comprehending rich contextual information dispersed across thousands of nucleotides. To address this need, we introduce GENA language model (GENA-LM), a suite of transformer-based foundational DNA language models capable of handling input lengths up to 36 000 base pairs. Notably, integrating the newly developed recurrent memory mechanism allows these models to process even larger DNA segments. We provide pre-trained versions of GENA-LM, including multispecies and taxon-specific models, demonstrating their capability for fine-tuning and addressing a spectrum of complex biological tasks with modest computational demands. While language models have already achieved significant breakthroughs in protein biology, GENA-LM showcases a similarly promising potential for reshaping the landscape of genomics and multi-omics data analysis. All models are publicly available on GitHub (https://github.com/AIRI-Institute/GENA\_LM) and on HuggingFace (https://huggingface.co/AIRI-Institute). In addition, we provide a web service (https://dnalm.airi.net/) allowing user-friendly DNA annotation with GENA-LM models.},
	number = {2},
	urldate = {2025-07-22},
	journal = {Nucleic Acids Research},
	author = {Fishman, Veniamin and Kuratov, Yuri and Shmelev, Aleksei and Petrov, Maxim and Penzar, Dmitry and Shepelin, Denis and Chekanov, Nikolay and Kardymon, Olga and Burtsev, Mikhail},
	month = jan,
	year = {2025},
	note = {16 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast},
	pages = {gkae1310},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/22BPFWHY/2025-01-27 - Fishman et al. - GENA-LM a family of open-source foundational DNA language models for long sequences.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/8HV8GK6P/gkae1310.html:text/html},
}

@misc{cornman_glm2_2024,
	title = {[{gLM2}] {The} {OMG} dataset: {An} {Open} {MetaGenomic} corpus for mixed-modality genomic language modeling},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {The {OMG} dataset},
	url = {https://www.biorxiv.org/content/10.1101/2024.08.14.607850v1},
	doi = {10.1101/2024.08.14.607850},
	abstract = {Biological language model performance depends heavily on pretraining data quality, diversity, and size. While metagenomic datasets feature enormous biological diversity, their utilization as pretraining data has been limited due to challenges in data accessibility, quality filtering and deduplication. Here, we present the Open MetaGenomic (OMG) corpus, a genomic pretraining dataset totalling 3.1T base pairs and 3.3B protein coding sequences, obtained by combining two largest metagenomic dataset repositories (JGI’s IMG and EMBL’s MGnify). We first document the composition of the dataset and describe the quality filtering steps taken to remove poor quality data. We make the OMG corpus available as a mixed-modality genomic sequence dataset that represents multi-gene encoding genomic sequences with translated amino acids for protein coding sequences, and nucleic acids for intergenic sequences. We train the first mixed-modality genomic language model (gLM2) that leverages genomic context information to learn robust functional representations and coevolutionary signals in protein-protein interfaces. Furthermore, we show that deduplication in embedding space can be used to balance the corpus, demonstrating improved performance on downstream tasks. The OMG dataset is publicly hosted on the Hugging Face Hub at https://huggingface.co/datasets/tattabio/OMG and gLM2 is available at https://huggingface.co/tattabio/gLM2\_650M.},
	language = {en},
	urldate = {2025-07-22},
	publisher = {bioRxiv},
	author = {Cornman, Andre and West-Roberts, Jacob and Camargo, Antonio Pedro and Roux, Simon and Beracochea, Martin and Mirdita, Milot and Ovchinnikov, Sergey and Hwang, Yunha},
	month = aug,
	year = {2024},
	note = {21 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.08.14.607850
Section: New Results},
	keywords = {Printed},
	file = {2024-08-17 - Cornman et al. - [gLM2] The OMG dataset An Open MetaGenomic corpus for mixed-modality genomic language modeling:/Users/meehl.joshua/Zotero/storage/YH66T5V7/2024-08-17 - Cornman et al. - [gLM2] The OMG dataset An Open MetaGenomic corpus for mixed-modality genomic language modeling.pdf:application/pdf},
}

@article{vishniakov_genomic_2024,
	title = {Genomic {Foundationless} {Models}: {Pretraining} {Does} {Not} {Promise} {Performance}},
	shorttitle = {Genomic {Foundationless} {Models}},
	url = {https://openreview.net/forum?id=kDZKEtDnT1},
	abstract = {The success of Large Language Models has inspired the development of Genomic Foundation Models (GFMs) through similar pretraining techniques. However, the relationship between pretraining performance and effectiveness in downstream genomic tasks remains unclear. Additionally, the high computational cost of pretraining raises questions about its cost-efficiency. To assess the usefulness of pretraining in genomics, we evaluated seven different GFMs across various benchmarks, comparing them to their counterparts with randomly initialized weights. Surprisingly, we found that randomly initialized models can match or even surpass the performance of pretrained GFMs in finetuning and feature extraction tasks. We also discovered that pretrained GFMs fail to capture clinically relevant genetic mutations, which are crucial for understanding genetic disorders and phenotypic traits. Our results indicate that most of the current pretrained GFMs lack a ``foundational'' understanding of genomics and provide minimal utility, even for basic tasks such as sequence classification. These findings collectively highlight the need for critically rethinking the pretraining approaches for genomics. Our code is available at https://github.com/nxifemwt/GFMs.},
	language = {en},
	urldate = {2025-07-22},
	author = {Vishniakov, Kirill and Viswanathan, Karthik and Medvedev, Aleksandr and Kanithi, Praveenkumar and Pimentel, Marco AF and Khan, Shadab},
	month = oct,
	year = {2024},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/IC5GE6PJ/2024-10-04 - Vishniakov et al. - Genomic Foundationless Models Pretraining Does Not Promise Performance.pdf:application/pdf},
}

@misc{schiff_caduceus_2024,
	title = {Caduceus: {Bi}-{Directional} {Equivariant} {Long}-{Range} {DNA} {Sequence} {Modeling}},
	shorttitle = {Caduceus},
	url = {http://arxiv.org/abs/2403.03234},
	doi = {10.48550/arXiv.2403.03234},
	abstract = {Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the performance of 10x larger models that do not leverage bi-directionality or equivariance.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Schiff, Yair and Kao, Chia-Hsiang and Gokaslan, Aaron and Dao, Tri and Gu, Albert and Kuleshov, Volodymyr},
	month = jun,
	year = {2024},
	note = {arXiv:2403.03234 [q-bio]},
	keywords = {Printed, Podcast, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/PN2AQVMU/2024-06-05 - Schiff et al. - Caduceus Bi-Directional Equivariant Long-Range DNA Sequence Modeling.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/YIPW7H9E/2403.html:text/html},
}

@misc{nguyen_hyenadna_2023,
	title = {{HyenaDNA}: {Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide} {Resolution}},
	shorttitle = {{HyenaDNA}},
	url = {http://arxiv.org/abs/2306.15794},
	doi = {10.48550/arXiv.2306.15794},
	abstract = {Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context ({\textless}0.001\% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Nguyen, Eric and Poli, Michael and Faizi, Marjan and Thomas, Armin and Birch-Sykes, Callum and Wornow, Michael and Patel, Aman and Rabideau, Clayton and Massaroli, Stefano and Bengio, Yoshua and Ermon, Stefano and Baccus, Stephen A. and Ré, Chris},
	month = nov,
	year = {2023},
	note = {arXiv:2306.15794 [cs]},
	keywords = {Printed, Important, Podcast, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/JVEF4BBI/2023-11-14 - Nguyen et al. - HyenaDNA Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/G59U5Z77/2306.html:text/html},
}

@article{chen_deepsea_2022,
	title = {[{DeepSEA} {Sei}] {A} sequence-based global map of regulatory activity for deciphering human genetics},
	volume = {54},
	copyright = {2022 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-022-01102-2},
	doi = {10.1038/s41588-022-01102-2},
	abstract = {Epigenomic profiling has enabled large-scale identification of regulatory elements, yet we still lack a systematic mapping from any sequence or variant to regulatory activities. We address this challenge with Sei, a framework for integrating human genetics data with sequence information to discover the regulatory basis of traits and diseases. Sei learns a vocabulary of regulatory activities, called sequence classes, using a deep learning model that predicts 21,907 chromatin profiles across {\textgreater}1,300 cell lines and tissues. Sequence classes provide a global classification and quantification of sequence and variant effects based on diverse regulatory activities, such as cell type-specific enhancer functions. These predictions are supported by tissue-specific expression, expression quantitative trait loci and evolutionary constraint data. Furthermore, sequence classes enable characterization of the tissue-specific, regulatory architecture of complex traits and generate mechanistic hypotheses for individual regulatory pathogenic mutations. We provide Sei as a resource to elucidate the regulatory basis of human health and disease.},
	language = {en},
	number = {7},
	urldate = {2025-07-23},
	journal = {Nature Genetics},
	author = {Chen, Kathleen M. and Wong, Aaron K. and Troyanskaya, Olga G. and Zhou, Jian},
	month = jul,
	year = {2022},
	note = {194 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Read},
	pages = {940--949},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/Y5MUGXGK/2022-07-11 - Chen et al. - [DeepSEA Sei] A sequence-based global map of regulatory activity for deciphering human genetics.pdf:application/pdf},
}

@article{sanabria_grover_2024,
	title = {[{GROVER}] {DNA} language model {GROVER} learns sequence context in the human genome},
	volume = {6},
	copyright = {2024 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-024-00872-0},
	doi = {10.1038/s42256-024-00872-0},
	abstract = {Deep-learning models that learn a sense of language on DNA have achieved a high level of performance on genome biological tasks. Genome sequences follow rules similar to natural language but are distinct in the absence of a concept of words. We established byte-pair encoding on the human genome and trained a foundation language model called GROVER (Genome Rules Obtained Via Extracted Representations) with the vocabulary selected via a custom task, next-k-mer prediction. The defined dictionary of tokens in the human genome carries best the information content for GROVER. Analysing learned representations, we observed that trained token embeddings primarily encode information related to frequency, sequence content and length. Some tokens are primarily localized in repeats, whereas the majority widely distribute over the genome. GROVER also learns context and lexical ambiguity. Average trained embeddings of genomic regions relate to functional genomics annotation and thus indicate learning of these structures purely from the contextual relationships of tokens. This highlights the extent of information content encoded by the sequence that can be grasped by GROVER. On fine-tuning tasks addressing genome biology with questions of genome element identification and protein–DNA binding, GROVER exceeds other models’ performance. GROVER learns sequence context, a sense for structure and language rules. Extracting this knowledge can be used to compose a grammar book for the code of life.},
	language = {en},
	number = {8},
	urldate = {2025-07-25},
	journal = {Nature Machine Intelligence},
	author = {Sanabria, Melissa and Hirsch, Jonas and Joubert, Pierre M. and Poetsch, Anna R.},
	month = jul,
	year = {2024},
	note = {47 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed},
	pages = {911--923},
	file = {2024-07-23 - Sanabria et al. - [Preprint][GROVER] DNA language model GROVER learns sequence context in the human genome:/Users/meehl.joshua/Zotero/storage/WLAJANB6/2024-07-23 - Sanabria et al. - [Preprint][GROVER] DNA language model GROVER learns sequence context in the human genome.pdf:application/pdf;Full Text PDF:/Users/meehl.joshua/Zotero/storage/SNYYQVLT/2024-07-23 - Sanabria et al. - [GROVER] DNA language model GROVER learns sequence context in the human genome.pdf:application/pdf},
}

@article{gresova_genomic_2023,
	title = {Genomic benchmarks: a collection of datasets for genomic sequence classification},
	volume = {24},
	issn = {2730-6844},
	shorttitle = {Genomic benchmarks},
	url = {https://doi.org/10.1186/s12863-023-01123-8},
	doi = {10.1186/s12863-023-01123-8},
	abstract = {Recently, deep neural networks have been successfully applied in many biological fields. In 2020, a deep learning model AlphaFold won the protein folding competition with predicted structures within the error tolerance of experimental methods. However, this solution to the most prominent bioinformatic challenge of the past 50 years has been possible only thanks to a carefully curated benchmark of experimentally predicted protein structures. In Genomics, we have similar challenges (annotation of genomes and identification of functional elements) but currently, we lack benchmarks similar to protein folding competition.},
	number = {1},
	urldate = {2025-07-25},
	journal = {BMC Genomic Data},
	author = {Grešová, Katarína and Martinek, Vlastimil and Čechák, David and Šimeček, Petr and Alexiou, Panagiotis},
	month = may,
	year = {2023},
	note = {33 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Read},
	pages = {25},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/3ESJLWSM/2023-05-01 - Grešová et al. - Genomic benchmarks a collection of datasets for genomic sequence classification.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/62PT4KDI/s12863-023-01123-8.html:text/html},
}

@article{zhou_deepsea_2018,
	title = {[{DeepSEA} {Beluga}] {Deep} learning sequence-based ab initio prediction of variant effects on expression and disease risk},
	volume = {50},
	copyright = {2018 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-018-0160-6},
	doi = {10.1038/s41588-018-0160-6},
	abstract = {Key challenges for human genetics, precision medicine and evolutionary biology include deciphering the regulatory code of gene expression and understanding the transcriptional effects of genome variation. However, this is extremely difficult because of the enormous scale of the noncoding mutation space. We developed a deep learning–based framework, ExPecto, that can accurately predict, ab initio from a DNA sequence, the tissue-specific transcriptional effects of mutations, including those that are rare or that have not been observed. We prioritized causal variants within disease- or trait-associated loci from all publicly available genome-wide association studies and experimentally validated predictions for four immune-related diseases. By exploiting the scalability of ExPecto, we characterized the regulatory mutation space for human RNA polymerase II–transcribed genes by in silico saturation mutagenesis and profiled {\textgreater} 140 million promoter-proximal mutations. This enables probing of evolutionary constraints on gene expression and ab initio prediction of mutation disease effects, making ExPecto an end-to-end computational framework for the in silico prediction of expression and disease risk.},
	language = {en},
	number = {8},
	urldate = {2025-07-25},
	journal = {Nature Genetics},
	author = {Zhou, Jian and Theesfeld, Chandra L. and Yao, Kevin and Chen, Kathleen M. and Wong, Aaron K. and Troyanskaya, Olga G.},
	month = jul,
	year = {2018},
	note = {420 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Recall, Read},
	pages = {1171--1179},
	file = {2018-07-16 - Zhou et al. - [DeepSEA Beluga] Deep learning sequence-based ab initio prediction of variant effects on expression:/Users/meehl.joshua/Zotero/storage/GDYBADGZ/2018-07-16 - Zhou et al. - [DeepSEA Beluga] Deep learning sequence-based ab initio prediction of variant effects on expression.pdf:application/pdf},
}

@misc{khan_deepgene_2024,
	title = {{DeepGene} {Transformer}: {Transformer} for the gene expression-based classification of cancer subtypes},
	shorttitle = {{DeepGene} {Transformer}},
	url = {http://arxiv.org/abs/2108.11833},
	doi = {10.48550/arXiv.2108.11833},
	abstract = {Cancer and its subtypes constitute approximately 30\% of all causes of death globally and display a wide range of heterogeneity in terms of clinical and molecular responses to therapy. Molecular subtyping has enabled the use of precision medicine to overcome these challenges and provide significant biological insights to predict prognosis and improve clinical decision-making. Over the past decade, conventional machine learning (ML) and deep learning (DL) algorithms have been widely espoused for the classification of cancer subtypes from gene expression datasets. However, these methods are potentially biased toward the identification of cancer biomarkers. Hence, an end-to-end deep learning approach, DeepGene Transformer, is proposed which addresses the complexity of high-dimensional gene expression with a multi-head self-attention module by identifying relevant biomarkers across multiple cancer subtypes without requiring feature selection as a pre-requisite for the current classification algorithms. Comparative analysis reveals that the proposed DeepGene Transformer outperformed the commonly used traditional and state-of-the-art classification algorithms and can be considered an efficient approach for classifying cancer and its subtypes, indicating that any improvement in deep learning models in computational biologists can be reflected well in this domain as well.},
	urldate = {2025-07-25},
	publisher = {arXiv},
	author = {Khan, Anwar and Lee, Boreom},
	month = jul,
	year = {2024},
	note = {arXiv:2108.11833 [q-bio]},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/9FYPW4A9/2024-07-10 - Khan and Lee - DeepGene Transformer Transformer for the gene expression-based classification of cancer subtypes.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/GI9HVQPK/2108.html:text/html},
}

@article{consens_genomic_2025,
	title = {Genomic language models could transform medicine but not yet},
	volume = {8},
	copyright = {2025 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-025-01603-4},
	doi = {10.1038/s41746-025-01603-4},
	abstract = {Recently, a genomic language model (gLM) with 40 billion parameters known as Evo2 has reached the same scale as the most powerful text large language models (LLMs). gLMs have been emerging as powerful tools to decode DNA sequences over the last five years. This article examines the emergence of gLMs and highlights Evo2 as a milestone in genomic language modeling, assessing both the scientific promise of gLMs and the practical challenges facing their implementation in medicine.},
	language = {en},
	number = {1},
	urldate = {2025-07-28},
	journal = {npj Digital Medicine},
	author = {Consens, Micaela Elisa and Li, Ben and Poetsch, Anna R. and Gilbert, Stephen},
	month = apr,
	year = {2025},
	note = {6 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast},
	pages = {212},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/LWIWPNBW/2025-04-18 - Consens et al. - Genomic language models could transform medicine but not yet.pdf:application/pdf},
}

@article{rodriguez-mier_corneto_2025,
	title = {[{CORNETO}] {Unifying} multi-sample network inference from prior knowledge and omics data with {CORNETO}},
	volume = {7},
	copyright = {2025 European Molecular Biology Laboratory and Pablo Rodriguez-Mier, Attila Gabor},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-025-01069-9},
	doi = {10.1038/s42256-025-01069-9},
	abstract = {Understanding biological systems requires methods that extract interpretable insights from omics data. Networks offer a natural abstraction by representing molecules as vertices and their interactions as edges, providing a foundation for constructing context-specific models tailored to particular conditions—an essential step in many biological analyses. Most existing approaches fall into one of two categories: machine learning methods, which offer strong predictive power but lack interpretability and require large datasets, and knowledge-based methods, which are more interpretable but designed for analysing individual samples and difficult to generalize. Here we present CORNETO, a unified mathematical framework that generalizes a wide variety of methods that learn biological networks from omics data and prior knowledge. CORNETO reformulates these methods as mixed-integer optimization problems using network flows and structured sparsity, enabling joint inference across multiple samples. This improves the discovery of both shared and sample-specific molecular mechanisms while yielding sparser, more interpretable solutions. CORNETO supports a range of prior knowledge structures, including undirected, directed and signed (hyper)graphs. It extends a broad class of approaches, ranging from Steiner trees to flux balance analysis, within a unified optimization-based interface. We demonstrate CORNETO’s utility across diverse biological contexts, including signalling, metabolism and integration with biologically informed deep learning. We provide CORNETO as an open-source Python library for flexible network modelling.},
	language = {en},
	number = {7},
	urldate = {2025-07-28},
	journal = {Nature Machine Intelligence},
	author = {Rodriguez-Mier, Pablo and Garrido-Rodriguez, Martin and Gabor, Attila and Saez-Rodriguez, Julio},
	month = jul,
	year = {2025},
	note = {2 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast, Recall},
	pages = {1168--1186},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/CEW2VJEZ/2025-07-22 - Rodriguez-Mier et al. - [CORNETO] Unifying multi-sample network inference from prior knowledge and omics data with CORNETO.pdf:application/pdf},
}

@article{gomez-cabrero_corneto_2025,
	title = {[{CORNETO}] {Data} meets prior knowledge for interpretable mechanistic inference in biology},
	volume = {7},
	copyright = {2025 Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-025-01075-x},
	doi = {10.1038/s42256-025-01075-x},
	abstract = {A unified optimization framework, CORNETO, introduces a versatile approach to knowledge-driven biological network inference, bringing machine learning sensibilities to systems biology.},
	language = {en},
	number = {7},
	urldate = {2025-07-28},
	journal = {Nature Machine Intelligence},
	author = {Gomez-Cabrero, David and Tegnér, Jesper N.},
	month = jul,
	year = {2025},
	note = {0 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed},
	pages = {987--988},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/XPMJ3N27/2025-07-22 - Gomez-Cabrero and Tegnér - [CORNETO] Data meets prior knowledge for interpretable mechanistic inference in biology.pdf:application/pdf},
}

@misc{liu_genbench_2024,
	title = {{GenBench}: {A} {Benchmarking} {Suite} for {Systematic} {Evaluation} of {Genomic} {Foundation} {Models}},
	shorttitle = {{GenBench}},
	url = {http://arxiv.org/abs/2406.01627},
	doi = {10.48550/arXiv.2406.01627},
	abstract = {The Genomic Foundation Model (GFM) paradigm is expected to facilitate the extraction of generalizable representations from massive genomic data, thereby enabling their application across a spectrum of downstream applications. Despite advancements, a lack of evaluation framework makes it difficult to ensure equitable assessment due to experimental settings, model intricacy, benchmark datasets, and reproducibility challenges. In the absence of standardization, comparative analyses risk becoming biased and unreliable. To surmount this impasse, we introduce GenBench, a comprehensive benchmarking suite specifically tailored for evaluating the efficacy of Genomic Foundation Models. GenBench offers a modular and expandable framework that encapsulates a variety of state-of-the-art methodologies. Through systematic evaluations of datasets spanning diverse biological domains with a particular emphasis on both short-range and long-range genomic tasks, firstly including the three most important DNA tasks covering Coding Region, Non-Coding Region, Genome Structure, etc. Moreover, We provide a nuanced analysis of the interplay between model architecture and dataset characteristics on task-specific performance. Our findings reveal an interesting observation: independent of the number of parameters, the discernible difference in preference between the attention-based and convolution-based models on short- and long-range tasks may provide insights into the future design of GFM.},
	urldate = {2025-08-05},
	publisher = {arXiv},
	author = {Liu, Zicheng and Li, Jiahui and Li, Siyuan and Zang, Zelin and Tan, Cheng and Huang, Yufei and Bai, Yajing and Li, Stan Z.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01627 [q-bio]},
	keywords = {Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/FJU69JQE/2024-06-05 - Liu et al. - GenBench A Benchmarking Suite for Systematic Evaluation of Genomic Foundation Models.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/EYMAHF4F/2406.html:text/html},
}

@article{hudaiberdiev_trednet_2023,
	title = {[{TREDNet}] {Modeling} islet enhancers using deep learning identifies candidate causal variants at loci associated with {T2D} and glycemic traits},
	volume = {120},
	copyright = {Copyright © 2023 the Author(s). Published by PNAS.},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2206612120},
	doi = {10.1073/pnas.2206612120},
	abstract = {Genetic association studies have identified hundreds of independent signals associated
with type 2 diabetes (T2D) and related traits. Despite these...},
	language = {EN},
	number = {35},
	urldate = {2025-08-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hudaiberdiev, Sanjarbek and Taylor, D. Leland and Song, Wei and Narisu, Narisu and Bhuiyan, Redwan M. and Taylor, Henry J. and Tang, Xuming and Yan, Tingfen and Swift, Amy J. and Bonnycastle, Lori L. and Consortium, Diamante and Chen, Shuibing and Stitzel, Michael L. and Erdos, Michael R. and Ovcharenko, Ivan and Collins, Francis S.},
	month = aug,
	year = {2023},
	note = {7 citations (Crossref/DOI) [2025-10-22]
Company: National Academy of Sciences
Distributor: National Academy of Sciences
Institution: National Academy of Sciences
Label: National Academy of Sciences
Publisher: Proceedings of the National Academy of Sciences},
	keywords = {Printed, Read},
	pages = {e2206612120},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/98NP7MFB/2023-08-29 - Hudaiberdiev et al. - [TREDNet] Modeling islet enhancers using deep learning identifies candidate causal variants at loci.pdf:application/pdf;Hudaiberdiev et al. - 2023 - Modeling islet enhancers using deep learning identifies candidate causal variants at loci associated-supp.pdf:/Users/meehl.joshua/Zotero/storage/3HDBNHDI/2023-08-29 - Hudaiberdiev et al. - [TREDNet] Modeling islet enhancers using deep learning identifies candidate causal variants at loci-supp.pdf:application/pdf},
}

@article{de_paoli_varchat_2024,
	title = {{VarChat}: the generative {AI} assistant for the interpretation of human genomic variations},
	volume = {40},
	issn = {1367-4811},
	shorttitle = {{VarChat}},
	doi = {10.1093/bioinformatics/btae183},
	abstract = {MOTIVATION: In the modern era of genomic research, the scientific community is witnessing an explosive growth in the volume of published findings. While this abundance of data offers invaluable insights, it also places a pressing responsibility on genetic professionals and researchers to stay informed about the latest findings and their clinical significance. Genomic variant interpretation is currently facing a challenge in identifying the most up-to-date and relevant scientific papers, while also extracting meaningful information to accelerate the process from clinical assessment to reporting. Computer-aided literature search and summarization can play a pivotal role in this context. By synthesizing complex genomic findings into concise, interpretable summaries, this approach facilitates the translation of extensive genomic datasets into clinically relevant insights.
RESULTS: To bridge this gap, we present VarChat (varchat.engenome.com), an innovative tool based on generative AI, developed to find and summarize the fragmented scientific literature associated with genomic variants into brief yet informative texts. VarChat provides users with a concise description of specific genetic variants, detailing their impact on related proteins and possible effects on human health. In addition, VarChat offers direct links to related scientific trustable sources, and encourages deeper research.
AVAILABILITY AND IMPLEMENTATION: varchat.engenome.com.},
	language = {eng},
	number = {4},
	journal = {Bioinformatics (Oxford, England)},
	author = {De Paoli, Federica and Berardelli, Silvia and Limongelli, Ivan and Rizzo, Ettore and Zucca, Susanna},
	month = mar,
	year = {2024},
	pmid = {38579245},
	pmcid = {PMC11055464},
	note = {17 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Podcast, Read},
	pages = {btae183},
	file = {Submitted Version:/Users/meehl.joshua/Zotero/storage/P26UUKAC/2024-03-29 - De Paoli et al. - VarChat the generative AI assistant for the interpretation of human genomic variations.pdf:application/pdf},
}

@article{chafai_review_2023,
	title = {A review of machine learning models applied to genomic prediction in animal breeding},
	volume = {14},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2023.1150596/full},
	doi = {10.3389/fgene.2023.1150596},
	abstract = {The advent of modern genotyping technologies has revolutionized genomic selection in animal breeding. Large marker datasets have shown several drawbacks for traditional genomic prediction methods in terms of flexibility, accuracy, and computational power. Recently, the application of machine learning models in animal breeding has gained a lot of interest due to their tremendous flexibility and their ability to capture patterns in large noisy datasets. Here, we present a general overview of a handful of machine learning algorithms and their application in genomic prediction to provide a meta-picture of their performance in genomic estimated breeding values (GEBVs) estimation, genotype imputation, and feature selection. Finally, we discuss a potential adoption of ML models in genomic prediction in developing countries. The results of the reviewed studies showed that machine learning models have indeed performed well in fitting large noisy data sets and modeling minor nonadditive effects in some of the studies. However, sometimes conventional 1 methods outperformed machine learning models, which confirms that there's no universal method for genomic prediction. In summary, machine learning models have great potential for extracting patterns from single nucleotide polymorphism (SNP) datasets. Nonetheless, the level of their adoption in animal breeding is still low due to data limitations, complex genetic interactions, a lack of standardization and reproducibility, and the lack of interpretability of machine learning models when trained with biological data. Consequently, there is no remarkable outperformance of machine learning methods compared to traditional methods in genomic prediction. Therefore, more research should be conducted to discover new insights that could enhance livestock breeding programs.},
	language = {English},
	urldate = {2025-08-07},
	journal = {Frontiers in Genetics},
	author = {Chafai, Narjice and Hayah, Ichrak and Houaga, Isidore and Badaoui, Bouabid},
	month = sep,
	year = {2023},
	note = {44 citations (Crossref/DOI) [2025-10-22]
Publisher: Frontiers},
	keywords = {Printed, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/EFSCDS4R/2023-09-06 - Chafai et al. - A review of machine learning models applied to genomic prediction in animal breeding.pdf:application/pdf},
}

@misc{hegde_language_2025,
	title = {Language modelling techniques for analysing the impact of human genetic variation},
	url = {http://arxiv.org/abs/2503.10655},
	doi = {10.48550/arXiv.2503.10655},
	abstract = {Interpreting the effects of variants within the human genome and proteome is essential for analysing disease risk, predicting medication response, and developing personalised health interventions. Due to the intrinsic similarities between the structure of natural languages and genetic sequences, natural language processing techniques have demonstrated great applicability in computational variant effect prediction. In particular, the advent of the Transformer has led to significant advancements in the field. However, Transformer-based models are not without their limitations, and a number of extensions and alternatives have been developed to improve results and enhance computational efficiency. This review explores the use of language models for computational variant effect prediction over the past decade, analysing the main architectures, and identifying key trends and future directions.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Hegde, Megha and Nebel, Jean-Christophe and Rahman, Farzana},
	month = mar,
	year = {2025},
	note = {arXiv:2503.10655 [cs]
version: 1},
	keywords = {Printed, Podcast, Recall},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/HDKEKLAQ/2025-03-07 - Hegde et al. - Language modelling techniques for analysing the impact of human genetic variation.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/3ZFZ7X65/2503.html:text/html},
}

@misc{liu_unicorn_2025,
	title = {{UNICORN}: {Towards} {Universal} {Cellular} {Expression} {Prediction} with a {Multi}-{Task} {Learning} {Framework}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {{UNICORN}},
	url = {https://www.biorxiv.org/content/10.1101/2025.01.22.634371v3},
	doi = {10.1101/2025.01.22.634371},
	abstract = {Sequence-to-function analysis is a challenging task in human genetics, especially in predicting cell-type-specific multi-omic phenotypes from biological sequences such as individualized gene expression. Here, we present UNICORN, a new method with improved prediction performances than the existing methods. UNICORN takes the embeddings from biological sequences as well as external knowledge from pre-trained foundation models as inputs and optimizes the predictor with carefully-designed loss functions. We demonstrate that UNICORN outperforms the existing methods in both gene expression prediction and multiomic phenotype prediction at the cellular level and the cell-type level, and it can also generate uncertainty scores of the predictions. Moreover, UNICORN is able to link personalized gene expression profiles with corresponding genome information. Finally, we show that UNICORN is capable of characterizing complex biological systems for different disease states or perturbations. Overall, embeddings from foundation models can facilitate the understanding of the role of biological sequences in the prediction task, and incorporating multi-omic information can enhance prediction performances.},
	language = {en},
	urldate = {2025-08-07},
	publisher = {bioRxiv},
	author = {Liu, Tianyu and Huang, Tinglin and Wang, Lijun and Lin, Yingxin and Ying, Rex and Zhao, Hongyu},
	month = jul,
	year = {2025},
	note = {1 citations (Crossref/DOI) [2025-10-22]
ISSN: 2692-8205
Pages: 2025.01.22.634371
Section: New Results},
	keywords = {Printed, Podcast},
	file = {2025-07-30 - Liu et al. - UNICORN Towards Universal Cellular Expression Prediction with a Multi-Task Learning Framework:/Users/meehl.joshua/Zotero/storage/HCX5VWC4/2025-07-30 - Liu et al. - UNICORN Towards Universal Cellular Expression Prediction with a Multi-Task Learning Framework.pdf:application/pdf},
}

@misc{yang_trinitydna_2025,
	title = {{TrinityDNA}: {A} {Bio}-{Inspired} {Foundational} {Model} for {Efficient} {Long}-{Sequence} {DNA} {Modeling}},
	shorttitle = {{TrinityDNA}},
	url = {http://arxiv.org/abs/2507.19229},
	doi = {10.48550/arXiv.2507.19229},
	abstract = {The modeling of genomic sequences presents unique challenges due to their length and structural complexity. Traditional sequence models struggle to capture long-range dependencies and biological features inherent in DNA. In this work, we propose TrinityDNA, a novel DNA foundational model designed to address these challenges. The model integrates biologically informed components, including Groove Fusion for capturing DNA's structural features and Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA sequences. Additionally, we introduce a multi-scale attention mechanism that allows the model to attend to varying levels of sequence dependencies, and an evolutionary training strategy that progressively adapts the model to both prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and efficient approach to genomic sequence modeling, offering significant improvements in gene function prediction, regulatory mechanism discovery, and other genomics applications. Our model bridges the gap between machine learning techniques and biological insights, paving the way for more effective analysis of genomic data. Additionally, we introduced a new DNA long-sequence CDS annotation benchmark to make evaluations more comprehensive and oriented toward practical applications.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Yang, Qirong and Guo, Yucheng and Liu, Zicheng and Yang, Yujie and Yin, Qijin and Li, Siyuan and Ji, Shaomin and Chao, Linlin and Zhang, Xiaoming and Li, Stan Z.},
	month = jul,
	year = {2025},
	note = {arXiv:2507.19229 [cs]},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/93F65B2N/2025-07-25 - Yang et al. - TrinityDNA A Bio-Inspired Foundational Model for Efficient Long-Sequence DNA Modeling.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/BWV7GASM/2507.html:text/html},
}

@article{zhang_epi-dynfusion_2025,
	title = {{EPI}-{DynFusion}: enhancer-promoter interaction prediction model based on sequence features and dynamic fusion mechanisms},
	volume = {16},
	issn = {1664-8021},
	shorttitle = {{EPI}-{DynFusion}},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2025.1614222/full},
	doi = {10.3389/fgene.2025.1614222},
	abstract = {IntroductionEnhancer–promoter interactions (EPIs) play a vital role in the regulation of gene expression. Although traditional wet-lab methods provide valuable insights into EPIs, they are often constrained by high costs and limited scalability. As a result, the development of efficient computational models has become essential. However, many current deep learning and machine learning approaches utilize simplistic feature fusion strategies, such as direct averaging or concatenation, which fail to effectively model complex relationships and dynamic importance across features. This often results in suboptimal performance in challenging biological contexts.MethodsTo address these limitations, we propose a deep learning model named EPI-DynFusion. This model begins by encoding DNA sequences using pre-trained DNA embeddings and extracting local features through convolutional neural networks (CNNs). It then integrates a Transformer and Bidirectional Gated Recurrent Unit (BiGRU) architecture with a Dynamic Feature Fusion mechanism to adaptively learn deep dependencies among features. Furthermore, we incorporate the Convolutional Block Attention Module (CBAM) to enhance the model’s ability to focus on informative regions. Based on this core architecture, we develop two variants: EPI-DynFusion-gen, a general model, and EPI-DynFusion-best, a fine-tuned version for cell line–specific data.ResultsWe evaluated the performance of our models across six benchmark cell lines. The average area under the receiver operating characteristic curve (AUROC) scores achieved by the specific, generic, and best models were 94.8\%, 95.0\%, and 96.2\%, respectively. The average area under the precision-recall curve (AUPR) scores were 81.2\%, 71.1\%, and 83.3\%, respectively, demonstrating the superior performance of the fine-tuned model in the precision-recall space. These results confirm that the proposed fusion strategies and attention mechanisms contribute to significant improvements in performance.DiscussionIn conclusion, EPI-DynFusion presents a robust and scalable framework for predicting enhancer–promoter interactions solely based on DNA sequence information. By addressing the limitations of conventional fusion techniques and incorporating attention mechanisms alongside sequence modeling, our method achieves state-of-the-art performance while enhancing the interpretability and generalizability of enhancer–promoter interaction prediction tasks.},
	language = {English},
	urldate = {2025-08-07},
	journal = {Frontiers in Genetics},
	author = {Zhang, Ao and Jia, Jianhua and Sun, Mingwei and Wei, Xin},
	month = jul,
	year = {2025},
	note = {0 citations (Crossref/DOI) [2025-10-22]
Publisher: Frontiers},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/I56BYTFI/2025-07-23 - Zhang et al. - EPI-DynFusion enhancer-promoter interaction prediction model based on sequence features and dynamic.pdf:application/pdf},
}

@article{ligeti_prokbert_2024,
	title = {{ProkBERT} family: genomic language models for microbiome applications},
	volume = {14},
	issn = {1664-302X},
	shorttitle = {{ProkBERT} family},
	url = {https://www.frontiersin.org/journals/microbiology/articles/10.3389/fmicb.2023.1331233/full},
	doi = {10.3389/fmicb.2023.1331233},
	abstract = {Background
In the evolving landscape of microbiology and microbiome analysis, the integration of machine learning is crucial for understanding complex microbial interactions, and predicting and recognizing novel functionalities within extensive datasets. However, the effectiveness of these methods in microbiology faces challenges due to the complex and heterogeneous nature of microbial data, further complicated by low signal-to-noise ratios, context-dependency, and a significant shortage of appropriately labeled datasets. This study introduces the ProkBERT model family, a collection of large language models, designed for genomic tasks. It provides a generalizable sequence representation for nucleotide sequences, learned from unlabeled genome data. This approach helps overcome the above-mentioned limitations in the field, thereby improving our understanding of microbial ecosystems and their impact on health and disease.

Methods
ProkBERT models are based on transfer learning and self-supervised methodologies, enabling them to use the abundant yet complex microbial data effectively. The introduction of the novel Local Context-Aware (LCA) tokenization technique marks a significant advancement, allowing ProkBERT to overcome the contextual limitations of traditional transformer models. This methodology not only retains rich local context but also demonstrates remarkable adaptability across various bioinformatics tasks.

Results
In practical applications such as promoter prediction and phage identification, the ProkBERT models show superior performance. For promoter prediction tasks, the top-performing model achieved a Matthews Correlation Coefficient (MCC) of 0.74 for E. coli and 0.62 in mixed-species contexts. In phage identification, ProkBERT models consistently outperformed established tools like VirSorter2 and DeepVirFinder, achieving an MCC of 0.85. These results underscore the models' exceptional accuracy and generalizability in both supervised and unsupervised tasks.

Conclusions
The ProkBERT model family is a compact yet powerful tool in the field of microbiology and bioinformatics. Its capacity for rapid, accurate analyses and its adaptability across a spectrum of tasks marks a significant advancement in machine learning applications in microbiology. The models are available on GitHub (https://github.com/nbrg-ppcu/prokbert) and HuggingFace (https://huggingface.co/nerualbioinfo) providing an accessible tool for the community.},
	language = {English},
	urldate = {2025-08-07},
	journal = {Frontiers in Microbiology},
	author = {Ligeti, Balázs and Szepesi-Nagy, István and Bodnár, Babett and Ligeti-Nagy, Noémi and Juhász, János},
	month = jan,
	year = {2024},
	note = {15 citations (Crossref/DOI) [2025-10-22]
Publisher: Frontiers},
	keywords = {Printed, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/XQ3IYAJJ/2024-01-12 - Ligeti et al. - ProkBERT family genomic language models for microbiome applications.pdf:application/pdf},
}

@article{yan_recent_2025,
	title = {Recent advances in deep learning and language models for studying the microbiome},
	volume = {15},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2024.1494474/full},
	doi = {10.3389/fgene.2024.1494474},
	abstract = {Recent advancements in deep learning, particularly large language models (LLMs), made a significant impact on how researchers study microbiome and metagenomics data. Microbial protein and genomic sequences, like natural languages, form a language of life, enabling the adoption of LLMs to extract useful insights from complex microbial ecologies. In this paper, we review applications of deep learning and language models in analyzing microbiome and metagenomics data. We focus on problem formulations, necessary datasets, and the integration of language modeling techniques. We provide an extensive overview of protein/genomic language modeling and their contributions to microbiome studies. We also discuss applications such as novel viromics language modeling, biosynthetic gene cluster prediction, and knowledge integration for metagenomics studies.},
	language = {English},
	urldate = {2025-08-07},
	journal = {Frontiers in Genetics},
	author = {Yan, Binghao and Nam, Yunbi and Li, Lingyao and Deek, Rebecca A. and Li, Hongzhe and Ma, Siyuan},
	month = jan,
	year = {2025},
	note = {3 citations (Crossref/DOI) [2025-10-22]
Publisher: Frontiers},
	keywords = {Printed, Podcast},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/M4HQEBBI/2025-01-07 - Yan et al. - Recent advances in deep learning and language models for studying the microbiome.pdf:application/pdf},
}

@article{brandes_genome-wide_2023,
	title = {Genome-wide prediction of disease variant effects with a deep protein language model},
	volume = {55},
	copyright = {2023 The Author(s)},
	issn = {1546-1718},
	url = {https://www.nature.com/articles/s41588-023-01465-0},
	doi = {10.1038/s41588-023-01465-0},
	abstract = {Predicting the effects of coding variants is a major challenge. While recent deep-learning models have improved variant effect prediction accuracy, they cannot analyze all coding variants due to dependency on close homologs or software limitations. Here we developed a workflow using ESM1b, a 650-million-parameter protein language model, to predict all {\textasciitilde}450 million possible missense variant effects in the human genome, and made all predictions available on a web portal. ESM1b outperformed existing methods in classifying {\textasciitilde}150,000 ClinVar/HGMD missense variants as pathogenic or benign and predicting measurements across 28 deep mutational scan datasets. We further annotated {\textasciitilde}2 million variants as damaging only in specific protein isoforms, demonstrating the importance of considering all isoforms when predicting variant effects. Our approach also generalizes to more complex coding variants such as in-frame indels and stop-gains. Together, these results establish protein language models as an effective, accurate and general approach to predicting variant effects.},
	language = {en},
	number = {9},
	urldate = {2025-08-07},
	journal = {Nature Genetics},
	author = {Brandes, Nadav and Goldman, Grant and Wang, Charlotte H. and Ye, Chun Jimmie and Ntranos, Vasilis},
	month = aug,
	year = {2023},
	note = {312 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Read},
	pages = {1512--1522},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/8TS597B9/2023-09 - Brandes et al. - Genome-wide prediction of disease variant effects with a deep protein language model.pdf:application/pdf},
}

@article{jagota_cross-protein_2023,
	title = {Cross-protein transfer learning substantially improves disease variant prediction},
	volume = {24},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-023-03024-6},
	doi = {10.1186/s13059-023-03024-6},
	abstract = {Genetic variation in the human genome is a major determinant of individual disease risk, but the vast majority of missense variants have unknown etiological effects. Here, we present a robust learning framework for leveraging saturation mutagenesis experiments to construct accurate computational predictors of proteome-wide missense variant pathogenicity.},
	number = {1},
	urldate = {2025-08-07},
	journal = {Genome Biology},
	author = {Jagota, Milind and Ye, Chengzhong and Albors, Carlos and Rastogi, Ruchir and Koehl, Antoine and Ioannidis, Nilah and Song, Yun S.},
	month = aug,
	year = {2023},
	note = {51 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Read},
	pages = {182},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/9FMQIK48/2023-08-07 - Jagota et al. - Cross-protein transfer learning substantially improves disease variant prediction.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/CT8STNSG/s13059-023-03024-6.html:text/html},
}

@misc{li_genomic_2023,
	title = {Genomic {Interpreter}: {A} {Hierarchical} {Genomic} {Deep} {Neural} {Network} with {1D} {Shifted} {Window} {Transformer}},
	shorttitle = {Genomic {Interpreter}},
	url = {http://arxiv.org/abs/2306.05143},
	doi = {10.48550/arXiv.2306.05143},
	abstract = {Given the increasing volume and quality of genomics data, extracting new insights requires interpretable machine-learning models. This work presents Genomic Interpreter: a novel architecture for genomic assay prediction. This model outperforms the state-of-the-art models for genomic assay prediction tasks. Our model can identify hierarchical dependencies in genomic sites. This is achieved through the integration of 1D-Swin, a novel Transformer-based block designed by us for modelling long-range hierarchical data. Evaluated on a dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter demonstrates superior performance in chromatin accessibility and gene expression prediction and unmasks the underlying `syntax' of gene regulation.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Li, Zehui and Das, Akashaditya and Beardall, William A. V. and Zhao, Yiren and Stan, Guy-Bart},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05143 [cs]},
	keywords = {Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/ZLYBN6R5/2023-06-28 - Li et al. - Genomic Interpreter A Hierarchical Genomic Deep Neural Network with 1D Shifted Window Transformer.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/GWG7QIFV/2306.html:text/html},
}

@article{hwang_glm_2024,
	title = {[{gLM}] {Genomic} language model predicts protein co-regulation and function},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-46947-9},
	doi = {10.1038/s41467-024-46947-9},
	abstract = {Deciphering the relationship between a gene and its genomic context is fundamental to understanding and engineering biological systems. Machine learning has shown promise in learning latent relationships underlying the sequence-structure-function paradigm from massive protein sequence datasets. However, to date, limited attempts have been made in extending this continuum to include higher order genomic context information. Evolutionary processes dictate the specificity of genomic contexts in which a gene is found across phylogenetic distances, and these emergent genomic patterns can be leveraged to uncover functional relationships between gene products. Here, we train a genomic language model (gLM) on millions of metagenomic scaffolds to learn the latent functional and regulatory relationships between genes. gLM learns contextualized protein embeddings that capture the genomic context as well as the protein sequence itself, and encode biologically meaningful and functionally relevant information (e.g. enzymatic function, taxonomy). Our analysis of the attention patterns demonstrates that gLM is learning co-regulated functional modules (i.e. operons). Our findings illustrate that gLM’s unsupervised deep learning of the metagenomic corpus is an effective and promising approach to encode functional semantics and regulatory syntax of genes in their genomic contexts and uncover complex relationships between genes in a genomic region.},
	language = {en},
	number = {1},
	urldate = {2025-08-07},
	journal = {Nature Communications},
	author = {Hwang, Yunha and Cornman, Andre L. and Kellogg, Elizabeth H. and Ovchinnikov, Sergey and Girguis, Peter R.},
	month = apr,
	year = {2024},
	note = {61 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast, Read},
	pages = {2880},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/Y4QAETJC/2024-04-03 - Hwang et al. - Genomic language model predicts protein co-regulation and function.pdf:application/pdf},
}

@article{karollus_species-aware_2024,
	title = {Species-aware {DNA} language models capture regulatory elements and their evolution},
	volume = {25},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-024-03221-x},
	doi = {10.1186/s13059-024-03221-x},
	abstract = {The rise of large-scale multi-species genome sequencing projects promises to shed new light on how genomes encode gene regulatory instructions. To this end, new algorithms are needed that can leverage conservation to capture regulatory elements while accounting for their evolution.},
	number = {1},
	urldate = {2025-08-07},
	journal = {Genome Biology},
	author = {Karollus, Alexander and Hingerl, Johannes and Gankin, Dennis and Grosshauser, Martin and Klemon, Kristian and Gagneur, Julien},
	month = apr,
	year = {2024},
	note = {30 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed, Read},
	pages = {83},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/5577H49F/2024-04-02 - Karollus et al. - Species-aware DNA language models capture regulatory elements and their evolution.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/PFZXDTN6/s13059-024-03221-x.html:text/html},
}

@article{he_lucaone_2025,
	title = {[{LucaOne}] {Generalized} biological foundation model with unified nucleic acid and protein language},
	volume = {7},
	copyright = {2025 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-025-01044-4},
	doi = {10.1038/s42256-025-01044-4},
	abstract = {The language of biology, encoded in DNA, RNA and proteins, forms the foundation of life but remains challenging to decode owing to its complexity. Traditional computational methods often struggle to integrate information across these molecules, limiting a comprehensive understanding of biological systems. Advances in natural language processing with pre-trained models offer possibilities for interpreting biological language. Here we introduce LucaOne, a pre-trained foundation model trained on nucleic acid and protein sequences from 169,861 species. Through large-scale data integration and semi-supervised learning, LucaOne shows an understanding of key biological principles, such as DNA–protein translation. Using few-shot learning, it effectively comprehends the central dogma of molecular biology and performs competitively on tasks involving DNA, RNA or protein inputs. Our results highlight the potential of unified foundation models to address complex biological questions, providing an adaptable framework for bioinformatics research and enhancing the interpretation of life’s complexity.},
	language = {en},
	number = {6},
	urldate = {2025-08-07},
	journal = {Nature Machine Intelligence},
	author = {He, Yong and Fang, Pan and Shan, Yongtao and Pan, Yuanfei and Wei, Yanhong and Chen, Yichang and Chen, Yihao and Liu, Yi and Zeng, Zhenyu and Zhou, Zhan and Zhu, Feng and Holmes, Edward C. and Ye, Jieping and Li, Jun and Shu, Yuelong and Shi, Mang and Li, Zhaorong},
	month = jun,
	year = {2025},
	note = {5 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast},
	pages = {942--953},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/229DEYNE/2025-06-18 - He et al. - [LucaOne] Generalized biological foundation model with unified nucleic acid and protein language.pdf:application/pdf},
}

@misc{consens_transformers_2023,
	title = {To {Transformers} and {Beyond}: {Large} {Language} {Models} for the {Genome}},
	shorttitle = {To {Transformers} and {Beyond}},
	url = {http://arxiv.org/abs/2311.07621},
	doi = {10.48550/arXiv.2311.07621},
	abstract = {In the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational challenges. This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the transformer architecture, in genomics. Building on the foundation of traditional convolutional neural networks and recurrent neural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics. Additionally, we contemplate the future of genomic modeling beyond the transformer architecture based on current trends in research. The paper aims to serve as a guide for computational biologists and computer scientists interested in LLMs for genomic data. We hope the paper can also serve as an educational introduction and discussion for biologists to a fundamental shift in how we will be analyzing genomic data in the future.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Consens, Micaela E. and Dufault, Cameron and Wainberg, Michael and Forster, Duncan and Karimzadeh, Mehran and Goodarzi, Hani and Theis, Fabian J. and Moses, Alan and Wang, Bo},
	month = nov,
	year = {2023},
	note = {arXiv:2311.07621 [q-bio]},
	keywords = {Printed, Read},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/GFUCWWFK/2023-11-13 - Consens et al. - To Transformers and Beyond Large Language Models for the Genome.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/LYTK2X6N/2311.html:text/html},
}

@misc{zhou_dnabert-s_2024,
	title = {{DNABERT}-{S}: {Pioneering} {Species} {Differentiation} with {Species}-{Aware} {DNA} {Embeddings}},
	shorttitle = {{DNABERT}-{S}},
	url = {http://arxiv.org/abs/2402.08777},
	doi = {10.48550/arXiv.2402.08777},
	abstract = {We introduce DNABERT-S, a tailored genome model that develops species-aware embeddings to naturally cluster and segregate DNA sequences of different species in the embedding space. Differentiating species from genomic sequences (i.e., DNA and RNA) is vital yet challenging, since many real-world species remain uncharacterized, lacking known genomes for reference. Embedding-based methods are therefore used to differentiate species in an unsupervised manner. DNABERT-S builds upon a pre-trained genome foundation model named DNABERT-2. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enhance it with the proposed Curriculum Contrastive Learning (C\${\textasciicircum}2\$LR) strategy. Empirical results on 23 diverse datasets show DNABERT-S's effectiveness, especially in realistic label-scarce scenarios. For example, it identifies twice more species from a mixture of unlabeled genomic sequences, doubles the Adjusted Rand Index (ARI) in species clustering, and outperforms the top baseline's performance in 10-shot species classification with just a 2-shot training. Model, codes, and data are publicly available at {\textbackslash}url\{https://github.com/MAGICS-LAB/DNABERT\_S\}.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Zhou, Zhihan and Wu, Weimin and Ho, Harrison and Wang, Jiayi and Shi, Lizhen and Davuluri, Ramana V. and Wang, Zhong and Liu, Han},
	month = oct,
	year = {2024},
	note = {arXiv:2402.08777 [q-bio]},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/6UMRVUCC/2024-10-22 - Zhou et al. - DNABERT-S Pioneering Species Differentiation with Species-Aware DNA Embeddings.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/FE2M43VQ/2402.html:text/html},
}

@article{de_almeida_chatnt_2025,
	title = {[{ChatNT}] {A} multimodal conversational agent for {DNA}, {RNA} and protein tasks},
	volume = {7},
	copyright = {2025 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-025-01047-1},
	doi = {10.1038/s42256-025-01047-1},
	abstract = {Language models are thriving, powering conversational agents that assist and empower humans to solve a number of tasks. Recently, these models were extended to support additional modalities including vision, audio and video, demonstrating impressive capabilities across multiple domains, including healthcare. Still, conversational agents remain limited in biology as they cannot yet fully comprehend biological sequences. Meanwhile, high-performance foundation models for biological sequences have been built through self-supervision over sequencing data, but these need to be fine-tuned for each specific application, preventing generalization between tasks. In addition, these models are not conversational, which limits their utility to users with coding capabilities. Here we propose to bridge the gap between biology foundation models and conversational agents by introducing ChatNT, a multimodal conversational agent with an advanced understanding of biological sequences. ChatNT achieves new state-of-the-art results on the Nucleotide Transformer benchmark while being able to solve all tasks at once, in English, and to generalize to unseen questions. In addition, we have curated a set of more biologically relevant instruction tasks from DNA, RNA and proteins, spanning multiple species, tissues and biological processes. ChatNT reaches performance on par with state-of-the-art specialized methods on those tasks. We also present a perplexity-based technique to help calibrate the confidence of our model predictions. By applying attribution methods through the English decoder and DNA encoder, we demonstrate that ChatNT’s answers are based on biologically coherent features such as detecting the promoter TATA motif or splice site dinucleotides. Our framework for genomics instruction tuning can be extended to more tasks and data modalities (for example, structure and imaging), making it a widely applicable tool for biology. ChatNT provides a potential direction for building generally capable agents that understand biology from first principles while being accessible to users with no coding background.},
	language = {en},
	number = {6},
	urldate = {2025-08-07},
	journal = {Nature Machine Intelligence},
	author = {de Almeida, Bernardo P. and Richard, Guillaume and Dalla-Torre, Hugo and Blum, Christopher and Hexemer, Lorenz and Pandey, Priyanka and Laurent, Stefan and Rajesh, Chandana and Lopez, Marie and Laterre, Alexandre and Lang, Maren and Şahin, Uğur and Beguir, Karim and Pierrot, Thomas},
	month = jun,
	year = {2025},
	note = {2 citations (Crossref/DOI) [2025-10-22]
Publisher: Nature Publishing Group},
	keywords = {Printed, Podcast},
	pages = {928--941},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/4K8RQ9LR/2025-06-06 - de Almeida et al. - [ChatNT] A multimodal conversational agent for DNA, RNA and protein tasks.pdf:application/pdf},
}

@misc{almeida_segmentnt_2024,
	title = {{SegmentNT}: annotating the genome at single-nucleotide resolution with {DNA} foundation models},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{SegmentNT}},
	url = {https://www.biorxiv.org/content/10.1101/2024.03.14.584712v1},
	doi = {10.1101/2024.03.14.584712},
	abstract = {Foundation models have achieved remarkable success in several fields such as natural language processing, computer vision and more recently biology. DNA foundation models in particular are emerging as a promising approach for genomics. However, so far no model has delivered granular, nucleotide-level predictions across a wide range of genomic and regulatory elements, limiting its practical usefulness. In this paper, we build on our previous work on the Nucleotide Transformer (NT) to develop a segmentation model, SegmentNT, that processes input DNA sequences up to 30kb length to predict 14 different classes of genomics elements at single nucleotide resolution. By utilizing pre-trained weights from NT, SegmentNT surpasses the performance of several ablation models, including convolution networks with one-hot encoded nucleotide sequences and models trained from scratch. SegmentNT can process multiple sequence lengths with zero-shot generalization for sequences of up to 50kb. We show improved performance on the detection of splice sites throughout the genome and demonstrate strong nucleotide-level precision. Because it evaluates all gene elements simultaneously, SegmentNT can predict the impact of sequence variants not only on splice site changes but also on exon and intron rearrangements in transcript isoforms. Finally, we show that a SegmentNT model trained on human genomics elements can generalize to elements of different species and that a trained multispecies SegmentNT model achieves stronger generalization for all genic elements on unseen species. In summary, SegmentNT demonstrates that DNA foundation models can tackle complex, granular tasks in genomics at a single-nucleotide resolution. SegmentNT can be easily extended to additional genomics elements and species, thus representing a new paradigm on how we analyze and interpret DNA. We make our SegmentNT-30kb human and multispecies models available on our github repository in Jax and HuggingFace space in Pytorch.},
	language = {en},
	urldate = {2025-08-07},
	publisher = {bioRxiv},
	author = {Almeida, Bernardo P. de and Dalla-Torre, Hugo and Richard, Guillaume and Blum, Christopher and Hexemer, Lorenz and Gélard, Maxence and Pandey, Priyanka and Laurent, Stefan and Laterre, Alexandre and Lang, Maren and Şahin, Uğur and Beguir, Karim and Pierrot, Thomas},
	month = mar,
	year = {2024},
	note = {16 citations (Semantic Scholar/DOI) [2025-10-22]
10 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.03.14.584712
Section: New Results},
	keywords = {Printed, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/Y5S5EUZG/2024-03-15 - Almeida et al. - SegmentNT annotating the genome at single-nucleotide resolution with DNA foundation models.pdf:application/pdf},
}

@misc{zhu_cd-gpt_2024,
	title = {{CD}-{GPT}: {A} {Biological} {Foundation} {Model} {Bridging} the {Gap} between {Molecular} {Sequences} {Through} {Central} {Dogma}},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {{CD}-{GPT}},
	url = {https://www.biorxiv.org/content/10.1101/2024.06.24.600337v1},
	doi = {10.1101/2024.06.24.600337},
	abstract = {The central dogma serves as a fundamental framework for understanding the flow and expression of genetic information within living organisms, facilitating the connection of diverse biological sequences across molecule types. In this study, we present CD-GPT (Central Dogma Generative Pretrained Transformer), a generative biological foundation model comprising 1 billion parameters, aiming to capture the intricate system-wide molecular interactions in biological systems. We introduce the concept of a unified representational space and employ a shared, multi-molecule vocabulary to effectively represent biological sequences and narrow their distance in the embedding space. Through extensive pretraining on comprehensive full molecular level data, CD-GPT exhibits exceptional performance in a wide range of predictive and generative downstream tasks, encompassing mono-molecular and multi-molecular analyses. Notably, CD-GPT excels in tasks such as genomic element detection, protein property prediction, RNA-protein interaction identification and also generative tasks like de novo protein generation and reverse translation. The versatility of CD-GPT opens up promising avenues for advanced multi-omics analysis.},
	language = {en},
	urldate = {2025-08-07},
	publisher = {bioRxiv},
	author = {Zhu, Xiao and Qin, Chenchen and Wang, Fang and Yang, Fan and He, Bing and Zhao, Yu and Yao, Jianhua},
	month = jun,
	year = {2024},
	note = {0 citations (Crossref/DOI) [2025-10-22]
Pages: 2024.06.24.600337
Section: New Results},
	keywords = {Printed, Read},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/C62UGDQI/2024-06-28 - Zhu et al. - CD-GPT A Biological Foundation Model Bridging the Gap between Molecular Sequences Through Central D.pdf:application/pdf},
}

@article{marquet_vespag_2024,
	title = {[{VespaG}] {Expert}-guided protein language models enable accurate and blazingly fast fitness prediction},
	volume = {40},
	issn = {1367-4811},
	url = {https://doi.org/10.1093/bioinformatics/btae621},
	doi = {10.1093/bioinformatics/btae621},
	abstract = {Exhaustive experimental annotation of the effect of all known protein variants remains daunting and expensive, stressing the need for scalable effect predictions. We introduce VespaG, a blazingly fast missense amino acid variant effect predictor, leveraging protein language model (pLM) embeddings as input to a minimal deep learning model.To overcome the sparsity of experimental training data, we created a dataset of 39 million single amino acid variants from the human proteome applying the multiple sequence alignment-based effect predictor GEMME as a pseudo standard-of-truth. This setup increases interpretability compared to the baseline pLM and is easily retrainable with novel or updated pLMs. Assessed against the ProteinGym benchmark (217 multiplex assays of variant effect—MAVE—with 2.5 million variants), VespaG achieved a mean Spearman correlation of 0.48 ± 0.02, matching top-performing methods evaluated on the same data. VespaG has the advantage of being orders of magnitude faster, predicting all mutational landscapes of all proteins in proteomes such as Homo sapiens or Drosophila melanogaster in under 30 min on a consumer laptop (12-core CPU, 16 GB RAM).VespaG is available freely at https://github.com/jschlensok/vespag. The associated training data and predictions are available at https://doi.org/10.5281/zenodo.11085958.},
	number = {11},
	urldate = {2025-08-07},
	journal = {Bioinformatics},
	author = {Marquet, Céline and Schlensok, Julius and Abakarova, Marina and Rost, Burkhard and Laine, Elodie},
	month = nov,
	year = {2024},
	note = {20 citations (Crossref/DOI) [2025-10-22]},
	keywords = {Printed},
	pages = {btae621},
	file = {Full Text PDF:/Users/meehl.joshua/Zotero/storage/H38497SI/2024-11-01 - Marquet et al. - [VespaG] Expert-guided protein language models enable accurate and blazingly fast fitness prediction.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/V53IB62U/btae621.html:text/html},
}

@misc{liu_metagene-1_2025,
	title = {{METAGENE}-1: {Metagenomic} {Foundation} {Model} for {Pandemic} {Monitoring}},
	shorttitle = {{METAGENE}-1},
	url = {http://arxiv.org/abs/2501.02045},
	doi = {10.48550/arXiv.2501.02045},
	abstract = {We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Liu, Ollie and Jaghouar, Sami and Hagemann, Johannes and Wang, Shangshang and Wiemels, Jason and Kaufman, Jeff and Neiswanger, Willie},
	month = jan,
	year = {2025},
	note = {arXiv:2501.02045 [q-bio]},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/N2CD9PIT/2025-01-03 - Liu et al. - METAGENE-1 Metagenomic Foundation Model for Pandemic Monitoring.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/YBYVLV3L/2501.html:text/html},
}

@misc{liu_life-code_2025,
	title = {Life-{Code}: {Central} {Dogma} {Modeling} with {Multi}-{Omics} {Sequence} {Unification}},
	shorttitle = {Life-{Code}},
	url = {http://arxiv.org/abs/2502.07299},
	doi = {10.48550/arXiv.2502.07299},
	abstract = {The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.},
	urldate = {2025-08-07},
	publisher = {arXiv},
	author = {Liu, Zicheng and Li, Siyuan and Chen, Zhiyuan and Wu, Fang and Yu, Chang and Yang, Qirong and Guo, Yucheng and Yang, Yujie and Zhang, Xiaoming and Li, Stan Z.},
	month = jun,
	year = {2025},
	note = {arXiv:2502.07299 [cs]},
	keywords = {Printed, Podcast},
	file = {Preprint PDF:/Users/meehl.joshua/Zotero/storage/LFXSCF3K/2025-06-15 - Liu et al. - Life-Code Central Dogma Modeling with Multi-Omics Sequence Unification.pdf:application/pdf;Snapshot:/Users/meehl.joshua/Zotero/storage/P626N5L8/2502.html:text/html},
}


