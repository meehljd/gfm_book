::: {.callout-warning .content-visible when-profile="draft"}

:::

# Transformer Architecture for Genomics {#sec-transformers}

The transformer architecture revolutionized natural language processing in 2017 and rapidly spread to other domains, including genomics. Between 2018 and 2023, transformers became the dominant paradigm for modeling biological sequences, powering protein language models like ESM and genomic models like DNABERT and Nucleotide Transformer. This chapter examines the core mechanisms that make transformers effective for genomic data, their architectural components, and the practical considerations for applying them to DNA, RNA, and protein sequences.

We begin by motivating why transformers matter for genomics, then systematically unpack the self-attention mechanism, positional encodings, and the standard transformer block. We discuss scaling considerations specific to genomic sequences, survey transformer variants used in genomic applications, and address training challenges. Finally, we consider the limitations that led to the development of alternative architectures like state space models, which we cover in depth in Chapter @sec-dna.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (chapter overview)**  
Figure: Timeline showing major transformer milestones in NLP (Vaswani 2017, BERT 2018, GPT-3 2020) alongside genomic applications (DNABERT 2020, Enformer 2021, Nucleotide Transformer 2023, Evo 2024), with arrows indicating architectural innovations flowing from NLP to genomics.
:::


## Why Transformers for Genomics?

Convolutional neural networks dominated early genomic deep learning because they naturally capture local patterns like transcription factor binding motifs, splice sites, and promoter elements. CNNs apply the same learned filters across all positions, implementing a form of translation equivariance that matches the biological intuition that a motif has similar effects regardless of where it appears in a regulatory region.

However, CNNs have fundamental limitations when modeling genomic regulation. Their receptive fields are inherently local. Even with stacked layers and dilated convolutions, the effective context a CNN can integrate remains bounded, often spanning only a few kilobases. Gene regulation in eukaryotes operates over vastly longer ranges. Enhancers routinely act hundreds of kilobases from their target promoters. Topologically associating domains (TADs) organize chromatin contacts at megabase scales. Distal regulatory variants identified through GWAS often lie far from coding sequences, exerting their effects through long-range interactions that CNNs struggle to capture directly.

Transformers address this limitation through their self-attention mechanism, which allows each position in a sequence to directly attend to all other positions in a single operation. This global receptive field enables modeling of long-range dependencies without requiring information to propagate through many layers. For genomics, this means enhancer-promoter interactions can be learned directly, regulatory context from distant elements can inform predictions at any position, and models can integrate information across entire genes or regulatory domains.

The attention mechanism also offers flexibility in how models aggregate information. Rather than applying fixed convolutional kernels, attention learns position-specific aggregation patterns that can adapt to the biological context. Different attention heads can specialize in different types of interactions, potentially capturing diverse regulatory mechanisms like enhancer-promoter communication, insulator boundaries, or promoter competition.

These properties made transformers particularly attractive for genomic foundation models. Proteins evolved through deep evolutionary time, creating complex structure-function relationships that benefit from global context integration. DNA regulatory elements interact across large genomic distances in ways that local CNNs miss. RNA secondary structure brings distant nucleotides into close three-dimensional proximity, creating long-range dependencies in the linear sequence.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (CNN vs transformer receptive fields)**  
Figure: Side-by-side comparison showing a CNN's limited, layered receptive field (triangular expansion through convolution stack) versus a transformer's global receptive field (all positions connect to all positions in attention layer). Overlay example showing enhancer-promoter pair 100kb apart that transformer captures in one layer but requires many CNN layers to connect.
:::


## Architectural Paradigms for Sequences

Before diving into transformer mechanics, we briefly situate them within the broader landscape of sequence modeling architectures. Three main paradigms have dominated genomic deep learning over the past decade.

CNNs were the first architecture to achieve strong performance on genomic tasks, pioneered by models like DeepSEA and DeepBind. Their strength lies in parameter efficiency (shared filters across positions) and the ability to learn hierarchical local features (motifs, motif combinations, regulatory grammar). Chapter 5 covers CNN-based regulatory prediction models in detail. CNNs remain competitive for tasks where local context dominates, such as splice site recognition or promoter classification. However, their limited receptive fields constrain performance on tasks requiring long-range integration.

Transformers emerged from natural language processing, where they demonstrated unprecedented ability to model linguistic context and semantic relationships. The core innovation is self-attention, which computes interactions between all sequence positions simultaneously. This provides global context but comes at a computational cost that scales quadratically with sequence length. Despite this expense, transformers dominated genomic modeling from roughly 2018 through 2023 because their ability to capture long-range dependencies outweighed their computational demands for many applications.

State space models (SSMs) represent a more recent development, offering linear computational complexity while maintaining long-range modeling capability. Architectures like Mamba have begun to challenge transformers' dominance, particularly for ultra-long genomic contexts where transformer quadratic scaling becomes prohibitive. We defer detailed discussion of SSMs to Chapter @sec-dna, focusing here on transformers as the foundation from which these newer architectures emerged.

The progression from CNNs to transformers to SSMs reflects an ongoing tension between computational efficiency and modeling capacity. Each paradigm offers distinct trade-offs that remain relevant for different genomic applications. Understanding transformers in depth provides essential context for appreciating both why they succeeded and why the field is now exploring alternatives.


## The Self-Attention Mechanism

Self-attention is the defining component of transformer architecture. It allows each position in a sequence to aggregate information from all other positions through learned, input-dependent weights. This section unpacks the mathematical formulation and intuition behind self-attention.

At each position in the input sequence, self-attention computes three vectors: a query, a key, and a value. These are produced by multiplying the input embedding at that position by three learned weight matrices $W^Q$, $W^K$, and $W^V$. The query represents "what this position is looking for," the key represents "what this position offers," and the value represents "what information this position carries."

The attention mechanism then computes similarity scores between each query and all keys. Specifically, for position $i$, we compute the dot product between its query $q_i$ and every key $k_j$ for positions $j = 1, \ldots, L$, where $L$ is the sequence length. These scores are scaled by the square root of the key dimension $\sqrt{d_k}$ to prevent gradient instability when dimensions are large, yielding:

$$
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
$$

A softmax function converts these scores into a probability distribution over positions, producing attention weights $\alpha_{ij}$ that sum to one across all $j$:

$$
\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_{j'=1}^L \exp(\text{score}(q_i, k_{j'}))}
$$

These weights determine how much each position $i$ attends to each other position $j$. High attention weight means position $i$ strongly aggregates information from position $j$; low weight means that position contributes little to the output at position $i$.

Finally, the output at position $i$ is computed as a weighted sum of all value vectors, where the weights are the attention scores:

$$
\text{output}_i = \sum_{j=1}^L \alpha_{ij} v_j
$$

This weighted aggregation is the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with the mixture proportions learned through backpropagation based on task objectives.

Multi-head attention extends this mechanism by running multiple attention operations in parallel, each with different learned projections. If we use $H$ heads, we split the model dimension $d$ into $H$ subspaces of dimension $d/H$, compute separate queries, keys, and values for each head, run attention independently, then concatenate the outputs and project back to dimension $d$. This allows different heads to capture different types of relationships. In genomic models, different heads might specialize in different regulatory patterns, such as one head attending to nearby positions (local context) while another attends to distal elements (long-range interactions).

The computational cost of self-attention is quadratic in sequence length because we compute $L \times L$ attention scores. For a sequence of length $L$ and model dimension $d$, computing all queries, keys, and values requires $O(Ld^2)$ operations, while computing the $L^2$ attention scores requires $O(L^2d)$ operations. The quadratic term dominates for long sequences, making standard self-attention expensive for genomic contexts spanning hundreds of kilobases.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (attention computation flow)**  
Figure: Schematic showing input embeddings transformed into Q, K, V matrices, computation of attention scores as QK^T heatmap, softmax normalization, and weighted aggregation of values. Include small example with L=6 positions showing how output at position 3 aggregates information from all positions with different weights.
:::


## Positional Encoding: Representing Sequence Order

Transformers have a critical limitation: self-attention is permutation invariant. If you shuffle the order of input tokens, each position still attends to the same set of other positions with the same values, just in different order. The model has no inherent notion of sequence position. For genomic data where order matters fundamentally (5' to 3' directionality, strand orientation, spatial organization), this poses a serious problem.

Positional encodings solve this by injecting information about token positions into the model. The original Transformer [@vaswani_attention_2017] used sinusoidal functions with different frequencies for each dimension. For position $pos$ and dimension $i$, the encoding is:

$$
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$
$$
\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

These fixed sinusoidal patterns have useful properties: they are deterministic (same for all sequences), allow the model to learn to attend by relative positions (since $\text{PE}(pos+k)$ can be written as a linear function of $\text{PE}(pos)$), and generalize to sequence lengths not seen during training.

However, many genomic models use learned positional embeddings instead. These are simply lookup tables where each position $pos$ has a learned vector $E_{pos}$ that is added to the input embedding. Learned positional embeddings offer more flexibility, allowing the model to discover position-dependent patterns specific to genomic data. The trade-off is that they must be trained and do not automatically extrapolate to longer sequences than those seen during training.

Relative positional encodings represent a middle ground. Rather than encoding absolute positions, these schemes encode the relative distance or relationship between positions. T5-style relative position bias adds a learnable scalar bias to attention scores based on the distance between query and key positions. This helps the model learn that nearby positions often have stronger interactions than distant ones, while remaining agnostic about absolute position.

Attention with Linear Biases (ALiBi) takes this further by adding a fixed linear penalty to attention scores based on distance, without any learned parameters. For a head with slope $m$, the attention score between positions separated by distance $|i - j|$ is penalized by $m|i - j|$. Different heads use different slopes, encouraging some to focus locally and others globally. ALiBi has shown strong generalization to longer contexts than seen during training, making it attractive for genomic applications where sequence length varies.

Rotary Position Embeddings (RoPE) encode positions by rotating the query and key vectors in a high-dimensional space. The rotation angle depends on position, ensuring that the dot product between query and key depends on their relative distance. RoPE has become popular in recent language models and is beginning to appear in genomic transformers because it combines the benefits of relative encoding with efficient implementation.

For genomics, positional encoding must respect biological semantics. DNA has strand directionality: the sequence ACGT on the forward strand has different regulatory meaning than ACGT on the reverse strand. Positional encodings should allow the model to learn strand-specific patterns. Some genomic transformers use separate positional embeddings for forward and reverse strands. Others rely on the model to learn strand orientation from sequence content itself.

Genomic coordinates pose another challenge. Should position 1 in the model correspond to a fixed genomic coordinate (e.g., transcription start site), or should it be relative to some local landmark? Different models make different choices based on their tasks. Models predicting regulatory activity often center sequences on gene promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings that reflect sequence order without reference to genomic coordinates.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (positional encoding schemes)**  
Figure: Four panels comparing positional encoding methods. (a) Sinusoidal: wave patterns at different frequencies. (b) Learned: discrete embedding vectors. (c) ALiBi: attention score modification with distance-dependent penalty. (d) RoPE: rotation in 2D subspace. Include small example showing how different schemes affect attention between positions separated by different distances.
:::


## The Transformer Block Architecture

A transformer model consists of stacked blocks, each containing self-attention and feed-forward components connected by residual connections and layer normalization. Understanding this block structure is essential for grasping how information flows through transformer models.

Each transformer block has two main components: a multi-head self-attention layer and a position-wise feed-forward network. The attention layer enables global communication across all positions, allowing each position to gather information from the entire sequence. The feed-forward network processes each position independently, applying nonlinear transformations to the aggregated information.

Layer normalization stabilizes training by normalizing activations across the feature dimension at each position. Two conventions exist for where to place layer normalization relative to the self-attention and feed-forward sublayers. Post-norm places normalization after each sublayer, applying it to the output before the residual connection. Pre-norm places normalization before each sublayer, normalizing the input to that sublayer. Pre-norm has become more common in recent models because it improves training stability, particularly for deep networks, though post-norm can achieve slightly better final performance with careful tuning.

The feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four, applies a nonlinear activation function (often GELU), then projects back to the original dimension. This expansion allows the model to process the attention-aggregated information through a high-dimensional nonlinear transformation before producing the output for the next layer.

Residual connections wrap around both the attention and feed-forward sublayers, adding the input directly to the output. These connections serve two critical functions. First, they provide gradient highways during backpropagation, allowing gradients to flow directly through many layers without being repeatedly transformed. This enables training of very deep networks. Second, they create an inductive bias toward small, incremental refinements of the representation at each layer, rather than forcing each layer to construct an entirely new representation.

The flow through a transformer block with pre-norm looks like this: the input $X$ is first normalized, then processed by multi-head attention to produce $X'$, which is added back to the original input via residual connection, yielding $X + X'$. This sum is again normalized, passed through the feed-forward network to produce $X''$, and added to the input of the feed-forward layer via another residual connection, yielding the final output $(X + X') + X'' = X + X' + X''$.

Stacking depth determines how many times this refinement process occurs. Shallow transformers with few layers are parameter-efficient but may lack capacity for complex tasks. Deep transformers with many layers can learn more sophisticated representations but require more computation and careful optimization. Most genomic transformers use between 6 and 24 layers, though this varies by application. Models focused on short sequences (e.g., small RNA molecules) might use fewer layers, while foundation models trained on long genomic contexts often use deeper stacks.

The choice of depth involves balancing several considerations. Deeper networks can learn more complex functions and abstract representations, but they are harder to train, prone to overfitting without sufficient data, and more computationally expensive at both training and inference. For genomic applications, depth often correlates with the complexity of regulatory patterns being modeled. Simple motif-based tasks might benefit more from wider layers (larger $d$) than deeper stacks, while tasks requiring integration of hierarchical regulatory information (promoters → enhancers → TAD structure) may benefit from additional depth.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (transformer block schematic)**  
Figure: Detailed diagram of a single transformer block showing pre-norm configuration. Input flows through LayerNorm → Multi-head Attention → Add & Norm → Feed-forward (with dimension expansion) → Add & Norm → Output. Annotate with typical dimensions (e.g., d=512, dff=2048) and show residual connections as bypass arrows.
:::


## Scaling Transformers for Genomics

Genomic sequences present unique scaling challenges. DNA sequences can span millions of bases, far exceeding the typical context lengths in natural language processing. Tokenization choices (covered in Chapter @sec-token) interact with sequence length: single-nucleotide tokens create very long sequences, while k-mer tokens reduce length at the cost of vocabulary size. Scaling transformers to genomic applications requires careful consideration of how to balance model size, context length, and computational resources.

Parameter scaling in transformers comes from two main sources: width and depth. Width refers to the model dimension $d$, which determines the size of embeddings and hidden states. Increasing width allows the model to represent more complex patterns at each position but increases the number of parameters quadratically (since weight matrices are $d \times d$). Depth refers to the number of stacked transformer blocks. Increasing depth allows the model to learn hierarchical abstractions through repeated refinement but also increases parameters linearly with layers.

The transformer scaling laws studied in natural language processing suggest that model performance improves smoothly with increased parameters, data, and compute. For genomics, similar principles apply, though the optimal ratios differ. Genomic sequences are less compressible than natural language (each nucleotide carries less predictable information than words in structured text), suggesting that genomic models might benefit more from depth than width compared to language models of similar parameter count.

Context length scaling presents the central challenge for genomic transformers. Standard self-attention has $O(L^2)$ complexity, where $L$ is sequence length. A 10kb sequence tokenized at single-nucleotide resolution has 10,000 tokens, requiring 100 million attention computations per layer. A 200kb sequence has 200,000 tokens, requiring 40 billion attention computations per layer. This quadratic scaling rapidly becomes prohibitive.

Several strategies address this computational bottleneck. Sparse attention patterns restrict which positions can attend to which others, reducing the quadratic cost. For example, local windowing allows each position to attend only to positions within a fixed window, reducing complexity to $O(Lw)$ where $w$ is window size. This works well when most relevant interactions are local, as often holds for regulatory sequences where nearby elements interact more strongly than distant ones.

Strided attention patterns create a hierarchy where lower layers use local windows and upper layers attend to every $k$-th position. This captures both local fine-grained patterns and global coarse-grained structure while maintaining sub-quadratic complexity. Hybrid models like Enformer (Chapter @sec-hybrid) use this strategy, applying CNNs to downsample sequences before transformer layers.

Approximations to full attention offer another approach. Linformer approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length at the cost of some expressiveness. Performer uses random feature methods to approximate attention scores without explicitly computing the full $L \times L$ matrix. These approximations work well in practice for some tasks but may lose important long-range dependencies.

For genomic applications, the choice among these strategies depends on the biological context. Regulatory prediction often benefits from local windowing because nearby elements dominate enhancer-promoter interactions. Foundation models trained to predict masked tokens may require global attention because the model must integrate information from across the entire sequence to make coherent predictions. Variant effect prediction sometimes requires selective attention to specific distal elements, which sparse patterns may miss.

Memory requirements compound the computational challenge. Transformer training requires storing activations for backpropagation, and attention matrices can be particularly memory-intensive. Gradient checkpointing trades compute for memory by recomputing activations during the backward pass rather than storing them. This allows training larger models or longer sequences on fixed hardware at the cost of additional computation time.

Mixed precision training uses lower-precision (16-bit) floating point for most computations while maintaining higher precision (32-bit) for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2× speedup with minimal precision loss for most models. Genomic transformers routinely use mixed precision to enable training on longer sequences or with larger batch sizes.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (scaling trade-offs)**  
Figure: Multi-panel showing scaling relationships. (a) Parameter count vs model dimension and depth, with typical genomic model configurations marked. (b) Attention complexity vs sequence length for full, windowed, and sparse attention, with genomic length scales (1kb, 10kb, 100kb, 1Mb) indicated. (c) Memory usage vs sequence length showing impact of activation checkpointing.
:::


## Transformer Variants in Genomic Applications

The transformer architecture has been adapted in diverse ways for genomic modeling. This section surveys major variants and their design rationale.

Standard encoder-only transformers process sequences bidirectionally, allowing each position to attend to all other positions including future positions. DNABERT and Nucleotide Transformer exemplify this architecture. They are trained with masked language modeling objectives where random tokens are masked and the model predicts them from bidirectional context. This works well for understanding genomic sequences where both upstream and downstream context matters, such as transcription factor binding sites influenced by flanking sequence or protein structures determined by both N-terminal and C-terminal residues.

Encoder-only transformers excel at sequence representation tasks where the goal is to learn meaningful embeddings that capture biological properties. These embeddings can be used directly for downstream tasks like variant effect prediction or as features for other models. The bidirectional context allows rich representations but makes these models unsuitable for generative tasks where we need to sample sequences autoregressively.

Decoder-only transformers use causal attention where each position attends only to itself and preceding positions. This enables autoregressive generation: the model generates sequences one token at a time, conditioning each new token on all previous tokens. GenSLM and other genomic foundation models trained on next-token prediction use this architecture. Causal attention is essential for generative modeling but provides less rich representations than bidirectional attention for fixed sequences because each position has access to only partial context.

The trade-off between encoder and decoder architectures reflects a fundamental tension in genomic modeling. Representation learning benefits from bidirectional context, while sequence generation requires causal structure. Some applications use both: a bidirectional encoder produces initial representations, then a causal decoder refines them for generation or prediction tasks.

Hybrid CNN-transformer architectures combine convolutional layers with transformer blocks. Models like Enformer and Borzoi (Chapter @sec-hybrid) apply convolutional stems to long sequences, downsampling through pooling, then pass the compressed representation through transformer layers. This exploits CNNs' efficiency for local pattern extraction while using transformers for long-range integration. The downsampling addresses transformers' quadratic complexity by reducing sequence length before attention.

These hybrid models achieve state-of-the-art performance on regulatory prediction tasks but blur the line between pure transformers and other architectures. They work because genomic regulation involves both local patterns (motifs, nucleosome positioning) and long-range interactions (enhancer-promoter loops, chromatin domains). The CNN-transformer combination matches this multi-scale structure.

Long-range modifications adapt transformer attention for ultra-long genomic contexts. Genomic Interpreter uses 1D Swin transformers with hierarchical windowed attention, enabling megabase-scale modeling. Others use sparse attention patterns tailored to genomic structure, such as attending to fixed landmark positions (promoters, insulators) or using genomic distance-based sparsity patterns that assume nearby positions interact more strongly than distant ones.

These modifications share a common goal: reducing attention's quadratic cost while preserving long-range capability. Success depends on whether the imposed structure matches actual genomic interactions. If enhancer-promoter loops occur at specific, regular spacing, fixed sparse patterns might work well. If interactions are highly variable and data-dependent, full attention or learned sparsity may be necessary.

Bidirectional versus causal attention has implications beyond generation. For variant effect prediction, bidirectional context allows the model to see both upstream and downstream changes when scoring a mutation. This can improve prediction quality because regulatory effects often depend on surrounding context. However, causal models may better capture directionality in processes like transcription or replication where 5' to 3' order matters mechanistically.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (transformer variants)**  
Figure: Comparison of four architectures with attention pattern diagrams and use cases. (a) Bidirectional encoder (DNABERT): full attention matrix shown as filled triangle. (b) Causal decoder (GenSLM): lower triangular attention matrix. (c) Hybrid (Enformer): CNN stem → downsampled sequence → transformer layers. (d) Sparse (Genomic Interpreter): blocked attention pattern with local and strided blocks.
:::


## Training Considerations

Training genomic transformers involves optimization algorithms, regularization strategies, and infrastructure choices that differ in important ways from natural language models.

Optimization for transformers typically uses Adam or AdamW, adaptive learning rate algorithms that maintain per-parameter learning rates adjusted based on gradient statistics. AdamW adds weight decay directly to the parameter updates rather than to the loss function, which improves training stability and generalization for transformers. Learning rate schedules typically use warmup for the first few thousand steps, linearly increasing the learning rate from zero to a peak value, then decay (either linear or cosine) for the remainder of training.

Warmup is particularly important for transformers because large learning rates early in training can cause instability when parameters are randomly initialized. Gradients in early iterations can be extremely large or small depending on initialization, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates.

For genomics, learning rate tuning often requires domain-specific adjustment. Genomic sequences have different statistical properties than natural language, and the optimal learning rate may differ substantially from NLP defaults. Regulatory sequences with highly conserved motifs may require lower learning rates to avoid overfitting to these strong signals, while protein sequences with weaker conservation may benefit from higher learning rates that encourage exploration.

Regularization prevents overfitting, particularly important for genomic applications where training data may be limited compared to the large models we wish to train. Dropout randomly zeros out activations during training, forcing the network to learn robust features that do not depend on specific neurons. Attention dropout applies this to attention weights, randomly dropping connections between positions. This prevents the model from over-relying on specific position pairs and encourages learning of distributed representations.

Weight decay penalizes large parameter values, encouraging the model to use smaller, smoother weights. For transformers, weight decay is typically applied to all parameters except biases and layer normalization parameters. The weight decay coefficient must be tuned carefully: too little provides insufficient regularization, while too much overly constrains the model and reduces capacity.

Gradient issues plague deep network training. Vanishing gradients occur when gradients become extremely small as they backpropagate through many layers, preventing effective learning in early layers. Exploding gradients are the opposite problem where gradients grow exponentially, causing parameter updates that destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths through the network. Exploding gradients are addressed through gradient clipping, which rescales gradients when their norm exceeds a threshold.

For genomic transformers, gradient issues often manifest differently than in language models. Genomic sequences have less hierarchical structure than natural language (no grammatical sentence structure), which affects gradient flow through attention layers. Imbalanced token frequencies (certain k-mers or amino acids appear much more often than others) can create gradient imbalances where common tokens receive large gradients while rare but biologically important tokens receive tiny gradients. Addressing this may require reweighting losses or using adaptive batch sampling.

Computational infrastructure for genomic transformer training typically requires distributed approaches. Single-GPU training suffices only for small models on short sequences. Multi-GPU data parallelism replicates the model across GPUs, splitting batches across devices and aggregating gradients. This scales well up to batch sizes limited by convergence requirements. Model parallelism splits the model itself across devices, necessary when models are too large to fit on a single GPU. Pipeline parallelism divides layers across devices and pipelines the forward and backward passes.

Mixed precision training (mentioned earlier) is nearly universal in genomic transformer training. Modern GPUs provide specialized tensor cores that accelerate 16-bit operations dramatically. For genomics, mixed precision typically provides 1.5-2× speedup with no loss in final model quality, though careful attention to loss scaling and overflow detection is required.

Batch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use gradient accumulation to simulate large batches: small batches are processed sequentially, gradients accumulated across them, then a single parameter update made. This provides the benefits of large batches without the memory cost.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (training dynamics)**  
Figure: Two-panel figure showing typical training curves. (a) Learning rate schedule showing warmup and decay phases. (b) Training and validation loss over time, with annotations for common issues (overfitting, underfitting, gradient instability) and when to apply different interventions (increase regularization, reduce learning rate, add gradient clipping).
:::


## Limitations and Alternatives

Despite their success, transformers have fundamental limitations that motivated development of alternative architectures. Understanding these limitations contextualizes recent innovations and guides architecture choice for specific genomic applications.

The quadratic complexity bottleneck is transformers' most severe limitation. Computing all pairwise attention scores for a sequence of length $L$ requires $O(L^2)$ operations and memory. For genomic contexts of 100kb or more (roughly 100,000 single-nucleotide tokens), this becomes prohibitive. Even with sparse attention approximations, transformers struggle to scale to megabase-length contexts where some regulatory interactions occur.

Recent genomic models have pushed context lengths to impressive scales (Enformer handles 200kb, AlphaGenome approaches 1Mb), but these often rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns. Pure transformers without such modifications remain limited to shorter contexts.

Alternative sub-quadratic architectures address this limitation directly. State space models (SSMs) like S4 and Mamba achieve linear complexity by representing sequences as continuous-time dynamical systems rather than discrete token-to-token interactions. These models maintain long-range memory through recurrent state updates while avoiding the quadratic attention bottleneck. Chapter @sec-dna covers SSMs in detail, including Hyena DNA and the Evo family of genomic foundation models.

Hyena operators replace attention with long convolutions implemented efficiently in the Fourier domain, achieving sub-quadratic complexity. Mamba uses selective state space models that dynamically adjust state transitions based on input content. Both approaches show promise for ultra-long genomic contexts, potentially enabling whole-chromosome or even whole-genome modeling.

When should we prefer transformers over these alternatives? Transformers excel when global context matters but sequences are not extremely long (under 10-50kb depending on resources). Their attention maps provide some interpretability, showing which positions the model considers relevant for specific predictions. Transformers have extensive tooling and pretrained models available from NLP research that transfer readily to genomics.

CNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where the relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters and less compute. The inductive bias toward local patterns also provides regularization that helps with limited training data.

Hybrid approaches combining CNNs for local feature extraction with transformers for long-range integration often achieve the best of both worlds. As discussed in Chapter @sec-hybrid, models like Enformer demonstrate that architectural combinations can outperform pure transformers or pure CNNs on real genomic tasks.

The transition to sub-quadratic architectures is ongoing. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. However, transformers benefit from years of engineering optimization and extensive pretrained models. The genomics community is actively exploring whether SSMs' theoretical advantages translate to practical improvements across diverse tasks.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (architecture comparison)**  
Figure: Comparison table or radar chart showing transformers, CNNs, and SSMs across multiple axes: sequence length capacity, parameter efficiency, interpretability, training stability, inference speed, and ecosystem maturity. Highlight trade-offs rather than declaring one superior.
:::


## Summary and Forward References

This chapter examined transformer architecture from first principles through genomic applications. Transformers introduced global self-attention that addresses CNNs' limited receptive fields, enabling modeling of long-range genomic interactions. The core mechanisms involve queries, keys, and values computing position-dependent aggregation weights that determine how information flows through the sequence.

Positional encodings address transformers' permutation invariance by injecting sequence order information. Multiple approaches exist (sinusoidal, learned, relative, RoPE), each with different properties for generalization and computational efficiency. Genomic applications must consider strand directionality and genomic coordinate systems when choosing positional encoding schemes.

The transformer block architecture combines multi-head attention with position-wise feed-forward networks, connected by residual connections and layer normalization. Stacking these blocks creates deep networks that learn hierarchical representations through repeated refinement. Design choices about depth, width, normalization placement, and component sizing affect model capacity and training stability.

Scaling transformers to genomic sequences involves trade-offs between parameters, context length, and compute. Quadratic attention complexity limits context lengths, motivating sparse attention patterns, hierarchical windowing, and hybrid architectures. Mixed precision training and gradient checkpointing enable larger models on available hardware.

Transformer variants for genomics include bidirectional encoders for representation learning (DNABERT, Nucleotide Transformer), causal decoders for generation (GenSLM), and hybrid architectures combining CNNs with attention (Enformer, Borzoi). Each variant suits different tasks based on whether bidirectional context or autoregressive structure is more important.

Training considerations specific to genomics include learning rate schedules with warmup, dropout and weight decay regularization, gradient clipping for stability, and distributed training infrastructure. Genomic sequences' unique statistical properties (token frequency imbalance, weaker hierarchical structure than language) affect optimization in subtle ways that may require task-specific tuning.

Transformers' quadratic complexity motivates alternatives, particularly state space models that achieve linear scaling while maintaining long-range capability. When to prefer transformers versus SSMs or CNNs depends on sequence length requirements, local versus global pattern importance, computational resources, and whether extensive pretrained models are available.

Looking forward, transformers remain central to genomic foundation models despite emerging alternatives. Chapter @sec-dna surveys DNA language models built on transformer and SSM architectures, trained through self-supervised objectives. Chapter @sec-prot covers protein language models where transformers have achieved remarkable success at learning structure and function from sequence alone. Chapter @sec-hybrid examines how regulatory prediction models combine transformers with CNNs to handle long genomic contexts efficiently.

The transformer architecture represents a watershed moment in genomic deep learning, enabling models that genuinely capture long-range dependencies essential for understanding gene regulation. While newer architectures may supersede transformers for some applications, the conceptual framework they introduced, particularly the idea that global context aggregation through learned attention is tractable and effective, continues to shape genomic modeling.
