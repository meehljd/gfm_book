# GWAS and Polygenic Scores {#sec-ch03-gwas}

**Genome-wide association studies (GWAS)** do not identify causal variants; they identify signposts. When a GWAS reports that a particular **single nucleotide polymorphism (SNP)** associates with coronary artery disease, that SNP is almost certainly not the variant that alters cardiac biology. It is correlated with the causal variant through linkage disequilibrium, the non-random association of nearby alleles that persists across generations. The statistical machinery of GWAS is exquisitely sensitive to these correlations but fundamentally agnostic about mechanism. It can identify a region of the genome that harbors trait-relevant variation without distinguishing the causal variant from its correlated neighbors, without explaining which genes or pathways are affected, and without revealing whether the same associations hold in populations with different linkage patterns.

This distinction between association and causation defines the central intellectual challenge of statistical genetics. GWAS have identified thousands of genomic regions associated with hundreds of complex traits, from height and blood pressure to schizophrenia and type 2 diabetes. These associations replicate across studies with remarkable consistency, confirming that the signals are real. Yet the path from associated region to biological mechanism remains obscure for most loci. The majority of GWAS signals fall in non-coding regions where there is no obvious gene to implicate. Even when a signal overlaps a gene, whether it affects expression, splicing, or protein function is rarely apparent from the association alone.

Polygenic scores aggregate these associations into predictions, summing risk alleles across thousands of loci to estimate an individual's genetic predisposition. For some traits, these scores achieve clinically meaningful discrimination: individuals in the top percentile of coronary artery disease risk have odds ratios comparable to monogenic familial hypercholesterolemia. Yet polygenic scores inherit all the limitations of the associations they aggregate. They predict without explaining, correlate without identifying mechanism, and transfer poorly across populations with different allele frequencies and linkage patterns. Understanding both their power and their limitations is essential for mechanistic approaches where regulatory sequence models (@sec-reg) and variant effect predictors (@sec-veps) attempt to move from statistical association to biological explanation [@khera_genome-wide_2017].


## GWAS Framework {#sec-ch03-gwas-framework}

Consider a clinician counseling a patient about cardiovascular disease risk. Traditional risk factors (age, smoking, cholesterol, blood pressure) explain roughly 50% of the variation in who develops disease [@khera_genome-wide_2017]. Family history suggests that genetics contributes substantially to the remainder, but which genetic variants matter, and how much does each contribute? GWAS provide a systematic approach to answering these questions by testing each of millions of variants for association with the trait of interest.

The scale required for well-powered GWAS explains why large-scale biobanks (@sec-data) have become essential infrastructure for statistical genetics. UK Biobank, with its 500,000 participants genotyped across hundreds of thousands of variants and linked to extensive phenotypic data, has enabled GWAS for thousands of traits at sample sizes that were unimaginable a decade ago. Similar resources, including the Million Veteran Program, FinnGen, and All of Us, continue to expand the scope of discoverable associations. The biobank paradigm of combining dense genotyping with rich phenotyping at population scale has transformed GWAS from underpowered fishing expeditions into reliable discovery engines.

The core logic is straightforward. For each variant in turn, researchers ask whether individuals carrying more copies of a particular allele tend to have higher or lower values of the phenotype (for quantitative traits) or higher or lower probability of disease (for binary outcomes). They estimate an effect size, compute a test statistic under the null hypothesis of no association, and record a p-value. After testing millions of variants, those exceeding a stringent significance threshold are identified, the associated loci reported, and interpretation begins regarding which genes and pathways might be involved.

This apparently simple procedure requires careful attention to study design, quality control, and statistical modeling. The phenotype must be measured consistently across individuals. The genotypes must be accurate and the variants well-defined. Confounders that correlate with both genotype and phenotype (most notably population structure) must be controlled. Multiple testing across millions of variants demands stringent significance thresholds. Only after addressing these challenges can GWAS results be trusted and translated into downstream applications.

### Association Models for Quantitative Traits {#sec-ch03-association-models-quantitative-traits}

Choosing the wrong statistical model for a GWAS does not merely introduce imprecision; it distorts effect size estimates in ways that propagate through every downstream analysis, from fine-mapping to polygenic scores to drug target prioritization. A height GWAS and a schizophrenia GWAS require fundamentally different approaches because one outcome is continuous and the other binary. Applying linear regression to a binary outcome produces fitted values outside the 0-1 probability range and residuals that violate normality assumptions.

For continuous phenotypes such as height, LDL cholesterol, or blood pressure, the standard approach is linear regression. Let *y_i* denote the phenotype for individual *i*, and let *g_ij* denote the genotype dosage at variant *j*, encoded as 0, 1, or 2 copies of the alternative allele (or as a fractional value for imputed genotypes). The model is:

$$
y_i = \alpha + \beta_j g_{ij} + \gamma^\top c_i + \varepsilon_i
$$

The coefficient *β_j* represents the expected change in phenotype per additional copy of the alternative allele, holding covariates *c_i* fixed. When phenotypes are standardized to zero mean and unit variance, *β_j* is expressed in standard deviation units per allele. The vector *c_i* typically includes age, sex, genotyping batch, and principal components capturing ancestry (discussed below). The residual *ε_i* captures unexplained variation, assumed to be independent and identically distributed across individuals.

For each variant, a test statistic is computed for the null hypothesis *H₀: β_j = 0*. In large samples, the t-statistic follows approximately a standard normal distribution under the null, yielding a two-sided p-value. With *M* variants tested (typically 10⁶ to 10⁷ after imputation), multiple comparison correction is essential. The conventional **genome-wide significance threshold** of 5 × 10⁻⁸ approximates a Bonferroni correction for roughly one million effectively independent tests, accounting for correlation among variants due to linkage disequilibrium [@risch_future_1996; @peer_estimation_2008].

### Association Models for Disease Outcomes {#sec-ch03-association-models-disease-outcomes}

Binary outcomes create a specific statistical problem that, if ignored, systematically distorts effect size estimates in ways that compound through downstream applications. When the phenotype is disease status (affected or unaffected), linear regression produces nonsensical predictions: fitted values outside the 0-1 probability range and residuals that violate normality assumptions. The consequence extends beyond statistical inelegance. Effect sizes estimated under the wrong model propagate into polygenic scores and risk prediction, potentially misclassifying patients who sit near clinical decision thresholds where intervention recommendations change.

For binary phenotypes, **logistic regression** replaces linear regression. The model relates genotype to the log-odds of disease:

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \beta_j g_{ij} + \gamma^\top c_i
$$

Here *β_j* is the log-odds ratio per allele, and exp(*β_j*) gives the **odds ratio (OR)**. An odds ratio of 1.2 means that each additional copy of the alternative allele increases the odds of disease by 20%. For rare diseases (prevalence below approximately 10%), odds ratios approximate relative risks, but the distinction matters for common conditions and when communicating absolute risk to patients.

Case-control sampling, in which cases are enriched relative to their population frequency, distorts absolute risk estimates but preserves the validity of odds ratio estimation. This mathematical property explains why GWAS conducted in case-control designs can still produce effect sizes useful for polygenic scores, provided downstream applications account for baseline disease incidence. The likelihood function conditions on disease status, making the odds ratio identifiable regardless of sampling scheme.

### Manhattan Plots and Q-Q Plots {#sec-ch03-manhattan-plots-q-q-plots}

The **Manhattan plot** has become the iconic visualization of GWAS results, named for its resemblance to the New York City skyline. Each point represents a tested variant, with genomic position along the x-axis (ordered by chromosome) and negative log-transformed p-value on the y-axis. Variants with stronger associations rise higher; those exceeding the genome-wide significance threshold of 5 × 10⁻⁸ (typically drawn as a horizontal line at −log₁₀(5 × 10⁻⁸) ≈ 7.3) are considered significant hits.

The Manhattan plot reveals both the successes and limitations of GWAS at a glance. Prominent peaks indicate genomic regions harboring trait-associated variants, but each peak typically contains dozens or hundreds of correlated variants rather than a single causal nucleotide. The width of peaks reflects local linkage disequilibrium structure: broader peaks indicate regions where many variants are correlated with the lead signal. The height reflects statistical strength, which depends on effect size, allele frequency, and sample size. Tall, narrow peaks suggest strong, well-localized signals; broad peaks spanning megabases indicate that fine-mapping will be challenging.

Quantile-quantile (Q-Q) plots complement Manhattan plots by assessing whether the observed p-value distribution matches theoretical expectations under the null hypothesis. Systematic deviation from the diagonal (genomic inflation) suggests either true polygenic signal or residual confounding from population structure. The genomic inflation factor *λ* quantifies this deviation, with values substantially above 1.0 warranting investigation of potential confounders.

::: {#fig-manhattan-anatomy}
![FIGURE PLACEHOLDER](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[Essential] Annotated Manhattan plot from a real or realistic GWAS (e.g., height or CAD). Key annotations: (1) Genome-wide significance threshold at −log₁₀(5×10⁻⁸) ≈ 7.3; (2) Example peak with lead SNP labeled; (3) Width of peak showing LD extent (many correlated variants, not just one); (4) Chromosomes color-alternated along x-axis; (5) Inset zoom on one peak showing how multiple variants exceed threshold, illustrating that GWAS identifies loci, not causal variants. Include Q-Q plot inset showing expected vs observed p-value distribution with genomic inflation factor *λ* annotated.
:::

### Population Structure Control {#sec-ch03-population-structure-control}

Population structure poses a fundamental challenge to GWAS interpretation because it can generate association signals indistinguishable from true biological effects. If allele frequencies differ systematically across subpopulations and the phenotype also varies across these groups for non-genetic reasons (differences in environment, diet, healthcare access, socioeconomic status), naive association testing will detect variants that mark ancestry rather than causal biology. A variant that is simply more common in one population will appear associated with any trait that differs between populations, regardless of biological mechanism. The resulting false positives waste resources on follow-up studies and, more insidiously, can embed ancestry-related confounding into polygenic scores that are then deployed as if they measured pure genetic risk.

**Principal component analysis (PCA)** on the genotype matrix captures the major axes of genetic variation across individuals [@price_pca_2006; @patterson_population_2006]. The leading principal components often correspond to continental ancestry gradients or finer-scale population structure within a study. Including these PCs as covariates in the regression model attenuates spurious associations driven by ancestry stratification.

This correction is imperfect. Subtle structure not captured by the included PCs, cryptic relatedness among individuals, and the interweaving of genetic ancestry with environmental exposures all complicate interpretation. The challenges extend far beyond technical statistical adjustment: ancestry is entangled with healthcare access, environmental exposures, and socioeconomic factors in ways that simple covariate correction cannot fully resolve. These issues become critical when translating GWAS results to clinical applications and when evaluating whether polygenic scores perform equitably across populations. The full complexity of ancestry as a confounder is addressed in @sec-confound.

## Heritability: What Genetics Can Explain {#sec-ch03-heritability-genetics-can-explain}

Before GWAS can identify specific variants, a more fundamental question must be answered: how much of the variation in a trait is attributable to genetics at all? A trait entirely determined by environment would yield no GWAS hits regardless of sample size. A trait entirely determined by genetics would, in principle, be fully predictable from genotype. **Heritability** quantifies where traits fall along this spectrum, but the concept is more subtle than it first appears, and different estimation methods yield systematically different answers.

::: {#fig-heritability-decomposition}
![FIGURE PLACEHOLDER](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[High] Conceptual diagram showing nested components of phenotypic variance. Outer ring: Total phenotypic variance (100%). First partition: Genetic vs Environmental components (e.g., 80/20 for height). Within genetic: Additive (narrow-sense h²) vs non-additive (dominance, epistasis). Within additive: SNP-heritability (what GWAS can capture) vs "missing heritability" (rare variants, structural variants, imperfect tagging). Annotate with approximate values for a well-studied trait like height.
:::

### Pedigree Heritability {#sec-ch03-pedigree-heritability}

Classical genetics estimated heritability by comparing phenotypic similarity among relatives. Identical twins share all their genetic variation; fraternal twins share on average half; full siblings also share half; parents and offspring share half; cousins share one-eighth. If genetic variation influences a trait, closer relatives should be more similar. The correlation structure across relationship types allows partitioning of phenotypic variance into genetic and environmental components.

**Narrow-sense heritability** (*h²*) represents the proportion of phenotypic variance attributable to additive genetic effects. For height, pedigree studies consistently estimate *h²* around 0.80, meaning that 80% of the variation in height across individuals in the studied population can be attributed to genetic differences [@visscher_heritability_2008]. For schizophrenia, twin studies estimate *h²* around 0.80 as well [@hilker_heritability_2018]. For body mass index, estimates cluster around 0.50 to 0.80 depending on the population, age group, and study design [@elks_variability_2012].

These high heritability estimates established that genetics substantially influences most traits of biomedical interest, motivating the search for specific causal variants. If 80% of height variation is genetic, then genetic variants collectively must explain most of that variation. Finding those variants became the goal of GWAS.

### SNP-Heritability and the Missing Heritability Problem {#sec-ch03-snp-heritability-missing-heritability}

GWAS delivered a puzzle. For height, even the largest studies with hundreds of significant hits explained only a fraction of the heritability estimated from family studies. Early GWAS collectively explained perhaps 5% of height variance when pedigree studies suggested 80% should be genetic. This gap, termed **missing heritability**, sparked intense debate about where the remaining genetic variance might hide [@manolio_finding_2009].

The concept of **SNP-heritability** (*h²_SNP*) emerged to parse this puzzle into more tractable components. Rather than asking how much variance is explained by genome-wide significant variants, researchers asked how much variance is explained by all common SNPs on genotyping arrays, including those that fail to reach significance. Methods such as GCTA-GREML estimate this quantity by modeling phenotypic similarity as a function of genetic similarity computed across all SNPs [@yang_common_2010]. For height, SNP-heritability estimates reach approximately 0.50 to 0.60, substantially higher than variance explained by significant hits alone but still below pedigree estimates.

This intermediate value revealed that missing heritability actually comprises two distinct gaps. The first gap separates SNP-heritability from variance explained by GWAS-significant variants. This **polygenic gap** reflects the architecture of complex traits: thousands of variants each contribute effects too small to reach genome-wide significance individually, yet they collectively explain substantial variance when modeled together. As sample sizes grow and more variants cross the significance threshold, this gap narrows. The polygenic gap is not truly "missing" heritability; the variance is captured by common SNPs, just distributed across too many variants to detect individually.

The second gap separates pedigree heritability from SNP-heritability. This **hidden heritability** reflects genetic variation genuinely absent from common SNP arrays: rare variants below minor allele frequency thresholds, structural variants poorly tagged by single nucleotide polymorphisms, copy number variations, and variants not in linkage disequilibrium with array content. Unlike the polygenic gap, this component cannot be recovered by increasing GWAS sample size; it requires different data types entirely, such as whole-genome sequencing that captures rare variation directly.

The distinction matters for how foundation models might contribute. Models trained on common variant data inherit the SNP-heritability ceiling; they cannot learn patterns from variation they never observe. Integrating rare variant data, structural variant calls, or multi-omic measurements represents not merely incremental improvement but access to a fundamentally different component of genetic architecture.

### Implications for GWAS and Polygenic Scores {#sec-ch03-implications-gwas-polygenic-scores}

The heritability landscape carries practical implications for what GWAS and polygenic scores can achieve. SNP-heritability sets an upper bound on the predictive accuracy of polygenic scores built from common variants: a PGS cannot explain more variance than is captured by the SNPs it uses. For height, with SNP-heritability around 0.50, the best possible common-variant PGS could explain at most half of phenotypic variance. Current PGS for height in European-ancestry populations approach this bound, explaining roughly 25% of variance with continued gains as sample sizes grow [@yengo_meta-analysis_2022].

For diseases, the relationship between heritability and predictive accuracy is more complex. A highly heritable disease might have low predictive accuracy if the causal variants are rare, if gene-environment interactions dominate, or if the heritability is distributed across thousands of variants each with tiny effects. Conversely, a moderately heritable disease with a few common variants of large effect might be more predictable. The architecture of genetic effects matters as much as total heritability.

Missing heritability also motivates the integration of rare variant analysis with GWAS of common variants. Whole-genome sequencing studies can capture rare variants invisible to genotyping arrays, potentially recovering some of the genetic variance missing from common-variant analyses. Foundation models trained on sequence data, rather than genotype arrays, may ultimately capture genetic effects across the full allele frequency spectrum, a possibility explored in @sec-veps.

## Linkage Disequilibrium and the Association-Causation Gap {#sec-ch03-linkage-disequilibrium-association}

GWAS test variants one at a time, but the genome is not inherited one variant at a time. Nearby variants travel together on haplotypes and are co-inherited across generations except when recombination separates them. This correlation structure, known as **linkage disequilibrium (LD)**, is both essential to GWAS power and the source of their fundamental interpretive limitation. Without LD, GWAS would need to genotype every variant in the genome directly; with LD, statistical association cannot distinguish cause from correlation.

When a GWAS identifies a significant association at variant *j*, three possibilities exist. The variant itself may be causal, directly influencing the phenotype through some molecular mechanism. Alternatively, variant *j* may simply be correlated with a nearby causal variant *k* due to LD, with the association signal reflecting this correlation rather than direct causation. In complex regions, multiple causal variants may exist, and the observed association pattern reflects their joint effects filtered through the local LD structure. Distinguishing these scenarios from GWAS summary statistics alone is often impossible. The causal variant and its tag look identical in the association data, yet only the causal variant represents a valid drug target or mechanistic insight.

::: {#fig-ld-tag-causal layout-ncol=3}
![FIGURE PLACEHOLDER A](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20A)

![FIGURE PLACEHOLDER B](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20B)

![FIGURE PLACEHOLDER C](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20C)

[Essential] Three-panel figure. Panel A: Haplotype diagram showing how LD creates correlation between variants on the same chromosome segment; show example haplotypes with causal variant (star) and tag variants traveling together. Panel B: r² matrix (triangular heatmap) for a genomic region showing block structure of LD. Panel C: Same causal variant shown in two populations with different LD structure: in one population, tags are highly correlated with causal variant; in another, the correlation is weaker, illustrating why portability fails.
:::

### Structure of Linkage Disequilibrium {#sec-ch03-structure-linkage-disequilibrium}

Understanding why LD creates interpretive ambiguity requires understanding how LD arises and decays. Recombination during meiosis shuffles genetic material between parental chromosomes, with crossover events occurring at an average rate of roughly one per 100 megabases, meaning each chromosome arm typically experiences only one or two exchanges per generation. Over many generations, recombination breaks down long-range correlations between variants while preserving short-range structure. The timescale of this decay matters: LD between variants separated by a few kilobases persists for thousands of generations, while correlations spanning megabases decay within tens of generations. The result is a mosaic pattern: regions of high LD (haplotype blocks) where many variants are strongly correlated, interspersed with recombination hotspots where LD decays rapidly.

Recombination does not occur uniformly across the genome. Crossover hotspots, typically spanning 1 to 2 kilobases, concentrate the majority of recombination events into a small fraction of genomic sequence. These hotspots are enriched for specific sequence motifs recognized by the zinc finger protein PRDM9, which directs the recombination machinery to particular locations. The consequence is that haplotype blocks can extend for hundreds of kilobases across regions lacking hotspots, while adjacent blocks may be separated by sharp boundaries where recombination has effectively randomized allelic associations.

The squared correlation coefficient *r²* quantifies LD between pairs of variants. Unlike Pearson correlation, which measures linear relationships between continuous variables, *r²* for LD is computed from allele frequencies and haplotype counts in a 2×2 contingency table. The metric equals (*p_AB* − *p_A* *p_B*)² / (*p_A* *p_a* *p_B* *p_b*), where *p_AB* is the frequency of the AB haplotype and *p_A*, *p_a*, *p_B*, *p_b* are the individual allele frequencies. This formulation captures the deviation from random association expected under linkage equilibrium. The notation unfortunately overlaps with the coefficient of determination from linear regression, but the quantities measure different phenomena: regression *R²* captures variance explained by a fitted model, while LD *r²* captures non-random association between alleles at two loci. When *r²* approaches 1, the two variants are nearly always observed together on the same haplotypes; when *r²* approaches 0, they segregate independently. From a GWAS perspective, if a causal variant *k* has strong association with the phenotype and variant *j* is in high LD with *k* (high *r²*), then variant *j* will also show strong association even if it has no direct causal role. The statistical signal propagates through LD, creating ambiguity about which variant is actually functional.

LD patterns vary across populations because demographic history shapes which haplotypes persist and at what frequencies. Founder effects occur when a small number of individuals establish a new population, carrying only a subset of the ancestral haplotype diversity. The Finnish population, descended from a small founder group roughly 4,000 years ago, exhibits extended LD blocks and elevated frequencies of otherwise rare disease alleles. Bottlenecks produce similar effects: dramatic population contractions eliminate rare haplotypes and reduce diversity, leaving survivors with correlated genetic backgrounds. In contrast, large stable populations accumulate recombination events over many generations, breaking down LD more completely. A variant that tags a causal allele effectively in one population may be a poor proxy in another where different recombination history has decoupled the correlation. This population-specificity of LD structure is one reason polygenic scores fail to transfer across ancestries, a problem examined in detail in @sec-ch03-portability.

### Causal Variants, Tag Variants, and GWAS Catalogs {#sec-ch03-causal-variants-tag-variants-gwas}

The distinction between causal and tag variants determines whether GWAS results can translate into biological insight or clinical action. A **causal variant** directly influences the phenotype, whether by altering protein sequence, disrupting transcription factor binding, affecting splicing, or modifying chromatin state. A **tag variant** is merely correlated with a causal variant through LD, serving as a statistical proxy without direct functional consequence. The distinction is invisible to GWAS: both produce association signals, and in the presence of strong LD, those signals are statistically indistinguishable.

GWAS catalogs therefore report associated loci, not causal variants. The "lead SNP" at each locus (the variant with the smallest p-value) is often a tag rather than the causal variant, particularly when the causal variant is rare, poorly genotyped, or not present on the array. Even when a locus is robustly associated, dozens or hundreds of correlated variants may be statistically indistinguishable from the lead SNP.

This limitation has concrete practical consequences. Drug development requires identifying causal genes and mechanisms, not just associated regions; targeting a tag variant or the wrong gene wastes years of development effort. Clinical variant interpretation needs to distinguish functional mutations from neutral passengers; reporting a tag as pathogenic misleads patients and clinicians. Polygenic scores built on tag SNPs may lose power when applied to populations with different LD patterns, since the tag-causal correlation that made the tag useful may not hold. The gap between association and causation motivates the fine-mapping approaches considered next.

## Fine-Mapping: From Loci to Causal Variants {#sec-ch03-fine-mapping-loci-causal-variants}

A pharmaceutical company evaluating a GWAS hit for drug development faces a concrete problem: the associated locus spans 500 kilobases, contains 200 correlated variants, and overlaps three genes. Which gene should they target? Which variant drives the association? Investing hundreds of millions of dollars in a program targeting the wrong gene would be catastrophic, yet GWAS summary statistics alone cannot resolve the ambiguity. **Fine-mapping** attempts to address this gap, moving from "this region is associated" to "these specific variants are most likely causal" by exploiting the joint behavior of correlated variants under explicit statistical models.

### Statistical Framework {#sec-ch03-statistical-framework}

The core insight of fine-mapping is that while multiple variants may show similar marginal association statistics, their joint behavior under a model that accounts for LD can discriminate among them. A causal variant should show association beyond what can be explained by LD with its neighbors; a tag variant should not. This distinction, invisible when variants are tested one at a time, becomes apparent when their correlations are modeled jointly.

Bayesian fine-mapping methods approach the problem by specifying a prior distribution over which variants in a region might be causal, then computing posterior probabilities given the observed association statistics and local LD structure [@maller_bayesian_2012; @hormozdiari_identifying_2014]. The key outputs are **posterior inclusion probabilities (PIPs)**, which estimate the probability that each variant is among the causal set, and **credible sets**, which are minimal sets of variants that contain the true causal variant(s) with specified probability (commonly 95%).

The mathematical foundation rests on comparing models that differ in which variants are causal. Consider a region containing *m* variants, and let *γ* denote a configuration specifying which variants are causal (a binary vector of length *m*). Under a linear model, the observed GWAS summary statistics (effect estimates *β̂* and their standard errors) follow a multivariate normal distribution whose mean depends on the true causal effects and whose covariance depends on the LD matrix *Σ*. The likelihood of observing the data under configuration *γ* can be written as:

$$P(\hat{\beta} \mid \gamma, \Sigma) = \int P(\hat{\beta} \mid \beta_\gamma, \Sigma) \, P(\beta_\gamma) \, d\beta_\gamma$$

where *β_γ* represents the true effect sizes for causal variants in configuration *γ*, and *P(β_γ)* is the prior on effect sizes (typically Gaussian with variance *σ²*) [@benner_finemap_2016]. This integral has a closed-form solution when both the likelihood and prior are Gaussian, yielding a Bayes factor comparing each configuration to the null model of no associations.

The posterior probability of configuration *γ* follows from Bayes' theorem:

$$P(\gamma \mid \hat{\beta}, \Sigma) = \frac{P(\hat{\beta} \mid \gamma, \Sigma) \, P(\gamma)}{\sum_{\gamma'} P(\hat{\beta} \mid \gamma', \Sigma) \, P(\gamma')}$$

The prior *P(γ)* encodes assumptions about the number and distribution of causal variants. Common choices include a fixed maximum number of causal variants *K* (often 1 to 5) with uniform probability across configurations of equal size, or a binomial prior where each variant has independent probability *π* of being causal [@wang_simple_2020]. The denominator sums over all possible configurations, a computation that becomes intractable for large regions (with *m* variants and up to *K* causal variants, the number of configurations scales as C(*m*, *K*)).

The posterior inclusion probability for variant *j* marginalizes over all configurations in which that variant appears:

$$\text{PIP}_j = \sum_{\gamma : j \in \gamma} P(\gamma \mid \hat{\beta}, \Sigma)$$

This quantity answers the question: given everything we observed, what is the probability that variant *j* is among the causal variants? Methods like *FINEMAP*, *CAVIAR*, and *SuSiE* differ primarily in how they approximate the intractable sum over configurations and in their prior specifications, but all produce PIPs as their primary output [@benner_finemap_2016; @hormozdiari_identifying_2014; @wang_simple_2020].

Credible sets provide a complementary summary. A 95% credible set is the smallest set of variants whose cumulative PIP exceeds 0.95. When a single variant dominates (PIP above 0.95), the credible set contains only that variant. When LD distributes probability across many variants, credible sets expand accordingly. The *SuSiE* method produces one credible set per inferred causal signal, enabling regions with multiple independent associations to be decomposed into distinct sets [@wang_simple_2020].

Variants with high PIPs (above 0.5 or 0.9) are strong candidates for functional follow-up. Credible sets that contain few variants are more actionable than those containing dozens. The width of credible sets reflects both the strength of the association signal and the local LD structure: tight LD means many variants remain plausible even with strong statistical evidence. In some regions, fine-mapping narrows thousands of candidates to a handful; in others, the ambiguity remains irreducible given available data.

### Functional Annotation Priors {#sec-ch03-functional-annotation-priors}

Statistical fine-mapping alone cannot resolve regions where multiple variants are in near-perfect LD; the data simply cannot distinguish variants that are always co-inherited. Functional annotations offer a path forward by incorporating biological plausibility: not all genomic positions are equally likely to harbor causal variants. Variants disrupting coding sequences, altering transcription factor binding sites, or falling within active enhancers carry higher prior probability of functional relevance than variants in unannotated intergenic regions.

Annotation-informed approaches update fine-mapping priors based on these external data sources. Variants in coding regions, promoters, enhancers, or regions of evolutionary constraint may be assigned higher prior probability of causality. Integration with chromatin accessibility data (from ATAC-seq or DNase-seq), transcription factor binding maps (from ChIP-seq), or expression quantitative trait loci (eQTL) can further prioritize variants with plausible regulatory mechanisms.

The functional scores introduced in @sec-data provide systematic frameworks for quantifying variant-level annotations. Scores such as *CADD*, *DANN*, and *Eigen* integrate diverse genomic features into single numbers that can inform fine-mapping priors [@kircher_general_2014; @quang_dann_2015; @ionita-laza_spectral_2016]. More recently, foundation models trained on genomic sequence have produced variant effect predictions that capture functional information beyond what traditional annotations provide (@sec-veps). These scores transform fine-mapping from a purely statistical exercise into an integrative analysis that combines association evidence with mechanistic plausibility.

Large-scale resources now link GWAS summary statistics, fine-mapping results, and functional genomic annotations across hundreds of traits and thousands of loci [@buniello_open_2025; @mountjoy_open_2021]. These datasets enable systematic identification of variants that are both statistically prioritized and functionally plausible, though the biological validation required to confirm causal mechanisms remains laborious and is completed for only a small fraction of associated loci.

### Multi-Ancestry Fine-Mapping {#sec-ch03-multi-ancestry-fine-mapping}

Single-ancestry fine-mapping encounters a fundamental resolution limit: when variants are in tight LD within the study population, no amount of statistical sophistication can distinguish them. Multi-ancestry approaches break through this limit by exploiting the population-specificity of LD structure. A variant in tight LD with twenty neighbors in Europeans may have only three correlated variants in African-ancestry populations, where shorter LD blocks (reflecting larger historical effective population size) provide greater resolution.

Joint fine-mapping across ancestries leverages these differences systematically [@kichaev_improved_2017]. When a variant remains strongly associated across populations despite different local LD structure, confidence in its causal role increases. The logic is straightforward: a true causal variant should show consistent association regardless of which other variants happen to be correlated with it in any particular population. A tag variant, by contrast, may appear associated in one population (where it correlates with the causal variant) but not in another (where that correlation is absent).

Multi-ancestry approaches grow increasingly important as large biobanks expand to include diverse populations, though they require careful attention to potential effect size heterogeneity across populations. The core assumption that causal variants produce consistent effects worldwide can be violated through several mechanisms. Gene-environment interactions represent one such mechanism: a variant's phenotypic effect may depend on environmental exposures that differ systematically across populations. The *FTO* obesity-associated variants, for instance, show stronger effects in sedentary populations than in physically active ones, and lactase persistence variants in *LCT* produce metabolic consequences only where dairy consumption is common. When populations differ in relevant environmental contexts, effect sizes will differ even for genuinely causal variants.

Genetic background effects present a second complication. A variant's impact may depend on epistatic interactions with other loci, and if modifier variants differ in frequency across populations, the focal variant will appear to have population-specific effects. A causal variant might produce large effects only when a particular haplotype is present at an interacting locus; if that haplotype is common in one population but rare in another, the apparent effect of the causal variant will vary despite its genuine causal role. These complexities do not invalidate multi-ancestry fine-mapping, but they do mean that variants showing heterogeneous effects across populations should not be automatically dismissed. The method gains statistical power by assuming effect consistency, yet biological reality sometimes violates this assumption in ways that could exclude true causal variants or reduce confidence in them.

## Polygenic Score Construction {#sec-ch03-polygenic-score-construction}

A 35-year-old woman with a family history of breast cancer asks her physician whether she should begin mammography screening earlier than guidelines recommend. Traditional risk models incorporate family history, age, and reproductive factors, but cannot capture the cumulative effect of thousands of common variants, each conferring small increases in risk, that together may substantially elevate her probability of disease. **Polygenic scores** address this gap by aggregating variant effects across the genome into a single number:

$$
\text{PGS}_i = \sum_{j} w_j g_{ij}
$$

The weight *w_j* reflects the estimated effect of variant *j*, and *g_ij* is the genotype dosage for individual *i*. The simplest approach uses GWAS effect size estimates directly as weights; more sophisticated methods adjust for LD, apply shrinkage, or incorporate fine-mapping information. The clinical promise is substantial: for diseases with significant genetic components, polygenic scores can identify individuals at elevated risk years or decades before disease onset, potentially enabling targeted screening or prevention.

::: {#fig-pgs-construction layout-ncol=2}
![FIGURE PLACEHOLDER A](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20A)

![FIGURE PLACEHOLDER B](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20B)

[High] Side-by-side comparison of C+T vs LD-aware Bayesian methods. Left panel (C+T): Start with GWAS summary statistics → apply p-value threshold → clump by LD → retain independent lead SNPs → weight by effect size. Show visually how most variants are discarded. Right panel (LDpred/PRS-CS): Same summary statistics → model LD structure jointly → shrink effects toward zero based on prior → retain all variants with modulated weights. Highlight that C+T discards information while Bayesian methods model it.
:::

::: {.callout-note title="Terminology: PGS versus PRS"}

The literature uses overlapping terminology. **Polygenic risk score (PRS)** is common in clinical contexts, emphasizing disease risk prediction. **Polygenic score (PGS)** is more general, encompassing both disease and quantitative trait prediction. **Genomic risk score** and related terms also appear, often interchangeably. This book uses PGS as the default, adding "risk" when specifically discussing disease outcomes. Methodological overviews provide detailed guidance on construction and evaluation [@choi_prs_2020].

:::

### Clumping and Thresholding {#sec-ch03-clumping-thresholding}

The challenge of constructing a useful polygenic score is not mathematical but statistical: GWAS provide noisy estimates of millions of effects, many of which are correlated through LD, and naive summation produces scores dominated by noise rather than signal. **Clumping and thresholding (C+T)** represents the simplest solution: reduce both the noise and the correlation by aggressive filtering, accepting substantial information loss in exchange for robustness.

The procedure begins with clumping: variants are ranked by p-value, then iteratively the most significant variant is selected and all variants within a specified window (typically 250 kb) in LD above a threshold (typically *r²* > 0.1) are removed. This yields a set of approximately independent index variants. A p-value cutoff then retains only variants below threshold. Finally, weights are set equal to the GWAS effect size estimate for retained variants, and zero otherwise.

The hyperparameters (LD window, *r²* threshold, p-value threshold) are typically chosen by grid search to maximize predictive performance in a held-out validation set. This tuning introduces overfitting risk, particularly in small samples or when the validation population differs from the eventual deployment population.

C+T is transparent and computationally simple, but it discards substantial information. Most variants are excluded, LD is handled only through coarse pruning, and variants with modest p-values that collectively explain meaningful variance may be entirely omitted. For highly polygenic traits where thousands of variants each contribute small effects, this information loss substantially degrades prediction accuracy. The method treats LD as a problem to be eliminated rather than a correlation structure to be modeled, an approach that sacrifices power for simplicity.

### LD-Aware Bayesian Methods {#sec-ch03-ld-aware-bayesian-methods}

The information discarded by C+T is not random noise; it contains genuine signal about genetic effects distributed across correlated variants. Rather than pruning away this structure, a more principled approach models the joint distribution of effect sizes explicitly, treating the true effects *β* = (*β₁*, ..., *β_M*) as random variables drawn from a prior distribution. Given GWAS summary statistics and an LD reference panel, these methods infer posterior mean effect sizes that serve as PGS weights. The key insight is that LD becomes information rather than nuisance: correlated variants constrain each other's likely effects, improving estimation for all.

*LDpred* assumes that a fraction *p* of variants have nonzero effects drawn from a Gaussian distribution, while the remainder have zero effect [@vilhjalmsson_modeling_2015]. The method uses GWAS summary statistics and LD from a reference panel (computed from a subset of individuals or external dataset matching the target ancestry) to compute approximate posterior effect sizes. These posteriors shrink noisy estimates toward zero, borrow strength across correlated variants, and generally outperform C+T when properly tuned.

*PRS-CS* extends this framework by placing a continuous shrinkage prior on effect sizes, which better accommodates the highly polygenic architecture of complex traits and reduces sensitivity to the sparsity hyperparameter [@ge_polygenic_2019]. The continuous prior assigns most variants small but nonzero effects rather than forcing a binary causal/non-causal distinction. The method has shown strong performance across a range of traits and ancestries, though like all methods it requires an LD reference that reasonably matches the target population.

Related approaches (*lassosum*, *SBayesR*, and others) use different priors or optimization strategies but share the core insight: jointly modeling effect sizes under LD yields better predictions than pruning LD away. Performance differences among methods are often modest when each is well-tuned, and the choice may depend on computational resources, availability of validation data, and specific trait architecture.

### Fine-Mapping-Informed Scores {#sec-ch03-fine-mapping-informed-scores}

Polygenic scores built on tag SNPs face a fundamental portability problem: the tag-causal correlation that justified including a variant may not hold in populations with different LD structure. Fine-mapping outputs, particularly posterior inclusion probabilities, offer a potential solution by identifying variants more likely to be causal. Causal variants should remain predictive regardless of population-specific LD patterns, since their effects are direct rather than mediated through correlation.

Two strategies incorporate fine-mapping information into PGS construction. Selection approaches retain only variants above a PIP threshold (typically 0.1 or 0.5), focusing the score on high-confidence causal candidates. Weighting approaches modulate each variant's contribution by its PIP, downweighting likely tags while preserving information from variants with intermediate evidence.

Fine-mapping-informed approaches aim to concentrate weight on variants that are biologically meaningful rather than merely statistically associated. In principle, this should improve cross-ancestry transferability since causal variants remain causal regardless of population-specific LD patterns. In practice, gains depend on fine-mapping resolution, which is limited in regions of tight LD. The approaches remain an active area of methodological development, with potential for substantial improvement as multi-ancestry fine-mapping resources expand.

## Polygenic Score Interpretation {#sec-ch03-polygenic-score-interpretation}

A polygenic score is a number, but numbers do not make clinical decisions. A patient told they are in the 95th percentile of genetic risk may interpret this as near-certain disease development, while a physician may recognize it as modest risk elevation insufficient to change management. Converting a score into actionable information requires understanding what it represents, how it relates to disease risk or trait values, and where its interpretation breaks down. Miscommunication at this stage can transform a useful risk stratification tool into a source of inappropriate anxiety or false reassurance.

### Relative Risk and Percentiles {#sec-ch03-relative-risk-percentiles}

The most immediate clinical question about a high polygenic score is: how much does it increase risk? Polygenic scores are most naturally interpreted in relative terms by fitting a logistic regression in a validation cohort:

$$
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \theta \cdot \text{PGS}_i + \eta^\top z_i
$$

where *z_i* contains covariates and *θ* captures the effect of the PGS. After standardizing the score to unit variance, exp(*θ*) gives the odds ratio per standard deviation of the PGS. This metric allows statements such as "individuals one standard deviation above the mean have 1.5-fold higher odds of disease."

Percentile-based communication is common in clinical contexts. The risk for individuals in the top 1% or 5% of the PGS distribution can be compared to those near the median or in the bottom percentiles. For some conditions, individuals in the top percentiles have risk comparable to or exceeding that conferred by single high-penetrance mutations: the top 8% of the coronary artery disease PGS distribution has risk equivalent to familial hypercholesterolemia carriers, and the top 1% of the breast cancer PGS distribution has lifetime risk approaching that of *BRCA2* mutation carriers [@khera_genome-wide_2017; @mavaddat_polygenic_2019]. This finding makes polygenic scores potentially relevant for clinical risk stratification, though the appropriate thresholds and clinical actions remain subjects of ongoing research and debate.

### Absolute Risk {#sec-ch03-absolute-risk}

A physician cannot act on relative risk alone; clinical decisions require knowing the probability that this specific patient will develop disease over a specified time horizon. Relative risk statements can mislead when baseline risk varies substantially. A 1.5-fold increase in odds for a disease with 1% baseline risk means absolute risk rises from 1% to roughly 1.5%; the same relative increase for a disease with 20% baseline risk means absolute risk rises from 20% to roughly 26%. A patient told they have "50% higher risk" may react very differently depending on whether baseline risk is low or high.

Converting PGS to absolute risk requires combining the score with baseline incidence rates, which vary by age, sex, and other factors. The **hazard ratio** per standard deviation of PGS, combined with age-specific **incidence curves** from population registries, can yield personalized risk trajectories. Such calculations demand careful attention to calibration: the model must produce well-calibrated probabilities in the population where it will be deployed, not just the population where it was trained. A model calibrated in UK Biobank may systematically over- or under-estimate risk when applied to a U.S. clinical population with different baseline incidence rates or healthcare practices. Clinical deployment of PGS is addressed in detail in @sec-clinical.

### Explained Variance and Discrimination {#sec-ch03-explained-variance-discrimination}

Population-level performance metrics determine whether a polygenic score has any utility, but they can mask the substantial uncertainty that remains for any individual patient. For quantitative traits, the coefficient of determination (*R²*) between PGS and phenotype provides a direct measure of explanatory power. Height PGS now explain roughly 25% of phenotypic variance in European-ancestry populations, approaching the theoretical maximum given current sample sizes and the heritability of the trait [@yengo_meta-analysis_2022]. For binary traits, the *R²* on the **liability scale** (the underlying continuous risk) is more interpretable than the observed-scale *R²*, which depends on disease prevalence.

Area under the receiver operating characteristic curve (auROC) measures discrimination: the probability that a randomly selected case has a higher PGS than a randomly selected control. auROC values of 0.5 indicate no discrimination (random guessing); values approaching 1.0 indicate near-perfect separation. For most complex diseases, PGS achieve auROC values in the 0.55 to 0.70 range when used alone, with incremental gains when combined with traditional risk factors [@torkamani_personal_2018; @lambert_polygenic_2019]. These values reflect meaningful stratification at the population level but limited utility for individual prediction.

Even a PGS that explains 10% of trait variance leaves 90% unexplained by factors genetic and environmental. High-risk individuals by PGS may never develop disease; low-risk individuals may be affected. Polygenic scores provide probabilistic risk stratification, not deterministic prediction. This distinction is critical for clinical communication and for setting appropriate expectations about what genomic risk information can and cannot offer.

## Ancestry, Portability, and Fairness {#sec-ch03-ancestry-portability-fairness}

The vast majority of GWAS participants have been of European ancestry: as of 2019, approximately 78% of participants were European despite Europeans comprising roughly 16% of the global population [@martin_clinical_2019]. This historical imbalance has profound consequences for who benefits from polygenic scores and who may be harmed by their limitations. A technology that works well for some populations and poorly for others is not merely incomplete; deployed without appropriate caution, it risks widening existing health disparities rather than narrowing them.

::: {#fig-pgs-portability}
![FIGURE PLACEHOLDER](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[High] Bar chart showing relative prediction accuracy (*R²* or auROC ratio) of European-derived PGS when applied to different ancestry groups. European as reference (100%), then decreasing accuracy for East Asian, South Asian, Hispanic/Latino, and African-ancestry populations (often 25-60% of European performance). Include error bars. Overlay or annotate factors contributing to the gap: LD differences, allele frequency differences, potential effect heterogeneity, training sample size disparities.
:::

### Portability Problem {#sec-ch03-portability-problem}

Polygenic scores derived from European-ancestry GWAS show markedly reduced performance in other populations. African-ancestry individuals typically experience 40% to 75% reductions in prediction accuracy compared to European-ancestry individuals, even for the same trait measured in the same study [@duncan_analysis_2019; @martin_clinical_2019]. The pattern holds across traits and across methods, though the magnitude varies with genetic architecture and the degree of shared causal variants.

Several factors contribute to this portability failure. LD structure differs across populations: tag SNPs that effectively proxy causal variants in Europeans may be poor proxies in populations with different recombination history. Allele frequencies differ: variants common in one population may be rare or absent in another. Effect sizes may genuinely differ across populations due to gene-environment interactions or genetic background effects. And GWAS in smaller non-European samples have less power to detect associations, yielding noisier effect estimates that further degrade prediction.

Multi-ancestry GWAS and methods designed to leverage diverse training data offer partial solutions. Including multiple ancestries in discovery improves transferability, and methods that explicitly model ancestry-specific LD or effect sizes can enhance performance [@marquez-luna_multiethnic_2017]. Yet even state-of-the-art approaches do not fully close the gap, and substantial research is needed before PGS perform equitably across populations.

### Fairness and Health Equity {#sec-ch03-fairness-health-equity}

The performance gap across ancestries is not merely a technical nuisance; it raises fundamental questions about fairness in precision medicine. If genomic models work primarily for individuals of European ancestry, deploying these models in diverse clinical populations risks exacerbating existing health disparities rather than ameliorating them. The communities historically excluded from genetic research would continue to receive inferior genomic medicine, now encoded in algorithmic form.

Consider a scenario where PGS are used for risk-stratified screening. If the score identifies high-risk individuals more accurately in Europeans than in other groups, Europeans receive more targeted and efficient screening while others receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research, while the costs of miscalibration fall on those historically excluded.

These challenges extend beyond PGS to every genomic model. Foundation models can learn to exploit ancestry signals as shortcuts, achieving high benchmark performance while performing poorly on underrepresented groups. Aggregate performance metrics mask inequities across populations. Deployment in diverse clinical settings requires explicit evaluation of performance stratified by ancestry, along with transparent reporting of limitations and appropriate caution in populations where validation is limited. These issues receive comprehensive treatment in @sec-confound, with governance and policy responses addressed in @sec-ethics.

::: {.callout-note title="The 78 Percent Problem"}

As of 2019, approximately 78% of GWAS participants were of European ancestry despite Europeans comprising roughly 16% of the global population [@martin_clinical_2019]. This disparity propagates through every layer of genomic medicine. Polygenic scores derived from European-ancestry GWAS show 40-75% reductions in prediction accuracy for African-ancestry individuals, even for the same trait measured in the same study. Variant databases like ClinVar contain far more pathogenic classifications for European-ancestry variants, leaving variants from underrepresented populations more likely to remain classified as VUS due to insufficient evidence. Foundation models inherit these biases at the root: a model trained on skewed data cannot be corrected *post hoc* to achieve the performance it would have achieved with representative training data.

The disparity is not merely statistical. If genomic risk scores are used for screening decisions, Europeans receive more accurate risk stratification while other populations receive either under-screening (if falsely classified as low risk) or over-screening (if falsely classified as high risk). The benefits of precision medicine accrue disproportionately to those already overrepresented in research.

This problem recurs throughout the book: in variant effect prediction (@sec-veps), model confounding (@sec-confound), clinical risk prediction (@sec-clinical), and the governance frameworks needed to address it (@sec-ethics). *[Citation Needed for VUS statistics]*

:::

## Phenome-Wide Association Studies {#sec-ch03-phenome-wide-association-studies}

GWAS answer a specific question: which variants associate with this phenotype? The reverse question is equally informative: which phenotypes associate with this variant, or with this set of variants aggregated into a polygenic score? **Phenome-wide association studies (PheWAS)** systematically test associations between genetic variants and hundreds or thousands of phenotypes, revealing pleiotropy that single-phenotype analyses cannot detect. A variant initially discovered for its association with coronary artery disease may also associate with type 2 diabetes, lipid levels, and blood pressure, connections that illuminate shared biology and inform variant interpretation.

This reversal of the GWAS paradigm has proven particularly valuable for understanding polygenic score biology. A polygenic score constructed for one trait often predicts other traits, sometimes to a surprising degree. The coronary artery disease PGS predicts not only heart attacks but also diabetes, hypertension, and mortality from other vascular causes. These cross-phenotype associations reflect the shared genetic architecture among related traits and the pleiotropic effects of common variants. They also reveal where phenotype definitions may be capturing overlapping constructs or where biological pathways connect seemingly distinct outcomes.

### PheWAS Framework {#sec-ch03-phewas-framework}

PheWAS implementations parallel GWAS but with dimensions transposed. Rather than testing millions of variants against one phenotype, PheWAS tests one variant (or score) against hundreds of phenotypes. The phenotype vocabulary typically derives from EHR codes grouped into clinically meaningful categories. **Phecodes** collapse ICD-9 and ICD-10 billing codes into approximately 1,800 phenotype groups, aggregating related codes (such as the many ICD codes for diabetes mellitus) into unified disease concepts while distinguishing diseases that occupy nearby code ranges but represent different conditions.

The statistical framework mirrors GWAS: logistic regression for binary phenotypes, linear regression for quantitative traits, adjustment for covariates including age, sex, and genetic ancestry. Multiple testing correction accounts for the hundreds of tests performed; the Bonferroni threshold at 1,800 phecodes requires *p* < 2.8 × 10⁻⁵ for significance. False discovery rate control offers a less conservative alternative appropriate when characterizing the landscape of associations rather than declaring individual findings.

Interpretation requires attention to the correlation structure among phenotypes. A variant associated with obesity will, by mechanical consequence, associate with any phenotype more common in obese individuals. True pleiotropy (the variant affecting multiple traits through independent biological pathways) cannot be distinguished from mediated pleiotropy (the variant affecting one trait that causes others) through PheWAS alone. Colocalization analysis, conditional testing, and Mendelian randomization provide complementary evidence about whether associations reflect shared causal variants or confounded correlations.

### PheWAS for Polygenic Score Interpretation {#sec-ch03-phewas-polygenic-score-interpretation}

Single variants have modest effects on complex traits, limiting the power of variant-level PheWAS for common diseases. Polygenic scores aggregate these effects across thousands of variants, providing sufficient signal for phenome-wide characterization. PRS-PheWAS tests the association between a polygenic score and each phenotype in the vocabulary, revealing the full spectrum of traits that share genetic architecture with the index phenotype.

Xu et al. applied this framework to interpret EHR-embedding-based polygenic scores, finding that scores derived from cardiovascular-related embedding dimensions associated strongly with circulatory system diagnoses across the phenome [@xu_improving_2025]. The PheWAS results explained why cardiovascular traits showed the largest improvements from embedding-enhanced prediction: the embeddings captured genetic signal shared across the cardiovascular phenotype cluster. This approach provides a systematic method for understanding what a polygenic score actually predicts and whether its cross-phenotype associations match biological expectations.

PRS-PheWAS also reveals unexpected associations that may indicate shared biology, confounding, or phenotype definition artifacts. A diabetes PGS that associates with billing codes for insulin pumps reflects healthcare utilization rather than disease biology. A depression PGS that associates with chronic pain diagnoses may indicate shared genetic liability, diagnostic conflation, or the medical consequences of depression. Distinguishing these possibilities requires domain knowledge and follow-up analyses that the PheWAS itself cannot provide.

### Phenotype Quality and PheWAS Power {#sec-ch03-phenotype-quality-phewas-power}

The power of PheWAS depends critically on phenotype quality, which varies enormously across the hundreds of conditions in a typical phecode vocabulary. Well-captured phenotypes with clear diagnostic criteria (type 2 diabetes, hypothyroidism) yield stronger associations than poorly captured phenotypes that depend on documentation practices (chronic fatigue syndrome, fibromyalgia). Rare phenotypes with few cases lack power regardless of effect size. Common phenotypes with high misclassification rates suffer attenuated effects.

This heterogeneity complicates interpretation of phenome-wide results. The absence of an association may reflect genuine lack of pleiotropy or insufficient power to detect it. The pattern of associations across phenotype categories may reflect genuine biological clustering or differential phenotype quality across clinical domains. Cardiovascular phenotypes in hospital-based EHRs are typically well-captured because they drive admissions and procedures; psychiatric phenotypes are poorly captured because they are often managed in outpatient settings that may not feed into the research EHR.

Recognition of these limitations has motivated phenotype quality assessment as a prerequisite for PheWAS. Metrics such as the proportion of cases with supporting laboratory values, the consistency of coding over time, and the agreement between algorithmic definitions and chart review provide evidence about which phenotypes can support reliable association testing. Restricting analyses to high-quality phenotypes improves specificity at the cost of comprehensiveness.

### Deep Phenotyping and Embedding-Enhanced GWAS {#sec-ch03-deep-phenotyping-embedding-enhanced-gwas}

The limitations of binary phecode phenotypes have motivated alternative approaches that leverage richer phenotypic representations. Rather than testing association with categorical disease labels, these methods test association with continuous phenotypic embeddings that capture clinical similarity and co-occurrence structure. A patient's position in embedding space reflects their full clinical profile rather than the presence or absence of specific diagnoses.

EHR-embedding GWAS treats each dimension of a phenotype embedding as a quantitative trait, conducting standard GWAS to identify variants associated with that dimension. Xu et al. found that such embedding dimensions show significant heritability and genetic correlation with diverse clinical traits, suggesting they capture biologically meaningful phenotypic variation [@xu_improving_2025]. Hierarchical clustering of traits by their genetic correlation profiles with embedding dimensions recovered clinically coherent groups, including a cardiovascular cluster comprising coronary artery disease, ischemic stroke, peripheral artery disease, type 2 diabetes, and related conditions.

These embedding-based approaches offer several advantages over traditional phenotype definitions. They avoid the information loss inherent in binary case-control classification. They capture phenotypic relationships that expert-defined definitions may miss. They can be constructed from the same EHR data used for genotype-phenotype analysis, requiring no additional phenotyping effort. Their limitations include interpretability (what does association with embedding dimension 7 mean biologically?) and potential circularity (if embeddings capture coding practices, GWAS may identify variants associated with healthcare utilization rather than disease). *[Citation Needed for interpretability challenges]*

The integration of phenotype embeddings with polygenic prediction represents an active research frontier. Embedding-enhanced polygenic risk scores combine traditional single-trait scores with scores derived from EHR-embedding GWAS, leveraging the genetic correlations among related phenotypes to improve prediction for the target trait. For cardiovascular outcomes where phenotypes cluster together and share genetic architecture, this integration has shown substantial improvements over single-trait scores. The approach is examined in detail in @sec-clinical.

## From Association to Mechanism {#sec-ch03-association-mechanism}

GWAS and polygenic scores have delivered thousands of robust trait associations, clinically useful risk stratification for some conditions, and fundamental insights into the polygenic architecture of complex phenotypes. They have also exposed a persistent gap between statistical association and biological understanding. Most GWAS hits lie in noncoding regions, often within enhancers, promoters, or other regulatory elements. The variant is associated; the mechanism is obscure. Fine-mapping narrows the list of candidates but rarely identifies a single causal nucleotide with confidence. Even when a variant is prioritized, the path from sequence change to molecular consequence to cellular phenotype to disease remains opaque.

This mechanistic gap limits translation in concrete ways. Drug development requires actionable targets, not associated regions. Clinical variant interpretation needs to explain why a variant matters, not just that it correlates with disease. Polygenic scores stratify population risk but offer little guidance on individual intervention. Multiple complementary strategies address this gap: regulatory sequence models predict how variants alter transcription factor binding and chromatin accessibility (@sec-reg), variant effect predictors assess functional impact at nucleotide resolution (@sec-veps), and multi-omics integration approaches connect genetic variation to intermediate molecular phenotypes (@sec-multi).

The goal is not to replace statistical genetics but to build on it. Association provides the map of where trait-relevant variation resides; mechanistic modeling attempts to explain how that variation produces its effects. The combination of statistical association and mechanistic interpretation offers the most promising path toward genomic medicine that is both predictive and understood.