# Data Landscape {#sec-ch02-data}

Genomic models learn from labels, and those labels come from somewhere. A variant effect predictor trained on ClinVar classifications learns whatever biases clinical laboratories embedded in those classifications. A chromatin accessibility model trained on Encyclopedia of DNA Elements (ENCODE) cell lines may fail on primary tissues absent from the training compendium. A constraint metric derived from European-ancestry cohorts will be poorly calibrated for variants private to other populations. Every machine learning model in genomics inherits both the signal and the systematic gaps of its training data. Understanding what genomic resources contain, and what they systematically miss, is prerequisite to interpreting what models learn.

No single dataset captures the complexity of genomic function. The field depends on a mosaic of complementary resources: reference genomes and gene annotations that define the coordinate system, population variant catalogs that reveal what survives in healthy individuals, biobank datasets that link genetic variation to phenotypes at scale, functional genomics atlases that map biochemical activity across cell types and conditions, and clinical databases that aggregate expert variant interpretations. Each resource contributes a different type of evidence. Reference genomes provide the scaffold against which all variants are defined. Population databases like the Genome Aggregation Database (gnomAD) establish baseline expectations for variant frequency. Functional assays from ENCODE and Roadmap Epigenomics indicate where the genome shows evidence of regulatory activity. Clinical databases like ClinVar provide ground-truth labels for pathogenic and benign variants, at least for the subset of variants that have been expertly reviewed.

::: {#fig-data-ecosystem}
![FIGURE PLACEHOLDER](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[Essential] Conceptual map showing how major data resources interconnect and flow into downstream applications. Organize into layers: (1) Foundation layer: Reference genomes (GRCh38, T2T-CHM13) and gene annotations (GENCODE, RefSeq, MANE); (2) Population layer: Variant catalogs (gnomAD, 1000 Genomes) and biobanks (UK Biobank, All of Us); (3) Functional layer: ENCODE, Roadmap Epigenomics, Cistrome, GTEx; (4) Clinical layer: ClinVar, ClinGen, OMIM, PharmGKB. Draw arrows showing dependencies. Indicate which resources provide "coordinates," "frequencies," "functions," or "labels."
:::

The goal of this chapter is not encyclopedic completeness but critical literacy: the ability to recognize when training data may not represent the population, condition, or variant class at hand. This literacy becomes essential when deploying models in clinical contexts where failures have consequences. The biases introduced by population structure, label ascertainment, and technical artifacts create systematic confounding that propagates through every downstream model. These challenges receive detailed treatment, where @sec-ch22-ancestry-confounding examines ancestry-related confounding, @sec-ch22-label-bias addresses ascertainment patterns in clinical labels, and @sec-ch22-batch-effects covers technical batch effects. The present chapter establishes the data landscape from which those confounders arise.


## Reference Genomes and Gene Annotations {#sec-ch02-reference}

A family arrives at a genetics clinic after their newborn's screening reveals a potential metabolic disorder. The clinical team orders whole-genome sequencing and receives a report identifying a novel variant in a gene associated with the condition. The variant's coordinates place it at the boundary between an exon and an intron, potentially disrupting splicing. Yet whether this interpretation is correct depends on decisions made years before the child was born: which positions constitute exon boundaries, which transcript model defines the canonical gene structure, and which sequence serves as the reference against which "variant" is defined. Reference genomes and gene annotations are so foundational that their assumptions often become invisible, yet every downstream analysis inherits the choices embedded in these resources. A model cannot learn about a regulatory element for a transcript that does not exist in the annotation.


### Reference Assemblies {#sec-ch02-reference-assemblies}

A patient's clinical sequencing reveals a potentially pathogenic variant in a duplicated region of chromosome 17. The variant calling pipeline reports a confident genotype, the annotation tool predicts a frameshift, and the clinical team prepares to discuss the finding with the family. Yet the "variant" may be an artifact of misalignment: reads from a paralogous sequence elsewhere in the genome mapped incorrectly because the reference assembly collapsed two distinct loci into one. Whether this error occurs, whether it can be detected, and whether the clinical interpretation has any foundation in biological reality all depend on the choice of reference genome.

Most modern pipelines align reads to a small number of reference assemblies, predominantly Genome Reference Consortium Human Build 38 (GRCh38) or the newer telomere-to-telomere CHM13 assembly (T2T-CHM13) [@nurk_complete_2022]. A reference genome is not simply a consensus sequence; it encodes a series of consequential decisions about how to represent duplications, alternate haplotypes, and unresolved gaps. These decisions determine which regions are mappable by short reads, how structural variants are represented, and how comparable results will be across cohorts built on different assemblies. The variant calling pipelines described in @sec-ch01-ngs depend fundamentally on these reference choices. Variant callers that rely on these reference assemblies face characteristic failures in difficult regions, as detailed in @sec-ch01-difficult.

Graph-based and **pangenome** references relax the assumption of a single linear reference, representing multiple haplotypes and ancestries within a unified coordinate system [@liao_draft_2023]. Comparative multi-species references, such as those used in mammalian constraint maps from the Zoonomia consortium [@sullivan_leveraging_2023], extend this idea across species, providing evolutionary conservation scores that feed directly into deleteriousness predictors and gene-level constraint metrics discussed in @sec-ch04-constraint-metrics for evolutionary approaches and @sec-ch04-cadd for integrated scoring.

For most genomic deep learning applications, the practical reality is still GRCh37 or GRCh38 coordinates, often with incremental patches. Models trained on these resources therefore inherit their blind spots: incomplete or collapsed segmental duplications, underrepresented ancestries in pangenome construction, and uneven quality across chromosomes and regions. These limitations concentrate in precisely the regions where variant interpretation matters most (such as the HLA locus, pharmacogenes with structural variation, and segmental duplications harboring disease genes), creating a systematic mismatch between clinical importance and reference quality.


### Gene Models {#sec-ch02-gene-models}

A child presents with developmental delay and muscle weakness. Whole-genome sequencing identifies a novel variant near the *DMD* gene, which encodes dystrophin and causes Duchenne muscular dystrophy when disrupted. The annotation pipeline reports the variant as intronic and unlikely to affect protein function. Yet *DMD* spans 2.2 megabases and includes 79 exons with complex alternative splicing; whether this variant disrupts a tissue-specific isoform depends entirely on which transcript model the annotation tool uses. The clinical implications are entirely different, yet the underlying sequence is identical: only the annotation changes.

Gene annotation databases such as GENCODE and RefSeq define the biological vocabulary overlaid on reference coordinates: exon-intron structures, canonical and alternative transcripts, start and stop codons, and untranslated regions [@frankish_gencode_2019; @oleary_refseq_2016]. These annotations distinguish coding from non-coding variants, identify splice-disrupting mutations, and map functional genomics signals to genes. They also establish the units (genes, transcripts, exons) that downstream models implicitly operate on. Splicing prediction models in @sec-ch06-spliceai learn splice site grammar from annotated exon-intron boundaries, then apply those patterns to detect both canonical and cryptic sites.

The MANE Select project provides a single matched transcript per protein-coding gene that is identical between GENCODE and RefSeq, simplifying clinical interpretation and variant reporting [@morales_mane_2022]. This standardization makes variant descriptions consistent across laboratories, yet it privileges a single isoform over biological complexity. In contexts where tissue-specific or developmentally regulated isoforms drive disease (alternative splicing in muscular dystrophies, isoform-specific expression in neuropsychiatric conditions), the canonical transcript may miss the relevant biology.

New isoforms continue to be discovered, alternative splicing remains incompletely cataloged, and cell-type-specific transcripts may be absent from bulk-derived annotations. Non-coding RNA genes and pseudogenes are even more unevenly annotated. These gaps propagate through every tool built on them: variant effect predictors cannot score consequences for transcripts that do not exist in their reference annotation, and expression models cannot predict isoforms they were never trained on.


## Population Variant Catalogs and Allele Frequencies {#sec-ch02-population}

A clinical geneticist evaluates a child with an undiagnosed syndrome and identifies a novel missense variant in a candidate gene. The question that determines what happens next is deceptively simple: has anyone else carried this variant? If the variant appears in thousands of healthy adults, it is almost certainly benign. If it has never been observed across hundreds of thousands of sequenced genomes, that absence becomes evidence of selective pressure against the variant, strongly suggesting functional consequence. Without population-scale variant catalogs, this inference is impossible, and every rare variant would demand the same level of scrutiny regardless of its actual likelihood of causing disease.

::: {#fig-ancestry-representation}
![FIGURE PLACEHOLDER](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[High] Stacked bar chart or waffle plot showing ancestry composition across key resources: gnomAD v4, UK Biobank, ClinVar submissions, GTEx donors, GWAS Catalog participants. Highlight the persistent European overrepresentation (approximately 78% of GWAS participants as of 2019) against global population proportions [@sirugo_diversity_2019]. Include a small world map inset showing which continental ancestries are represented vs underrepresented.
:::

**Allele frequency**, the proportion of chromosomes in a reference population carrying a given variant, serves as one of the most powerful priors in variant interpretation. Beyond simple filtering, allele frequencies inform statistical frameworks for case-control association, provide training signal for deleteriousness predictors, and enable **imputation** of ungenotyped variants through **linkage disequilibrium** (see @sec-ch03-gwas). The catalogs described below have progressively expanded in sample size, ancestral diversity, and annotation depth, transforming variant interpretation from an ad hoc exercise into a quantitative discipline.

A crucial nuance shapes model interpretation: these catalogs record variants that are compatible with being sampled in the first place. Gene-lethal variants that cause embryonic death or severe childhood disease rarely appear, even when they are biologically informative. Variants causing late-onset conditions (Alzheimer's risk alleles, adult-onset cancer predisposition) can persist at appreciable frequencies because selection has not had time to remove them. Models trained on population data can only learn from variants present in these catalogs, which means they systematically underrepresent the most severe loss-of-function mutations.


### dbSNP and Variant Identifiers {#sec-ch02-dbsnp}

Two laboratories sequence the same patient and report their findings to a tumor board. Laboratory A describes a variant using genomic coordinates on GRCh38; Laboratory B uses HGVS nomenclature relative to a specific transcript. Are they discussing the same variant? Without standardized identifiers, this simple question can consume hours of manual reconciliation. The database of Single Nucleotide Polymorphisms (dbSNP) provides the common currency that cuts through this ambiguity: stable identifiers (rsIDs) that enable integration across tools and publications [@sherry_dbsnp_2001].

When a laboratory reports a variant, when a researcher publishes a GWAS finding, and when a clinician queries a pathogenicity database, they need a common language to ensure they are discussing the same genomic position. Modern whole-exome and whole-genome sequencing routinely discovers millions of previously unseen variants per large cohort, but dbSNP identifiers remain the standard way to reference known **single nucleotide polymorphisms (SNPs)** and link disparate resources. When a GWAS publication reports an association at rs12345, that identifier traces back to dbSNP and enables integration with functional annotations, clinical databases, and population variant catalogs.


### 1000 Genomes and Early Reference Panels {#sec-ch02-1000genomes}

Genotyping arrays measure only a sparse subset of genomic positions, yet disease-associated variants may lie anywhere in the genome. How can researchers infer variants at unmeasured positions? The answer lies in patterns of co-inheritance: variants that travel together on ancestral chromosome segments can be inferred from neighboring measured positions. This process of imputation depends entirely on having reference panels that capture the haplotype structure of the population being studied.

The 1000 Genomes Project provided one of the first widely used multi-population panels for imputation, sampling individuals from African, European, East Asian, South Asian, and admixed American populations [@auton_1kgp_2015]. The resulting haplotype structure underlies many imputation servers and downstream analyses, enabling genotyping arrays with millions of markers to impute tens of millions of untyped variants through linkage disequilibrium [@yun_accurate_2021]. Although its sample size (approximately 2,500 individuals) is modest by current standards, 1000 Genomes established the template for how to build and distribute multi-population reference panels, and its samples continue to serve as benchmarks for variant calling performance. The role of imputation in GWAS is discussed further in @sec-ch03-gwas.


### Genome Aggregation Database (gnomAD) {#sec-ch02-gnomad}

Distinguishing genuinely rare variants from sampling gaps requires population-scale catalogs with two properties: sufficient sample size to detect low-frequency variants reliably, and sufficient ancestral diversity to avoid misclassifying variants common in underrepresented populations. A variant at 1% frequency in African populations but absent from European cohorts would be incorrectly flagged as novel by any database sampling only European individuals. The Genome Aggregation Database (gnomAD) addresses both requirements by aggregating exome and genome sequencing data from research and clinical cohorts worldwide into harmonized allele frequency resources spanning hundreds of thousands of individuals [@karczewski_mutational_2020].

gnomAD provides high-resolution allele frequencies stratified by genetic ancestry, enabling population-matched filtering that accounts for variants common in one ancestry but rare in others. This stratification matters because a variant observed at 1% frequency in African populations but absent from European cohorts would be incorrectly flagged as ultra-rare by a model trained predominantly on European data.

gnomAD also introduced **constraint metrics** that have become standard features in variant prioritization. The probability of loss-of-function intolerance (pLI) and loss-of-function observed/expected upper bound fraction (LOEUF) summarize how depleted a gene is for protein-truncating variants relative to expectation. Genes essential for viability show far fewer loss-of-function variants than neutral mutation rates would predict; this depletion provides evidence of selective constraint that transfers to variant interpretation. A novel truncating variant in a highly constrained gene warrants more concern than the same variant class in an unconstrained gene. These constraint metrics serve as important features in many variant effect predictors discussed in @sec-ch04-cadd and @sec-ch14-vep-fm.

Population frequencies from gnomAD provide critical filtering steps in clinical variant interpretation pipelines, as detailed in @sec-ch26-frequency-filters. The constraint metrics derived from gnomAD form a foundation for variant effect prediction discussed in @sec-ch04-constraint-metrics. Allele frequency distributions also inform fine-mapping approaches that assign causal probability to GWAS-associated variants (@sec-ch03-ld)."

::: {.callout-note title="Interpreting Constraint Metrics"}

pLI (probability of loss-of-function intolerance) estimates the probability that a gene falls into the class of haploinsufficient genes where loss of one copy causes disease. Scores range from 0 to 1; genes with pLI > 0.9 are considered highly constrained. pLI's categorical nature (genes are classified as tolerant, intermediate, or intolerant) limits its resolution for genes with intermediate constraint.

LOEUF (loss-of-function observed/expected upper bound fraction) provides a continuous measure by computing the ratio of observed to expected loss-of-function variants, with a 90% confidence upper bound. Lower LOEUF values indicate stronger constraint. A gene with LOEUF of 0.2 has observed only 20% as many truncating variants as expected under neutral evolution. LOEUF has largely superseded pLI in contemporary analyses due to its continuous scale and more intuitive interpretation.

:::

These resources are indispensable for filtering common variants in Mendelian disease diagnostics, distinguishing ultra-rare variants from recurrent ones, and providing population genetics priors for deleteriousness scores like *CADD* [@rentzsch_cadd_2019; @schubach_cadd_2024]. At the same time, they reflect the composition of the cohorts they aggregate: ancestry representation remains uneven despite ongoing efforts, structural variants and repeat expansions are less completely cataloged than SNVs and short indels, and individuals with severe early-onset disease are underrepresented by design. These biases propagate into every model that uses gnomAD frequencies or constraint scores as features.


## Biobanks and GWAS Data {#sec-ch02-biobanks}

A pharmaceutical company developing a new cardiac drug needs to understand which genetic variants influence drug response. A health system implementing pharmacogenomic testing needs to know which patients are at risk for adverse reactions. A researcher studying the genetics of depression needs cases and controls with standardized phenotyping. None of these questions can be answered by sequencing alone; they require linking genetic variation to phenotypes at scale, across thousands or hundreds of thousands of individuals. Yet assembling such cohorts introduces its own biases: participants must consent, provide samples, and have phenotypes recorded in standardized ways. The populations enrolled in major biobanks reflect patterns of healthcare access, research infrastructure, and historical priorities that do not represent global genetic diversity.

The overrepresentation of European-ancestry individuals in most major biobanks creates systematic gaps in variant discovery, effect-size estimation, and **polygenic score** portability that propagate through downstream analyses [@sirugo_diversity_2019]. A variant common in West African populations may be absent or rare in European-dominated catalogs, rendering it invisible to association studies and underrepresented in predictive models. This tension between scientific utility and representational equity shapes every biobank-derived resource and is discussed in detail in @sec-ch22-confounding.


### Large Population Cohorts {#sec-ch02-cohorts}

A variant that increases heart disease risk by 5% (OR = 1.05) requires approximately 50,000 cases and 50,000 controls to detect reliably at genome-wide significance ($\alpha = 5 \times 10^{-8}$). A variant shifting a continuous trait like blood pressure by 0.05 standard deviations demands even larger samples, often exceeding 100,000 individuals. The fundamental constraint is statistical: detecting small effect sizes against a backdrop of millions of tested variants requires both stringent significance thresholds and massive sample sizes to achieve adequate power. Required sample size scales with the inverse square of effect size, meaning a variant with half the effect requires four times the sample. This relationship explains why genetic discovery accelerated dramatically when biobanks reached the scale of hundreds of thousands of participants. @sec-ch25-clinical-risk provides a detailed treatment of these statistical foundations and their implications for clinical risk prediction.

UK Biobank, with approximately 500,000 participants and deep phenotyping across thousands of traits, has become a dominant resource for methods development and benchmarking [@bycroft_uk_2018]. FinnGen leverages Finland's population history and unified healthcare records for large-scale disease association discovery [@kurki_finngen_2023]. The All of Us Research Program prioritizes diversity, aiming to enroll one million participants with deliberate oversampling of historically underrepresented groups [@null_all-of-us_2019]. deCODE genetics has genotyped a substantial fraction of Iceland's population, enabling unique studies of rare variants and founder effects in a population with detailed genealogical records [@gudbjartsson_decode_2015]. Additional resources include the Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie Biobank, and emerging African genomics initiatives such as H3Africa [@sirugo_diversity_2019].

Together, these efforts enable **genome-wide association studies (GWAS)** for thousands of traits, development and evaluation of polygenic scores, and fine-mapping of causal variants and genes [@marees_gwas_2018; @mountjoy_open_2021]. From a modeling perspective, they provide the large-scale genotype-phenotype matrices that power architectures ranging from classical linear mixed models to foundation models trained on biobank-scale data. The practical reality for most GWAS and polygenic score methods in @sec-ch03-gwas is data from either array genotyping with imputation or whole-exome/whole-genome sequencing with joint calling, as in *DeepVariant*/GLnexus-style pipelines [@yun_accurate_2021].


### GWAS Summary Statistics {#sec-ch02-gwas-summary}

Individual-level genotype and phenotype data are powerful but sensitive. Sharing such data across institutions requires complex data use agreements, institutional review board approvals, and secure computing infrastructure. These barriers would slow scientific progress if every analysis required access to raw data. Summary statistics offer an alternative: per-variant effect sizes, standard errors, and p-values that capture the essential association signal without revealing individual genotypes.

The GWAS Catalog compiles published results across thousands of traits, while the PGS Catalog provides curated polygenic score weights and metadata for reproducibility [@sollis_gwas-catalog_2023; @lambert_pgs-catalog_2021]. Frameworks like Open Targets Genetics integrate fine-mapped signals with functional annotations to prioritize candidate causal genes at associated loci [@mountjoy_open_2021].

Summary statistics enable meta-analysis across cohorts without sharing individual-level data, transfer of genetic findings to new populations through methods like PRS-CSx, and integration with functional annotations to distinguish causal variants from linked bystanders [@ruan_improving_2022]. For deep learning, summary statistics provide a sparse, trait-level view of the genome. This sparsity creates different challenges than the dense labels available in functional genomics, but also different opportunities: the genetic architecture revealed through GWAS informs polygenic score construction (@sec-ch03-pgs-construction) and indicates which variants and pathways merit follow-up with regulatory models (@sec-ch13-regulatory) and variant effect predictors (@sec-ch14-vep-fm).


## Functional Genomics and Regulatory Landscapes {#sec-ch02-functional}

Protein-coding exons constitute roughly 1.5% of the human genome, yet most disease-associated variants from GWAS fall outside coding regions. A massive study identifies 100 loci associated with schizophrenia, but 90 of them lie in non-coding regions with no obvious connection to any gene. This mismatch creates a fundamental interpretability problem: we can identify non-coding loci that harbor disease risk, but we cannot easily determine which base pairs matter, which genes they regulate, or in which cell types they act. Understanding these non-coding variants requires mapping the regulatory logic that governs when, where, and how much each gene is expressed. Functional genomics assays provide this map, identifying **transcription factor** binding sites, nucleosome positioning, **chromatin accessibility**, **histone modifications**, and three-dimensional genome organization across cell types and conditions.

Functional genomics datasets serve a dual role in genomic deep learning. First, they supply the biological vocabulary for interpreting non-coding variants, linking sequence changes to potential regulatory consequences. Second, and more directly, they provide the training labels for sequence-to-function models. When a model learns to predict chromatin accessibility or histone marks from DNA sequence alone, it compresses into its parameters the regulatory code implicit in thousands of functional genomics experiments.


### ENCODE, Roadmap, and Related Consortia {#sec-ch02-encode}

A single ChIP-seq experiment for one transcription factor in one cell line provides useful signal, but models that learn general regulatory grammar require thousands of such experiments spanning many factors, marks, and cell types. A researcher training a regulatory model on her own laboratory's data will produce a model that works well in her specific experimental context but fails to generalize. The key insight behind ENCODE and Roadmap was that coordinated experimental campaigns, with standardized methods and quality control, could create reference datasets serving the entire field.

The Encyclopedia of DNA Elements (ENCODE) and Roadmap Epigenomics consortia designed coordinated experimental campaigns that profiled transcription factor binding (ChIP-seq), histone modifications, chromatin accessibility (DNase-seq, ATAC-seq), and chromatin conformation (Hi-C) across cell lines and primary tissues [@kagda_encode_2025; @kundaje_roadmap_2015]. Gene Expression Omnibus (GEO) archives these and many other functional genomics datasets with standardized metadata [@edgar_geo_2002].

The significance of these consortia lies less in any individual experiment than in the scale and standardization they provide. By generating hundreds of assays across dozens of cell types with consistent protocols, ENCODE and Roadmap created canonical reference datasets that define the regulatory landscape for the cell types they profiled. These resources enabled multiple generations of regulatory models. *DeepSEA* (@sec-ch06-deepsea) pioneered multi-task learning on ENCODE chromatin accessibility and transcription factor binding, where each prediction task corresponds to a ChIP-seq or accessibility experiment. *Enformer* (@sec-ch13-enformer) extended this paradigm with transformer attention mechanisms and longer context windows. The progression from convolutional to attention-based architectures reflects both the richness of ENCODE data and its limitations: models trained on these resources inherit ENCODE's choices about which cell types, factors, and experimental conditions merit inclusion.

::: {#fig-functional-genomics-matrix}
![FIGURE PLACEHOLDER](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[High] Heatmap-style visualization showing the ENCODE/Roadmap data compendium structure. Rows represent cell types or tissues (group by category: cell lines, primary cells, tissues). Columns represent assay types (ChIP-seq for various TFs and histone marks, DNase-seq, ATAC-seq, RNA-seq). Color intensity indicates data availability (present/absent or coverage depth). Highlight which cell types have comprehensive coverage vs sparse coverage. Annotate example cell types that are well-profiled (K562, GM12878, HepG2) vs disease-relevant tissues that remain undersampled.
:::


### Cistrome Data Browser {#sec-ch02-cistrome}

ENCODE and Roadmap provide authoritative datasets for their chosen cell types and factors, but they represent only a fraction of publicly available functional genomics experiments. A researcher interested in a specific transcription factor or a disease-relevant cell type may find that ENCODE lacks the relevant data, even though dozens of laboratories have generated relevant ChIP-seq experiments. These experiments exist scattered across GEO with heterogeneous processing and quality.

The Cistrome Data Browser addresses this gap by aggregating thousands of human and mouse ChIP-seq and chromatin accessibility datasets from ENCODE, Roadmap, GEO, and individual publications into a uniformly reprocessed repository [@zheng_cistrome_2019]. All datasets pass through standardized quality control and peak calling, enabling comparisons across experiments originally generated with different protocols.

Cistrome provides uniform peak calls, signal tracks, and metadata for cell type, factor, and experimental conditions. The tradeoff is heterogeneity: while reprocessing harmonizes computational steps, the underlying experiments vary in sample preparation, antibody quality, sequencing depth, and experimental design. Cistrome expands coverage at the cost of the tight experimental control found in the primary consortia, a tradeoff that matters when models learn from noisy or inconsistent labels.


### From Assays to Training Labels {#sec-ch02-assays-labels}

Sequence-to-function models transform functional genomics resources into supervised learning problems. Models like *DeepSEA* (see @sec-ch13-regulatory) draw training labels from ENCODE, Roadmap, and Cistrome-style datasets: each genomic window is associated with binary or quantitative signals indicating transcription factor binding, histone modifications, or chromatin accessibility across hundreds of assays and cell types [@zhou_deepsea_2015; @zhou_expecto_2018].

The quality, coverage, and biases of these labels directly constrain what models can learn. Cell types absent from the training compendium cannot be predicted reliably. Factors with few high-quality ChIP-seq experiments will have noisier labels. Systematic differences between assay types (binary peak calls versus quantitative signal tracks) shape whether models learn to predict occupancy, accessibility, or something in between. These considerations become central when examining model architectures and training strategies in @sec-ch13-regulatory.


### Deep Mutational Scanning and Multiplexed Variant Assays {#sec-ch02-dms}

Population variant catalogs tell us which variants survive in healthy individuals, but they cannot tell us what happens when a specific amino acid is changed to every possible alternative. Functional genomics experiments reveal where the genome is active, but they do not directly measure the consequence of each possible mutation. **Deep mutational scanning (DMS)** fills this gap by measuring the fitness or functional impact of thousands of protein or regulatory variants in a single experiment.

These assays systematically introduce mutations (often approaching saturation mutagenesis for a protein domain or regulatory element), subject the resulting library to selection or screening, and use sequencing to quantify the representation of each variant before and after selection. The result is dense, quantitative measurements of variant effects under controlled conditions. Benchmarks such as ProteinGym compile large DMS datasets across proteins to evaluate variant effect predictors. TraitGym curates multiplexed reporter assays and other high-throughput readouts of regulatory variant effects [@notin_proteingym_2023; @benegas_traitgym_2025].

These resources sit at the interface between genomic and protein-level modeling. Where gnomAD and biobanks catalog sparse, naturally occurring variation, DMS datasets offer dense, quantitative functional measurements across systematic variant libraries that test most or all possible substitutions. DMS data differ fundamentally from population catalogs: they measure functional impact directly under controlled conditions rather than inferring it from population survival. Protein sequence models (@sec-ch12-protein-lm) and regulatory variant predictors (@sec-ch14-vep-fm) use these DMS-style datasets as key benchmarks and training sources.


## Expression and eQTL Resources {#sec-ch02-expression}

Functional genomics assays reveal where transcription factors bind and which chromatin regions are accessible, but they do not directly answer the downstream question: does regulatory activity actually change how much RNA a gene produces? A transcription factor may bind a genomic region without altering expression of nearby genes; an accessible chromatin region may not contain active regulatory elements. Regulatory binding and gene expression exist in a many-to-many relationship that cannot be resolved by either measurement alone. Expression datasets complete this link, measuring transcript abundance across tissues, cell types, and genetic backgrounds.

Connecting non-coding GWAS variants to their effector genes requires mechanistic hypotheses: some indication of which gene a regulatory variant actually regulates. **Expression quantitative trait loci (eQTLs)** provide exactly this connection, identifying genetic variants statistically associated with transcript-level changes. When a GWAS signal colocalizes with an eQTL for a nearby gene in disease-relevant tissue, that gene becomes a candidate effector. For model training, expression data provide quantitative labels that integrate across many regulatory inputs converging on a single promoter.


### Bulk Expression Atlases {#sec-ch02-gtex}

A GWAS identifies a locus associated with coronary artery disease in a non-coding region. Dozens of genes lie within the associated interval. Which one mediates the disease risk? If the lead variant also associates with expression of a nearby gene specifically in arterial endothelial cells, that gene becomes the prime candidate. Without tissue-specific expression data linked to genotypes, this inference is impossible.

The Genotype-Tissue Expression (GTEx) consortium provides the most comprehensive resource linking genetic variation to gene expression across human tissues, with RNA-seq profiles from 948 post-mortem donors across 54 tissues [@gtex_2020]. GTEx established foundational insights that inform regulatory genomics models: most genes harbor tissue-specific eQTLs, regulatory variants typically act *in cis* over distances of hundreds of kilobases, and expression variation explains a meaningful fraction of complex trait heritability.

GTEx underlies expression prediction models such as *PrediXcan*, which trains tissue-specific models to impute gene expression from genotypes alone [@gamazon_predixcan_2015]. Transcriptome-wide association studies (TWAS) extend this idea to associate imputed expression with phenotypes [@gusev_twas_2016]. Colocalization methods ask whether a GWAS signal and an eQTL share the same causal variant, providing evidence that the associated gene mediates the trait effect.

The GTEx design has limitations worth acknowledging. Post-mortem collection introduces agonal stress artifacts that may not reflect living tissue biology. Sample sizes vary considerably across tissues (hundreds for some, dozens for others), affecting statistical power. Some disease-relevant tissues, such as pancreatic islets or specific brain subregions, remain undersampled. Complementary resources like the eQTLGen Consortium aggregate eQTL results from blood across much larger sample sizes, trading tissue diversity for statistical power [@vosa_eqtl-gen_2021].


### Single-Cell and Context-Specific Expression {#sec-ch02-single-cell}

Bulk RNA-seq averages expression across all cells in a tissue sample, obscuring the cell-type-specific programs that often mediate disease biology. A bulk eQTL in brain tissue might reflect astrocytes, neurons, microglia, or oligodendrocytes; the causal cell type matters for understanding mechanism. This averaging creates a fundamental resolution problem: variants may have strong effects in rare cell populations that are diluted to undetectability when mixed with other cell types.

Single-cell RNA-seq resolves this heterogeneity, identifying expression signatures for individual cell types, rare populations, and transitional states. Large-scale efforts including the Human Cell Atlas and Tabula Sapiens are building reference atlases that catalog cell types across organs and developmental stages [@regev_cell-atlas_2017; @tabula_sapiens_2022]. For variant interpretation, single-cell data enable cell-type-specific eQTL mapping, revealing that a variant may influence expression in one cell type but not others within the same tissue. Spatial transcriptomics adds anatomical context, preserving tissue architecture while measuring gene expression.

These technologies introduce computational challenges: sparsity from dropout effects, batch variation across samples and technologies, and massive scale with millions of cells per study. They also offer an increasingly fine-grained view of the link between genotype, regulatory state, and cellular phenotype. Multi-omics integration (@sec-ch19-multi-omics) and systems-level modeling draw heavily on single-cell and spatial resources.

## Protein Databases {#sec-ch02-protein-databases}

... Update with PDB; UniRef; BFD; etc...

## Phenotype Definition and Data Quality {#sec-ch02-phenotypes}

Every model in genomics learns from labels, but phenotype labels carry their own biases distinct from variant annotations or functional genomics measurements. A GWAS for type 2 diabetes depends entirely on how diabetes is defined: by self-report, ICD-10 codes, hemoglobin A1c thresholds, medication records, or clinical adjudication. Each definition captures a different slice of the underlying biology. Self-report misses undiagnosed cases. ICD codes reflect billing practices as much as clinical reality. Laboratory thresholds impose sharp boundaries on continuous metabolic dysregulation. The "same" phenotype defined differently yields different genetic architectures, different effect sizes, and different polygenic score performance.

This sensitivity to phenotype definition compounds as biobanks scale. UK Biobank's 500,000 participants enable discovery at unprecedented statistical power, but that power is limited by the precision of the phenotypes being tested. A GWAS with millions of participants but noisy case definitions may have less effective power than a smaller study with carefully adjudicated outcomes. The trade-off between sample size and phenotype quality pervades modern statistical genetics, and understanding its contours is essential for interpreting what models trained on biobank data actually learn.

Phenotype quality issues create systematic confounding in GWAS (@sec-ch03-phenotype-quality) and clinical risk prediction models (@sec-ch25-ehr-integration). Deep phenotyping approaches that extract richer representations from EHR data are examined in @sec-ch03-deep-phenotyping for GWAS contexts and @sec-ch25-feature-integration for clinical deployment."


### Problem of Binary Disease Definitions {#sec-ch02-binary-phenotypes}

Most GWAS treat disease as binary: case or control, affected or unaffected. This simplification enables standard statistical machinery but discards information about disease severity, age of onset, trajectory, and subtype. Two patients both labeled "coronary artery disease" may differ in clinically meaningful ways: one experienced an acute myocardial infarction at age 45, the other underwent elective stenting for stable angina at 72. Collapsing this heterogeneity into a single binary label forces genetic analyses to identify variants associated with an artificial composite rather than biologically coherent disease entities.

The consequences extend beyond reduced statistical power. Phenotype heterogeneity can induce genetic heterogeneity, where different genetic variants predispose to different subtypes that have been artificially combined. A GWAS for "depression" that includes melancholic depression, atypical depression, and adjustment disorders will identify variants associated with the mixture rather than any specific syndrome. The resulting polygenic scores predict the mixture, potentially missing stronger associations with homogeneous subtypes and providing weaker stratification than would be achievable with cleaner phenotype definitions.

Clinical endpoints also differ in their proximity to genetic effects. Biomarkers such as LDL cholesterol or blood pressure lie closer to gene function than clinical outcomes such as myocardial infarction or stroke, which require the biomarker dysregulation to persist, interact with environmental factors, and culminate in tissue damage. Genetic effects are typically larger and more readily detected for intermediate phenotypes than for distal clinical outcomes. This motivates strategies that analyze biomarkers as outcomes in their own right, then connect genetic effects on biomarkers to disease risk through Mendelian randomization or mediation analysis.


### Electronic Health Record Quality and Completeness {#sec-ch02-ehr}

Electronic health records promise comprehensive phenotyping at scale: every diagnosis, procedure, medication, and laboratory result captured in structured or semi-structured form. In practice, EHR data are messy, incomplete, and shaped by processes far removed from biology. A diagnosis code reflects not just what the patient has but what the clinician chose to document, what the billing system required, and what the coding specialist interpreted. The same clinical presentation may receive different codes depending on the setting, the clinician's documentation habits, and institutional coding policies.

Missing data pervades EHR phenotyping. Laboratory values are measured when clinically indicated, not at random, creating informative missingness where the absence of a measurement conveys information about the patient's health status. Patients who transfer between health systems appear to have incomplete histories. Conditions managed by specialists outside the health system may be entirely absent from the record. These gaps are not random but systematically related to patient characteristics, healthcare access, and disease severity in ways that can bias genetic analyses.

Temporal dynamics add further complexity. Disease onset rarely corresponds to diagnosis date; patients carry pathology for years before clinical recognition. Medication records indicate prescriptions but not adherence. Procedure dates capture interventions but not the progression of disease that motivated them. Time-to-event analyses must grapple with left truncation (patients entering observation after disease onset), interval censoring (disease status observed only at discrete timepoints), and the distinction between incident and prevalent cases that confounds cross-sectional analyses.


### Coding Inconsistencies and Label Noise {#sec-ch02-label-noise}

The International Classification of Diseases provides a standardized vocabulary, but standardized vocabulary does not guarantee standardized application. ICD-10 contains over 70,000 codes, and clinical coders must choose among them based on physician documentation that may be ambiguous, incomplete, or inconsistent. Studies comparing chart review to coded diagnoses find substantial discordance: some patients with clear clinical disease lack corresponding codes, while others have codes without supporting clinical evidence. *[Citation Needed]*

Code usage also evolves over time. The transition from ICD-9 to ICD-10 in the United States (October 2015) created discontinuities in phenotype definitions built on specific codes. Clinical practice changes alter what conditions are tested for, diagnosed, and coded. COVID-19's emergence created entirely new codes and altered coding patterns for respiratory illness more broadly. Longitudinal analyses spanning coding transitions or practice changes must account for these artifacts or risk confusing temporal trends in coding with temporal trends in disease.

**Label noise** from coding errors propagates into every downstream analysis. A phenotype definition with 10% misclassification (5% false positives, 5% false negatives) substantially attenuates genetic effect sizes and reduces GWAS power. For rare diseases where cases are precious, false positives among controls matter less than false negatives among cases, which dilute the genetic signal. For common diseases where controls are presumed healthy, false negatives among controls (undiagnosed cases) similarly attenuate associations. The magnitude of this attenuation depends on disease prevalence, misclassification rates, and their correlation with genetic risk.


### Deep Phenotyping Approaches {#sec-ch02-deep-phenotyping}

Recognition of these limitations has motivated **deep phenotyping** strategies that move beyond binary disease definitions. Quantitative phenotypes, when available, preserve information that binary thresholds discard. Rather than dichotomizing blood pressure into hypertensive versus normotensive, analyzing systolic and diastolic pressure as continuous traits captures the full distribution of genetic effects. Similarly, imaging-derived phenotypes (cardiac MRI measurements, brain volume, bone density) provide precise quantitative endpoints with higher heritability than clinical disease outcomes.

Phenotype refinement uses clinical features to identify more homogeneous subgroups. Clustering patients by age of onset, comorbidity patterns, or biomarker profiles can reveal subtypes with distinct genetic architectures. Type 2 diabetes, for instance, has been decomposed into clusters defined by age, BMI, insulin resistance, and beta-cell function, with different clusters showing different genetic associations and different disease trajectories. *[Citation Needed]* Such stratification requires sufficient clinical data to define subgroups, limiting its application to well-phenotyped cohorts.

A more radical approach abandons expert-specified phenotype criteria entirely. Instead of encoding clinical knowledge through hierarchical ontologies, embedding methods learn vector representations of clinical concepts from co-occurrence patterns in EHR data. Word2Vec models trained on ICD-10 code sequences position clinically related codes near each other in this learned space; codes that co-occur in patient records cluster together regardless of their position in the ICD ontology. Large language models can generate similar **phenotype embeddings** from textual descriptions, capturing semantic relationships encoded in clinical language.

These embeddings can serve as phenotypes themselves. Xu et al. demonstrated that GWAS conducted on EHR-embedding dimensions identified heritable components of clinical phenotype structure, with genetic correlations revealing coherent trait clusters such as cardiovascular disease risk factors [@xu_improving_2025]. The embeddings capture phenotypic relationships that binary disease definitions obscure, potentially improving the power to detect genetic associations and the transferability of polygenic scores across related traits.


### Impact on Downstream Modeling {#sec-ch02-phenotype-impact}

Phenotype quality constraints propagate through every analysis built on biobank data, creating systematic confounding that affects both GWAS (@sec-ch03-phenotype-quality) and clinical risk prediction models (@sec-ch25-ehr-integration). Polygenic scores trained on noisy phenotypes learn to predict the noise alongside the signal, potentially inheriting coding artifacts, temporal discontinuities, and population-specific documentation practices. Transfer learning from one biobank to another may fail not because the underlying genetic architecture differs but because the phenotype definitions differ in ways that alter what the model learned.

Foundation models face analogous challenges. A model that learns associations between genetic variants and EHR-derived phenotypes absorbs whatever systematic distortions those phenotypes contain. If a diagnosis is more likely to be coded in patients who receive specialist care, the model learns a genetic signature for healthcare access as much as for disease biology. If a biomarker is measured only in symptomatic patients, the model learns from a biased sample that may not represent the population distribution. Deep phenotyping approaches that extract richer representations from EHR data offer partial solutions, examined in @sec-ch03-deep-phenotyping for GWAS contexts and @sec-ch25-feature-integration for clinical deployment.

These considerations motivate careful phenotype documentation in model development. Specifying exactly how a phenotype was defined, which codes or criteria were applied, what exclusions were made, and how temporal boundaries were established enables assessment of whether findings will generalize to settings with different definitions. The goal is not perfect phenotyping, which remains unattainable, but transparent phenotyping that allows downstream users to understand what the model actually learned and where its assumptions may break down.


## Variant Interpretation Databases and Clinical Labels {#sec-ch02-clinical}

A family receives whole-exome sequencing results for their child with developmental delay. The laboratory report lists 50 rare variants in genes associated with neurodevelopmental disorders. For each variant, the clinical team must answer: is this the cause? Allele frequencies tell us what variants survive in healthy populations, and functional genomics data reveal where the genome is biochemically active, but neither directly answers this question. That determination requires integrating multiple lines of evidence (family segregation, functional assays, computational predictions, phenotypic observations) into a structured framework that can be applied consistently.

Clinical variant interpretation databases aggregate these assessments from laboratories, expert panels, and research groups. These databases have become critical infrastructure for both clinical genomics and computational method development, providing labels that inform diagnostic decisions and serve as training data for machine learning models. Their labels carry biases and circularity that propagate through any analysis built on them, yet no viable alternative exists for large-scale model training and evaluation.


### ClinVar and Clinical Assertions {#sec-ch02-clinvar}

A clinical laboratory sequences a patient with suspected hereditary cancer syndrome and identifies a missense variant in *BRCA2*. Before returning results, the laboratory searches ClinVar and finds that three other laboratories have evaluated this variant: two classified it as likely pathogenic, one as a variant of uncertain significance. How should this conflicting evidence inform the final report? ClinVar aggregates assertions of variant pathogenicity from clinical laboratories and researchers worldwide, making it the central clearinghouse for clinical variant interpretations [@landrum_clinvar_2018].

ClinVar provides standardized classifications following ACMG/AMP guidelines (pathogenic, likely pathogenic, benign, likely benign, **variant of uncertain significance**) that are central to diagnostic pipelines and to benchmarking variant effect predictors. It has become the *de facto* reference for variant pathogenicity labels, but its contents reflect systematic biases that affect any downstream use. These biases operate at multiple levels and warrant careful consideration.

Submission heterogeneity poses a fundamental challenge. Annotations come from diverse submitters, including diagnostic laboratories, research groups, expert panels, and database exports. Submitters apply varying evidentiary standards; some provide detailed supporting evidence while others offer only assertions. Conflicting interpretations are common, particularly for variants of uncertain significance.

Classifications evolve as evidence accumulates. A variant classified as VUS in 2018 may be reclassified as likely pathogenic by 2023 based on new functional studies or additional patient observations. ClinVar releases monthly snapshots rather than maintaining formal version control, so models trained on older releases may learn outdated classifications that have since been revised. Specifying the exact ClinVar release date is essential for reproducibility.

Ancestry and gene coverage biases create uneven representation. Variants in well-studied populations (particularly European ancestry) and well-characterized disease genes are heavily overrepresented. Variants from underrepresented populations are more likely to remain classified as VUS due to insufficient evidence. This creates feedback loops: predictive models perform better on European-ancestry variants because training data is richer, reinforcing the disparity [@landrum_clinvar_2018].

Clinical assertions in ClinVar become training labels for variant effect predictors like CADD (@sec-ch04-cadd) and evaluation benchmarks for foundation model approaches (@sec-ch14-vep-fm). The role of ClinVar in ACMG-AMP variant classification workflows is detailed in @sec-ch26-acmg-amp. Calibration of computational scores to ClinVar pathogenicity assertions is examined in @sec-ch14-acmg-mapping, while systematic evaluation of ClinVar as a benchmark resource appears in @sec-ch20-clinical-databases.

Circularity with computational predictors represents a subtle but important concern. Clinical submissions increasingly incorporate computational scores like *CADD*, *REVEL*, and *AlphaMissense* as supporting evidence for pathogenicity classification. When these same ClinVar labels are then used to train or evaluate computational predictors, circularity emerges [@schubach_cadd_2024]. If a laboratory used a high *CADD* score as supporting evidence for classifying a variant as likely pathogenic, and that variant later appears as a positive label in ClinVar, models trained on ClinVar may partly learn to reproduce *CADD* itself rather than discovering independent signal. This circularity operates at two levels: evaluation circularity (when models are assessed on benchmarks influenced by the model's own predictions) and training circularity (when features used in training derive from the same underlying information as the labels). Both forms inflate apparent performance without demonstrating genuine predictive power.

Variants of uncertain significance constitute the majority of rare variant classifications, reflecting genuinely limited evidence. These variants are both targets for predictive modeling (can computational methods resolve uncertainty?) and potential pitfalls (models trained only on confidently classified variants may not generalize to VUS with different characteristics).

Despite these limitations, ClinVar remains invaluable. The key is using it appropriately: recognizing biases when training models, accounting for version differences when comparing studies, stratifying performance by ancestry and gene coverage, and treating computational predictions as one line of evidence rather than definitive classifications.

::: {#fig-clinvar-landscape layout-ncol=3}
![FIGURE PLACEHOLDER A](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20A)

![FIGURE PLACEHOLDER B](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20B)

![FIGURE PLACEHOLDER C](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20C)

[High] Three-panel figure. Panel A: Pie or bar chart showing distribution of ClinVar classifications (Pathogenic, Likely Pathogenic, VUS, Likely Benign, Benign, Conflicting). Highlight that VUS dominates. Panel B: Heatmap showing classification density by gene, with well-studied genes (BRCA1, BRCA2, CFTR) having many submissions vs sparse coverage elsewhere. Panel C: Timeline showing how classifications evolve (example of a variant reclassified from VUS to Pathogenic over time, illustrating version sensitivity).
:::


### Complementary Clinical Databases {#sec-ch02-clinical-other}

ClinVar's open-access model and broad submission base make it the most widely used resource, but it is not the only source of clinical variant interpretations. The Human Gene Mutation Database (HGMD) maintains a curated collection of disease-causing mutations compiled from the published literature, with particular depth in rare Mendelian disorders [@stenson_hgmd_2017]. HGMD's professional version includes variants not yet publicly released, and its curation emphasizes literature-reported pathogenic variants rather than the full spectrum of classifications in ClinVar. The Leiden Open Variation Database (LOVD) takes a gene-centric approach, with individual databases maintained by gene experts who curate variants according to locus-specific knowledge [@fokkema_lovd_2011]. LOVD instances often capture variants and functional evidence specific to particular disease communities that may not appear in broader databases.

These resources complement ClinVar in important ways: HGMD provides literature-derived pathogenic variants that may precede ClinVar submissions, while LOVD captures expert knowledge from disease-specific research communities. For model development and benchmarking, awareness of these alternative sources matters because training exclusively on ClinVar may miss variants documented elsewhere, and apparent novel predictions may simply reflect incomplete training data rather than genuine generalization.


### ClinGen and Expert Curation {#sec-ch02-clingen}

Clinical laboratories submitting to ClinVar vary enormously in expertise and evidentiary standards. A submission from a general diagnostic laboratory applying ACMG guidelines to an unfamiliar gene may differ substantially from an assessment by researchers who have studied that gene for decades. The Clinical Genome Resource (ClinGen) addresses this heterogeneity by providing expert-curated assessments at multiple levels [@rehm_clingen_2015].

ClinGen expert panels evaluate **gene-disease validity** (whether variation in a gene can cause a specific disease) and **dosage sensitivity** (whether haploinsufficiency or triplosensitivity leads to clinical phenotypes). These evaluations build on the catalog of Mendelian phenotypes maintained by OMIM, which provides curated gene-disease associations and clinical synopses [@amberger_omimorg_2015].

ClinGen also develops calibrated thresholds for computational predictors, specifying score intervals that justify different strengths of evidence (supporting, moderate, strong) for pathogenicity or benignity [@pejaver_calibration_2022]. The FDA has recognized these curations as valid scientific evidence for clinical validity. These calibrations directly inform how computational scores should be incorporated into variant classification workflows and are discussed further in @sec-ch14-acmg-mapping for score calibration to ACMG evidence levels and @sec-ch14-combining-evidence for integration of multiple computational predictors.


### Pharmacogenomics Resources {#sec-ch02-pharmacogenomics}

Most variant interpretation focuses on rare mutations that cause or predispose to disease. **Pharmacogenomics** presents a different paradigm: common polymorphisms that individually may have no disease consequences but profoundly influence how individuals respond to medications. These variants matter not because they cause disease but because they determine whether a drug will work, fail, or cause harm.

Implementing pharmacogenomics in clinical practice requires three capabilities: curating variant-drug associations from published literature, translating that evidence into actionable dosing guidelines, and automating the path from a patient's `VCF` file to a clinical report. PharmGKB addresses the first need, cataloging over 800 genes, 700 drugs, and thousands of variant-drug-phenotype relationships with evidence levels [@whirl-PharmGKB_2012]. CPIC translates this knowledge into standardized guidelines specifying how to adjust drug selection or dosing based on metabolizer phenotype [@relling_clinical_2019]. PharmCAT automates annotation, taking `VCF` files as input and producing CPIC-compliant reports [@sangkuhl_pharmacogenomics_2019]. ClinPGx integrates all three into a unified framework spanning variant detection through clinical recommendation [@gong_integrating_2025].

::: {.callout-note title="Star-Allele Nomenclature"}

Pharmacogenes use a specialized nomenclature where haplotypes (combinations of variants on the same chromosome) are designated by star alleles. The reference haplotype is \*1, with variant haplotypes numbered sequentially (\*2, \*3, etc.) as they were discovered. Each star allele represents a specific combination of SNVs, indels, or structural variants that travel together.

For *CYP2D6*, over 150 star alleles have been defined. Some reduce enzyme function (\*4, \*5), others increase it through gene duplication (\*1xN), and many have unknown functional consequences. A patient's **diplotype** (the combination of maternal and paternal star alleles) determines their metabolizer phenotype: poor, intermediate, normal, or ultrarapid.

Star-allele calling requires phasing to determine which variants co-occur on the same chromosome, plus structural variant detection to identify gene deletions and duplications. Standard SNV-focused pipelines miss critical information, which is why specialized tools like PharmCAT exist.

:::

The *CYP2D6* gene exemplifies the complexity. This cytochrome P450 enzyme metabolizes approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants [@nofziger_pharmvar_2019]. Patients with loss-of-function *CYP2D6* variants cannot activate codeine to morphine, rendering the drug ineffective for pain relief; patients with gene duplications may convert codeine too efficiently, experiencing dangerous opioid toxicity from standard doses. The difference between these scenarios depends entirely on accurate star-allele diplotyping.

From a modeling perspective, pharmacogenomic resources offer a complementary type of label linking variants to molecular and clinical outcomes through different mechanisms than Mendelian disease pathogenicity. Where ClinVar labels indicate whether a variant causes disease, PharmGKB labels indicate how a variant affects drug response in individuals who may be otherwise healthy.




## Inherited Constraints {#sec-ch02-constraints}

Every genomic model inherits both the power and the biases of its training data. A variant effect predictor trained on ClinVar labels absorbs the ascertainment patterns of clinical sequencing: European ancestry overrepresented, rare diseases enriched, incidental findings undersampled. A chromatin model trained on ENCODE immortalized cell lines learns regulatory patterns that may not generalize to primary tissues with different epigenetic landscapes. Models that estimate genetic constraint quantify how strongly purifying selection acts against damaging variants in each gene, comparing observed variant counts to expectations. But when trained on human population databases, these models systematically miss the most severe cases: gene-lethal variants never appear because carriers do not survive to be sequenced.

These biases compound as data flows through analysis pipelines. GWAS summary statistics carry ancestry composition forward into polygenic scores. Conservation scores calculated from biased multiple sequence alignments propagate into variant effect predictions. Foundation model pretraining on reference genomes from limited populations shapes the representations available for all downstream applications. Each transformation amplifies some biases while masking others, making the provenance of model behavior increasingly difficult to trace.

The critical question is not whether models trained on these data contain biases; they do. The question is whether those biases can be characterized, bounded, and ultimately corrected. These foundational datasets appear throughout genomic AI as training labels, evaluation benchmarks, and sometimes both simultaneously. Recognizing when the same data sources serve multiple roles is essential for interpreting model performance honestly and anticipating where generalization will fail. Part VI examines these challenges in depth: data partitioning strategies that account for shared ancestry and homology (@sec-ch21-homology-aware-splitting), population structure effects that confound genetic associations (@sec-ch22-ancestry-confounding), and ascertainment patterns that create circularity in clinical labels (@sec-ch22-label-bias).