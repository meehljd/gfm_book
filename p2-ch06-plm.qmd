::: {.callout-warning .content-visible when-profile="draft"}
**TODO:**
- ADD TAPE; ProGen
- Add figure: ESM architecture diagram showing transformer layers, attention heads, and masked token prediction
- Add figure: ESMFold pipeline diagram showing embedding extraction → structure module
- Add figure: AlphaMissense workflow showing integration of PLM embeddings with structural context
- Consider adding visualization of attention patterns capturing residue contacts
- Add table comparing PLM architectures (ESM, ProtTrans variants, ESM-2 scaling)
- Add discussion somewhere (here or VEP chapter) on marginal VEP calculations of log likelihoods
:::


# Protein Language Models  {#sec-prot}

Before transformers revolutionized genomic sequence modeling, they first transformed our ability to model proteins. The field built upon decades of computational progress, from David Baker's Rosetta framework that pioneered physics-based protein structure prediction and design through sampling and energy minimization, to the deep learning revolution of the past decade. This progression moved from early generative models like DeepSequence that captured epistatic interactions in protein families [@riesselman_deepsequence_2018], to EVE's unsupervised prediction of disease variants from evolutionary data [@frazer_eve_2021], to the ESM family's demonstration that transformers trained on massive sequence databases learn representations encoding structure and function [@rives_esm_2021]. The trajectory culminated in AlphaFold2's solution to the protein structure prediction problem [@jumper_alphafold2_2021] and AlphaMissense's proteome-wide variant pathogenicity scoring [@cheng_alphamissense_2023], establishing that self-supervised learning on biological sequences could match or exceed decades of specialized computational methods. The transformative impact of this work was recognized with the 2024 Nobel Prize in Chemistry, awarded to Demis Hassabis and John Jumper for AlphaFold2 and to David Baker for computational protein design. These advances validated and extended the central dogma's sequence → structure → function paradigm, demonstrating that deep learning models could compress the entire chain of causation into learned representations that predict functional consequences directly from amino acid sequences.

The success of protein language models (PLMs) established a paradigm that would later inspire genomic foundation models: treat biological sequences as a form of natural language, train large transformer models on massive unlabeled sequence databases, and extract functional knowledge through self-supervised learning. The analogy between protein sequences and natural language runs deeper than mere metaphor. Both encode complex information in linear strings of discrete tokens, whether amino acids or words. Both exhibit hierarchical structure, with motifs combining into domains as words combine into phrases. Both have syntax in the form of structural constraints and semantics in the form of functional meaning. And crucially, both are shaped by evolutionary pressure: natural selection filters protein sequences just as cultural selection shapes language.

This chapter examines how protein language models pioneered biological foundation modeling, from the ESM family's demonstration that transformers can learn protein structure and function from sequence alone, to their application in variant effect prediction and structure determination. Understanding PLMs provides essential context for the genomic language models covered in subsequent chapters, as many architectural choices and training strategies transfer directly from proteins to DNA.

## The ESM Model Family

### ESM-1b: Establishing the Paradigm

The Evolutionary Scale Modeling (ESM) project, developed at Meta AI Research, demonstrated that transformer language models trained on protein sequences learn biologically meaningful representations without explicit supervision [@rives_esm_2021]. The key insight was that masked language modeling, the same objective that powers BERT in natural language processing, could be applied directly to amino acid sequences.

ESM-1b was trained on UniRef50, a clustered database of approximately 33 million protein sequences covering the known diversity of protein families. UniRef50 clusters sequences at 50% identity, providing broad coverage while reducing redundancy [@suzek_uniref_2007]. This curation strategy ensures the model sees diverse evolutionary solutions to protein function rather than memorizing overrepresented families.

The architecture follows the BERT-style bidirectional transformer design with 650 million parameters distributed across 33 layers, a hidden dimension of 1,280, and 20 attention heads. The maximum sequence length of 1,024 amino acids accommodates most individual protein domains and many complete proteins. The training objective is masked language modeling: the model learns to predict randomly masked amino acids given surrounding context. This is analogous to BERT's masked token prediction, but operates on amino acids rather than words.

### Emergent Biological Knowledge

Despite never seeing structural or functional labels during training, ESM learns representations that capture fundamental biological properties. This emergent knowledge manifests across multiple levels of protein organization.

At the level of secondary structure, attention patterns in ESM correlate with alpha helices and beta sheets. The model implicitly learns that certain amino acid patterns form specific structural elements, encoding this knowledge in its internal representations without any explicit supervision on structure labels.

ESM's attention heads also capture residue-residue contacts, identifying amino acids that are distant in sequence but close in three-dimensional space. This emergent capability suggests the model learns aspects of protein folding from sequence statistics alone. When researchers analyzed which sequence positions attend to each other in trained ESM models, they found strong correspondence with experimentally determined contact maps.

The model's masked token predictions correlate with position-specific conservation scores from multiple sequence alignments. ESM effectively learns which positions tolerate variation and which are evolutionarily constrained, extracting this information from the statistical patterns in sequence databases rather than from explicit conservation annotations.

Attention also concentrates on catalytic residues, binding sites, and other functionally important positions, even without explicit functional annotation in the training data. The model discovers that certain sequence positions are more informative about surrounding context, and these positions frequently correspond to sites of biological importance.

### ESM-2: Scaling Up

ESM-2 extended the ESM approach with larger models and improved training [@lin_esm-2_2022]. The model family spans several orders of magnitude in scale, from 8 million to 15 billion parameters, enabling systematic study of how biological knowledge scales with model capacity.

| Model | Parameters | Layers | Contact Prediction Performance |
|-------|------------|--------|-------------------------------|
| ESM-2 (8M) | 8M | 6 | Baseline |
| ESM-2 (35M) | 35M | 12 | +5% |
| ESM-2 (150M) | 150M | 30 | +8% |
| ESM-2 (650M) | 650M | 33 | +12% |
| ESM-2 (3B) | 3B | 36 | +15% |
| ESM-2 (15B) | 15B | 48 | State-of-the-art |

::: {.callout-warning .content-visible when-profile="draft"}
Validate contact prediction performance values
:::

Performance scales smoothly with model size across structure prediction, contact prediction, and variant effect tasks. This phenomenon mirrors the scaling laws observed in natural language processing, where larger models consistently capture more nuanced patterns and achieve better downstream performance. The predictable scaling relationship suggests that continued investment in model size yields reliable returns in biological accuracy.

## Alternative Architectures: The ProtTrans Family

The ProtTrans family explored multiple transformer architectures for protein sequences, demonstrating that the protein language modeling paradigm generalizes beyond the specific design choices of ESM [@elnaggar_prottrans_2021].

ProtBERT applies the BERT-style bidirectional encoder to protein sequences, trained on the Big Fantastic Database (BFD) comprising approximately 2.1 billion protein sequences [@devlin_bert_2019; @jumper_alphafold2_2021]. This massive training corpus, substantially larger than UniRef50, provides even broader coverage of protein sequence space.

ProtT5 adapts the encoder-decoder architecture from T5, enabling both understanding and generation tasks [@raffel_t5_2019]. The encoder processes input sequences to produce contextual representations, while the decoder can generate output sequences conditioned on those representations. This architecture is particularly valuable for tasks that require sequence generation, such as protein design or sequence completion.

ProtXLNet explores permutation language modeling based on XLNet, capturing bidirectional context without the artificial [MASK] token that BERT-style models require during training [@yang_xlnet_2020]. By training on all possible token orderings, XLNet-style models learn to predict each token from any subset of context tokens, potentially capturing richer dependencies.

These architectural variants demonstrate that the protein language modeling paradigm generalizes across architectures. The choice between encoder-only (BERT-style) and encoder-decoder (T5-style) models depends on the downstream application: encoders excel at classification and embedding tasks, while encoder-decoders enable sequence generation.

## Zero-Shot Variant Effect Prediction

A critical application of protein language models is predicting the effects of amino acid substitutions. Missense variants are the most common type of protein-coding mutation, and clinical genetics pipelines must routinely assess whether specific substitutions are likely to be pathogenic or benign. Traditionally, this required either direct experimental characterization or computational methods trained on labeled pathogenicity data.

### The Zero-Shot Paradigm

ESM-1v demonstrated that PLMs can predict variant effects without any training on variant labels [@meier_esm-1v_2021]. The approach exploits the masked language modeling objective: for a variant at position $i$ changing amino acid $a$ to amino acid $b$, compute the log-likelihood ratio:

$$\Delta \text{score} = \log P(b \mid \text{context}) - \log P(a \mid \text{context})$$

If the model assigns higher probability to the mutant amino acid than the wild-type, the variant is predicted benign; if lower, deleterious. This zero-shot prediction requires no labeled training data. The model's evolutionary knowledge, learned from sequence databases, directly informs variant interpretation.

The intuition is straightforward. If evolution has shaped protein sequences such that certain positions strongly prefer certain amino acids, substitutions that violate these preferences are likely to disrupt function. The language model captures these preferences through its training on millions of evolutionarily successful sequences. Variants that the model finds surprising, in the sense of assigning low probability, are more likely to be functionally disruptive.

### Genome-Wide Application

Brandes and colleagues applied ESM-1b to predict effects for all approximately 450 million possible missense variants in the human genome [@brandes_genome-wide_2023]. This comprehensive annotation covers every position in every human protein multiplied by every possible amino acid substitution, providing precomputed effect scores that can be queried for any missense variant without running the model.

On ClinVar, the database of clinically annotated variants, ESM-1b outperformed existing methods in classifying approximately 150,000 missense variants as pathogenic or benign. The model achieved strong correlation with experimental measurements across 28 deep mutational scanning datasets, demonstrating that PLM predictions capture genuine functional information rather than merely correlating with annotation artifacts.

The analysis also identified approximately 2 million variants annotated as damaging only in specific protein isoforms, highlighting the importance of considering alternative splicing when interpreting variant effects. A variant that disrupts function in one isoform may have no effect if that isoform is not expressed in relevant tissues, underscoring the need to integrate PLM predictions with expression context.

### The ProteinGym Benchmark

ProteinGym provides a comprehensive benchmark for variant effect predictors, aggregating 217 deep mutational scanning assays covering diverse proteins [@notin_proteingym_2023]. Deep mutational scanning experiments systematically measure the functional effects of thousands of variants in a protein, providing ground truth for computational method evaluation.

| Method | Mean Spearman ρ | Information Sources |
|--------|-----------------|---------------------|
| ESM-1v | 0.48 | Single sequence (PLM) |
| EVE (evolutionary model) | 0.46 | MSA (generative model) |
| DeepSequence | 0.44 | MSA (VAE) |
| PolyPhen-2 | 0.32 | Conservation + structure |
| SIFT | 0.30 | Conservation |

: Performance of protein language models and traditional methods on the ProteinGym deep mutational scanning benchmark [@notin_proteingym_2023]. Shown are mean Spearman correlations between predicted and experimentally measured variant effects across 217 assays [@meier_esm-1v_2021; @frazer_eve_2021; @riesselman_deepsequence_2018; @ng_sift_2003; @adzhubei_polyphen_2010].


PLMs achieve competitive or superior performance to methods that explicitly model evolutionary conservation from multiple sequence alignments, despite using only single sequences as input. This suggests that transformer attention over large sequence databases captures similar information to traditional alignment-based approaches, but in a form that generalizes more readily to novel sequence contexts.

## ESMFold: Structure from Sequence

### Eliminating the Alignment Bottleneck

The most dramatic demonstration of PLM capabilities came with ESMFold, which predicts protein 3D structure directly from ESM-2 embeddings [@lin_esm-2_2022]. Traditional structure prediction, including AlphaFold2, relies heavily on multiple sequence alignments (MSAs). These computationally expensive searches against sequence databases can take hours per protein, and the quality of predictions depends critically on finding informative homologs.

ESMFold eliminates this requirement entirely. The architecture couples ESM-2 (15 billion parameters) with a structure module adapted from AlphaFold2. The language model embeddings replace MSA-derived features, providing the evolutionary context that the structure module needs to predict atomic coordinates.

The computational speedup is substantial: approximately 60-fold faster than AlphaFold2 for typical proteins, enabling metagenomic-scale structure prediction. This speed advantage makes it feasible to predict structures for the millions of protein sequences emerging from environmental sequencing projects, where computing MSAs would be prohibitively expensive.

ESMFold achieves atomic-level accuracy for many proteins, though slightly below AlphaFold2 for proteins that benefit from MSA information. The accuracy gap is largest for proteins with sparse evolutionary sampling, where MSAs provide information that single-sequence analysis cannot recover. For well-represented protein families, ESMFold approaches AlphaFold2 accuracy at a fraction of the computational cost.

### What ESMFold Reveals About PLMs

ESMFold's success demonstrates that ESM-2's internal representations encode sufficient information to determine 3D structure. The language model has learned not just local sequence patterns but global folding principles, capturing what makes a sequence fold into a particular shape.

This has profound implications for understanding what PLMs learn. The attention that transformers pay to distant sequence positions during masked prediction is, in some sense, learning the physics of protein folding. Residues that need to be close in 3D space attend to each other in the transformer's attention matrices. The statistical patterns in protein sequences, shaped by billions of years of evolution and the physical constraints of protein folding, encode structural information that sufficiently powerful language models can decode.

## Integration into Variant Interpretation Pipelines

### CADD v1.7: PLM Features for Ensemble Methods

The Combined Annotation Dependent Depletion (CADD) framework integrates diverse annotations to score variant deleteriousness (@sec-cadd). CADD v1.7 incorporated ESM-1v predictions as features within its existing integrative architecture [@schubach_cadd_2024].

The integration approach treats PLM scores as additional annotations alongside conservation scores, functional annotations, and regulatory predictions. For each missense variant, ESM-1v scores are computed and included as features in CADD's gradient-boosted tree classifier. This allows the ensemble to learn how PLM predictions complement other evidence sources, potentially capturing cases where PLM and conservation signals provide independent information.

Performance gains from PLM integration are consistent across benchmarks. On ClinVar pathogenic versus common variant classification, CADD v1.7 improves from 0.94 to 0.95 AUROC. On deep mutational scanning datasets (31 assays), performance improves from 0.78 to 0.81 Spearman correlation. The PLM features particularly improve scoring for variants in regions with limited evolutionary conservation data, where traditional methods struggle but language models can still extract contextual information.

### AlphaMissense: Combining PLM and Structure

AlphaMissense represents the current state-of-the-art in missense variant effect prediction, combining PLM representations with structural context [@cheng_alphamissense_2023]. Rather than treating PLMs as a feature source for an external classifier, AlphaMissense adapts AlphaFold's architecture directly for pathogenicity prediction.

The model learns to predict pathogenicity by combining three information sources. Sequence embeddings from ESM-style language modeling provide evolutionary context about amino acid preferences at each position. Structural context from predicted protein structures captures whether a position is buried or exposed, in a secondary structure element or loop, near active sites or binding interfaces. Evolutionary information from cross-species comparisons supplements the single-sequence PLM signal with explicit alignment-derived conservation.

The training data comes from population frequency databases, primarily gnomAD [@gnomAD]. Common variants, those observed frequently in healthy populations, provide weak labels for benign effects. Variants absent from large population databases, particularly those in constrained positions, provide weak labels for deleterious effects. Critically, AlphaMissense never trains on clinical pathogenicity labels from ClinVar, yet achieves state-of-the-art performance on clinical benchmarks. This demonstrates that the combination of PLM representations, structural context, and population genetics signals captures genuine functional information rather than memorizing clinical annotations.

AlphaMissense provides predictions for all approximately 71 million possible single amino acid substitutions across the human proteome. Of these, 89% are classified as either likely benign or likely pathogenic with sufficient confidence to be actionable, providing interpretable predictions for the vast majority of possible missense variants.

| Method | ClinVar AUC | DMS Correlation | Information Sources |
|--------|-------------|-----------------|---------------------|
| SIFT | 0.78 | 0.30 | Conservation |
| PolyPhen-2 | 0.82 | 0.32 | Conservation + structure |
| CADD v1.7 | 0.95 | 0.81 | Multi-feature integration |
| ESM-1v | 0.89 | 0.48 | Sequence only (zero-shot) |
| AlphaMissense | 0.94 | 0.52 | PLM + structure + population |

: Comparative performance of missense variant effect predictors on clinical (ClinVar) and experimental (deep mutational scanning) benchmarks [@ng_sift_2003; @adzhubei_polyphen_2010; @schubach_cadd_2024; @meier_esm-1v_2021; @cheng_alphamissense_2023].

AlphaMissense achieves top performance by integrating the strengths of multiple approaches: PLM-derived sequence understanding, AlphaFold-derived structural context, and population genetics-derived evolutionary constraint signals.

## Lessons for Genomic Foundation Models

The success of protein language models established several principles that inform genomic foundation modeling. These lessons transfer, with appropriate modifications, to the DNA language models covered in subsequent chapters.

### Self-Supervision Works

PLMs demonstrated that massive amounts of biological knowledge can be learned from unlabeled sequences. The same evolutionary pressures that shape proteins also shape DNA. Purifying selection removes deleterious variants, leaving statistical signatures in sequence databases that self-supervised models can learn to exploit. This principle underlies the entire foundation model paradigm: if sufficiently large models are trained on sufficiently large datasets with appropriate self-supervised objectives, they will learn representations that capture biological function.

### Scale Matters

Performance improves predictably with model size, motivating the development of larger genomic models. The progression from 8 million to 15 billion parameters in ESM-2 showed consistent gains across structure prediction, contact prediction, and variant effect tasks. While the relationship between scale and performance is not linear indefinitely, current models remain in a regime where additional capacity yields reliable improvements. This scaling relationship justifies the substantial computational investment required to train genomic foundation models.

### Transfer Learning is Effective

Representations learned for one task (masked token prediction) transfer to other tasks (structure prediction, variant effects). This suggests that self-supervised pretraining captures fundamental biological knowledge rather than task-specific shortcuts. A model trained to predict masked amino acids is simultaneously learning about protein structure, function, evolutionary constraint, and disease relevance, even though none of these properties appear in the training objective. The same principle applies to genomic sequences: models trained to predict masked nucleotides may simultaneously learn about regulatory elements, evolutionary conservation, and variant effects.

### Architecture Choices Matter

The BERT-style bidirectional encoder proved highly effective for proteins, where the entire sequence context is typically available. However, genomic sequences present different challenges: much longer lengths spanning kilobases to megabases, different information density with proteins being information-dense while intergenic regions are less so, and different symmetries including the reverse-complement structure absent in proteins. These differences motivate architectural adaptations in genomic language models, including hybrid architectures that combine convolutional and attention mechanisms, longer context windows, and specialized tokenization schemes.

### Integration with Other Modalities

AlphaMissense showed that PLM embeddings combine effectively with structural information. Similarly, genomic models benefit from integration with epigenomic data, gene annotations, and other biological context. The most powerful variant effect predictors combine multiple information sources, using PLMs as one component of larger systems. This principle extends to genomic foundation models, where sequence-based representations complement rather than replace other genomic annotations.

## Limitations and Ongoing Challenges

Despite their success, protein language models face several limitations that inform the development of genomic models.

### Sequence Length Constraints

Most PLMs handle sequences up to 1,000 to 2,000 amino acids. While sufficient for most individual protein domains, this limits modeling of large protein complexes and does not directly transfer to the much longer sequences in genomics. Genomic language models must handle sequences spanning millions of bases, requiring architectural innovations beyond simple scaling of transformer attention.

### Orphan Proteins

PLMs struggle with proteins that have few homologs in training databases. Orphan or dark proteins, those unique to specific lineages, lack the evolutionary signal that PLMs exploit. For these proteins, the statistical patterns learned from diverse sequence families provide less informative context. This limitation is less severe for genomic models trained on reference genomes, where even unique sequences exist in the context of conserved flanking regions.

### Epistasis

Most variant effect predictions assume independence: the effect of mutation A does not depend on whether mutation B is present. Real proteins exhibit epistasis, where variant effects depend on sequence context. Two individually benign variants may be jointly deleterious if they disrupt compensatory interactions. Current PLM-based predictors do not explicitly model these interaction effects, though the contextual embeddings may capture some epistatic relationships implicitly.

### Interpretability

While attention patterns correlate with biological features, understanding exactly what PLMs learn remains challenging. The field is developing interpretation methods (@sec-interp), but PLMs remain partially opaque. For clinical applications where explanations are valued, this interpretability gap limits adoption. Future work must balance the accuracy gains from complex models against the transparency required for clinical decision-making.

## Beyond Language Models: Structure Prediction and Design

While protein language models demonstrate the power of self-supervised learning on sequences alone, the broader protein modeling landscape encompasses methods that explicitly incorporate structural information, evolutionary constraints, and physical principles. These approaches complement PLMs by addressing tasks where three-dimensional geometry, binding interactions, or design objectives are central.

### Structure Prediction Systems

AlphaFold2 revolutionized protein structure prediction by combining learned representations with explicit modeling of protein geometry [@jumper_alphafold2_2021]. Unlike pure sequence models, AlphaFold2 processes both sequence information through embeddings and structural information through an iterative refinement process that directly predicts atomic coordinates. The system requires multiple sequence alignments as input, using evolutionary information to infer residue-residue contacts and structural constraints. AlphaFold2 searches for homologous sequences using computationally expensive tools including HHblits against the BFD and Jackhmmer against other databases, a process that can take hours per protein.

AlphaFold3 extends this framework to model protein complexes, nucleic acids, and small molecules [@abramson_alphafold3_2024]. The architecture incorporates diffusion-based structure generation, allowing it to predict not only protein structures but also their interactions with other biomolecules. This expansion from single proteins to molecular complexes represents a shift toward modeling entire biological systems rather than isolated components.

ColabFold democratized access to AlphaFold-quality predictions by replacing the slow MSA search with MMseqs2, reducing search time from hours to minutes while maintaining prediction accuracy [@mirdita_colabfold_2022]. The system uses precomputed databases and optimized search algorithms to make structure prediction accessible through free cloud computing resources. ColabFold's efficiency enabled large-scale structural proteomics, with researchers generating predictions for entire proteomes in practical time frames.

OpenFold provides an open-source reimplementation of AlphaFold2, enabling researchers to modify and extend the architecture for specialized applications [@ahdritz_openfold_2024]. By making the full training and inference pipeline accessible, OpenFold supports development of domain-specific variants optimized for particular protein families or prediction tasks.

RosettaFold emerged as an alternative to AlphaFold2, demonstrating that similar accuracy could be achieved through different architectural choices [@baek_rosettafold_2021]. The three-track neural network architecture processes sequence, distance, and coordinate information simultaneously, with each track informing the others through carefully designed information exchange. RosettaFold's modular design facilitated subsequent extensions including RoseTTAFold2 for protein-protein interactions and RoseTTAFoldNA for nucleic acid modeling.

ESMFold, discussed earlier in this chapter, represents a distinct approach by eliminating MSA requirements entirely [@lin_esm-2_2022]. The model achieves AlphaFold2-level accuracy for many proteins while being orders of magnitude faster, enabling structural annotation of metagenomic sequences where traditional MSA construction fails due to lack of homologs.

### Generative Design Methods

ProteinMPNN applies message-passing neural networks to the inverse folding problem: designing sequences that fold into specified backbone structures [@dauparas_proteinmpnn_2022]. Given a target structure, the model learns to generate amino acid sequences likely to adopt that fold. ProteinMPNN's success at designing stable, functional proteins demonstrates that neural networks can capture the sequence-structure relationship in both directions, complementing the structure prediction capabilities of AlphaFold and ESM.

RFDiffusion extends diffusion models to protein backbone generation, enabling de novo design of proteins with specified functions [@watson_rfdiffusion_2023]. Rather than predicting structure from sequence or sequence from structure, RFDiffusion generates entirely novel protein backbones conditioned on design objectives such as binding a target molecule or forming particular geometric shapes. The method has been used to design proteins with enzymatic activity, binding specificity, and novel folds not found in nature.

Boltz-1 introduced a unified framework for predicting structures of protein-ligand, protein-protein, and protein-nucleic acid complexes using a diffusion-based approach [@wohlwend_boltz1_2025]. Boltz-2 builds on this foundation with improved accuracy and broader applicability across different types of biomolecular interactions. These methods address the critical challenge of predicting not just individual protein structures but how proteins interact with other molecules in cellular contexts.

### Molecular Docking and Binding

DiffDock applies diffusion models to molecular docking, predicting how small molecules bind to protein targets [@corso_diffdock_2022]. Traditional docking methods rely on physics-based scoring functions and extensive sampling, often requiring hours of computation per ligand-protein pair. DiffDock learns to generate binding poses directly, achieving comparable or better accuracy in a fraction of the time. This capability is particularly valuable for drug discovery, where thousands of potential compounds must be evaluated against protein targets.

### Infrastructure and Search Methods

The MSA construction pipeline underlying AlphaFold2 represents substantial engineering beyond the neural network architecture itself. HHblits performs iterative profile-profile searches to identify remote homologs, building deep alignments that capture evolutionary constraints [@remmert_hhblits_2012]. Jackhmmer provides complementary sensitivity using hidden Markov model searches [@finn_hmmer_2011]. These tools process multiple sequence databases including UniRef, BFD, and MGnify, each optimized for different coverage-redundancy tradeoffs.

MMseqs2 revolutionized sequence search by achieving BLAST-level sensitivity at hundreds of times the speed through careful algorithmic optimization and parallelization [@steinegger_mmseqs2_2017]. ColabFold's adoption of MMseqs2 for MSA construction demonstrated that the computational bottleneck in structure prediction lay not in the neural network but in the database search, motivating the development of faster search algorithms as a critical infrastructure problem.

### Integration with Protein Language Models

These structure-focused methods increasingly incorporate PLM representations as complementary information sources. AlphaFold3 integrates sequence embeddings from language models with its structure prediction network. RFDiffusion can condition generation on ESM embeddings to guide designs toward particular sequence properties. ProteinMPNN benefits from PLM-derived features when designing sequences for challenging structural targets. This trend toward hybrid architectures suggests that the future of protein modeling lies not in choosing between sequence models and structure models, but in intelligently combining their complementary strengths.

The protein modeling ecosystem thus spans a continuum from pure sequence models like ESM that never explicitly represent structure, through hybrid systems like AlphaFold that combine learned sequence representations with geometric constraints, to physics-based methods that emphasize structural principles. Each approach offers distinct advantages: PLMs provide fast, broadly applicable predictions without MSA construction; structure prediction systems achieve atomic-level accuracy when sufficient evolutionary data exists; generative methods enable design of novel proteins with specified functions. Understanding this landscape helps position genomic foundation models, which face analogous tradeoffs between sequence-only and structure-aware approaches.

## Significance

Protein language models established that transformer architectures can learn deep biological knowledge from sequence data alone. ESM's ability to predict structure, function, and variant effects without explicit labels demonstrated the power of self-supervised learning on evolutionary data. This success directly motivated the development of genomic language models. If proteins constitute a language that transformers can learn, perhaps DNA does too.

The genomic language models covered in @sec-dna adapt PLM architectures and training strategies to the distinct challenges of DNA sequences: longer contexts, different alphabets, and the full complexity of gene regulation. The integration path continues as well: just as CADD v1.7 and AlphaMissense incorporate PLM predictions, future models will integrate genomic and proteomic language models into unified frameworks for variant interpretation (@sec-vep) and multi-omic modeling (@sec-systems).