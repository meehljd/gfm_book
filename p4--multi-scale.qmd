# Part IV: Multi-Scale Modeling {.unnumbered}

The preceding parts traced genomic deep learning from its origins in sequence-to-function CNNs through transformer-based language models and into the foundation model paradigm. Yet biology operates across scales that sequence alone cannot capture. Cells of different types read the same genome differently. Genes function not in isolation but within networks of regulation and interaction. The three-dimensional folding of chromatin brings distal elements into contact, creating regulatory logic invisible to one-dimensional sequence models. This part examines how foundation model principles extend beyond sequence to embrace these multi-scale aspects of genome biology.

The transition from sequence-centric to multi-scale modeling reflects a deeper shift in how we conceptualize genomic computation. Early deep learning models asked what a sequence *encodes*; the models in this part ask what that sequence *becomes* in particular cellular contexts, interaction networks, and organizational architectures. This shift demands new data modalities, from single-cell transcriptomes that reveal cellular heterogeneity to Hi-C contact maps that expose spatial genome organization, and new computational approaches that can learn meaningful representations across these modalities.

Three chapters develop this theme. [Chapter @sec-epi] examines foundation models for single-cell transcriptomics, epigenomics, and three-dimensional genome structure, showing how transformer architectures adapt to the unique characteristics of these data types. [Chapter @sec-networks] turns to graph neural networks and network-based approaches that represent genes, proteins, and their interactions as structured graphs rather than flat sequences. [Chapter @sec-systems] broadens the view further to multi-omics integration and systems-level reasoning, exploring how models can jointly represent genomic, transcriptomic, proteomic, and clinical information.

Together, these chapters address a central challenge: connecting sequence variation to phenotype requires traversing multiple layers of biological organization, from DNA through RNA and protein to cellular state and tissue function. The models surveyed here represent early attempts to bridge these layers computationally, learning representations that capture not just what the genome says but what it does in living systems.