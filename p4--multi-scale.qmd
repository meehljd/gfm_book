# Part IV: Systems and Scale {.unnumbered}

Each architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information; they learn to recognize transcription factor binding sites, splice signals, and chromatin accessibility patterns from the sequence grammar immediately surrounding each position. Protein language models treat amino acid sequences as structured compositions whose meaning emerges from evolutionary context; they learn what substitutions are tolerated by observing which sequences survived natural selection. DNA language models extend this paradigm to nucleotides, learning regulatory grammar through self-supervised objectives that predict masked or next tokens. Hybrid architectures attempt to reconcile local and global perspectives, using convolutions to extract features efficiently while deploying attention to model interactions spanning tens or hundreds of kilobases. Understanding these assumptions clarifies what each model family can capture and where each will fail.

This part surveys the major foundation model families in genomic deep learning. @sec-fm-principles establishes what defines a foundation model and develops a practical taxonomy for navigating the rapidly expanding ecosystem. @sec-dna-lm examines DNA language models (DNABERT, Nucleotide Transformer, HyenaDNA) that apply self-supervised pretraining to genomic sequence, learning representations that transfer across diverse downstream tasks. @sec-protein-lm turns to protein language models, where the foundation model paradigm achieved its earliest and most dramatic successes; ESM, ProtTrans, and their descendants learn rich representations of structure and function that enabled AlphaFold's revolution in structure prediction and AlphaMissense's proteome-wide variant scoring.

@sec-regulatory examines hybrid architectures (Enformer, Borzoi, AlphaGenome) that combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases, enabling direct prediction of gene expression from sequence. @sec-vep-fm synthesizes these approaches in the context of variant effect prediction, showing how foundation model representations translate into pathogenicity scores across variant types and genomic contexts. By the end of this part, readers will understand not only how each model family works but when to deploy each and what limitations to anticipate.