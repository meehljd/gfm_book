::: {.callout-warning .content-visible when-profile="draft"}
RNNs deserve coverage but probably not a dedicated chapter. They're pedagogically valuable for explaining why attention was such a breakthrough, but they're not central to modern genomic foundation models.

Recommended approach: Integrate RNN coverage into Chapter 7 (Attention) as a "what came before" section that motivates attention mechanisms. This aligns with your "lead with why" structure: sequential processing seemed natural for sequences → vanishing gradients and LSTM/GRU partial solutions → fundamental parallelization bottleneck → attention resolves the tension.
Key points worth covering:

Why recurrence seemed intuitive for sequences (hidden state carries context)
Vanishing/exploding gradient problem, LSTM gates as partial fix
Sequential dependency prevents parallelization (O(n) vs O(1) for attention)
Some genomic applications existed (DanQ combined CNN+LSTM for regulatory prediction, ~2016)
Why 100kb+ contexts made recurrence computationally intractable

Alternative: Brief treatment in Part 2 introduction (3-4 paragraphs) covering the CNN→RNN→Transformer arc, leaving Chapter 7 to focus purely on attention mechanics.
The DanQ model [@quang_danq_2016] is probably the most cited genomic RNN application and worth a paragraph if you go this route. But full chapter coverage would overweight an approach that's essentially historical at this point.
:::



# Transformers and Attention {#sec-attention}

Where convolutional networks ask "what local pattern exists here," attention asks a different question: "what distant information matters here?" This reformulation changed what genomic models could learn. A convolutional filter scanning across a sequence detects motifs, chromatin marks, and regulatory grammar with high fidelity, but it remains blind to dependencies beyond its receptive field. Attention computes direct interactions between all positions simultaneously, allowing a nucleotide near a gene promoter to attend to an enhancer 100 kilobases away without information passing through intermediate layers. The shift is not merely architectural; it reflects a different assumption about how sequence encodes function. Local patterns matter, but so do long-range relationships that convolutions cannot capture.

The attention mechanism, introduced for machine translation by Vaswani et al. [@vaswani_attention_2017], resolved a tension that had constrained sequence modeling for years. Recurrent networks could maintain context across arbitrary distances but processed sequences one position at a time, creating training bottlenecks that limited practical sequence length. Convolutional networks processed sequences in parallel but could only integrate information within fixed receptive fields. Attention achieves both: parallel computation across all positions with direct modeling of arbitrary-range dependencies. For genomic sequences, where enhancer-promoter interactions span tens of kilobases and topologically associating domains organize contacts across megabases, this capacity for long-range modeling proved transformative.

This chapter examines the attention mechanism and transformer architecture from the perspective of genomic applications. The treatment emphasizes how attention enables capabilities that convolutions cannot achieve: modeling regulatory interactions across distances that exceed CNN receptive fields, learning position-dependent patterns through attention heads that specialize for different relationship types, and scaling to the context lengths that genomic applications require. The goal is not comprehensive coverage of transformer variants (which would fill its own textbook) but targeted understanding of the components most relevant to genomic foundation models. Understanding how attention works, what biological patterns attention heads learn to detect, and where attention mechanisms still struggle establishes the foundation for the models examined throughout the remainder of this book.


## The Self-Attention Mechanism

A 28-year-old woman presents with dilated cardiomyopathy and a variant of uncertain significance in the *LMNA* gene's promoter region. Her clinician needs to determine whether this variant disrupts regulatory elements that control *LMNA* expression in cardiac tissue. The relevant information spans thousands of base pairs: transcription factor binding sites flanking the variant, enhancers that drive cardiac-specific expression, and insulators that constrain regulatory interactions. A model that can only aggregate local context (through convolutional or recurrent operations) must pass information through many intermediate layers, each adding noise and limiting what survives the journey. When this information pathway fails, the variant appears as noise rather than the pathogenic regulatory disruption it may represent. The fundamental question is how to let any position in a sequence directly access information from any other position, regardless of distance.

::: {#fig-self-attention}
![**FIGURE PLACEHOLDER**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER)

[Essential] Step-by-step visualization of self-attention on a short regulatory sequence. Panel A: Input embeddings for ~10 positions. Panel B: Query, Key, Value projections (show W^Q, W^K, W^V matrices). Panel C: Attention score matrix (query-key dot products, pre-softmax). Panel D: Attention weight matrix (post-softmax, showing which positions attend to which). Panel E: Weighted sum of values producing output. Annotate the key equation at each step.
:::

Self-attention answers this question by computing all pairwise interactions simultaneously, allowing the model to directly relate any position to any other regardless of distance. Where convolutions apply fixed filters uniformly across the sequence, attention performs dynamic routing: each position queries the entire sequence and aggregates information based on content-dependent relevance scores. The routing changes for every input because attention weights depend on what the sequence contains, not just where positions sit relative to each other. For the *LMNA* variant, this means the model can directly assess whether the variant position interacts with known cardiac enhancers without that signal degrading through layer after layer of local aggregation.

### Query, Key, and Value Vectors

At each position in the input sequence, self-attention computes three vectors: a **query**, a **key**, and a **value**. These vectors emerge from multiplying the input embedding at that position by three learned weight matrices $W^Q$, $W^K$, and $W^V$. The query represents what information this position seeks from other positions. The key represents what information this position offers to queries from elsewhere. The value represents the actual information this position contributes when attended to. This query-key-value structure separates the question of "which positions should interact" (determined by query-key similarity) from "what information flows between them" (determined by values).

The attention mechanism computes similarity scores between each query and all keys. For position $i$, we compute the dot product between its query $q_i$ and every key $k_j$ across all positions $j = 1, \ldots, L$, where $L$ is sequence length. These scores are scaled by $\sqrt{d_k}$ (the square root of the key dimension) to prevent the dot products from growing large in high dimensions, which would push softmax outputs toward extreme values and create vanishing gradients:

$$
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
$$

A softmax function converts these scores into attention weights $\alpha_{ij}$ that form a probability distribution over positions:

$$
\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_{j'=1}^L \exp(\text{score}(q_i, k_{j'}))}
$$

These weights determine how strongly position $i$ attends to each other position. High weight means position $i$ aggregates substantial information from position $j$; low weight means position $j$ contributes little to the output at position $i$. The final output at position $i$ is a weighted sum of all value vectors:

$$
\text{output}_i = \sum_{j=1}^L \alpha_{ij} v_j
$$

This weighted aggregation forms the core of self-attention. Each output position receives a mixture of information from across the entire sequence, with mixture proportions learned through backpropagation. For genomic sequences, this means a position near a splice site can attend to both the upstream exon and downstream intron, integrating context that determines whether splicing occurs. A position in a promoter can attend to distant enhancers, learning which distal elements influence expression at this gene. When predicting the pathogenicity of a variant in the *SCN5A* promoter (mutations in which cause Brugada syndrome and long QT syndrome, affecting approximately 1 in 2,000 individuals), the model can simultaneously consider the core promoter elements, upstream enhancers that drive cardiac-specific expression, and downstream regulatory regions that modulate expression levels.

[FIGURE RECOMMENDATION: Attention weight visualization showing a genomic sequence with attention patterns. Panel A: heatmap of attention weights between all position pairs, highlighting strong attention between a promoter region and a distal enhancer. Panel B: the same attention pattern overlaid on a linear genome diagram showing the biological interpretation of learned attention. Emphasize how attention learns to connect biologically related positions without explicit supervision.]

### Multi-Head Attention

::: {#fig-multihead-attention}
![**FIGURE PLACEHOLDER**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER)

[High] Panel showing 4-6 attention heads from a trained genomic transformer, each displaying different learned patterns. Head 1: Local attention (attending to nearby positions). Head 2: Periodic attention (nucleosome spacing ~200bp). Head 3: Motif-specific attention (attending to CTCF sites). Head 4: Long-range attention (enhancer-promoter). Annotate the biological interpretation of each pattern.
:::


A patient presenting with a complex arrhythmia may carry variants affecting both a cardiac ion channel's coding sequence and its distal enhancer. Understanding this case requires the model to simultaneously track local splice site context around the coding variant and enhancer-promoter relationships spanning 50 kilobases. A single attention operation cannot capture both patterns effectively: when forced to learn one pattern of position interactions, the model faces an impossible choice between attending strongly to nearby positions for local regulatory context or attending to distant positions for enhancer-gene relationships. Genomic sequences exhibit multiple types of dependencies simultaneously, and forcing all these interaction types through a single attention pattern creates destructive competition.

**Multi-head attention** extends the basic mechanism by running multiple attention operations in parallel, each with independent learned projections [@vaswani_attention_2017]. If we use $H$ heads, we split the model dimension $d$ into $H$ subspaces of dimension $d/H$, compute separate queries, keys, and values for each head, run attention independently, concatenate outputs, and project back to dimension $d$. Different heads can specialize in different interaction types without competing for attention capacity.

In genomic models, one head might attend to nearby positions (capturing local motif context) while another attends to positions at characteristic distances (capturing nucleosome spacing or enhancer-promoter loops). Empirical analysis of trained genomic transformers reveals diverse attention patterns: some heads attend locally regardless of content, others attend to specific sequence motifs like TATA boxes or CTCF binding sites, and still others show distance-dependent patterns suggestive of chromatin organization [@avsec_effective_2021]. This specialization emerges from training without explicit supervision, reflecting the model's discovery that different types of interactions require different aggregation patterns.

::: {#fig-attention-patterns layout-ncol=3}
![**FIGURE PLACEHOLDER A**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20A)

![**FIGURE PLACEHOLDER B**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20B)

![**FIGURE PLACEHOLDER C**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20C)

[Essential] Heatmap visualization of attention weights from a trained genomic transformer (e.g., Enformer). Panel A: Attention pattern showing strong weights between a promoter region and a distal enhancer ~50kb away, demonstrating learned enhancer-promoter relationships. Panel B: Same attention overlaid on linear genome diagram showing the biological interpretation. Panel C: Different attention head showing local patterns (attending to nearby positions), demonstrating head specialization.
:::


The multi-head structure also provides redundancy that aids training. If one head fails to learn useful patterns, others can compensate. Gradient flow through multiple parallel paths stabilizes optimization. For genomic applications where training data may be limited compared to natural language corpora, this redundancy helps prevent individual heads from overfitting to spurious correlations. The number of heads represents a design choice: too few heads limit the diversity of learnable patterns, while too many heads reduce the dimensionality available to each head, potentially limiting their individual expressiveness. Most genomic transformers use 8 to 16 heads, balancing diversity against per-head capacity.


## Positional Encoding {#sec-positional-encoding}

A patient with hypertrophic cardiomyopathy carries a variant in the *MYH7* gene's promoter region. Determining pathogenicity requires knowing precisely where the variant sits relative to the transcription start site: a variant at position -30 (where the TATA box resides) carries entirely different implications than the same sequence at position +500 (within the 5' UTR). Position is not merely bookkeeping for genomic sequences; it encodes biological function. The canonical TATA box must appear 25 to 30 base pairs upstream of transcription initiation to function; the same sequence elsewhere carries no regulatory significance. Splice site recognition depends on the invariant GT and AG dinucleotides appearing at precise distances from exon boundaries. Enhancer-promoter communication requires specific distance relationships that vary by locus and cell type. A model that cannot distinguish position 100 from position 10,000 cannot learn the positional grammar that governs gene regulation.

Self-attention, by design, computes interactions based purely on content: the attention weight between positions depends only on their query and key vectors, not on where they sit in the sequence. Shuffling input token order changes nothing about how attention weights are computed. The model has no inherent notion of sequence order, a property called **permutation invariance**. For genomic data where position matters fundamentally, this blindness to order would be catastrophic. DNA has 5' to 3' directionality that determines transcription direction. Distance from transcription start sites determines promoter versus enhancer classification. Strand orientation distinguishes sense from antisense transcription. **Positional encodings** inject information about token positions into the model, breaking permutation invariance by making the model aware of where each token sits in the sequence.

### Absolute Position Encodings

The original transformer used sinusoidal functions with different frequencies for each embedding dimension [@vaswani_attention_2017]. For position $pos$ and dimension $i$:

$$
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
$$

$$
\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
$$

These fixed patterns have useful properties. They are deterministic (the same for all sequences), allow the model to learn to attend by relative positions (since $\text{PE}(pos+k)$ can be expressed as a linear function of $\text{PE}(pos)$), and generalize to sequence lengths not seen during training. The different frequencies across dimensions create a unique "fingerprint" for each position that the model can learn to decode.

Many genomic models use **learned positional embeddings** instead: lookup tables where each position has a learned vector added to the input embedding. DNABERT [@ji_dnabert_2021] and Nucleotide Transformer [@dalla-torre_nucleotide_2023] both employ learned positional embeddings, allowing the model to discover position-dependent patterns specific to genomic data. The trade-off is that learned embeddings require training and do not automatically extrapolate to longer sequences. A model trained with maximum sequence length of 512 tokens has no learned embedding for position 513, creating a hard boundary on sequence length at inference time.

### Relative Position Encodings

Absolute encodings treat position 1,000 and position 1,001 as having different representations even though their relative relationship (adjacent positions) may matter more than their absolute locations. For genomic applications, relative distance often carries more biological meaning than absolute coordinates: nucleosomes are spaced approximately 200 base pairs apart regardless of genomic location, and enhancer-promoter interactions depend on distance rather than absolute position. **Relative positional encodings** address this by encoding distances between positions rather than absolute coordinates.

T5-style relative position bias adds a learnable scalar to attention scores based on the distance between query and key positions [@raffel_exploring_2020]. This bias helps the model learn that nearby positions often interact more strongly than distant ones while remaining agnostic about absolute position in the sequence. The learned biases can capture genomic-specific distance preferences, such as the characteristic spacing of regulatory elements or the periodicity of nucleosome positioning.

**Attention with Linear Biases (ALiBi)** adds a fixed linear penalty to attention scores based on distance, without learned parameters [@press_train_2022]. For a head with slope $m$, attention between positions separated by distance $|i - j|$ is penalized by $m|i - j|$. Different heads use different slopes, encouraging some to focus locally and others globally. ALiBi generalizes well to longer contexts than seen during training because the linear penalty extrapolates naturally, making it attractive for genomic applications where sequence length varies dramatically. A model trained on 1-kilobase sequences can process 10-kilobase sequences at inference time without architectural modification.

**Rotary Position Embeddings (RoPE)** encode positions by rotating query and key vectors in a high-dimensional space, with rotation angle depending on position [@su_roformer_2024]. The dot product between rotated query and key depends on their relative distance, combining benefits of relative encoding with efficient implementation. RoPE has become standard in recent language models and appears increasingly in genomic transformers, offering a balance between the flexibility of learned embeddings and the extrapolation capability of fixed schemes.

[FIGURE RECOMMENDATION: Comparison of positional encoding schemes. Panel A: sinusoidal absolute encodings showing the wave patterns across positions. Panel B: learned embeddings as a heatmap of position vs. dimension. Panel C: ALiBi attention bias matrix showing linear decay with distance. Panel D: RoPE rotation visualization in 2D subspace. Include genomic context showing how each scheme handles a 10kb regulatory region.]

### Genomic Position Considerations

Genomic sequences impose additional requirements on positional encoding beyond what natural language demands. DNA has strand directionality: ACGT on the forward strand has different regulatory meaning than the same sequence on the reverse strand (which reads as ACGT from the complementary strand's perspective but represents TGCA in the reference orientation). Positional encodings should enable the model to learn strand-specific patterns. Some genomic transformers encode both strands separately and combine predictions; others rely on the model learning strand orientation from sequence content alone.

Genomic coordinates pose another design choice. Should position 1 correspond to a fixed genomic landmark (transcription start site, gene start) or simply indicate sequence order without biological reference? Models predicting regulatory activity often center sequences on promoters, using positions relative to the TSS. Foundation models trained on random genomic segments typically use positional encodings reflecting sequence order without genomic coordinate reference. The choice affects what the model can learn: TSS-relative positions enable learning of distance-dependent regulatory patterns (such as the preference for certain motifs at specific distances from transcription start), while sequence-order positions require the model to learn these patterns implicitly from content.

The choice of positional encoding interacts with tokenization strategies discussed in @sec-representations. K-mer tokenization reduces sequence length (and thus attention cost) but changes what "position" means: position 1 might represent nucleotides 1 through 6 rather than a single base. Positional encodings must be interpreted relative to the tokenization scheme. A model using 6-mer tokens with learned positional embeddings learns different position-dependent patterns than one using single-nucleotide tokens, even if both cover the same genomic region.


## The Transformer Block

A clinician interpreting a *BRCA1* variant needs a model that does more than identify isolated motifs or single long-range interactions. The variant's pathogenicity depends on how multiple regulatory signals integrate: local splice site grammar, enhancer contacts from 20 kilobases upstream, and transcription factor binding sites whose effects depend on chromatin context. Single attention layers identify pairwise relationships, but understanding complex regulatory logic requires building hierarchical representations where simple patterns combine into compound signals. This hierarchical integration emerges from stacking **transformer blocks**, the modular units that combine attention and nonlinear processing to build increasingly abstract representations through repeated application.

::: {#fig-transformer-block}
![**FIGURE PLACEHOLDER**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER)

[High] Detailed diagram of a single transformer block with pre-norm configuration. Show: Input → Layer Norm → Multi-Head Attention → Residual Add → Layer Norm → Feed-Forward Network (expand 4x, GELU, project back) → Residual Add → Output. Annotate dimension changes. Include small inset showing 2-3 stacked blocks to illustrate depth.
:::


### Block Components

Each transformer block accomplishes two distinct functions: enabling positions to share information across the sequence, and transforming that aggregated information through nonlinear processing. The **multi-head self-attention layer** handles global communication, allowing each position to gather information from the entire sequence. The **position-wise feed-forward network** processes each position independently, applying nonlinear transformations to the aggregated information. Separating these functions into distinct components allows each to be optimized independently and provides clear computational semantics: attention determines which positions are relevant to each other (the "what to consider" question), while the feed-forward network determines how to combine that information (the "what to conclude" question).

The feed-forward network consists of two linear transformations with a nonlinearity between them. Typically, this expands the dimension by a factor of four (from model dimension $d$ to $4d$), applies GELU or similar activation, then projects back to dimension $d$. This expansion allows processing through a high-dimensional nonlinear transformation before producing output for the next layer. The position-wise nature means each position is transformed identically but independently; cross-position information flows only through attention.

**Layer normalization** stabilizes training by normalizing activations across the feature dimension at each position. Two conventions exist for placement. Post-norm places normalization after each sublayer, applying it to output before the residual connection. Pre-norm places normalization before each sublayer, normalizing input to attention or feed-forward operations. Pre-norm has become standard because it improves training stability for deep networks, though post-norm can achieve slightly better final performance with careful tuning [@xiong_layer_2020].

**Residual connections** (which add input directly to sublayer output, providing gradient highways during backpropagation) wrap around both attention and feed-forward sublayers. These connections serve two critical functions. First, they allow gradients to flow directly through many layers without repeated transformation, enabling training of very deep networks that would otherwise suffer from vanishing gradients. Second, they create an inductive bias toward incremental refinement: each layer makes small adjustments to the representation rather than constructing entirely new representations from scratch. For genomic models, this incremental refinement maps naturally onto biological interpretation: early layers might identify motifs, middle layers might recognize motif combinations, and later layers might integrate these patterns into regulatory predictions.

### Information Flow and Depth

The flow through a pre-norm transformer block proceeds as follows. Input $X$ is normalized, processed by multi-head attention to produce $X'$, and added back via residual connection, yielding $X + X'$. This sum is normalized, passed through the feed-forward network to produce $X''$, and added via another residual connection, yielding final output $X + X' + X''$. Each layer thus adds refinements to the representation while preserving information from earlier processing.

Stacking depth determines how many times this refinement occurs. Shallow transformers (6 layers or fewer) are parameter-efficient but may lack capacity for complex hierarchical patterns. Deep transformers (12 to 24 layers) can learn sophisticated representations that capture how promoter elements, enhancer contacts, and chromatin state combine to determine expression. Most genomic transformers use 6 to 24 layers, varying by application. Models for short sequences (small RNAs, individual binding sites) might use fewer layers, while foundation models for long genomic contexts often use deeper stacks to build representations that integrate information across multiple biological scales.

The choice of depth balances capacity against trainability. Deeper networks learn more complex functions but are harder to optimize, prone to overfitting without sufficient data, and more expensive at training and inference. For genomic models, depth often correlates with the complexity of patterns being modeled. Simple motif recognition tasks might benefit more from wider layers (larger $d$) than deeper stacks, while tasks requiring hierarchical integration (understanding how promoter-enhancer-insulator relationships determine expression) may benefit from additional depth that builds increasingly abstract representations layer by layer.

[FIGURE RECOMMENDATION: Transformer block architecture diagram. Show the complete block structure with multi-head attention, feed-forward network, layer normalization, and residual connections. Include annotations showing information flow and dimension changes. Highlight how residual connections create gradient highways. Consider showing 2-3 stacked blocks to illustrate how representations build through depth.]


## Scaling to Genomic Sequences

A 52-year-old patient presents with unexplained cardiomyopathy, and whole-genome sequencing reveals a structural variant spanning 500 kilobases on chromosome 14, disrupting the *MYH7* locus and several upstream regulatory elements. The clinical team needs to assess whether this variant explains the patient's phenotype. Standard transformers cannot help: the quadratic complexity of self-attention makes 500-kilobase contexts computationally intractable. This gap between clinical need and computational capability defines a central challenge for genomic AI. The attention mechanism enables long-range modeling in principle, but practical constraints on memory and computation limit what contexts can actually be processed. Effective application of transformers to genomics requires strategies for managing these constraints.

### The Quadratic Barrier

Computing all pairwise attention scores requires $O(L^2)$ operations, where $L$ is sequence length. For a 10-kilobase sequence tokenized at single-nucleotide resolution, this means 100 million attention computations per layer. A 200-kilobase sequence requires 40 billion computations per layer. Memory requirements scale similarly because the attention matrix must be stored for backpropagation.

This scaling constraint directly limits what clinical questions transformers can address. The *HLA* region (critical for transplant matching and autoimmune disease risk in the approximately 40,000 organ transplants performed annually in the United States) spans approximately 4 megabases and contains the most polymorphic genes in the human genome. Modeling this region with standard self-attention would require $16 \times 10^{12}$ attention computations per layer, far exceeding practical limits. Structural variant detection often requires analyzing megabase-scale contexts to identify breakpoints and assess functional impact, yet these contexts remain computationally intractable for standard transformers. A patient with a suspected chromosomal translocation cannot benefit from transformer-based analysis when the relevant context exceeds computational capacity.

### Parameter Considerations

The number of parameters a transformer can effectively utilize depends on both training data quantity and the complexity of patterns to be learned. Transformer parameters come primarily from two sources. Width (model dimension $d$) determines embedding and hidden state sizes; increasing width allows more complex pattern representation at each position but increases parameters quadratically because weight matrices scale as $d \times d$. Depth (number of layers) determines how many refinement steps occur; increasing depth allows hierarchical abstractions through repeated processing but increases parameters linearly.

Scaling laws from natural language processing suggest performance improves smoothly with increased parameters, data, and compute [@kaplan_scaling_2020]. Similar principles apply to genomics, though optimal ratios may differ. Genomic sequences are less compressible than natural language: each nucleotide carries less predictable information than words in structured sentences. The entropy of DNA sequence is higher than English text, meaning more parameters may be needed to model the same sequence length. This asymmetry suggests genomic models might benefit relatively more from depth (more processing of high-entropy information) than from width (more dimensions per position when each position carries limited structure).

Genomic foundation models span a wide parameter range. DNABERT uses approximately 110 million parameters, comparable to BERT-base [@ji_dnabert_2021]. Nucleotide Transformer scales from 50 million to 2.5 billion parameters across model variants [@dalla-torre_nucleotide_2023]. The largest genomic transformers approach 10 billion parameters, though whether this scale provides commensurate benefit for genomic tasks remains under investigation. The relationship between parameter count and downstream task performance is not always monotonic: a well-trained smaller model can outperform a poorly trained larger one, and task-specific fine-tuning often matters more than pretraining scale for focused clinical applications.

### Context Length Strategies

Standard self-attention's $O(L^2)$ complexity becomes prohibitive for long genomic contexts, forcing architectural choices that trade expressiveness for tractability. The strategies employed reflect different assumptions about which interactions matter most for genomic modeling.

**Sparse attention patterns** restrict which positions attend to which others. Local windowing allows each position to attend only within a fixed window, reducing complexity to $O(Lw)$ where $w$ is window size. This approach works when most relevant interactions are local, as often holds for regulatory sequences where nearby elements interact more strongly than distant ones. For clinical variant interpretation in coding sequences, where splice sites and reading frame context typically lie within a few hundred bases, local attention may capture the relevant biology. The trade-off is missing long-range interactions that fall outside windows, potentially critical for understanding distal enhancer effects or structural variant consequences.

**Strided attention** creates hierarchy: lower layers use local windows while upper layers attend to every $k$-th position. This configuration captures both local fine-grained patterns and global coarse-grained structure while maintaining sub-quadratic complexity. Hybrid models like Enformer apply CNNs to downsample sequences before transformer layers, reducing the effective sequence length that attention must handle [@avsec_effective_2021]. A 200-kilobase genomic region might be compressed to roughly 1,500 positions after CNN processing, making full attention tractable at the cost of single-nucleotide resolution in transformer layers.

**Approximations to full attention** offer another approach. Linformer approximates the attention matrix through low-rank decomposition, reducing complexity to linear in sequence length [@wang_linformer_2020]. Performer uses random feature methods to approximate attention scores without explicitly computing the full $L \times L$ matrix [@choromanski_rethinking_2021]. These approximations trade some expressiveness for efficiency and may miss certain long-range dependencies that low-rank structure cannot capture.

### Memory and Precision

Memory requirements compound computational challenges for genomic transformers. Training requires storing activations for backpropagation, and attention matrices are particularly memory-intensive. A 100-kilobase sequence with 16 attention heads and 12 layers requires storing $16 \times 12 \times 100,000 \times 100,000$ attention weights, approximately 2 terabytes at 32-bit precision before considering other activations.

**Gradient checkpointing** trades compute for memory by recomputing activations during the backward pass rather than storing them. This technique enables training larger models or longer sequences on fixed hardware at the cost of additional computation time, typically increasing training time by 20 to 30 percent. **Mixed precision training** uses 16-bit floating point for most computations while maintaining 32-bit precision for critical operations like loss computation and optimizer updates. Modern GPUs accelerate 16-bit arithmetic substantially, providing near 2× speedup with minimal precision loss. Flash Attention implements memory-efficient attention computation that avoids materializing the full attention matrix, enabling longer contexts within fixed memory budgets [@dao_flashattention_2022].


## Architectural Variants for Genomics

A clinical laboratory evaluating a novel missense variant needs a model that integrates bidirectional protein context to assess pathogenicity. A biotechnology company designing synthetic promoters needs a model that generates novel sequences with specified expression properties. A research team studying protein-RNA interactions needs a model that predicts binding from sequence features. These distinct requirements have driven development of architectural variants that make different trade-offs between representation learning, generation capability, and computational efficiency. The choice of architecture shapes what questions can be asked and how answers emerge.

::: {#fig-encoder-decoder layout-ncol=3}
![**FIGURE PLACEHOLDER A**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20A)

![**FIGURE PLACEHOLDER B**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20B)

![**FIGURE PLACEHOLDER C**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20C)

[Enhancing] Three-panel comparison. Panel A (Encoder-only, e.g., BERT/DNABERT): Bidirectional attention pattern (full matrix), typical use for classification/embedding. Panel B (Decoder-only, e.g., GPT/Evo): Causal attention pattern (lower triangular), typical use for generation. Panel C (Hybrid CNN-Transformer, e.g., Enformer): CNN downsampling followed by transformer, showing how hybrid reduces sequence length before attention.
:::

### Encoder-Only Transformers

When a clinical laboratory queries a pathogenicity database for a novel missense variant, they need a model that integrates information from the entire protein sequence: upstream domains that establish structural context, downstream regions that complete functional units, and evolutionary patterns that distinguish tolerated from deleterious changes. **Encoder-only transformers** process sequences bidirectionally, allowing each position to attend to all other positions including those that follow in the sequence. This bidirectional context produces richer representations than unidirectional processing because each position's representation incorporates information from the entire sequence.

DNABERT exemplifies this architecture, trained with **masked language modeling** objectives where random tokens are masked and predicted from bidirectional context [@ji_dnabert_2021]. The model learns to predict held-out k-mers based on surrounding sequence, implicitly learning sequence patterns and constraints that transfer to downstream tasks. Nucleotide Transformer follows similar principles at larger scale [@dalla-torre_nucleotide_2023]. These models excel at **representation learning**: producing embeddings that capture biological properties useful for variant effect prediction, function classification, or other tasks that require fixed-length representations of variable-length sequences.

Bidirectional attention suits tasks where both upstream and downstream context matters for understanding a position. Transcription factor binding depends on flanking sequence in both directions. Splice site recognition requires seeing both exonic and intronic context. Variant pathogenicity may depend on protein domain context from both N-terminal and C-terminal directions. The limitation is that encoder-only architectures cannot generate sequences autoregressively because they require seeing the full sequence to produce any output; they answer "what does this sequence mean" rather than "what sequence should come next."

### Decoder-Only Transformers

Generating synthetic genomic sequences for therapeutic design, creating diverse antibody libraries for drug discovery, or sampling from learned regulatory grammars all require models that produce sequences one token at a time. **Decoder-only transformers** use **causal attention** where each position attends only to itself and preceding positions. This structure enables **autoregressive generation**: the model produces sequences one token at a time, conditioning each new token on all previous tokens.

GenSLM applies this architecture to genomic data, training on next-token prediction to learn sequence distributions [@zvyagin_genslm_2023]. The model learns to predict the next nucleotide or k-mer given all preceding context, implicitly learning the statistical regularities of genomic sequence. This objective aligns naturally with generation tasks: sampling proceeds by repeatedly predicting the next token and appending it to the sequence. Causal attention is essential for generation because the model must produce each position before it can condition subsequent positions on that output.

The trade-off is that causal attention produces less rich representations for fixed sequences because each position sees only partial context. Position 100 in a 1000-position sequence has access to only the first 100 positions, not the remaining 900 that might provide relevant information. For variant effect prediction where downstream context matters, this limitation can be substantial. The choice between encoder and decoder architectures reflects a fundamental tension: representation learning benefits from bidirectional context, while generation requires causal structure.

### Encoder-Decoder Transformers

Some genomic tasks require transforming one sequence into another of different length or structure. Predicting protein sequence from coding DNA, generating variant descriptions from sequence context, or translating between sequence representations all involve input-output relationships that neither pure encoder nor pure decoder architectures handle naturally. **Encoder-decoder architectures** combine bidirectional encoding with autoregressive decoding [@vaswani_attention_2017].

The encoder processes an input sequence with full bidirectional attention, producing contextualized representations. The decoder then generates output tokens autoregressively, attending both to its own previous outputs (through causal self-attention) and to encoder representations (through **cross-attention**). This cross-attention allows each decoder position to query the full encoded input when generating output, combining the benefits of bidirectional understanding with autoregressive generation.

Encoder-decoder models are less common in genomic applications than encoder-only or decoder-only variants because most genomic tasks either need representations (favoring encoders) or generation (favoring decoders), not both simultaneously. Machine translation exemplifies the encoder-decoder use case: encode a sentence in one language, decode into another. Genomic analogs might include predicting protein sequences from codon-optimized DNA or generating clinical variant reports from sequence features, but these applications remain less developed than pure representation or generation tasks.

### Hybrid CNN-Transformer Models

The most successful genomic transformers combine convolutional and attention mechanisms rather than using transformers alone. This hybrid approach exploits CNNs' efficiency for local pattern extraction while using transformers for long-range integration, matching the multi-scale structure of genomic regulation where both local motifs and distal interactions determine function.

Enformer and Borzoi apply convolutional stems to long sequences, downsampling through pooling, then pass compressed representations through transformer layers [@avsec_effective_2021]. The CNN layers handle motif recognition, nucleosome positioning signals, and local chromatin features with parameter efficiency that pure transformers cannot match. Transformer layers then integrate across the broader regulatory landscape, learning enhancer-promoter relationships and TAD boundary effects. This division of labor achieves state-of-the-art performance on regulatory prediction tasks while remaining computationally tractable for 200-kilobase contexts.

The hybrid approach also addresses the quadratic attention bottleneck indirectly. By downsampling sequences before transformer layers (often by factors of 128 or more), hybrids reduce effective sequence length and thus attention cost. A 200-kilobase genomic region compressed to 1,500 positions requires only 2.25 million attention computations per layer rather than 40 billion for the uncompressed sequence. The cost is loss of single-nucleotide resolution in transformer layers, though the CNN stem preserves local detail that attention layers integrate but do not need to resolve. @sec-regulatory examines Enformer and related hybrid architectures in detail.

[FIGURE RECOMMENDATION: Architecture comparison diagram showing encoder-only, decoder-only, encoder-decoder, and hybrid CNN-transformer architectures. For each, show the attention pattern (bidirectional vs. causal), typical training objective, and example genomic applications. Highlight how hybrid architectures reduce sequence length before attention through CNN downsampling.]


## Training Dynamics

When a model trained to predict pathogenic variants misclassifies a disease-causing *BRCA1* mutation as benign, the consequences extend beyond benchmark metrics. Clinical laboratories may return incorrect results; patients may forego preventive surgeries that could save their lives. Training failures matter clinically because they determine what models learn and what they miss. A model that overfits to common polymorphisms in training data will fail on the rare variants that matter most for diagnosis. A model whose gradients vanish during training will never learn the subtle regulatory patterns that distinguish pathogenic from benign promoter variants. Understanding training dynamics helps predict and prevent these failures.

### Optimization

Genomic transformers inherit their training foundations from natural language processing but require adjustments for biological data. The Adam optimizer and its variant AdamW remain standard, using adaptive learning rates that maintain per-parameter estimates adjusted based on gradient statistics [@loshchilov_decoupled_2019]. AdamW applies weight decay directly to parameter updates rather than to the loss function, improving generalization and training stability.

Learning rate schedules typically use warmup (linearly increasing learning rate from near-zero to peak over the first several thousand steps) followed by decay (linear or cosine decrease over the remaining training). Warmup addresses a specific instability: transformers with random initialization can produce extreme gradients early in training, and adaptive optimizers need time to build accurate gradient statistics. Warmup allows the optimizer to stabilize before applying full learning rates. Skipping warmup often causes training collapse in the first few hundred steps, manifesting as loss spikes or NaN values.

For genomic data, learning rate tuning may require adjustment from NLP defaults. Regulatory sequences with highly conserved motifs (TATA boxes, splice site dinucleotides) create strong signals that models can overfit quickly; lower learning rates may prevent latching onto these patterns before learning subtler regulatory grammar. Protein sequences exhibit weaker positional conservation than regulatory DNA, potentially benefiting from higher rates that encourage broader exploration of the loss landscape. Empirically, genomic transformers often use peak learning rates of 1e-4 to 5e-4, somewhat lower than the 1e-3 to 3e-3 common in language modeling.

### Regularization

Regularization prevents overfitting, particularly important when training data is limited relative to model size. Genomic datasets, while growing rapidly, remain smaller than the trillion-token corpora used for large language models. A model with 100 million parameters trained on 10 billion nucleotides faces different overfitting risks than one trained on 1 trillion tokens.

**Dropout** randomly zeros activations during training, forcing the network to learn robust features independent of specific neurons. **Attention dropout** applies this to attention weights, randomly dropping connections between positions. This mechanism prevents over-reliance on specific position pairs and encourages distributed representations that remain informative even when some information pathways are blocked. Dropout rates of 0.1 to 0.2 are typical for genomic transformers.

**Weight decay** penalizes large parameter values, encouraging smaller, smoother weights. For transformers, weight decay is typically applied to all parameters except biases and layer normalization parameters. The coefficient requires careful tuning: too little provides insufficient regularization; too much constrains capacity and reduces model expressiveness. Values of 0.01 to 0.1 are common, with higher values for smaller datasets where overfitting risk is greater.

### Gradient Stability

Gradient issues plague deep network training and require specific attention for genomic transformers. **Vanishing gradients** occur when gradients become extremely small through many layers, preventing learning in early layers. **Exploding gradients** are the opposite: gradients grow exponentially and destabilize training. Transformers mitigate vanishing gradients through residual connections that provide direct gradient paths, allowing gradients to flow from output to early layers without passing through potentially attenuating transformations. Exploding gradients are addressed through **gradient clipping**, which rescales gradients when their norm exceeds a threshold.

For genomic transformers, gradient issues manifest differently than in language models. Genomic sequences have less hierarchical structure than natural language (no grammatical sentence organization), which affects gradient flow through attention layers. Imbalanced token frequencies create gradient imbalances: common k-mers receive large gradients from frequent occurrence while rare but biologically important tokens (such as k-mers containing the stop codon TAG) receive small gradients from infrequent occurrence. Addressing this imbalance may require loss reweighting or adaptive sampling that ensures rare tokens appear frequently enough for effective learning.

### Distributed Training

The computational scale of genomic foundation models typically exceeds single-GPU capacity, requiring distributed training strategies. **Data parallelism** replicates the model across GPUs, splitting batches across devices and aggregating gradients. This approach scales well up to batch sizes limited by convergence requirements but does not help when the model itself exceeds GPU memory. **Model parallelism** splits the model across devices, necessary when parameters exceed single-GPU memory. **Pipeline parallelism** divides layers across devices and pipelines forward and backward passes, interleaving computation to improve device utilization.

Batch size selection involves competing considerations. Larger batches provide more stable gradient estimates and better GPU utilization but require more memory and may reduce generalization. Genomic transformers often use **gradient accumulation** to simulate large batches: small batches process sequentially, gradients accumulate, then a single parameter update occurs. This strategy provides large-batch gradient stability without the memory cost, though it increases training time proportionally. Effective batch sizes of 256 to 4096 sequences are common for genomic transformers, achieved through accumulation over many smaller physical batches.


## Limitations and Emerging Alternatives

A 48-year-old patient presents with a suspected Lynch syndrome diagnosis, and genetic testing reveals a structural variant spanning 3 megabases on chromosome 2 that may disrupt the *MSH2* gene and its upstream regulatory region. The clinical team needs to determine whether this variant explains the patient's early-onset colorectal cancer and guides surveillance recommendations for family members. Standard transformers cannot address this question: the quadratic complexity of self-attention makes 3-megabase contexts computationally intractable. Current models can span 200 kilobases with hybrid architectures, yet the structural variants and chromosomal rearrangements that cause many inherited cancer syndromes remain beyond reach. This gap between clinical need and computational capability defines the frontier of genomic AI.

::: {#fig-quadratic-ceiling}
![**FIGURE PLACEHOLDER**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER)

[High] Log-log plot showing computational cost (FLOPs or memory) vs. sequence length for different architectures. Show: Standard attention (quadratic curve, O(L²)), sparse/local attention (sub-quadratic), state space models like Hyena/Mamba (linear). Annotate biologically relevant context lengths: promoter (~1kb), gene (~10kb), enhancer-promoter (~100kb), TAD (~1Mb), chromosome arm (~100Mb). Draw vertical lines at each scale showing which architectures are tractable.
:::

### The Quadratic Ceiling

The quadratic complexity of self-attention remains transformers' most severe limitation for genomics. Computing all pairwise attention scores requires $O(L^2)$ operations and memory. For genomic contexts exceeding 100 kilobases (roughly 100,000 single-nucleotide tokens), this becomes prohibitive. Even with sparse approximations and efficient implementations, transformers struggle at megabase scales where many regulatory interactions occur and structural variants manifest their effects.

Recent models have pushed context lengths substantially. Enformer handles 200 kilobases; emerging models approach 1 megabase. But these achievements rely on hybrid architectures with significant downsampling or hierarchical windowing that may miss certain long-range patterns or single-nucleotide resolution details. Pure transformers without such modifications remain limited to shorter contexts. The fundamental constraint shapes what questions transformers can address and motivates alternatives that escape quadratic scaling.

### State Space Models

**State space models (SSMs)** address the quadratic barrier directly by achieving linear complexity while maintaining long-range modeling capability. Rather than computing all pairwise interactions, SSMs represent sequences as continuous-time dynamical systems, maintaining memory through recurrent state updates that propagate information across positions without explicit pairwise computation.

Architectures like S4 [@gu_efficiently_2022], Hyena [@poli_hyena_2023], and Mamba [@gu_mamba_2024] have demonstrated competitive or superior performance to transformers on various sequence modeling tasks while scaling to much longer contexts. For genomics, this capability enables whole-chromosome or potentially whole-genome modeling that remains intractable for standard transformers. HyenaDNA processes sequences up to 1 million nucleotides at single-nucleotide resolution, enabling analysis of structural variants and long-range regulatory interactions that transformers cannot approach [@nguyen_hyenadna_2024]. The Evo model extends this further, achieving context lengths sufficient for bacterial genome-scale modeling. @sec-dna-lm examines these architectures in detail, exploring how linear complexity enables new categories of genomic analysis.

### Choosing Architectures

The choice between transformers and alternatives depends on the biological question and computational constraints. Transformers excel when global context matters but sequences are not extremely long (under 10 to 50 kilobases depending on computational resources). Attention maps provide interpretability, showing which positions the model considers relevant for predictions. Transformers benefit from extensive tooling and pretrained models from NLP that transfer readily to genomics.

CNNs remain preferable when computational efficiency is paramount and local patterns dominate. For splice site prediction or promoter classification where relevant context spans at most a few hundred base pairs, a well-designed CNN may outperform transformers while using far fewer parameters. The inductive bias toward local patterns also regularizes against overfitting when training data is limited.

Hybrid approaches often achieve the best practical results for intermediate-scale problems. Models combining CNNs for local feature extraction with transformers for long-range integration outperform pure architectures on regulatory prediction tasks, as @sec-regulatory demonstrates with Enformer and related models. The optimal combination depends on the specific biological question and the scale of relevant interactions.

The transition toward sub-quadratic architectures continues. Early results suggest SSMs match or exceed transformers on some genomic benchmarks while scaling to longer contexts. The question is no longer whether alternatives to quadratic attention exist, but which tasks benefit most from linear-complexity architectures and which retain advantages from explicit pairwise attention computation.

[FIGURE RECOMMENDATION: Scaling comparison showing computational cost vs. context length for standard attention (quadratic curve), sparse attention variants (sub-quadratic), and state space models (linear). Annotate with biologically relevant context lengths: promoter regions (~1kb), enhancer-gene distances (~100kb), TAD sizes (~1Mb), chromosome arms (~100Mb). Show which architectures are tractable at each scale.]


## Architecture and Learning

The transformer architecture provides the computational substrate for modern genomic foundation models, but architecture alone does not determine what models learn. Attention mechanisms enable pairwise interaction modeling across arbitrary sequence distances. Position encodings break permutation invariance to preserve the sequential structure essential to regulatory grammar. Stacked blocks build hierarchical representations through iterative refinement. These components create capacity; training objectives and data determine how that capacity is used.

Self-supervised pretraining transforms architectural capacity into biological knowledge. Masked language modeling teaches models to predict held-out tokens from context, implicitly learning the sequence patterns and evolutionary constraints that determine biological function. Next-token prediction in autoregressive models captures sequential dependencies required for sequence generation. Applied to massive genomic datasets, these objectives enable transformers to learn representations that transfer across diverse downstream tasks without task-specific supervision. The foundation models examined in subsequent chapters (DNABERT for regulatory sequence, ESM-2 for proteins, Enformer for expression prediction) each demonstrate that transformers trained on biological sequence capture patterns that generalize beyond their training objectives.

Attention introduced a paradigm shift in how genomic models access context. Where convolutional networks aggregate local information through hierarchical composition, attention enables direct communication between any two positions regardless of distance. The computational challenge shifts from extending receptive fields to managing the quadratic complexity of pairwise attention. State space models and linear attention variants address this bottleneck while maintaining long-range capability, and whether these alternatives ultimately complement or displace standard transformers remains an open question. What is clear is that attention-based architectures have become the default substrate for genomic foundation models, with the pretraining objectives examined next determining what biological knowledge they acquire.