% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Genomic Foundation Models},
  pdfauthor={Josh Meehl},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Genomic Foundation Models}
\author{Josh Meehl}
\date{2025-12-01}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Introduction}\label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

Genomics is in the middle of a quiet phase change.\\
On one side, sequencing has become routine: biobanks now contain
hundreds of thousands to millions of genomes, exomes, and
transcriptomes, cataloging billions of variants across diverse
populations. On the other side, deep learning and large-scale sequence
modeling have transformed how we represent language, proteins, and now
DNA itself (J. Zhou and Troyanskaya 2015; Ž. Avsec et al. 2021; He et
al. 2023; Benegas, Batra, and Song 2023; Benegas, Ye, et al. 2024; Zhang
et al. 2024).

This book is about the intersection: \textbf{genomic foundation models
(GFMs)}---large, reusable models trained on genomic and related data
that can be adapted to many downstream tasks, from variant
interpretation to clinical risk prediction.

Rather than offering a general introduction to genomics or machine
learning, the goal here is narrower and more opinionated:

\begin{quote}
To give you a \emph{conceptual and practical map} of how modern deep
models for DNA, RNA, and proteins are built, what they actually learn,
and how they can be used responsibly in research and clinical workflows.
\end{quote}

The chapters that follow connect classic genomics pipelines, early deep
regulatory models, sequence language models, and multi-omic GFMs into a
single narrative arc.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{Why Genomic Foundation
Models?}\label{why-genomic-foundation-models}
\addcontentsline{toc}{section}{Why Genomic Foundation Models?}

\markright{Why Genomic Foundation Models?}

Traditional genomic modeling has typically been \textbf{task-specific}:

\begin{itemize}
\tightlist
\item
  A variant caller designed only to distinguish sequencing errors from
  real variants.
\item
  A convolutional network trained to predict chromatin marks in a fixed
  panel of cell types.
\item
  A risk score tuned for coronary artery disease in one ancestry group.
\end{itemize}

These models can work well within their narrow domains, but they do not
usually offer a \textbf{single, reusable representation} of genomic
variation.

By contrast, the ``foundation model'' paradigm---popularized in natural
language and protein modeling---rests on three ideas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Scale}\\
  Train large models on massive, heterogeneous datasets (e.g., whole
  genomes across species, genome-wide chromatin maps, population
  variation).
\item
  \textbf{Self-supervision}\\
  Use objectives such as masked-token prediction, next-token modeling,
  or multi-task sequence-to-function training that do not require dense
  human labels (Benegas, Batra, and Song 2023; Zvyagin et al. 2022;
  Nguyen et al. 2023; Schiff et al. 2024).
\item
  \textbf{Reusability}\\
  Treat the model as a \emph{backbone}: for any new task, you probe or
  fine-tune it, rather than training from scratch. Variant effect
  prediction, enhancer classification, GWAS fine-mapping priors, and
  clinical risk models all become ``adapters'' on top of a shared
  representation (Benegas, Ye, et al. 2024; Fishman et al. 2025).
\end{enumerate}

In genomics, this paradigm is still evolving and remains empirically
contested: some studies report large gains from pretraining, while
others argue that simple baselines remain competitive (Vishniakov et al.
2024). This book deliberately leans into that tension. Throughout the
chapters we will ask:

\begin{itemize}
\tightlist
\item
  \textbf{When} does large-scale pretraining actually help?
\item
  \textbf{What} do these models learn about regulatory logic, evolution,
  and molecular mechanisms?
\item
  \textbf{How} can they be plugged into real decision-making pipelines
  without overclaiming?
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{What This Book Is (and Is
Not)}\label{what-this-book-is-and-is-not}
\addcontentsline{toc}{section}{What This Book Is (and Is Not)}

\markright{What This Book Is (and Is Not)}

This is \textbf{not}:

\begin{itemize}
\tightlist
\item
  A full genomics textbook.
\item
  A comprehensive review of every published DNA or protein model.
\item
  A mathematical introduction to deep learning from first principles.
\end{itemize}

Instead, it aims to be:

\begin{itemize}
\tightlist
\item
  A \textbf{roadmap} to the main families of models and data that matter
  for genomic foundation modeling today.
\item
  A \textbf{bridge} between classic statistical genetics (GWAS, PRS,
  deleteriousness scores) and modern sequence models.
\item
  A \textbf{practical guide} to how these models are built, trained,
  evaluated, and deployed---emphasizing caveats, confounders, and
  failure modes.
\end{itemize}

The assumed background is a working familiarity with basic genomics
(variants, genes, regulatory elements) and some exposure to machine
learning. Where needed, earlier chapters provide primers before moving
into more advanced models.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{How the Book Is Organized}\label{how-the-book-is-organized}
\addcontentsline{toc}{section}{How the Book Is Organized}

\markright{How the Book Is Organized}

The book is organized into five parts. Each part can be read on its own,
but they are designed to build on each other.

\subsection*{Part I --- Data \& Pre-DL Methods (Chapters
1--4)}\label{part-i-data-pre-dl-methods-chapters-14}
\addcontentsline{toc}{subsection}{Part I --- Data \& Pre-DL Methods
(Chapters 1--4)}

Part I lays the \textbf{genomic and statistical foundation} that later
models rest on.

\begin{itemize}
\item
  \textbf{Chapter 1 --- NGS \& Variant Calling}\\
  Introduces next-generation sequencing, alignment, and variant calling.
  It traces the evolution from hand-crafted statistical pipelines (e.g.,
  GATK) to deep learning-based callers such as DeepVariant, highlighting
  how even ``upstream'' tools have transitioned to learned models.
\item
  \textbf{Chapter 2 --- PRS \& GWAS Basics}\\
  Covers the genome-wide association study (GWAS) paradigm, summary
  statistics, linkage disequilibrium, and polygenic risk scores (PRS).
  This chapter sets up the statistical framing of ``variant-to-trait''
  associations that later GFMs must interface with.
\item
  \textbf{Chapter 3 --- Deleteriousness Scores}\\
  Surveys conservation-based and machine learning-based variant
  pathogenicity scores (e.g., CADD and related tools), emphasizing how
  hand-crafted features and population constraint metrics are combined
  into early genome-wide predictors (Rentzsch et al. 2019; Schubach et
  al. 2024).
\item
  \textbf{Chapter 4 --- Foundational Genomics Data}\\
  Reviews the major data resources that fuel modern models: ENCODE and
  Roadmap epigenomics for chromatin and accessibility, large expression
  atlases, population resources like gnomAD, and biobanks linking
  genomes to phenotypes. These datasets become the training targets and
  evaluation benchmarks for the deep models in later parts.
\end{itemize}

Together, Part I answers: \emph{What are the core data, statistical
tools, and baseline methods that any genomic foundation model must
respect, integrate with, or improve upon?}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Part II --- CNN Seq-to-Function Models (Chapters
5--7)}\label{part-ii-cnn-seq-to-function-models-chapters-57}
\addcontentsline{toc}{subsection}{Part II --- CNN Seq-to-Function Models
(Chapters 5--7)}

Part II turns to the first wave of \textbf{deep sequence-to-function
models}, largely built on convolutional neural networks (CNNs).

\begin{itemize}
\item
  \textbf{Chapter 5 --- Regulatory Prediction}\\
  Focuses on models that predict chromatin accessibility and
  transcription factor binding from DNA sequence (e.g., DeepSEA and
  successors) (J. Zhou and Troyanskaya 2015; Chen et al. 2022; Ž. Avsec
  et al. 2021). It introduces one-hot sequence encodings, convolutional
  filters as motif detectors, and multi-task training over many assays.
\item
  \textbf{Chapter 6 --- Transcriptional Effects}\\
  Moves from local chromatin states to gene-level transcriptional
  readouts, including models such as ExPecto and related architectures.
  It emphasizes the challenge of mapping sequences to expression in
  specific cell types and conditions, and introduces ideas like
  distance-dependent kernels.
\item
  \textbf{Chapter 7 --- Splicing Prediction}\\
  Examines specialized sequence models for splicing, such as SpliceAI,
  which use deep receptive fields to capture intronic and exonic
  context. This chapter shows how tailored architectures can achieve
  state-of-the-art performance on particular mechanisms, foreshadowing
  the mechanism-aware GFMs of Part IV.
\end{itemize}

Part II illustrates how \textbf{task-specific supervised models} can
uncover rich regulatory logic, but also how their narrow training
objectives limit reuse.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Part III --- Transformer Models (Chapters
8--11)}\label{part-iii-transformer-models-chapters-811}
\addcontentsline{toc}{subsection}{Part III --- Transformer Models
(Chapters 8--11)}

Part III explores the \textbf{sequence modeling architectures} that
underpin many GFMs, with an emphasis on transformers and long-range
variants.

\begin{itemize}
\item
  \textbf{Chapter 8 --- Sequence Representation \& Tokens}\\
  Discusses how to tokenize DNA, RNA, and protein sequences (nucleotides
  vs k-mers vs learned tokens) and how those choices affect model
  capacity, context length, and interpretability. It also introduces
  positional encodings and other tricks needed for very long sequences.
\item
  \textbf{Chapter 9 --- Protein Language Models}\\
  Provides a primer on protein language models---ESM,
  AlphaMissense-style models, and related architectures---that learn
  from large multiple sequence alignments or unaligned protein corpora
  (Cheng et al. 2023). These models serve as a conceptual and technical
  template for DNA-based LMs and illustrate how representations can
  support both structure prediction and variant effect prediction.
\item
  \textbf{Chapter 10 --- DNA Foundation Models}\\
  Introduces DNA language models and related self-supervised
  architectures (e.g., GPN, GenSLMs, HyenaDNA) (Benegas, Batra, and Song
  2023; Zvyagin et al. 2022; Nguyen et al. 2023; Benegas, Ye, et al.
  2024). It contrasts them with the supervised CNNs of Part II, focusing
  on objectives, pretraining corpora, and usage modes (frozen probes,
  fine-tuning, zero-shot).
\item
  \textbf{Chapter 11 --- Long-range Hybrid Models}\\
  Surveys architectures that integrate long-range context and multi-task
  supervision, including models like Enformer, Caduceus, and hybrid
  convolution--transformer--SSM designs (Ž. Avsec et al. 2021; Schiff et
  al. 2024). These models blur the line between ``language model'' and
  ``sequence-to-function'' network and motivate the broader GFM framing
  in Part IV.
\end{itemize}

Part III answers: \emph{What architectural tools are available for
modeling genomic sequences at kilobase to megabase scale, and how do
they differ from classic CNNs?}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Part IV --- GFMs \& Multi-omics (Chapters
12--16)}\label{part-iv-gfms-multi-omics-chapters-1216}
\addcontentsline{toc}{subsection}{Part IV --- GFMs \& Multi-omics
(Chapters 12--16)}

Part IV is the conceptual heart of the book, focusing explicitly on
\textbf{genomic foundation models} and their multi-omic extensions.

\begin{itemize}
\item
  \textbf{Chapter 12 --- Genomic FMs: Principles \& Practice}\\
  Provides a working definition of GFMs, tracing the progression from
  deleteriousness scores and CNNs to self-supervised DNA and protein LMs
  (He et al. 2023; Benegas, Ye, et al. 2024; Dalla-Torre et al. 2023;
  Nguyen et al. 2023). It distills design principles around pretraining
  objectives, context length, conditioning (cell type, species), and
  deployment patterns.
\item
  \textbf{Chapter 13 --- Variant Effect Prediction}\\
  Recasts variant effect prediction in the GFM era, spanning
  conservation-based scores, protein LMs, sequence-to-function models,
  and multi-modal systems like AlphaMissense and MSA-enhanced models
  (Cheng et al. 2023; Benegas, Albors, et al. 2024; Brixi et al. 2025).
  It emphasizes how GFMs act as feature generators for downstream
  prioritization and association analyses.
\item
  \textbf{Chapter 14 --- Confounders in Model Training}\\
  Details the many sources of confounding---batch effects, ancestry and
  population structure, data leakage between train and test, label
  bias---that can inflate reported performance or create spurious
  signals. This chapter provides a checklist for stress-testing GFMs and
  their derivatives.
\item
  \textbf{Chapter 15 --- Interpretability \& Mechanisms}\\
  Explores how to probe GFMs and related models for mechanistic insight:
  motif discovery, saliency maps, in silico mutagenesis, causal
  perturbation experiments, and connections to biophysical or
  evolutionary models. The emphasis is on turning black-box predictions
  into hypotheses about regulatory grammar and molecular mechanisms.
\item
  \textbf{@sec-multiomics --- Multi-omics and Systems Context}\\
  Broadens the view from isolated sequences to \textbf{multi-omic and
  network-level models}, integrating chromatin, expression,
  protein--protein interaction networks, and other modalities. It
  discusses graph-based models, multi-modal encoders, and how GFMs can
  provide a common representation layer for multi-omic integration (X.
  Li et al. 2022; Chandak, Huang, and Zitnik 2023; Cornman et al. 2024).
\end{itemize}

Part IV pulls together themes from every earlier chapter, focusing on
how to design, train, and interrogate models that aspire to be genuinely
\emph{foundational} within genomics.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection*{Part V --- Applications (Chapters
17--19)}\label{part-v-applications-chapters-1719}
\addcontentsline{toc}{subsection}{Part V --- Applications (Chapters
17--19)}

Part V turns outward, focusing on \textbf{applications and deployment}
in clinical and translational settings.

\begin{itemize}
\item
  \textbf{Chapter 17 --- Clinical Risk Prediction}\\
  Describes how GFMs and related models can be used to build clinical
  risk scores and decision support tools, often combining genomic
  features with electronic health record (EHR) and other data sources
  (Cao and Gao 2022; Georgantas, Kutalik, and Richiardi 2024; Clarke et
  al. 2024; Rakowski and Lippert 2025). It covers calibration,
  uncertainty, fairness, and regulatory considerations for high-stakes
  predictions.
\item
  \textbf{Chapter 18 --- Pathogenic Variant Discovery}\\
  Focuses on discovery workflows: ranking variants and genes for
  follow-up, integrating GFMs into fine-mapping, network-based gene
  prioritization, and the design of CRISPR and MPRA experiments for
  functional validation (Benegas, Albors, et al. 2024; Ž. Avsec et al.
  2021; Linder et al. 2025; H. Li et al. 2024; Wu et al. 2024).
\item
  \textbf{Chapter 19 --- Drug Discovery \& Biotech}\\
  Zooms out to drug discovery and biotech pipelines: target
  identification, genetic validation, functional genomics screens, and
  early-stage safety and efficacy prediction. It highlights where GFMs
  are already useful, where they remain speculative, and how they might
  integrate with broader scientific LLM ecosystems (Ma et al. 2023;
  Zhang et al. 2024).
\end{itemize}

These chapters emphasize \textbf{end-to-end workflows}: how a model
trained on sequences ends up influencing a clinical report, a gene
nomination list, or a preclinical portfolio.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{How to Read This Book}\label{how-to-read-this-book}
\addcontentsline{toc}{section}{How to Read This Book}

\markright{How to Read This Book}

Different readers will want to enter at different points:

\begin{itemize}
\tightlist
\item
  If you are primarily a \textbf{computational biologist or statistical
  geneticist}, you may want to skim Part I (to align terminology) and
  then focus on Parts II--IV to understand how deep models are
  constructed and evaluated.
\item
  If you are a \textbf{machine learning researcher}, Chapters 5--11 and
  12--16 provide the most relevant architectural and modeling details,
  with Part I serving as biological context and Part V illustrating
  application constraints.
\item
  If you are a \textbf{clinician, translational scientist, or industry
  practitioner}, you may wish to read Chapters 1--4 and 12--19, treating
  the technical architectural chapters as optional references.
\end{itemize}

The chapters are intentionally cross-referenced. When a later chapter
relies heavily on earlier material---for example, Chapter 13 on variant
effect prediction drawing from Chapters 3, 5--7, 9, and 10---this is
called out explicitly.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{A Moving Target}\label{a-moving-target}
\addcontentsline{toc}{section}{A Moving Target}

\markright{A Moving Target}

Genomic foundation models are a \textbf{moving target}. New
architectures, training corpora, and evaluation benchmarks continue to
appear at a rapid pace (He et al. 2023; Benegas, Ye, et al. 2024; Zhang
et al. 2024; Vishniakov et al. 2024; Fishman et al. 2025). This book
does not aim to be the final word on any particular model; instead, it
offers a framework for understanding and comparing them.

If the book succeeds, you should finish it able to:

\begin{itemize}
\tightlist
\item
  Read a new GFM paper and place it in the landscape of data,
  architecture, objective, and application.
\item
  Design experiments that use GFMs as components---features, priors, or
  simulators---without overclaiming.
\item
  Recognize common pitfalls in training, evaluation, and deployment,
  especially in clinical and translational contexts.
\item
  Form your own views about where genomic foundation models are
  genuinely transformative, and where simpler baselines or traditional
  methods may suffice.
\end{itemize}

The rest of the book now turns to the foundations: how we get from raw
sequencing reads to high-confidence variants and the statistical tools
that predate deep learning, setting the stage for the models to come.

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

Genomics is in the middle of a quiet revolution.

For years, ``machine learning in genetics'' meant hand-crafted features,
linear models, and carefully tuned statistical pipelines. Variant
calling relied on logistic regression and hidden Markov models.
Genome-wide association studies (GWAS) and polygenic risk scores (PRS)
summarized common variant effects. Deleteriousness scores like CADD
distilled curated annotations into prioritization heuristics. Functional
genomics resources---ENCODE, GTEx, large biobanks---arrived in waves,
but each new dataset was bolted onto bespoke tools and task-specific
models.

Over roughly the last decade, that picture has changed. Convolutional
neural networks (CNNs) began to learn regulatory code directly from
sequence. Transformers and protein language models showed how
self-supervision on massive unlabeled corpora could extract rich
biological structure. Genomic foundation models (GFMs) now promise
reusable representations of DNA, RNA, protein, and multi-omic context
that can be adapted to everything from variant effect prediction to
clinical risk scoring.

This book is my attempt to make sense of that transition---\emph{from
sequence variation to genomic foundation models and back again to
clinical and biological questions}---in a way that is coherent,
historically grounded, and practically useful.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{Who This Book Is For}\label{who-this-book-is-for}
\addcontentsline{toc}{section}{Who This Book Is For}

\markright{Who This Book Is For}

This book is written for readers who sit somewhere in the triangle
between \textbf{genomics}, \textbf{statistics}, and \textbf{machine
learning}:

\begin{itemize}
\item
  \textbf{Computational biologists and statistical geneticists}\\
  who want to understand how deep learning and foundation models fit
  alongside GWAS, PRS, rare variant association, and functional
  genomics.
\item
  \textbf{Machine learning researchers and engineers}\\
  who are comfortable with CNNs, transformers, and language models, and
  want a pragmatic guide to what makes genomic data different from text
  or images.
\item
  \textbf{Clinicians and translational researchers}\\
  who increasingly see ``AI scores,'' ``foundation models,'' or
  ``genomic LLMs'' in papers, reports, and grant proposals, and want a
  grounded view of what these tools can and cannot do in practice.
\end{itemize}

Some familiarity with probability, linear models, and basic genomics
(variants, genes, regulatory elements) will help, but the early chapters
are designed to be a gentle ramp rather than a gate.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{How the Book Is Organized}\label{how-the-book-is-organized-1}
\addcontentsline{toc}{section}{How the Book Is Organized}

\markright{How the Book Is Organized}

The book is organized into five parts that mirror how the field itself
has evolved---from data and pre-deep-learning methods to CNNs,
transformers, foundation models, and finally clinical and translational
applications. :contentReference{oaicite:0}

\begin{itemize}
\item
  \textbf{Part I: Data \& Pre-DL Methods (Chapters 1--4)}\\
  We start from the raw materials: next-generation sequencing and
  variant calling, GWAS and PRS basics, classical deleteriousness
  scores, and foundational functional genomics resources. This part
  establishes the statistical and biological context that later deep
  models must respect.
\item
  \textbf{Part II: CNN Seq-to-Function Models (Chapters 5--7)}\\
  We then move into supervised sequence-to-function models like DeepSEA,
  ExPecto, and SpliceAI. These chapters show how CNNs can learn
  regulatory code, link chromatin to expression, and predict splicing,
  and they surface early lessons about architecture, context windows,
  and training data.
\item
  \textbf{Part III: Transformer Models (Chapters 8--11)}\\
  With that foundation, we broaden to representation learning and
  long-range modeling. We discuss sequence tokenization and context
  representation, protein language models, DNA foundation models, and
  hybrid architectures that couple CNNs and transformers to capture
  long-range regulatory interactions.
\item
  \textbf{Part IV: GFMs \& Multi-omics (Chapters 12--16)}\\
  Here we step back and ask: what makes a genomic model a
  \emph{foundation model}? We examine principles for designing GFMs, how
  variant effect prediction is being reshaped by foundation models, and
  how confounding, interpretability, and multi-omic integration become
  more critical as models grow larger and more general.
\item
  \textbf{Part V: Applications (Chapters 17--19)}\\
  Finally, we move from methods to workflows: clinical risk prediction,
  pathogenic variant discovery, and drug discovery/biotech applications.
  These chapters focus less on novel architectures and more on how GFMs
  plug into broader pipelines, evaluation strategies, and
  decision-making processes.
\end{itemize}

You do not have to read the book linearly. If you are already
comfortable with GWAS and classical variant scores, you might skim Part
I and start with the CNN or transformer chapters. If you care primarily
about clinical or biotech applications, you may wish to read Part V
early, referring back to earlier chapters as needed.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{Key Themes Across the Book}\label{key-themes-across-the-book}
\addcontentsline{toc}{section}{Key Themes Across the Book}

\markright{Key Themes Across the Book}

Several themes recur throughout the chapters, and the rest of the book
can be read as a deepening of each of these:

\subsection*{Data → Architecture
Evolution}\label{data-architecture-evolution}
\addcontentsline{toc}{subsection}{Data → Architecture Evolution}

We trace how \textbf{data availability and representation} drove model
design:

\begin{itemize}
\tightlist
\item
  From hand-engineered features and shallow models (e.g., CADD-style
  scores)\\
\item
  To supervised CNNs that learn regulatory motifs and grammars directly
  from sequence\\
\item
  To transformers and structured state-space models (SSMs) that operate
  on longer contexts and richer tokenizations\\
\item
  To GFMs trained to be reused across tasks, tissues, and species
\end{itemize}

At each stage, the goal is not simply ``more complex models,'' but
better alignment between \emph{what the data contain} and \emph{what the
model can express}.

\subsection*{Context Length Scaling: 1 kb → 100 kb → 1
Mb}\label{context-length-scaling-1-kb-100-kb-1-mb}
\addcontentsline{toc}{subsection}{Context Length Scaling: 1 kb → 100 kb
→ 1 Mb}

Many of the most interesting genomic questions are \textbf{non-local}:

\begin{itemize}
\tightlist
\item
  Enhancers regulate genes hundreds of kilobases away.\\
\item
  Chromatin conformation creates long-range dependencies.\\
\item
  Polygenic traits aggregate effects across the entire genome.
\end{itemize}

We follow how context windows grew from \textasciitilde1 kb
promoter-centric CNNs, to tens of kilobases in ExPecto-style models, to
100 kb--1 Mb in modern hybrid and SSM-based architectures---and how each
jump in context length changes both modeling tactics and biological
interpretation.

\subsection*{Self-Supervision: Leveraging Unlabeled Genomic
Data}\label{self-supervision-leveraging-unlabeled-genomic-data}
\addcontentsline{toc}{subsection}{Self-Supervision: Leveraging Unlabeled
Genomic Data}

Unlike labeled regulatory or clinical datasets, the \textbf{raw genome
is cheap and abundant}. We explore how self-supervised
objectives---masked language modeling, next-token prediction, denoising
tasks---allow models to learn useful representations from unlabeled
sequences, and how these representations transfer to:

\begin{itemize}
\tightlist
\item
  Variant effect prediction\\
\item
  Regulatory element annotation\\
\item
  Cross-species generalization\\
\item
  Downstream clinical prediction tasks
\end{itemize}

\subsection*{Transfer Learning: Protein LMs → DNA LMs; Human →
Cross-Species}\label{transfer-learning-protein-lms-dna-lms-human-cross-species}
\addcontentsline{toc}{subsection}{Transfer Learning: Protein LMs → DNA
LMs; Human → Cross-Species}

Genomic foundation models are fundamentally about \textbf{transfer}:

\begin{itemize}
\tightlist
\item
  From protein language models to variant effect prediction and
  structure-informed tasks\\
\item
  From human-focused models to cross-species models that incorporate
  evolutionary signal\\
\item
  From pretraining on large unlabeled corpora to fine-tuning on smaller,
  task-specific or cell-type-specific datasets
\end{itemize}

We emphasize both the opportunities and the pitfalls: where transfer
learning works remarkably well, and where naive reuse can bake in biases
or violate domain assumptions.

\subsection*{Clinical Translation: VEP, PRS, Rare Disease, and
Beyond}\label{clinical-translation-vep-prs-rare-disease-and-beyond}
\addcontentsline{toc}{subsection}{Clinical Translation: VEP, PRS, Rare
Disease, and Beyond}

Throughout the book we keep returning to a basic question:

\begin{quote}
When, if ever, do these models \emph{change decisions} in medicine and
biology?
\end{quote}

We examine how GFMs augment or reshape:

\begin{itemize}
\tightlist
\item
  Variant effect prediction pipelines for rare disease diagnosis\\
\item
  Polygenic and multi-omic risk prediction for complex traits\\
\item
  Fine-mapping and gene prioritization in drug target discovery\\
\item
  Biomarker development, patient stratification, and trial design
\end{itemize}

The focus is on workflows and evaluation: calibration, robustness,
fairness, and reproducibility in real clinical and translational
settings.

\subsection*{Interpretability: From Black Box to Mechanistic
Insight}\label{interpretability-from-black-box-to-mechanistic-insight}
\addcontentsline{toc}{subsection}{Interpretability: From Black Box to
Mechanistic Insight}

Finally, we treat interpretability not as an afterthought, but as a
central design constraint. As models grow larger and more flexible, we
ask:

\begin{itemize}
\tightlist
\item
  Can we extract motifs, grammars, and regulatory programs from sequence
  models in a way that aligns with experimental biology?\\
\item
  How can we distinguish genuine mechanism from confounded shortcuts and
  benchmark leakage?\\
\item
  What tools---from saliency and attribution to mechanistic
  interpretability---are actually useful in genomics practice?
\end{itemize}

These themes tie together discussions of confounders, evaluation, and
mechanistic modeling across Parts II--V.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{\texorpdfstring{What This Book Is
\emph{Not}}{What This Book Is Not}}\label{what-this-book-is-not}
\addcontentsline{toc}{section}{What This Book Is \emph{Not}}

\markright{What This Book Is \emph{Not}}

To keep the narrative focused, this book deliberately avoids trying to
be:

\begin{itemize}
\tightlist
\item
  A \textbf{comprehensive survey} of every published model or dataset.\\
\item
  A \textbf{software manual} for specific tools, APIs, or cloud
  platforms.\\
\item
  A \textbf{proof-heavy textbook} on statistical genetics or deep
  learning theory.
\end{itemize}

Instead, the goal is to provide a \textbf{conceptual map and a set of
worked examples} that will help you navigate the rapidly evolving space
of genomic foundation models, understand their assumptions and
limitations, and decide where they fit into your own research or
clinical practice.

Where detailed tutorials, code, or benchmarks are helpful, I point to
external resources rather than reproducing them in full.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{How to Read and Use This
Book}\label{how-to-read-and-use-this-book}
\addcontentsline{toc}{section}{How to Read and Use This Book}

\markright{How to Read and Use This Book}

A few practical suggestions:

\begin{itemize}
\item
  \textbf{Use the early chapters as shared vocabulary.}\\
  Even if you are already familiar with NGS pipelines or GWAS, skimming
  Chapters 1--4 ensures we share terminology and assumptions.
\item
  \textbf{Pick application-driven paths.}\\
  If your primary interest is clinical risk prediction, you might follow
  a path like: Chapters 2--3 → 5--7 → 10--13 → 17. For variant
  discovery: Chapters 1--4 → 9--13 → 18. For biotech and drug discovery:
  Chapters 3--4 → 9--12 → 16--19.
\item
  \textbf{Move back and forth between methods and applications.}\\
  The later application chapters are written to be readable on their
  own, but they gain depth if you revisit relevant method chapters
  (e.g., SpliceAI in Chapter 7 when thinking about splice-disrupting
  variants in Chapter 18).
\item
  \textbf{Treat the references as a roadmap.}\\
  The citations are curated to point to primary papers and
  representative follow-ups rather than exhaustive lists. Following
  those threads is often the fastest way to get from this conceptual
  overview to the cutting edge in a particular niche.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{Acknowledgments}\label{acknowledgments}
\addcontentsline{toc}{section}{Acknowledgments}

\markright{Acknowledgments}

This book grew directly out of work with the Mayo Clinic GenAI team, and
I owe a special debt of gratitude to the colleagues who made that
environment so generative.

To the \textbf{principal investigators and clinicians} who grounded our
models in real clinical questions:\\
Drs. \textbf{Shant Ayanian}, \textbf{Elena Myasoedova}, and
\textbf{Alexander Ryu}.

To \textbf{leadership} for carving out the space, support, and vision
for this work:\\
Dr.~\textbf{Matthew Callstrom}, Dr.~\textbf{Panos Korfiatis}, and
\textbf{Matt Redlon}.

To my \textbf{Data Science and Machine Learning Engineering colleagues},
whose ideas, code, and patience shaped many of the workflows and
examples in this book:\\
\textbf{Bridget Toomey}, \textbf{Carl Molnar}, \textbf{Zach Jensen}, and
\textbf{Marc Blasi}.

I am also grateful for the architectural creativity and technical depth
of our \textbf{collaborators at Cerebras}:\\
\textbf{Natalia Vassilieva}, \textbf{Jason Wolfe}, \textbf{Omid Shams
Solari}, \textbf{Vinay Pondenkandath}, \textbf{Bhargav Kanakiya}, and
\textbf{Faisal Al-khateeb}.

And to our \textbf{collaborators at GoodFire}, whose partnership helped
push these ideas closer to interpretable and deployable systems:\\
\textbf{Daniel Balsam}, \textbf{Nicholas Wang}, \textbf{Michael Pearce},
and \textbf{Mark Bissell}.

I also want to thank my former colleagues at \textbf{LGC} for
foundational work on protein language models and for the conversations
that helped shape my thinking about PLMs:\\
\textbf{Prasad Siddavatam} and \textbf{Robin Butler}.

Finally, beyond these named groups, this book owes a great deal to the
broader community of people building and using genomic models: the teams
who generated large-scale sequencing and functional genomics datasets;
the authors of classical tools like CADD and PGS; the many groups
pushing forward CNNs, transformers, and foundation models for DNA, RNA,
and protein; and the clinicians, statisticians, and experimental
biologists who keep all of us honest about what actually matters.

If this book helps you connect a new model to a real biological
question, or use genomic foundation models a bit more thoughtfully in
your own work, then it will have done its job.

--- \emph{Josh Meehl}

\part{Part I: Data \& Pre-DL Methods}

\chapter{NGS \& Variant Calling}\label{ngs-variant-calling}

\section{The Challenge of NGS Data}\label{the-challenge-of-ngs-data}

Next-generation sequencing has revolutionized genomics by enabling the
rapid generation of billions of short sequence reads from an
individual's genome. However, these reads are inherently error-prone,
with error rates ranging from approximately 0.1\% to 10\% depending on
the platform and chemistry (Poplin et al. 2018). The errors arise from a
complex process that depends on properties of the sequencing instrument,
preceding data processing tools, and the genome sequence itself.

Traditional variant calling methods like the Genome Analysis Toolkit
(GATK) rely on a combination of hand-crafted statistical models to
distinguish true genetic variants from sequencing artifacts:

\begin{itemize}
\tightlist
\item
  \textbf{Logistic regression} to model base errors
\item
  \textbf{Hidden Markov models} to compute read likelihoods
\item
  \textbf{Naive Bayes classification} to identify variants
\item
  \textbf{Gaussian mixture models} with hand-crafted features to filter
  likely false positives
\end{itemize}

While these techniques achieve high accuracy on the Illumina platform,
generalizing them to other sequencing technologies (e.g., Ion Torrent,
PacBio) has proven difficult due to the need to manually retune or
extend the statistical models for each platform's unique error profile
(Poplin et al. 2018).

\section{DeepVariant: Deep Learning for Variant
Calling}\label{deepvariant-deep-learning-for-variant-calling}

DeepVariant (Poplin et al. 2018) replaces the assortment of statistical
modeling components with a single deep convolutional neural network
(CNN), demonstrating that deep learning can learn to call genetic
variants more accurately than state-of-the-art methods without
specialized knowledge about genomics or sequencing.

\subsection{Architecture}\label{architecture}

DeepVariant uses a three-stage pipeline:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Candidate Generation}: Standard algorithmic preprocessing
  identifies candidate SNPs and indels with high sensitivity but low
  specificity.
\item
  \textbf{Pileup Image Encoding}: Read alignments around each candidate
  variant are encoded as multi-channel images (pileup images),
  capturing:

  \begin{itemize}
  \tightlist
  \item
    Reference sequence
  \item
    Read bases and quality scores
  \item
    Mapping quality
  \item
    Strand orientation
  \end{itemize}
\item
  \textbf{CNN Classification}: An Inception-architecture CNN processes
  each pileup image and emits probabilities for each of the three
  diploid genotypes (homozygous reference, heterozygous, homozygous
  alternate).
\end{enumerate}

\subsection{Learning Complex
Dependencies}\label{learning-complex-dependencies}

A key limitation of traditional variant callers like GATK is their
assumption that read errors are independent. Though this has long been
recognized as invalid, the true likelihood function that models multiple
reads simultaneously is unknown. Deep neural networks are universal
function approximators, enabling DeepVariant to learn an approximation
to this complex, interdependent likelihood function directly from data
(Poplin et al. 2018).

\subsection{Cross-Technology
Generalization}\label{cross-technology-generalization}

DeepVariant demonstrates remarkable generalization across sequencing
technologies. When retrained on platform-specific data, the model
achieves high positive predictive values (PPVs) across diverse
technologies:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Technology & Initial PPV & Final PPV & Notes \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Illumina WGS & 96.5\% & 99.9\% & Standard short-read \\
PacBio WGS & 22.1\% & 97.3\% & High indel error rate \\
SOLiD WGS & 14.3\% & 99.0\% & Color-space artifacts \\
Ion Ampliseq & 8.1\% & 99.7\% & Exome, amplification artifacts \\
\end{longtable}

The model even generalizes across species---a model trained on human
data achieved high accuracy (F1 = 98.29\%) when applied to mouse
sequencing data, outperforming training on mouse data alone (Poplin et
al. 2018).

\section{Clinical Validation}\label{clinical-validation}

In a clinical comparison using whole-exome sequencing (WES) trio
analysis of 80 families, DeepVariant demonstrated superior performance
over GATK HaplotypeCaller (Y.-L. Lin et al. 2022):

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Metric & DeepVariant & GATK & p-value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mendelian error rate & 3.09 ± 0.83\% & 5.25 ± 0.91\% & \textless{}
0.001 \\
Ti/Tv ratio & 2.38 ± 0.02 & 2.04 ± 0.07 & \textless{} 0.001 \\
Execution time (trio) & \textasciitilde1.5 hours & \textasciitilde2.5
hours & 0.046 \\
\end{longtable}

The higher transition-to-transversion (Ti/Tv) ratio achieved by
DeepVariant suggests proportionally more true positive calls, as
biological SNVs exhibit a characteristic Ti/Tv ratio near 2.1 for
exomes. The lower Mendelian error rate---variants in a child that
violate expected inheritance patterns---provides direct evidence of
improved accuracy in real clinical samples.

Both pipelines detected nearly identical sets of disease-causing
variants (62 vs.~61 of 63 total), with DeepVariant detecting one
additional variant missed by GATK due to low coverage (Y.-L. Lin et al.
2022).

\section{Significance for Genomic Deep
Learning}\label{significance-for-genomic-deep-learning}

DeepVariant established several paradigms that recur throughout this
book:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Image-based encoding}: Representing genomic data as images
  enables the application of powerful computer vision architectures.
\item
  \textbf{End-to-end learning}: Replacing hand-crafted feature
  engineering with learned representations from raw data.
\item
  \textbf{Transfer learning}: Models trained on data-rich settings
  (human genomes with ground truth) can generalize to data-poor settings
  (non-model organisms).
\item
  \textbf{Technology agnosticism}: A single architecture can adapt to
  diverse sequencing platforms through retraining, rather than requiring
  platform-specific statistical models.
\end{enumerate}

These principles---learning from data rather than encoding expert
knowledge, and generalizing across contexts---form the foundation of the
deep learning approaches to genomic interpretation covered in subsequent
chapters.

\chapter{PRS \& GWAS Basics}\label{prs-gwas-basics}

\section{The GWAS Paradigm}\label{the-gwas-paradigm}

Genome-wide association studies (GWAS) have transformed our
understanding of the genetic architecture of complex traits and
diseases. By testing millions of common genetic variants across large
populations, GWAS identify single nucleotide polymorphisms (SNPs)
statistically associated with phenotypes of interest. The NHGRI-EBI GWAS
Catalog now contains tens of thousands of robust associations across
hundreds of traits.

\subsection{Key Concepts}\label{key-concepts}

\textbf{Effect sizes and summary statistics}: For each tested variant,
GWAS produces summary statistics including:

\begin{itemize}
\tightlist
\item
  Effect size (β): The estimated change in phenotype per copy of the
  effect allele
\item
  Standard error: Uncertainty in the effect estimate
\item
  P-value: Statistical significance of the association
\item
  Allele frequency: Population prevalence of the variant
\end{itemize}

Most GWAS-identified variants have small individual effects---typically
explaining less than 1\% of phenotypic variance each---reflecting the
highly polygenic nature of complex traits.

\textbf{Linkage disequilibrium (LD)}: Variants that are physically close
on a chromosome tend to be inherited together, creating correlations in
genotype data. This LD structure means that GWAS signals often implicate
blocks of correlated variants rather than pinpointing causal variants
directly. LD patterns vary substantially across populations due to
different demographic histories, creating challenges for cross-ancestry
analyses.

\section{The Fine-Mapping Challenge}\label{the-fine-mapping-challenge}

A central challenge in post-GWAS analysis is distinguishing causal
variants from those merely correlated with causal variants through LD.
Fine-mapping methods attempt to resolve this by computing posterior
inclusion probabilities (PIPs) for each variant being causal.

Statistical fine-mapping approaches like SuSiE identify credible sets of
variants likely to contain the causal variant (Ž. Avsec et al. 2021).
For 48 well-powered traits in the UK Biobank, genome-wide fine-mapping
identified causal variants collectively explaining 17\% of SNP-based
heritability, though reaching 50\% would require approximately 2 million
samples on average (Wu et al. 2024).

The majority of GWAS-identified variants are spuriously correlated with
the phenotype through LD rather than being truly causal.
Fine-mapping---predicting which variants are causal---is crucial for
downstream tasks including uncovering biological mechanisms and
constructing robust polygenic risk scores (Rakowski and Lippert 2025).

\section{Constructing Polygenic Risk
Scores}\label{constructing-polygenic-risk-scores}

Polygenic risk scores (PRS) aggregate the effects of many genetic
variants into a single measure of an individual's genetic predisposition
to a trait or disease.

\subsection{Traditional PRS Methods}\label{traditional-prs-methods}

\textbf{Clumping and thresholding (C+T)}: The simplest approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select variants below a p-value threshold
\item
  ``Clump'' correlated variants, keeping only the most significant per
  LD block
\item
  Sum effect sizes weighted by genotype dosage
\end{enumerate}

\textbf{LDpred and Bayesian methods}: More sophisticated approaches
model the joint distribution of effect sizes while accounting for LD
structure, typically improving prediction accuracy over C+T methods.

\textbf{Linear assumptions}: Most PRS methods assume that variant
effects:

\begin{itemize}
\tightlist
\item
  Scale linearly with allele count (additive model)
\item
  Are constant across individuals
\item
  Combine additively across the genome
\end{itemize}

While computationally convenient, these assumptions increase error,
particularly for individuals from underrepresented populations
(Georgantas, Kutalik, and Richiardi 2024).

\section{Heritability and Its
Partitioning}\label{heritability-and-its-partitioning}

\textbf{SNP-based heritability} (\(h^2_{SNP}\)) estimates the proportion
of phenotypic variance explained by all measured common variants. This
is typically lower than family-based heritability estimates, a
phenomenon termed ``missing heritability.''

\textbf{LD Score Regression (LDSR)} enables partitioning of heritability
using only GWAS summary statistics. By correlating test statistics with
LD scores, LDSR can estimate:

\begin{itemize}
\tightlist
\item
  Total SNP heritability
\item
  Heritability enrichment in functional annotations
\item
  Genetic correlations between traits
\end{itemize}

Analysis of UK Biobank GWAS revealed that heritability partitions
distinctly across regulatory sequence classes, with tissue-specific
enhancers explaining substantial heritability for relevant traits---for
example, monocyte/macrophage enhancers for monocyte count, and brain
enhancers for cognitive traits (Chen et al. 2022).

\section{Limitations of Current
Approaches}\label{limitations-of-current-approaches}

\subsection{Ancestry Bias}\label{ancestry-bias}

The vast majority of GWAS have been conducted in populations of European
ancestry. This creates systematic problems:

\begin{itemize}
\tightlist
\item
  \textbf{Reduced prediction accuracy}: PRS derived from European GWAS
  perform substantially worse in non-European populations due to
  differences in LD structure and allele frequencies
\item
  \textbf{Health disparities}: Clinical deployment of ancestry-biased
  PRS could exacerbate existing health inequities
\item
  \textbf{Missed variants}: Causal variants common in non-European
  populations but rare in Europeans may be missed entirely
\end{itemize}

\subsection{Missing Heritability}\label{missing-heritability}

SNP-based heritability estimates typically capture only 20-50\% of
family-based heritability, attributed to:

\begin{itemize}
\tightlist
\item
  \textbf{Rare variants}: GWAS are underpowered to detect rare variant
  associations
\item
  \textbf{Structural variants}: Most arrays and imputation panels miss
  copy number variants and other structural variation
\item
  \textbf{Gene-gene interactions (epistasis)}: Standard additive models
  ignore non-linear interactions
\item
  \textbf{Gene-environment interactions}: Effects that depend on
  environmental context
\end{itemize}

\subsection{Mechanistic Opacity}\label{mechanistic-opacity}

GWAS associations rarely identify causal genes or mechanisms directly:

\begin{itemize}
\tightlist
\item
  Most significant variants lie in non-coding regions
\item
  The nearest gene is often not the causal gene
\item
  Regulatory effects may act over large genomic distances
\end{itemize}

\section{The Promise of Deep
Learning}\label{the-promise-of-deep-learning}

These limitations motivate the deep learning approaches covered in
subsequent chapters. Sequence-based models can:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Predict variant effects \emph{ab initio}}: Models like ExPecto
  and Enformer predict functional consequences from sequence alone,
  enabling prioritization of likely causal variants within LD blocks (J.
  Zhou et al. 2018; Ž. Avsec et al. 2021)
\item
  \textbf{Improve fine-mapping}: Functional predictions can be
  integrated with statistical fine-mapping to improve causal variant
  identification. Hybrid CNN-transformer models like Borzoi show
  superior performance in identifying causal SNPs within LD blocks
  (Manzo, Borkowski, and Ovcharenko 2025)
\item
  \textbf{Enable cross-ancestry transfer}: By learning functional
  effects from sequence rather than population-specific LD patterns,
  sequence models may generalize better across ancestries. Variants
  prioritized by sequence-based models construct PRS that transfer
  better to non-European target populations (Rakowski and Lippert 2025)
\item
  \textbf{Capture non-linear effects}: Deep learning architectures can
  model complex, non-linear relationships between genotype and
  phenotype. Methods like Delphi relax linear assumptions to produce
  more predictive PRS, with relative improvements of 11-35\% in variance
  explained across traits (Georgantas, Kutalik, and Richiardi 2024)
\item
  \textbf{Incorporate rare variants}: Approaches like DeepRVAT learn
  gene impairment scores from rare variant annotations, facilitating
  refinement of PRS by accounting for rare variant effects that standard
  GWAS miss.
\end{enumerate}

The chapters that follow trace the development of these capabilities,
from early CNN-based regulatory prediction through modern transformer
architectures that integrate long-range genomic context.

\chapter{Deleteriousness Scores}\label{deleteriousness-scores}

\section{The Variant Prioritization
Challenge}\label{the-variant-prioritization-challenge}

The human genome harbors millions of genetic variants, the vast majority
of which have unknown functional consequences. While Chapter 1 addressed
distinguishing true variants from sequencing artifacts and Chapter 2
examined statistical association with phenotypes, a complementary
challenge remains: predicting which variants are likely deleterious
across the entire genome, including both coding and non-coding regions.

Traditional approaches to variant prioritization suffer from critical
limitations:

\begin{itemize}
\tightlist
\item
  \textbf{Single-feature scores}: Most methods exploit a single
  information type (e.g., conservation) and cannot integrate diverse
  signals
\item
  \textbf{Restricted scope}: Many tools focus exclusively on missense
  variants or specific genomic contexts
\item
  \textbf{Training set bias}: Supervised methods trained on ClinVar or
  HGMD pathogenic variants inherit biases toward well-studied genes and
  variant types
\end{itemize}

Combined Annotation-Dependent Depletion (CADD) addresses these
limitations through a fundamentally different approach: rather than
training directly on known pathogenic variants, CADD learns to
distinguish variants that survived evolutionary selection from those
that did not (Rentzsch et al. 2019).

\section{The Evolutionary Proxy Training
Strategy}\label{the-evolutionary-proxy-training-strategy}

CADD's innovation lies in its training data construction. Rather than
relying on curated pathogenic/benign labels---which are sparse, biased,
and incomplete---CADD exploits evolutionary history as a proxy for
deleteriousness.

\subsection{Proxy-Neutral Variants}\label{proxy-neutral-variants}

The proxy-neutral training set consists of approximately 15 million SNVs
and 1.8 million indels that are:

\begin{itemize}
\tightlist
\item
  \textbf{Human-derived}: Present in modern humans but absent in the
  inferred human-ape ancestor genome
\item
  \textbf{Fixed or nearly fixed}: Allele frequency of 95--100\% in human
  populations
\end{itemize}

These variants have persisted through millions of years of purifying
selection since the human-chimpanzee divergence. By virtue of reaching
fixation, they are overwhelmingly neutral or at most weakly
deleterious---strong deleterious effects would have been purged by
natural selection.

\subsection{Proxy-Deleterious
Variants}\label{proxy-deleterious-variants}

The proxy-deleterious set is generated through simulation, matching the
sequence composition, substitution frequencies, and local mutation rate
patterns of the proxy-neutral variants. These simulated ``de novo''
variants are free from selective pressure---they represent the full
spectrum of possible mutations, including those that would be
deleterious in a real genome.

While many simulated variants are indeed neutral, an unknown but
substantial fraction would be harmful if they occurred in an individual.
The key insight is that the \emph{relative enrichment} of deleterious
variants differs systematically between the two sets: proxy-neutral
variants are depleted for deleterious alleles, while proxy-deleterious
variants contain a representative sample.

\subsection{Training Objective}\label{training-objective}

A machine learning classifier trained to distinguish proxy-neutral from
proxy-deleterious variants effectively learns which annotation features
characterize variants likely to be ``observed'' (survived selection)
versus ``simulated'' (potentially deleterious). Higher CADD scores
indicate variants more similar to the simulated set---and thus more
likely to have functional consequences.

This approach provides several advantages over pathogenicity-based
training:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Aspect & CADD Approach & Pathogenicity Training \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Training set size & \textasciitilde30 million variants & Thousands of
variants \\
Genome coverage & All regions equally & Biased toward coding \\
Gene bias & Minimal & Concentrated in disease genes \\
Variant type bias & Minimal & Biased toward missense \\
\end{longtable}

\section{Integration of Diverse
Annotations}\label{integration-of-diverse-annotations}

CADD's strength derives from integrating more than 60 distinct genomic
annotations into a unified framework. These annotations span multiple
categories:

\subsection{Gene Model Annotations}\label{gene-model-annotations}

Ensembl Variant Effect Predictor (VEP) provides consequence predictions
including:

\begin{itemize}
\tightlist
\item
  Transcript location (exon, intron, UTR, intergenic)
\item
  Protein-coding effects (synonymous, missense, nonsense, frameshift)
\item
  Distance to splice sites and transcript features
\item
  Amino acid properties for missense variants
\end{itemize}

\subsection{Conservation and
Constraint}\label{conservation-and-constraint}

Multiple evolutionary conservation scores capture selective pressure
across different timescales:

\begin{itemize}
\tightlist
\item
  \textbf{PhyloP}: Position-specific conservation across 46 vertebrate
  species
\item
  \textbf{PhastCons}: Probability of negative selection estimated from
  multiple alignments
\item
  \textbf{GERP++}: Rejected substitution scores indicating purifying
  selection
\item
  \textbf{GerpN/GerpS}: Neutral evolution and constrained element scores
\end{itemize}

\subsection{Epigenetic and Regulatory
Activity}\label{epigenetic-and-regulatory-activity}

Data from ENCODE and NIH Roadmap Epigenomics provide chromatin state
information:

\begin{itemize}
\tightlist
\item
  DNase I hypersensitivity
\item
  Histone modifications (H3K4me1, H3K4me3, H3K27ac, etc.)
\item
  Transcription factor binding sites
\item
  Chromatin accessibility across cell types
\end{itemize}

\subsection{Additional Features}\label{additional-features}

\begin{itemize}
\tightlist
\item
  CpG dinucleotide context and mutation rates
\item
  Sequence properties (GC content, repeat regions)
\item
  Population genetics metrics (allele frequency distributions)
\item
  Protein domain annotations
\end{itemize}

\section{Model Architecture and
Scoring}\label{model-architecture-and-scoring}

\subsection{Machine Learning
Framework}\label{machine-learning-framework}

CADD v1.0 employed a support vector machine (SVM) with a linear kernel.
Subsequent versions transitioned to logistic regression, which provides:

\begin{itemize}
\tightlist
\item
  Faster training and inference
\item
  Interpretable feature weights
\item
  Efficient handling of crossed features
\end{itemize}

To capture non-linear relationships between annotations, CADD creates
crossed feature annotations---for example, combining conservation scores
with consequence labels to allow the model to weight conservation
differently for missense versus synonymous variants.

\subsection{PHRED-Scaled Scores}\label{phred-scaled-scores}

Raw CADD scores are difficult to interpret across genomic contexts. To
enable meaningful comparisons, CADD transforms raw scores into
PHRED-like scaled scores based on genome-wide rank:

\[\text{CADD}_\text{PHRED} = -10 \cdot \log_{10}\left(\frac{\text{rank}}{N}\right)\]

where \(N\) ≈ 9 billion represents all possible single nucleotide
substitutions in the reference genome. A CADD score of 10 indicates the
variant is in the top 10\% most deleterious; a score of 20 indicates the
top 1\%; a score of 30 indicates the top 0.1\%.

This scaling provides intuitive interpretation: CADD 15--20 typically
indicates likely functional variants, while CADD \textgreater{} 30
suggests strong deleteriousness.

\section{CADD v1.7: Integration of Deep Learning
Predictions}\label{cadd-v1.7-integration-of-deep-learning-predictions}

The most recent CADD release (v1.7) demonstrates how the annotation
integration framework can incorporate predictions from deep learning
models (Schubach et al. 2024). This represents a bridge between
traditional annotation-based approaches and the sequence-to-function
models covered in subsequent chapters.

\subsection{Protein Language Model
Features}\label{protein-language-model-features}

CADD v1.7 incorporates scores from Meta ESM-1v, a protein language model
trained on millions of protein sequences. ESM-1v provides:

\begin{itemize}
\tightlist
\item
  Per-residue fitness predictions based on evolutionary plausibility
\item
  Improved missense variant prioritization
\item
  Context-aware amino acid substitution scoring
\end{itemize}

These features substantially improve CADD's performance on coding
variants, particularly for missense mutations in conserved protein
regions.

\subsection{Regulatory CNN
Predictions}\label{regulatory-cnn-predictions}

For non-coding variants, CADD v1.7 integrates predictions from a custom
convolutional neural network trained on regions of open chromatin. This
model:

\begin{itemize}
\tightlist
\item
  Predicts regulatory activity from DNA sequence
\item
  Provides variant effect scores for regulatory elements
\item
  Captures sequence patterns associated with transcription factor
  binding
\end{itemize}

\subsection{Extended Conservation
Scores}\label{extended-conservation-scores}

The Zoonomia project's alignment of \textgreater240 mammalian genomes
provides deeper evolutionary context through:

\begin{itemize}
\tightlist
\item
  Broader phylogenetic sampling
\item
  Improved resolution of constrained elements
\item
  Better detection of lineage-specific selection
\end{itemize}

\subsection{Performance Improvements}\label{performance-improvements}

Evaluation on multiple benchmarks demonstrates consistent gains:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Benchmark & CADD v1.6 & CADD v1.7 & Improvement \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ClinVar pathogenic vs.~common & 0.94 & 0.95 & +1\% \\
Deep mutational scanning (31 datasets) & 0.78 & 0.81 & +3\% \\
Regulatory saturation mutagenesis & 0.68 & 0.72 & +4\% \\
\end{longtable}

The regulatory variant improvements are particularly notable, as this
has historically been CADD's weakest domain.

\section{Benchmarking Against Alternative
Approaches}\label{benchmarking-against-alternative-approaches}

CADD's genome-wide applicability allows direct comparison with
specialized scores:

\subsection{Coding Variants}\label{coding-variants}

For missense variant classification separating ClinVar pathogenic from
high-frequency ExAC variants, CADD performs comparably to dedicated
tools like PolyPhen-2 and PROVEAN while maintaining broader
applicability. The integration of ESM-1v features in v1.7 further
improves coding variant prediction.

\subsection{Non-coding Variants}\label{non-coding-variants}

CADD demonstrates strong performance on regulatory variants, though
recent benchmarks suggest functional-genomics-supervised models like
Enformer and Borzoi may perform better for certain variant classes.
Interestingly, alignment-based models including CADD and GPN-MSA compare
favorably for Mendelian and complex disease traits, while
expression-predicting models excel for complex non-disease traits
(Benegas, Eraslan, and Song 2025).

\subsection{Population Frequency
Correlation}\label{population-frequency-correlation}

A key validation of CADD's evolutionary proxy approach is the strong
inverse correlation between CADD scores and population allele frequency.
Higher-scoring variants are systematically rarer in population databases
like gnomAD, consistent with purifying selection removing deleterious
alleles---the same principle underlying the training strategy.

\section{Significance for Genomic Deep
Learning}\label{significance-for-genomic-deep-learning-1}

CADD established several important paradigms that inform the deep
learning approaches covered in subsequent chapters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Annotation integration}: The principle of combining multiple
  information sources into unified predictions extends naturally to
  multi-headed neural network architectures that jointly predict diverse
  genomic features.
\item
  \textbf{Evolutionary training signals}: Using evolutionary
  conservation as a proxy for function---rather than curated
  pathogenicity labels---avoids biases and provides genome-wide
  coverage. This approach reappears in protein language models and
  genomic foundation models.
\item
  \textbf{Genome-wide applicability}: CADD demonstrated that a single
  framework could score any variant anywhere in the genome, setting
  expectations for the comprehensive coverage that sequence-to-function
  models now achieve.
\item
  \textbf{Continuous improvement through feature addition}: The
  progression from CADD v1.0 to v1.7 shows how deep learning predictions
  can be incorporated as features into existing frameworks, providing a
  practical integration path.
\end{enumerate}

The chapters that follow trace how deep learning models moved from
providing features to CADD-like integrators toward directly predicting
variant effects from DNA sequence---learning the regulatory grammar that
explains \emph{why} evolutionary conservation patterns emerge.

\chapter{Foundational Genomics Data}\label{foundational-genomics-data}

\section{Functional Genomics Data}\label{functional-genomics-data}

The preceding chapters established methods for identifying genetic
variants (Chapter 1), associating them with phenotypes (Chapter 2), and
scoring their likely deleteriousness (Chapter 3). These approaches and
the deep learning models that follow depend on large-scale public data
resources spanning chromatin profiling, population genetics, clinical
variant databases, expression atlases, biobanks, and experimental
benchmarks.

This chapter surveys the foundational data resources that underpin
modern genomic machine learning. These datasets serve multiple roles:
training targets for supervised models, validation benchmarks for
variant effect prediction, and population references for interpreting
rare variants.

\section{Chromatin Profiling: ENCODE and Roadmap
Epigenomics}\label{chromatin-profiling-encode-and-roadmap-epigenomics}

The ENCODE (Encyclopedia of DNA Elements) (Kagda et al. 2025) and
Roadmap Epigenomics consortia generated genome-wide profiles of
regulatory activity across hundreds of cell types, providing both
biological insight and training data for sequence-to-function models.

\subsection{Data Types and Scale}\label{data-types-and-scale}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1600}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3400}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3600}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Assay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Target
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ENCODE Coverage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Roadmap Coverage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
DNase-seq / ATAC-seq & Open chromatin & 684 datasets & 53 samples \\
ChIP-seq (TF) & Transcription factor binding & 2,131 datasets & --- \\
ChIP-seq (histone) & Histone modifications & 1,860 datasets & 127
samples \\
CAGE & Transcription start sites & 638 datasets & --- \\
\end{longtable}

The Cistrome Project systematically reprocesses public ChIP-seq data,
contributing \textasciitilde20,000 profiles that supplement ENCODE
(Zheng et al. 2019). Combined, these resources enable models like Sei to
predict 21,907 chromatin targets across \textgreater1,300 cell types
(Chen et al. 2022).

\subsection{Key Histone Modifications}\label{key-histone-modifications}

Histone modifications mark distinct regulatory states:

\begin{itemize}
\tightlist
\item
  \textbf{H3K4me3}: Active promoters
\item
  \textbf{H3K4me1 + H3K27ac}: Active enhancers
\item
  \textbf{H3K4me1 + H3K27me3}: Poised/bivalent enhancers
\item
  \textbf{H3K27me3}: Polycomb-repressed regions
\item
  \textbf{H3K9me3}: Constitutive heterochromatin
\end{itemize}

These combinatorial patterns define the training targets for models
including DeepSEA, ExPecto, and Enformer (Chapters 5, 6, and 11).

\section{Population Genetics
Resources}\label{population-genetics-resources}

\subsection{gnomAD}\label{gnomad}

The Genome Aggregation Database (gnomAD) aggregates exome and genome
sequencing data from large-scale projects, providing allele frequencies
and constraint metrics essential for variant interpretation.

\textbf{Current scale}: gnomAD v4 includes \textgreater800,000 exomes
and \textgreater76,000 genomes spanning diverse global populations.

\textbf{Key applications for deep learning}:

\begin{itemize}
\tightlist
\item
  \emph{Allele frequency filtering}: Rare variants (AF \textless{}
  0.01\%) are enriched for functional effects; common variants have
  largely survived purifying selection
\item
  \emph{Constraint metrics}: pLI (probability of loss-of-function
  intolerance) and LOEUF (loss-of-function observed/expected upper bound
  fraction) quantify selective pressure on genes
\item
  \emph{Training signal}: CADD's evolutionary proxy approach uses allele
  frequency distributions to validate that predicted deleterious
  variants are depleted from the population (Rentzsch et al. 2019)
\end{itemize}

\subsection{1000 Genomes Project}\label{genomes-project}

The 1000 Genomes Project catalogued genetic variation across 26
populations, providing:

\begin{itemize}
\tightlist
\item
  Reference panel for genotype imputation
\item
  LD structure for fine-mapping and PRS construction
\item
  Benchmarking variants for sequence models (e.g., Sei variant effect
  analyses use 1000 Genomes SNPs) (Chen et al. 2022)
\end{itemize}

\section{Clinical Variant Databases}\label{clinical-variant-databases}

\subsection{ClinVar}\label{clinvar}

ClinVar is the primary public archive for clinically interpreted genetic
variants, aggregating submissions from clinical laboratories, research
groups, and expert panels.

\textbf{Content}: \textgreater2.5 million variant submissions covering
pathogenic, likely pathogenic, benign, likely benign, and variants of
uncertain significance (VUS).

\textbf{Role in model development}:

\begin{itemize}
\tightlist
\item
  \emph{Benchmarking}: ClinVar pathogenic/benign classifications provide
  standard evaluation sets for variant effect predictors. CADD v1.7
  reports \textasciitilde1\% improvement on ClinVar variants compared to
  previous versions (Schubach et al. 2024).
\item
  \emph{Training labels}: Some supervised approaches use ClinVar
  annotations as training targets, though this risks circularity if
  evaluated on the same database.
\end{itemize}

\textbf{Limitations}: Ascertainment bias toward well-studied genes;
classification criteria vary across submitters; benign variants are
underrepresented relative to pathogenic.

\subsection{HGMD}\label{hgmd}

The Human Gene Mutation Database provides curated disease-causing
mutations, though access requires subscription. HGMD's ``regulatory''
category has been used to benchmark non-coding variant effect
predictions (Chen et al. 2022).

\section{Expression Resources}\label{expression-resources}

\subsection{GTEx}\label{gtex}

The Genotype-Tissue Expression (GTEx) project provides RNA-seq
expression profiles and eQTL maps across 54 human tissues from
\textasciitilde1,000 donors.

\textbf{Key applications}:

\begin{itemize}
\tightlist
\item
  \emph{eQTL validation}: Signed linkage disequilibrium profile (SLDP)
  regression tests whether model-predicted variant effects correlate
  with measured expression changes. Enformer showed improved SLDP
  Z-scores relative to Basenji2 across GTEx tissues (Ž. Avsec et al.
  2021).
\item
  \emph{Tissue-specific expression}: Enhancer sequence class scores
  correlate with tissue-specific gene expression in corresponding
  tissues (Chen et al. 2022).
\item
  \emph{Cross-ancestry analysis}: GTEx's population diversity enables
  evaluation of model generalization.
\end{itemize}

\subsection{FANTOM}\label{fantom}

The FANTOM consortium's CAGE atlas maps transcription start sites at
single-nucleotide resolution across hundreds of human and mouse samples.
CAGE data serve as direct training targets for expression-predicting
models---Enformer predicts 638 CAGE tracks, enabling TSS-level activity
predictions (Ž. Avsec et al. 2021).

\section{Biobank Resources}\label{biobank-resources}

Large-scale biobanks linking genomic data to electronic health records
and phenotypic measurements are transforming genetic research. These
resources enable GWAS discovery, PRS development, and validation of
variant effect predictions at population scale.

\subsection{UK Biobank}\label{uk-biobank}

The UK Biobank enrolled \textasciitilde500,000 participants aged 40--69
from across the United Kingdom, with comprehensive phenotyping and
genomic data.

\textbf{Data available}:

\begin{itemize}
\tightlist
\item
  Genotyping array data (all participants)
\item
  Whole exome sequencing (\textasciitilde470,000 participants)
\item
  Whole genome sequencing (\textasciitilde500,000 participants)
\item
  Electronic health records, imaging, and extensive phenotypic
  measurements
\end{itemize}

\textbf{Applications}: UK Biobank GWAS summary statistics are used for
heritability partitioning by sequence classes, revealing trait-specific
regulatory architectures (Chen et al. 2022). Genome-wide fine-mapping of
48 UK Biobank traits identified causal variants explaining 17\% of
SNP-based heritability (Wu et al. 2024).

\subsection{All of Us Research
Program}\label{all-of-us-research-program}

The NIH All of Us Research Program aims to enroll one million diverse US
participants, with explicit emphasis on historically underrepresented
populations.

\textbf{Current scale}: \textgreater245,000 whole genome sequences, with
\textasciitilde45\% from participants identifying with underrepresented
racial or ethnic groups. The dataset includes \textgreater1 billion
genetic variants, including \textgreater275 million previously
unreported variants.

\textbf{Unique features}:

\begin{itemize}
\tightlist
\item
  \emph{Diversity}: 77\% of participants are from communities
  historically underrepresented in biomedical research
\item
  \emph{Longitudinal EHR}: Linked electronic health records with median
  10+ years of data for many participants
\item
  \emph{Multi-modal data}: Surveys, physical measurements, Fitbit
  wearable data, and genomics
\item
  \emph{Rapid access}: Researcher Workbench enables data access with
  median 29 hours from registration
\end{itemize}

All of Us addresses a critical gap: over 90\% of participants in prior
large genomics studies were of European descent, limiting
generalizability of findings and perpetuating health disparities.

\subsection{Mayo Clinic Tapestry}\label{mayo-clinic-tapestry}

The Tapestry study represents Mayo Clinic's largest genomics initiative,
combining clinical exome sequencing with longitudinal EHR linkage.

\textbf{Scale}: \textgreater98,000 enrolled participants with Exome+
sequencing (whole exome plus \textasciitilde300,000 informative
non-coding SNPs).

\textbf{Clinical integration}: Unlike research-only biobanks, Tapestry
returns actionable results directly to participants and their providers:

\begin{itemize}
\tightlist
\item
  1.9\% of participants carry pathogenic/likely pathogenic variants in
  CDC Tier 1 genes
\item
  Results for BRCA1/2 (hereditary breast/ovarian cancer), Lynch syndrome
  genes, and familial hypercholesterolemia genes are entered into EHRs
\end{itemize}

\textbf{Research access}: \textgreater62,000 participants' exome data
are available for research, with 82 approved investigator requests
delivering \textgreater1.1 million datasets.

\subsection{Comparative Summary}\label{comparative-summary}

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Biobank & Participants & Sequencing & Diversity & EHR Linked \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
UK Biobank & \textasciitilde500,000 & WGS + WES & Primarily European &
Yes \\
All of Us & \textgreater245,000 WGS & WGS & 45\% underrepresented &
Yes \\
Mayo Tapestry & \textasciitilde98,000 & Exome+ & \textasciitilde11\%
underrepresented & Yes \\
\end{longtable}

\section{Experimental Benchmarks}\label{experimental-benchmarks}

\subsection{Deep Mutational Scanning}\label{deep-mutational-scanning}

Deep mutational scanning (DMS) experiments systematically measure the
functional effects of thousands of variants in a single assay, providing
gold-standard labels for benchmarking variant effect predictors.

\textbf{ProteinGym}: A comprehensive benchmark aggregating DMS data
across 217 assays covering diverse proteins. CADD v1.7 reports
\textasciitilde3\% improvement on DMS datasets with integration of
ESM-1v protein language model scores (Schubach et al. 2024).

\textbf{MaveDB}: The Multiplexed Assays of Variant Effect Database
archives functional scores from MAVE experiments, enabling standardized
model evaluation.

\textbf{CAGI}: The Critical Assessment of Genome Interpretation
organizes blinded prediction challenges using experimental data,
including saturation mutagenesis of regulatory elements.

\subsection{TraitGym}\label{traitgym}

TraitGym provides curated regulatory variant benchmarks for evaluating
causal variant prediction across Mendelian and complex traits (Benegas,
Eraslan, and Song 2025).

\textbf{Dataset composition}:

\begin{itemize}
\tightlist
\item
  113 Mendelian traits with known or high-confidence causal regulatory
  variants
\item
  83 complex traits with fine-mapped candidate causal variants
\item
  Carefully constructed control variants matched for genomic context
\end{itemize}

\textbf{Key findings}: Alignment-based models (CADD, GPN-MSA) perform
favorably for Mendelian and complex disease traits, while
functional-genomics-supervised models (Enformer, Borzoi) excel for
complex non-disease traits. This suggests complementary information
captured by evolutionary constraint versus molecular phenotype
prediction.

TraitGym addresses a critical gap: the field has lacked consistently
curated datasets with accurate labels for non-coding variants, making
comprehensive benchmarking difficult.

\subsection{Massively Parallel Reporter
Assays}\label{massively-parallel-reporter-assays}

MPRAs test thousands of regulatory sequences in parallel, measuring
their ability to drive reporter gene expression. MPRA data from
saturation mutagenesis experiments serve as benchmarks for regulatory
variant effect prediction---CADD v1.7 shows \textasciitilde4\%
improvement on regulatory saturation mutagenesis data (Schubach et al.
2024).

\section{Data Quality Considerations}\label{data-quality-considerations}

\subsection{Technical Artifacts}\label{technical-artifacts}

Functional genomics data contain systematic biases affecting model
training:

\begin{itemize}
\tightlist
\item
  \textbf{Batch effects}: Technical variation between experiments
\item
  \textbf{Antibody specificity}: ChIP-seq quality depends on antibody
  performance
\item
  \textbf{Peak calling thresholds}: Binary calls from continuous signals
  involve arbitrary cutoffs
\item
  \textbf{Cell line artifacts}: Immortalized lines may not reflect
  primary tissue biology
\end{itemize}

\subsection{Ascertainment Bias}\label{ascertainment-bias}

Clinical databases and biobanks exhibit ascertainment biases:

\begin{itemize}
\tightlist
\item
  ClinVar is enriched for variants in well-studied disease genes
\item
  Biobanks may oversample certain demographics or health-seeking
  populations
\item
  Benign variants are systematically underrepresented in clinical
  databases
\end{itemize}

\subsection{Population Structure}\label{population-structure}

Ancestry-specific allele frequencies and LD patterns affect model
generalization:

\begin{itemize}
\tightlist
\item
  PRS developed in European populations transfer poorly to other
  ancestries
\item
  Rare variant interpretation depends on population-matched frequency
  data
\item
  Initiatives like All of Us explicitly address historical
  underrepresentation
\end{itemize}

\section{From Data to Models}\label{from-data-to-models}

The resources described in this chapter enable the supervised learning
framework underlying sequence-to-function models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Chromatin profiles} (ENCODE, Roadmap) → Training targets for
  predicting regulatory activity from sequence
\item
  \textbf{Population genetics} (gnomAD, 1000 Genomes) → Evolutionary
  signal for constraint-based scoring
\item
  \textbf{Clinical databases} (ClinVar) → Benchmarking pathogenic
  variant detection
\item
  \textbf{Expression data} (GTEx, FANTOM) → Validation of predicted
  expression effects
\item
  \textbf{Biobanks} (UK Biobank, All of Us, Tapestry) → GWAS discovery
  and phenotype prediction
\item
  \textbf{Experimental benchmarks} (DMS, TraitGym, MPRA) → Ground-truth
  functional measurements
\end{enumerate}

The chapters that follow trace how deep learning models leverage these
resources: CNNs learning to predict chromatin profiles (DeepSEA, Chapter
5), connecting predictions to expression (ExPecto, Chapter 6), and
integrating long-range interactions (Enformer, Chapter 11).

\part{Part II: CNN Seq-to-Function Models}

\chapter{Regulatory Prediction}\label{regulatory-prediction}

\section{The Noncoding Variant
Challenge}\label{the-noncoding-variant-challenge}

The vast majority of disease-associated variants identified by GWAS lie
in noncoding regions of the genome. Yet in 2015, the field lacked
systematic methods to predict how these variants affect gene regulation.
Existing approaches relied on overlap with known annotations---if a
variant fell within a ChIP-seq peak or DNase hypersensitive site, it
might be flagged as potentially functional. But this strategy offered no
mechanism for predicting the \emph{direction} or \emph{magnitude} of
effect, and it could not score variants in regions lacking experimental
coverage.

DeepSEA, introduced by Zhou and Troyanskaya in 2015, fundamentally
changed this paradigm by learning to predict chromatin features directly
from DNA sequence (J. Zhou and Troyanskaya 2015). Rather than asking
``does this variant overlap a known regulatory element?'', DeepSEA asks
``what regulatory activities does this sequence encode, and how would a
mutation change them?''

\section{The Core Innovation: Learning Regulatory Code from
Sequence}\label{the-core-innovation-learning-regulatory-code-from-sequence}

DeepSEA's central insight was that deep convolutional networks could
learn the sequence patterns underlying regulatory activity without
explicit feature engineering. Previous methods like gapped k-mer SVMs
(gkm-SVM) required defining sequence features a priori---specifying
which k-mers to count and how to weight them. DeepSEA instead learned
relevant sequence features automatically from data.

\subsection{Architecture}\label{architecture-1}

The original DeepSEA architecture comprised:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Input layer}: 1000 bp DNA sequence, one-hot encoded (4
  channels × 1000 positions)
\item
  \textbf{Three convolutional layers}: Each followed by ReLU activation
  and max pooling, learning increasingly abstract sequence features
\item
  \textbf{Fully connected layer}: Integrating learned features across
  the sequence
\item
  \textbf{Output layer}: 919 sigmoid outputs predicting chromatin
  profile probabilities
\end{enumerate}

The convolutional layers function analogously to motif scanners, but
with crucial differences: they learn motifs from data rather than
requiring predefined position weight matrices, and deeper layers can
learn combinations of motifs (regulatory ``grammar'') rather than just
individual binding sites.

\subsection{Training Data}\label{training-data}

DeepSEA was trained on 919 chromatin profiles from ENCODE and Roadmap
Epigenomics:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Profile Type & Count & Examples \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Transcription factor binding & 690 & CTCF, p53, GATA1 \\
Histone modifications & 104 & H3K4me3, H3K27ac \\
DNase I hypersensitivity & 125 & Open chromatin across cell types \\
\end{longtable}

For each 1000 bp sequence, the model predicts the probability that the
central 200 bp region exhibits each chromatin feature. Training used
sequences from the human genome with chromosome 8 held out for testing.

\subsection{Multi-Task Learning}\label{multi-task-learning}

A key architectural decision was predicting all 919 features
simultaneously rather than training separate models. This multi-task
learning approach offers several advantages:

\begin{itemize}
\tightlist
\item
  \textbf{Shared representations}: Early convolutional layers learn
  general sequence features (e.g., GC content, common motifs) useful
  across tasks
\item
  \textbf{Regularization}: Jointly predicting correlated features
  prevents overfitting to any single task
\item
  \textbf{Efficiency}: One model serves all prediction tasks
\end{itemize}

\section{Predicting Variant Effects}\label{predicting-variant-effects}

DeepSEA enables variant effect prediction through a straightforward
procedure: predict chromatin profiles for both reference and alternative
allele sequences, then compute the difference. This produces a
919-dimensional vector describing how the variant is predicted to alter
regulatory activity across all profiled features.

\subsection{Single-Nucleotide
Sensitivity}\label{single-nucleotide-sensitivity}

The model achieves single-nucleotide sensitivity---changing one base can
substantially alter predictions. This was validated using allelic
imbalance data from digital genomic footprinting. For 57,407 variants
showing allele-specific DNase I sensitivity across 35 cell types,
DeepSEA predictions correlated strongly with the experimentally observed
allelic bias.

\subsection{In Silico Saturation
Mutagenesis}\label{in-silico-saturation-mutagenesis}

By systematically predicting effects of all possible single-nucleotide
substitutions within a sequence, DeepSEA enables ``in silico saturation
mutagenesis'' (ISM). This computational experiment reveals which
positions are most critical for regulatory function---equivalent to a
CRISPR tiling screen, but performed entirely computationally.

ISM analysis of regulatory elements reveals sequence positions where
mutations would most strongly perturb function, often corresponding to
transcription factor binding motifs learned by the model.

\section{Functional Variant
Prioritization}\label{functional-variant-prioritization}

Beyond predicting chromatin effects, DeepSEA introduced a framework for
prioritizing likely functional variants among large sets of candidates.

\subsection{eQTL Prioritization}\label{eqtl-prioritization}

Expression quantitative trait loci (eQTLs) represent variants associated
with gene expression changes. However, most eQTL signals reflect linkage
disequilibrium rather than causal variants. DeepSEA demonstrated
improved ability to distinguish true eQTLs from nearby non-causal
variants compared to overlap-based methods.

\subsection{GWAS Variant
Prioritization}\label{gwas-variant-prioritization}

Similarly, for GWAS-identified disease associations, DeepSEA helped
prioritize which variants in LD blocks were most likely causal. The
model outperformed contemporary methods including GWAVA (which was
trained on known regulatory mutations) on held-out benchmarks.

\subsection{Comparison to Prior
Methods}\label{comparison-to-prior-methods}

DeepSEA's performance advantage over gkm-SVM was particularly notable
for transcription factor binding prediction:

\begin{itemize}
\tightlist
\item
  Deep CNN achieved higher AUC for nearly all transcription factors
\item
  gkm-SVM showed no improvement with increased context sequence length
\item
  DeepSEA performance improved substantially with context (200 bp → 500
  bp → 1000 bp)
\end{itemize}

This demonstrated that the deep learning architecture could exploit
longer-range sequence context that simpler models could not capture.

\section{Evolution of the DeepSEA
Framework}\label{evolution-of-the-deepsea-framework}

The original DeepSEA established the sequence-to-chromatin prediction
paradigm. Subsequent work from the same group expanded and refined this
approach.

\subsection{DeepSEA Beluga (2018)}\label{deepsea-beluga-2018}

ExPecto, published in 2018, included an updated chromatin prediction
model nicknamed ``Beluga'' (J. Zhou et al. 2018). Key improvements
included:

\begin{itemize}
\tightlist
\item
  \textbf{Expanded prediction targets}: 2,002 chromatin profiles (up
  from 919)
\item
  \textbf{Deeper architecture}: Additional convolutional layers with
  residual connections
\item
  \textbf{Larger context}: 2000 bp input sequences
\item
  \textbf{Integration with expression prediction}: Chromatin predictions
  serve as intermediate features for tissue-specific expression
  prediction (Chapter 6)
\end{itemize}

\subsection{Sei (2022)}\label{sei-2022}

Sei represents the current state of the DeepSEA lineage, predicting
21,907 chromatin profiles---a 24-fold expansion over the original (Chen
et al. 2022). Architectural innovations include:

\begin{itemize}
\tightlist
\item
  \textbf{Dual linear/nonlinear paths}: Parallel convolution blocks, one
  with activation functions and one without, allowing the model to learn
  both complex nonlinear patterns and simpler linear relationships
\item
  \textbf{Dilated convolutions}: Expanding receptive field without
  reducing spatial resolution
\item
  \textbf{Spatial basis functions}: Memory-efficient integration of
  information across positions
\end{itemize}

Sei improved over Beluga by 19\% on average (measured by
AUROC/(1-AUROC)) on the 2,002 profiles predicted by both models.

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
Model & Year & Chromatin Targets & Input Length & Architecture \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
DeepSEA & 2015 & 919 & 1000 bp & 3 conv + FC \\
Beluga & 2018 & 2,002 & 2000 bp & Deep residual CNN \\
Sei & 2022 & 21,907 & 4000 bp & Dual-path + dilated conv \\
\end{longtable}

\section{What DeepSEA Learns}\label{what-deepsea-learns}

\subsection{Motif Discovery}\label{motif-discovery}

Analysis of DeepSEA's convolutional filters reveals learned sequence
patterns corresponding to known transcription factor binding motifs.
First-layer filters often match canonical motifs from databases like
JASPAR, while deeper layers capture more complex patterns including
motif combinations.

\subsection{Regulatory Grammar}\label{regulatory-grammar}

Beyond individual motifs, DeepSEA implicitly learns aspects of
regulatory ``grammar''---the rules governing how motifs combine to
produce regulatory activity. This includes:

\begin{itemize}
\tightlist
\item
  \textbf{Motif spacing}: Some TF pairs require specific distances
  between binding sites
\item
  \textbf{Motif orientation}: Directionality of certain motifs affects
  function
\item
  \textbf{Combinatorial logic}: Multiple weak motifs can synergize, or
  compete through overlapping sites
\end{itemize}

However, the original DeepSEA architecture's limited receptive field
(due to pooling operations) constrained its ability to learn long-range
dependencies. This limitation motivated later architectures with
expanded context windows (Enformer, Chapter 11).

\section{Limitations and
Considerations}\label{limitations-and-considerations}

\subsection{Cell Type Specificity}\label{cell-type-specificity}

DeepSEA predicts chromatin profiles for specific cell types included in
training, but the same sequence may have different regulatory activity
in cell types not represented. The model cannot predict activity in
novel cell types without relevant training data.

\subsection{Context Independence}\label{context-independence}

The model treats each input sequence independently, without considering:

\begin{itemize}
\tightlist
\item
  3D chromatin structure (which brings distant sequences into proximity)
\item
  Current transcriptional state (which affects chromatin accessibility)
\item
  Other variants in the same individual (epistasis)
\end{itemize}

\subsection{Quantitative Accuracy}\label{quantitative-accuracy}

While DeepSEA accurately predicts binary presence/absence of chromatin
features, quantitative predictions of signal strength are less reliable.
Later models like Basenji addressed this by predicting continuous
coverage rather than binary peaks.

\section{Significance for the Field}\label{significance-for-the-field}

DeepSEA established several paradigms that shaped subsequent genomic
deep learning:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Sequence-in, function-out}: Learning regulatory activity
  directly from sequence without hand-engineered features
\item
  \textbf{Multi-task chromatin prediction}: Jointly modeling many
  related tasks improves both performance and efficiency
\item
  \textbf{Variant effect prediction via comparison}: Score variants by
  comparing predictions for reference and alternative alleles
\item
  \textbf{Ab initio prediction}: Make predictions for any sequence,
  including novel mutations never observed in training data
\end{enumerate}

The approach demonstrated that deep learning could extract biologically
meaningful patterns from raw sequence data at scale. This opened the
door to increasingly sophisticated sequence-to-function
models---predicting not just chromatin state, but gene expression
(ExPecto, Chapter 6), splicing (SpliceAI, Chapter 7), and eventually
long-range regulatory interactions (Enformer, Chapter 11).

DeepSEA's public web server (http://deepsea.princeton.edu/) and code
release also established a model for making genomic deep learning tools
accessible to the broader research community---a practice that has
become standard in the field.

\chapter{Transcriptional Effects}\label{transcriptional-effects}

\section{From Chromatin to
Expression}\label{from-chromatin-to-expression}

DeepSEA (Chapter 5) demonstrated that deep learning could predict
chromatin features from DNA sequence alone. Yet chromatin accessibility
and transcription factor binding are intermediate phenotypes---the
ultimate functional readout for most regulatory variants is their effect
on gene expression. A variant might disrupt a transcription factor
binding site, but does that binding site actually regulate a nearby
gene? In which tissues? By how much?

ExPecto, introduced by Zhou et al.~in 2018, addressed these questions by
extending the sequence-to-chromatin paradigm to predict tissue-specific
gene expression levels (J. Zhou et al. 2018). The framework's name
reflects its core capability: \textbf{Ex}pression prediction. Rather
than stopping at chromatin predictions, ExPecto integrates predicted
regulatory signals across a 40 kb promoter-proximal region to predict
absolute expression levels in 218 tissues and cell types.

Critically, ExPecto predicts expression effects \emph{ab initio} from
sequence---without training on any variant data. This enables scoring of
rare variants, \emph{de novo} mutations, and even hypothetical mutations
never observed in any population.

\section{The Modular Architecture}\label{the-modular-architecture}

ExPecto comprises three sequential components, each addressing a
distinct computational challenge.

\subsection{Component 1: Epigenomic Effects Model (Beluga
CNN)}\label{component-1-epigenomic-effects-model-beluga-cnn}

The first component is an enhanced version of DeepSEA, predicting 2,002
chromatin profiles (histone marks, transcription factor binding, and
DNase hypersensitivity) across \textgreater200 cell types. Key
architectural improvements over the original DeepSEA include:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Feature & DeepSEA (2015) & ExPecto/Beluga (2018) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Chromatin targets & 919 & 2,002 \\
Input window & 1,000 bp & 2,000 bp \\
Convolution layers & 3 & 6 (with residual connections) \\
Cell types & \textasciitilde125 & \textgreater200 \\
\end{longtable}

The CNN scans the 40 kb region surrounding each transcription start site
(TSS) with a moving window (200 bp step size), generating chromatin
predictions at 200 spatial positions. For each gene, this produces 2,002
× 200 = 400,400 features representing the predicted spatial chromatin
organization around the TSS.

\subsection{Component 2: Spatial Feature
Transformation}\label{component-2-spatial-feature-transformation}

The 400,400-dimensional feature space poses optimization challenges for
downstream expression prediction. ExPecto addresses this through spatial
transformation---a biologically motivated dimensionality reduction that
captures the known distance-dependent relationship between regulatory
elements and their target promoters.

The transformation applies ten exponential decay functions separately to
upstream and downstream regions. The full model specification is:

\[
\text{expression} = \sum_{i,k} \left( \beta_{ik}^{\text{up}} \cdot \mathbf{1}(t_d < 0) + \beta_{ik}^{\text{down}} \cdot \mathbf{1}(t_d > 0) \right) \cdot \sum_{d \in D} p_{id} \cdot e^{-a_k \cdot |t_d|}
\]

where \(p_{id}\) is the predicted probability for chromatin feature
\(i\) at spatial bin \(d\), \(t_d\) is the mean distance to TSS for bin
\(d\), and \(a_k\) represents decay constants (0.01, 0.02, 0.05, 0.1,
0.2). The indicator functions \(\mathbf{1}(\cdot)\) allow separate
coefficients for upstream (\(\beta^{\text{up}}\)) and downstream
(\(\beta^{\text{down}}\)) regions.

This transformation reduces dimensionality 20-fold (to 20,020 features)
while preserving spatial information---features with higher decay rates
are concentrated near the TSS, while lower decay rates capture more
distal signals. The transformation is not learned but prespecified,
equivalent to constraining the model to learn smooth spatial patterns as
linear combinations of basis functions.

\subsection{Component 3: Tissue-Specific Linear
Models}\label{component-3-tissue-specific-linear-models}

The final component comprises 218 L2-regularized linear regression
models (one per tissue), each predicting log RPKM expression from
spatially-transformed features. Linear models were chosen deliberately:
they provide interpretability, prevent overfitting given the
high-dimensional feature space, and enable straightforward coefficient
analysis to identify which chromatin features drive expression in each
tissue.

Training used gradient boosting with L2 regularization (λ=100, shrinkage
η=0.01), with chromosome 8 held out for evaluation (990 genes). The
chromosome-level holdout prevents data leakage through overlapping
regulatory regions and sequence homology.

\section{Expression Prediction
Performance}\label{expression-prediction-performance}

ExPecto achieved 0.819 median Spearman correlation between predicted and
observed expression (log RPKM) across 218 tissues and cell types---a
substantial improvement over prior sequence-based expression models,
which were typically limited to narrower regulatory regions (\textless2
kb) and fewer cell types.

\subsection{Tissue Specificity}\label{tissue-specificity}

Beyond predicting absolute expression levels, ExPecto captures
tissue-specific expression patterns. Expression predictions correlate
more strongly with experimental measurements from the matching tissue
than from other tissues, indicating the model learns tissue-specific
regulatory logic rather than generic sequence features.

Analysis of model coefficients reveals automatic learning of
cell-type-relevant features without explicit tissue labels:

\begin{itemize}
\tightlist
\item
  \textbf{Liver model}: Top weighted features correspond to seven
  transcription factors in HepG2 (liver-derived) cells
\item
  \textbf{Breast model}: All top five positive features are estrogen
  receptor (ER-α) and glucocorticoid receptor (GR) in breast cancer cell
  lines T-47D and ECC-1
\item
  \textbf{Blood model}: All top five features derive from blood cell
  lines and erythroblast cells
\end{itemize}

\subsection{Feature Importance}\label{feature-importance}

Model coefficients reveal the relative contributions of different
chromatin feature types:

\begin{itemize}
\tightlist
\item
  \textbf{Transcription factors and histone marks} receive consistently
  higher weights, reflecting their direct mechanistic roles in
  transcriptional regulation
\item
  \textbf{DNase I features} receive significantly lower weights (p =
  6.9×10⁻²⁵, Wilcoxon rank sum test) despite indicating regulatory
  activity---likely because DNase hypersensitivity marks \emph{presence}
  of regulatory activity without specifying \emph{type} (activating
  vs.~repressing) or \emph{causal} relationship to expression
\end{itemize}

\section{Variant Effect Prediction}\label{variant-effect-prediction}

ExPecto's expression predictions enable scoring variant effects through
\emph{in silico} mutagenesis: predict expression with reference allele,
predict with alternative allele, and compute the difference. Because the
model never trains on variant data, predictions are unconfounded by
linkage disequilibrium---a fundamental advantage over statistical eQTL
approaches.

\subsection{Computing Variant Effects}\label{computing-variant-effects}

For any variant, ExPecto computes effects by comparing predictions:

\[
\Delta \text{expression} = f(\text{sequence}_{\text{alt}}) - f(\text{sequence}_{\text{ref}})
\]

This approach predicts the direction and magnitude of expression change
in each of 218 tissues for any single nucleotide variant within the 40
kb promoter region.

\subsection{eQTL Validation}\label{eqtl-validation}

ExPecto correctly predicted the direction of expression change for 92\%
of the top 500 strongest-effect GTEx eQTL variants. Prediction accuracy
increases with predicted effect magnitude: variants with stronger
predicted effects show higher eQTL direction concordance, consistent
with the expectation that true causal variants should have larger
predicted effects.

Unlike traditional eQTL studies, which are biased toward common variants
with sufficient statistical power, ExPecto predictions work equally well
across the allele frequency spectrum. This makes the framework
particularly valuable for rare variant interpretation where population
data is sparse.

\subsection{Advantages Over eQTL
Mapping}\label{advantages-over-eqtl-mapping}

Traditional eQTL studies face fundamental limitations:

\begin{itemize}
\tightlist
\item
  \textbf{LD confounding}: Only 3.5--11.7\% of GTEx lead variants are
  estimated to be truly causal, meaning \textless1\% of all reported
  eQTL variants directly affect expression
\item
  \textbf{Allele frequency bias}: Rare variants lack statistical power
  for detection
\item
  \textbf{Tissue availability}: eQTL mapping requires large sample sizes
  in the tissue of interest
\end{itemize}

ExPecto's sequence-based predictions sidestep all three limitations:
they score based on predicted functional impact rather than population
associations, work identically for any allele frequency, and leverage
expression training data from many tissues even when eQTL data is
unavailable.

\section{GWAS Causal Variant
Prioritization}\label{gwas-causal-variant-prioritization}

A major application of ExPecto is prioritizing causal variants within
GWAS-identified loci, where LD typically prevents identification of the
true functional variant.

\subsection{Systematic Prioritization}\label{systematic-prioritization}

Zhou et al.~applied ExPecto to prioritize variants from
\textasciitilde3,000 GWAS studies. Key findings:

\begin{itemize}
\tightlist
\item
  GWAS loci with stronger predicted effect variants were significantly
  more likely to replicate in independent studies (p = 6.3×10⁻¹⁸⁹, Wald
  test with logistic regression)
\item
  Stronger predicted effect variants were more likely to be the exact
  replicated variant (p = 5.6×10⁻¹⁴)
\end{itemize}

For example, an early venous thromboembolism GWAS identified rs3756008
as the lead variant near the F11 locus. ExPecto prioritized a different
LD variant, rs4253399, which was subsequently discovered as the true
association in a larger cohort study.

\subsection{Experimental Validation}\label{experimental-validation}

The authors experimentally validated three top-ranked ExPecto
predictions for immune-related diseases using luciferase reporter
assays. In all cases, the ExPecto-prioritized variants showed
significant allele-specific regulatory activity, while the original GWAS
lead variants showed no differential activity:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.3000}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0750}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1125}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1875}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Disease
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ExPecto-Prioritized SNP
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gene
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reporter Effect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p-value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
GWAS Lead SNP
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Crohn's disease / IBD & rs1174815 & IRGM & Decreased expression & 3×10⁻⁶
& Not significant \\
Behçet's disease & rs147398495 & CCR1 & Changed activity & 7×10⁻¹⁰ & Not
significant \\
Chronic HBV infection & rs381218 & HLA-DOA & 4-fold change & 1×10⁻⁹ &
Not significant \\
\end{longtable}

ExPecto correctly predicted the direction of expression change for all
three validated variants. These results demonstrate that sequence-based
expression models can identify functional variants that statistical
association studies cannot distinguish from linked non-functional
variants.

\section{In Silico Saturation
Mutagenesis}\label{in-silico-saturation-mutagenesis-1}

The computational efficiency of ExPecto enables exhaustive
characterization of the regulatory mutation space. The authors computed
predicted effects for all possible single nucleotide substitutions
within ±1 kb of each TSS---over 140 million mutations across 23,779
human Pol II-transcribed genes. This identified \textgreater1.1 million
mutations with strong predicted expression effects.

\subsection{Variation Potential}\label{variation-potential}

For each gene, the comprehensive mutagenesis profile defines its
``variation potential'' (VP)---the collective effects of all possible
mutations on that gene's expression. VP reflects the regulatory
sensitivity of each gene:

\begin{itemize}
\tightlist
\item
  \textbf{High VP genes}: Expression easily perturbed by sequence
  changes; regulatory regions densely packed with functional elements
\item
  \textbf{Low VP genes}: Expression robust to mutations; potentially
  fewer regulatory constraints or more redundant regulatory architecture
\end{itemize}

VP correlates with known biological properties: tissue-specific genes
show lower VP than broadly expressed genes, and genes under stronger
evolutionary constraint tend to have higher VP.

\subsection{Constraint Violation
Scores}\label{constraint-violation-scores}

By comparing predicted mutational effects to observed population
variation, ExPecto enables inference of evolutionary constraints. A
``constraint violation score'' measures whether observed variants push
expression in the ``wrong'' direction relative to inferred evolutionary
constraint:

\begin{itemize}
\tightlist
\item
  Genes with negative VP directionality (mutations tend to reduce
  expression) are typically actively expressed---loss-of-function
  mutations are deleterious
\item
  Genes with positive VP directionality (mutations tend to increase
  expression) are typically repressed---gain-of-expression mutations are
  deleterious
\end{itemize}

This framework successfully predicts GWAS risk alleles without any prior
variant-disease association data. Positive violation scores are
significantly associated with alternative alleles being risk alleles (p
= 0.002, Wilcoxon rank sum test, AUC = 0.67), demonstrating potential
for ab initio disease variant identification.

\section{The 40 kb Regulatory Window}\label{the-40-kb-regulatory-window}

ExPecto's ±20 kb window around each TSS represents an empirically
optimized trade-off:

\begin{itemize}
\tightlist
\item
  \textbf{Smaller windows}: Decreased prediction performance
\item
  \textbf{Larger windows (50--200 kb)}: Negligible performance
  improvement
\end{itemize}

This suggests that most regulatory information for promoter-proximal
expression lies within 40 kb of the TSS---at least within the linear
modeling framework employed by ExPecto. Distal enhancers beyond this
window, while biologically important, likely require more sophisticated
integration approaches to capture (addressed by Enformer, Chapter 11,
with its 200 kb effective receptive field).

\section{Relationship to the DeepSEA
Lineage}\label{relationship-to-the-deepsea-lineage}

ExPecto represents a conceptual extension of the DeepSEA framework:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Model & Year & Primary Output & Context Window \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
DeepSEA & 2015 & 919 chromatin profiles & 1 kb \\
ExPecto/Beluga & 2018 & Gene expression (218 tissues) & 40 kb \\
Sei & 2022 & 21,907 chromatin profiles + sequence classes & 4 kb \\
\end{longtable}

While DeepSEA predicts regulatory intermediate phenotypes, ExPecto
predicts the downstream transcriptional consequence. For GWAS variant
prioritization, ExPecto predictions proved more effective than DeepSEA
alone---variants may alter chromatin features without affecting
expression, but expression effects are more directly tied to phenotypic
consequences.

The chromatin prediction component of ExPecto (Beluga) became the
foundation for Sei (discussed in Chapter 5), which expanded chromatin
targets to 21,907 profiles and introduced sequence class annotations for
interpretability.

\section{Limitations and
Considerations}\label{limitations-and-considerations-1}

\subsection{Linear Expression Model}\label{linear-expression-model}

While the chromatin CNN captures nonlinear sequence patterns, the final
expression model is linear. This prevents modeling of complex regulatory
logic:

\begin{itemize}
\tightlist
\item
  Synergistic interactions between elements
\item
  Competitive binding or mutual exclusion
\item
  Threshold effects where element contributions are context-dependent
\end{itemize}

The choice was pragmatic---linear models require less data and offer
interpretability---but may sacrifice predictive power for genes with
complex regulatory logic.

\subsection{Context Window
Constraints}\label{context-window-constraints}

The 40 kb promoter-proximal window misses:

\begin{itemize}
\tightlist
\item
  Distal enhancers operating over hundreds of kilobases
\item
  3D chromatin interactions that bring distant elements into proximity
\item
  Enhancer-promoter specificity (which enhancer regulates which gene
  among nearby alternatives)
\end{itemize}

\subsection{TSS-Centric Framework}\label{tss-centric-framework}

ExPecto requires a defined TSS for each gene, potentially limiting
predictions for:

\begin{itemize}
\tightlist
\item
  Genes with multiple alternative promoters
\item
  Novel or unannotated transcription start sites
\item
  Tissue-specific promoter usage
\end{itemize}

\subsection{Training Data Biases}\label{training-data-biases}

Expression models trained on GTEx, Roadmap, and ENCODE data inherit
their biases:

\begin{itemize}
\tightlist
\item
  Ancestry composition (GTEx is primarily European)
\item
  Tissue representation (some tissues well-covered, others sparse)
\item
  Cell line artifacts (immortalized cells may not reflect primary tissue
  biology)
\end{itemize}

\section{Significance for the Field}\label{significance-for-the-field-1}

ExPecto established several paradigms that influenced subsequent genomic
deep learning:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Modular sequence-to-expression prediction}: Decomposing the
  problem into chromatin prediction, spatial integration, and expression
  modeling enables interpretability and component-wise improvement
\item
  \textbf{Ab initio variant effect prediction}: Training without variant
  data avoids LD confounding, enabling causal inference rather than
  association
\item
  \textbf{Scalable in silico mutagenesis}: Computational efficiency
  enables exhaustive characterization of mutational effects at genome
  scale
\item
  \textbf{Tissue-specific regulatory learning}: The framework learns
  tissue-relevant regulatory features without explicit tissue labels for
  chromatin inputs
\item
  \textbf{Experimental validation standard}: Demonstrating functional
  validation of computational predictions with reporter assays
\end{enumerate}

The framework demonstrated that deep learning could move beyond
predicting intermediate molecular phenotypes (chromatin state) to
predict cellular phenotypes (expression levels) directly from sequence.
This progression---from sequence to chromatin to expression to
disease---prefigured the increasingly ambitious goals of later genomic
foundation models.

ExPecto's public web portal (http://hb.flatironinstitute.org/expecto)
and code release (https://github.com/FunctionLab/ExPecto) maintained the
field's norm of open tool availability established by DeepSEA. The
framework continues to serve as a baseline for expression prediction
methods and as a component in variant prioritization pipelines.

\chapter{Splicing Prediction}\label{splicing-prediction}

\section{The Splicing Challenge}\label{the-splicing-challenge}

While DeepSEA and ExPecto (Chapters 5--6) addressed chromatin state and
gene expression, a distinct class of functional variants operates
through a different mechanism: disruption of pre-mRNA splicing. The
spliceosome---the cellular machinery that removes introns and joins
exons---achieves remarkable precision, recognizing the correct splice
sites among millions of potential candidates in the human transcriptome.
Yet the sequence determinants underlying this specificity remained
incompletely understood, limiting interpretation of variants that might
alter splicing.

SpliceAI, introduced by Jaganathan et al.~in 2019, demonstrated that
deep neural networks could learn the sequence rules governing splicing
with near-spliceosomal precision (Jaganathan et al. 2019). The model
predicts splice site locations directly from pre-mRNA sequence, enabling
identification of ``cryptic splice'' variants---mutations that create
novel splice sites or disrupt existing ones in ways that evade
traditional annotation-based detection.

The clinical implications are substantial: SpliceAI estimates that
9--11\% of pathogenic mutations in rare genetic disorders act through
cryptic splicing, representing a previously underappreciated class of
disease variation.

\section{Prior Approaches and
Limitations}\label{prior-approaches-and-limitations}

Before SpliceAI, splice site prediction relied on methods with limited
context:

\begin{itemize}
\tightlist
\item
  \textbf{MaxEntScan}: Models core splice motifs using maximum entropy,
  limited to \textasciitilde9 bp context around donor/acceptor sites
\item
  \textbf{GeneSplicer}: Combines Markov models with decision trees
\item
  \textbf{NNSplice}: Early neural network approach with narrow receptive
  fields
\end{itemize}

These methods captured the essential GT (donor) and AG (acceptor)
dinucleotides and surrounding consensus sequences, but could not model
the long-range determinants---exon/intron length constraints, branch
points, enhancers, and silencers---that contribute to splicing
specificity. As a result, they produced many false positive predictions
and missed variants acting through distal mechanisms.

\section{The SpliceAI Architecture}\label{the-spliceai-architecture}

SpliceAI employs an ultra-deep residual convolutional network that
integrates information across 10,000 nucleotides of sequence
context---orders of magnitude more than prior methods.

\subsection{Residual Block Design}\label{residual-block-design}

The architecture's fundamental unit is the residual block, comprising
batch normalization, ReLU activation, and dilated convolutions. Residual
connections address the vanishing gradient problem that had limited
earlier deep networks:

\[
\text{output} = \text{input} + F(\text{input})
\]

where \(F\) represents the transformation learned by the convolutional
layers. Skip connections from every fourth residual block feed directly
to the penultimate layer, accelerating training convergence.

\subsection{Dilated Convolutions for Long-Range
Context}\label{dilated-convolutions-for-long-range-context}

Each residual block uses dilated (atrous) convolutions parameterized by:

\begin{itemize}
\tightlist
\item
  \(N\): Number of convolutional kernels
\item
  \(W\): Window size
\item
  \(D\): Dilation rate
\end{itemize}

A kernel with window size \(W\) and dilation rate \(D\) spans
\((W-1) \cdot D\) neighboring positions. The total receptive field \(S\)
of the network is:

\[
S = \sum_{i=1}^{K} 2 \cdot (W_i - 1) \cdot D_i
\]

where \(K\) is the number of residual blocks. By progressively
increasing dilation rates through the network, SpliceAI achieves a
10,000 bp receptive field without the computational cost of processing
10,000 positions at full resolution.

\subsection{Architecture Variants}\label{architecture-variants}

Four architectures were developed with different context windows:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Model & Flanking Sequence & Total Context & Residual Blocks \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SpliceAI-80nt & 40 bp each side & 80 bp & 4 \\
SpliceAI-400nt & 200 bp each side & 400 bp & 8 \\
SpliceAI-2k & 1,000 bp each side & 2,000 bp & 16 \\
SpliceAI-10k & 5,000 bp each side & 10,000 bp & 32 \\
\end{longtable}

The 32-layer SpliceAI-10k model substantially outperformed
shorter-context variants, demonstrating that long-range sequence
features contribute meaningfully to splice site prediction.

\subsection{Output Format}\label{output-format}

For each nucleotide position, SpliceAI outputs three probabilities
summing to one:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Probability of being a splice \textbf{acceptor} (first nucleotide of
  an exon)
\item
  Probability of being a splice \textbf{donor} (last nucleotide of an
  exon)
\item
  Probability of being \textbf{neither}
\end{enumerate}

The model operates in sequence-to-sequence mode: given an input of
length \(S/2 + l + S/2\), it outputs predictions for the central \(l\)
positions. This enables efficient batch processing where overlapping
computations are shared.

\section{Training and Evaluation}\label{training-and-evaluation}

\subsection{Training Data}\label{training-data-1}

SpliceAI was trained on 20,287 protein-coding genes from GENCODE V24,
selecting principal transcripts when multiple isoforms existed. The
training/test split used odd versus even chromosomes:

\begin{itemize}
\tightlist
\item
  \textbf{Training}: Chromosomes 2, 4, 6, 8, 10--22, X, Y (13,384 genes,
  130,796 donor-acceptor pairs)
\item
  \textbf{Testing}: Chromosomes 1, 3, 5, 7, 9---excluding genes with
  paralogs on training chromosomes (1,652 genes, 14,289 donor-acceptor
  pairs)
\end{itemize}

The paralog exclusion prevents information leakage through sequence
homology.

For variant effect prediction, training was augmented with novel splice
junctions commonly observed in GTEx RNA-seq data (adding
\textasciitilde67,000 donor and \textasciitilde63,000 acceptor
annotations), improving sensitivity for detecting splice-altering
variants, particularly in deep intronic regions.

\subsection{Splice Site Prediction
Performance}\label{splice-site-prediction-performance}

SpliceAI-10k achieved:

\begin{itemize}
\tightlist
\item
  \textbf{Top-k accuracy}: 95\% (at threshold where predicted sites
  equal actual sites)
\item
  \textbf{PR-AUC}: 0.98
\end{itemize}

For comparison, MaxEntScan achieved only 57\% top-k accuracy under
equivalent conditions. The dramatic improvement reflects SpliceAI's
ability to reject false positive splice sites by considering sequence
context beyond the core motif.

Notably, performance improved substantially with context length (80 bp →
400 bp → 2,000 bp → 10,000 bp), confirming that distal sequence features
contribute to splice site recognition.

\section{Variant Effect Prediction}\label{variant-effect-prediction-1}

\subsection{The Delta Score}\label{the-delta-score}

SpliceAI predicts variant effects by comparing splice site predictions
for reference and alternative sequences:

\[
\Delta\text{score} = \max_{|p - v| \leq 50} \left| P_{\text{alt}}(p) - P_{\text{ref}}(p) \right|
\]

where \(v\) is the variant position and \(p\) ranges over positions
within 50 bp of the variant. The maximum change across all positions
captures variants that strengthen existing sites, weaken existing sites,
or create entirely new splice sites.

Critically, the model was trained only on reference transcript sequences
and splice junction annotations---it never saw variant data during
training. Variant effect prediction is thus a challenging test of
whether the network learned genuine sequence determinants of splicing.

\subsection{Cryptic Splice Variant
Classes}\label{cryptic-splice-variant-classes}

SpliceAI detects several classes of splice-altering variants:

\begin{itemize}
\tightlist
\item
  \textbf{Donor/acceptor loss}: Disruption of annotated splice sites
\item
  \textbf{Donor/acceptor gain}: Creation of novel splice sites
\item
  \textbf{Exon skipping}: Variants causing an exon to be spliced out
\item
  \textbf{Intron retention}: Variants causing an intron to remain in
  mature mRNA
\item
  \textbf{Cryptic exon activation}: Deep intronic variants creating
  novel exons
\end{itemize}

Traditional annotation-based methods can identify variants in the
essential GT/AG dinucleotides but miss the broader landscape of cryptic
splice variants operating through more subtle mechanisms.

\section{Validation on GTEx RNA-seq}\label{validation-on-gtex-rna-seq}

The authors validated SpliceAI predictions using RNA-seq data from 149
GTEx individuals with matched whole-genome sequencing. Private variants
(present in only one individual) predicted to alter splicing were tested
for association with aberrant splice junctions.

\subsection{Validation Rates}\label{validation-rates}

At a Δ score threshold of ≥0.5, cryptic splice variants validated at
three-quarters the rate of essential GT/AG splice disruptions:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Variant Class & Validation Rate \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Essential GT/AG disruption & \textasciitilde100\% (by definition) \\
Cryptic splice (Δ ≥ 0.8) & \textasciitilde85\% \\
Cryptic splice (Δ ≥ 0.5) & \textasciitilde75\% \\
Cryptic splice (Δ ≥ 0.2) & \textasciitilde50\% \\
\end{longtable}

Validation rate and effect size both tracked closely with Δ score,
confirming that the model's confidence correlates with functional
impact.

\subsection{Position-Dependent
Sensitivity}\label{position-dependent-sensitivity}

Sensitivity varied by genomic location:

\begin{itemize}
\tightlist
\item
  \textbf{Near exons} (≤50 bp from exon-intron boundaries): 71\%
  sensitivity at Δ ≥ 0.5
\item
  \textbf{Deep intronic} (\textgreater50 bp from boundaries): 41\%
  sensitivity at Δ ≥ 0.5
\end{itemize}

Deep intronic variants are more challenging because intronic regions
contain fewer of the specificity determinants selected to be present
near exons. Nevertheless, SpliceAI substantially outperformed prior
methods in both regions.

\subsection{Comparison to Prior
Methods}\label{comparison-to-prior-methods-1}

Benchmarking against MaxEntScan, GeneSplicer, and NNSplice demonstrated
SpliceAI's superior performance across all operating points. At matched
sensitivity, SpliceAI achieved higher validation rates; at matched
validation rates, SpliceAI achieved higher sensitivity.

\section{Population Genetics
Evidence}\label{population-genetics-evidence}

Beyond RNA-seq validation, the authors assessed whether predicted
cryptic splice variants show signatures of negative selection in human
populations.

\subsection{Allele Frequency
Depletion}\label{allele-frequency-depletion}

Using ExAC/gnomAD data, high-confidence cryptic splice variants (Δ ≥
0.8) showed 78\% depletion at common allele frequencies compared to
expectation---comparable to the 82\% depletion observed for frameshift,
stop-gain, and essential splice-disrupting variants. This indicates that
most confidently predicted cryptic splice variants are functional and
deleterious.

The depletion was stronger for variants predicted to cause frameshifts
versus in-frame alterations, consistent with the expectation that
frameshift-causing splice variants have more severe fitness
consequences.

\subsection{Rare Variant Burden}\label{rare-variant-burden}

The average human genome carries approximately:

\begin{itemize}
\tightlist
\item
  \textbf{11} rare protein-truncating variants (allele frequency
  \textless0.1\%)
\item
  \textbf{5} rare functional cryptic splice variants
\end{itemize}

Cryptic splice variants outnumber essential GT/AG splice-disrupting
variants roughly 2:1, highlighting the substantial mutational target
space beyond canonical splice sites.

\section{De Novo Mutations in Rare
Disease}\label{de-novo-mutations-in-rare-disease}

The central clinical finding of SpliceAI is that cryptic splice
mutations constitute a major, previously underappreciated cause of rare
genetic disorders.

\subsection{Case-Control Analysis}\label{case-control-analysis}

The authors analyzed de novo mutations in:

\begin{itemize}
\tightlist
\item
  \textbf{4,293} individuals with intellectual disability (Deciphering
  Developmental Disorders cohort)
\item
  \textbf{3,953} individuals with autism spectrum disorders (Simons
  Simplex Collection + Autism Sequencing Consortium)
\item
  \textbf{2,073} unaffected sibling controls
\end{itemize}

De novo mutations predicted to disrupt splicing (Δ ≥ 0.1) were
significantly enriched in affected individuals:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Cohort & Enrichment vs.~Controls & p-value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Intellectual disability (DDD) & 1.51-fold & 4.2×10⁻⁴ \\
Autism spectrum disorder & 1.30-fold & 0.020 \\
\end{longtable}

The enrichment remained significant when restricting to synonymous and
intronic mutations, excluding the possibility that results were driven
solely by variants with dual protein-coding and splicing effects.

\subsection{Fraction of Pathogenic
Mutations}\label{fraction-of-pathogenic-mutations}

Based on the excess of de novo mutations in cases versus controls:

\begin{itemize}
\tightlist
\item
  \textbf{9\%} of pathogenic de novo mutations in intellectual
  disability act through cryptic splicing
\item
  \textbf{11\%} of pathogenic de novo mutations in autism act through
  cryptic splicing
\end{itemize}

In absolute terms, \textasciitilde250 cases across the cohorts could be
explained by de novo cryptic splice mutations, compared to
\textasciitilde909 cases explained by de novo protein-truncating
variants.

\subsection{Clinical Penetrance}\label{clinical-penetrance}

Cryptic splice mutations showed roughly 50\% of the clinical penetrance
of classic protein-truncating mutations (stop-gain, frameshift,
essential splice). This reduced penetrance reflects that many cryptic
splice variants are hypomorphic---producing a mixture of normal and
aberrant transcripts rather than complete loss of function.

Well-characterized examples from Mendelian disease support this
interpretation: the c.315-48T\textgreater C variant in FECH and
c.-32-13T\textgreater G in GAA are both hypomorphic cryptic splice
alleles associated with milder phenotype or later age of onset.

\subsection{Novel Gene Discovery}\label{novel-gene-discovery}

Including cryptic splice mutations in gene discovery analyses
identified:

\begin{itemize}
\tightlist
\item
  \textbf{5 additional} candidate genes for intellectual disability
\item
  \textbf{2 additional} candidate genes for autism
\end{itemize}

These genes would have fallen below the discovery threshold (FDR
\textless0.01) when considering only protein-coding mutations.

\section{Experimental Validation in Autism
Patients}\label{experimental-validation-in-autism-patients}

To directly validate predicted cryptic splice effects, the authors
performed deep RNA-seq (\textasciitilde350 million reads per sample,
\textasciitilde10× GTEx coverage) on lymphoblastoid cell lines from 36
autism probands harboring predicted de novo cryptic splice mutations.

\subsection{Validation Results}\label{validation-results}

Among 28 cases with adequate RNA-seq coverage at the gene of interest:

\begin{itemize}
\tightlist
\item
  \textbf{21 (75\%)} showed unique aberrant splicing events associated
  with the predicted de novo variant
\item
  \textbf{7 (25\%)} showed no aberrant splicing in lymphoblastoid cells
\end{itemize}

The 75\% validation rate is remarkable given that the relevant tissue
(developing brain) was not accessible---some cryptic splice effects may
be tissue-specific and not observable in blood-derived cells.

\subsection{Aberrant Splicing Classes}\label{aberrant-splicing-classes}

Among the 21 validated cases:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Splicing Aberration & Count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Novel junction creation & 9 \\
Exon skipping & 8 \\
Intron retention & 4 \\
\end{longtable}

These aberrant events were absent from all other samples (the remaining
35 probands and 149 GTEx individuals), confirming their association with
the predicted de novo variants.

\section{What SpliceAI Learns}\label{what-spliceai-learns}

Analysis of SpliceAI's learned representations revealed that the network
captures known splicing biology:

\subsection{Core Splice Motifs}\label{core-splice-motifs}

The model correctly learned the essential GT donor and AG acceptor
dinucleotides, plus surrounding consensus sequences. In silico
mutagenesis of these positions produced the largest predicted effects.

\subsection{Branch Point Recognition}\label{branch-point-recognition}

Introducing the optimal branch point sequence (TACTAAC) at varying
distances from splice acceptors showed that SpliceAI learned the
expected distance constraints (20--45 bp upstream of acceptors). At
distances \textless20 bp, the branch point disrupts the polypyrimidine
tract, and SpliceAI correctly predicted reduced acceptor strength.

\subsection{Exonic Splicing Enhancers}\label{exonic-splicing-enhancers}

The SR-protein binding motif GAAGAA, introduced at various positions,
enhanced splice site strength when placed in expected locations within
exons, demonstrating that SpliceAI learned the contribution of exonic
splicing enhancers.

\subsection{Nucleosome Positioning}\label{nucleosome-positioning}

Novel exon-creation events (where variants activate cryptic exons in
introns) were significantly associated with existing nucleosome
positioning, supporting a causal role for nucleosome occupancy in exon
definition. SpliceAI implicitly captures this relationship despite not
being trained on chromatin data.

\section{Limitations and
Considerations}\label{limitations-and-considerations-2}

\subsection{Tissue Specificity}\label{tissue-specificity-1}

SpliceAI predicts splice sites based on sequence alone, without modeling
tissue-specific alternative splicing. The same variant may have
different effects across tissues depending on the expression of splicing
factors and regulatory RNAs.

\subsection{Incomplete Penetrance}\label{incomplete-penetrance}

Many cryptic splice variants produce partial shifts in splicing
(alternative splicing) rather than complete disruption. The Δ score
correlates with penetrance, but precise quantification of isoform ratios
requires experimental validation.

\subsection{Deep Intronic Predictions}\label{deep-intronic-predictions}

While SpliceAI substantially improves deep intronic variant prediction
over prior methods, sensitivity remains lower than for variants near
exons. The 41\% sensitivity (Δ ≥ 0.5) in deep intronic regions suggests
that additional sequence features beyond the 10 kb context may
contribute to splicing.

\subsection{Training on Canonical
Transcripts}\label{training-on-canonical-transcripts}

Training on principal transcripts may not fully capture the diversity of
alternative splicing. Augmentation with RNA-seq-derived junctions
improved performance, suggesting that expanded training data could
further enhance predictions.

\section{Significance for the Field}\label{significance-for-the-field-2}

SpliceAI established several important contributions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Clinical impact quantification}: The estimate that 9--11\% of
  pathogenic mutations act through cryptic splicing fundamentally
  changed understanding of the noncoding disease mutation landscape
\item
  \textbf{Deep context matters}: The 32-layer, 10 kb context
  architecture demonstrated that splicing involves long-range sequence
  integration, motivating similar approaches in other genomic prediction
  tasks
\item
  \textbf{Genome-wide variant scoring}: Precomputed Δ scores for all
  possible single nucleotide substitutions (available at
  https://github.com/Illumina/SpliceAI) enable routine clinical
  annotation
\item
  \textbf{Validation standards}: The combination of RNA-seq validation,
  population genetics evidence, and case-control analysis established a
  rigorous framework for evaluating variant effect predictors
\item
  \textbf{Specialized versus general models}: SpliceAI's success
  demonstrated that task-specific deep learning models could outperform
  general-purpose approaches by focusing computational capacity on a
  well-defined prediction problem
\end{enumerate}

SpliceAI has become a standard component of clinical variant
interpretation pipelines, complementing protein-effect predictors and
regulatory variant scores. The approach has influenced subsequent work
on tissue-specific splicing prediction and integration of splicing
effects into comprehensive variant effect models like Borzoi (Chapter
11).

The model's code and precomputed scores are publicly available
(https://github.com/Illumina/SpliceAI), enabling widespread adoption in
both research and clinical settings.

\part{Part III: Transformers Models}

\chapter{Sequence Representation \&
Tokens}\label{sequence-representation-tokens}

\section{From Sequence to Model: The Representation
Problem}\label{from-sequence-to-model-the-representation-problem}

Every genomic deep learning model must answer a fundamental question:
how should DNA sequence be represented as numerical input? The previous
chapters employed one-hot encoding---a simple, lossless representation
where each nucleotide becomes a 4-dimensional binary vector. This
approach worked well for CNN-based models like DeepSEA (Chapter 5) and
SpliceAI (Chapter 7), but the emergence of transformer-based language
models introduced new considerations around tokenization, vocabulary
design, and the trade-offs between sequence compression and resolution.

This chapter examines the evolution of sequence representation
strategies, from one-hot encoding through k-mer tokenization to modern
approaches including Byte Pair Encoding (BPE), single-nucleotide tokens,
and biologically-informed tokenization schemes. The choice of
representation profoundly affects what a model can learn, how
efficiently it trains, and what context lengths it can practically
achieve.

\section{One-Hot Encoding: The CNN
Baseline}\label{one-hot-encoding-the-cnn-baseline}

\subsection{Representation}\label{representation}

One-hot encoding represents each nucleotide as a sparse binary vector:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Nucleotide & Vector \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
A & {[}1, 0, 0, 0{]} \\
C & {[}0, 1, 0, 0{]} \\
G & {[}0, 0, 1, 0{]} \\
T & {[}0, 0, 0, 1{]} \\
\end{longtable}

A sequence of length \(L\) becomes a matrix of dimensions
\(4 \times L\), interpretable as 4 ``channels'' (like RGB channels in
images, plus one).

\subsection{Advantages}\label{advantages}

One-hot encoding offers several properties that made it the default for
CNN-based genomic models:

\begin{itemize}
\tightlist
\item
  \textbf{Lossless}: No information is discarded; every nucleotide is
  explicitly represented
\item
  \textbf{Single-nucleotide resolution}: Enables detection of effects
  from individual SNPs
\item
  \textbf{Translation equivariance}: Convolutional filters learn
  position-invariant motifs
\item
  \textbf{Simplicity}: No preprocessing, vocabulary construction, or
  tokenizer training required
\end{itemize}

\subsection{Limitations}\label{limitations}

For transformer architectures, one-hot encoding presents challenges:

\begin{itemize}
\tightlist
\item
  \textbf{Sequence length}: A 10 kb sequence requires 10,000 tokens,
  straining attention's \(O(L^2)\) complexity
\item
  \textbf{No learned embeddings}: Each nucleotide has a fixed, sparse
  representation rather than a learned dense embedding
\item
  \textbf{Context constraints}: Practical transformer context windows of
  512--4,096 tokens translate to only 512--4,096 bp---a tiny fraction of
  genes or regulatory regions
\end{itemize}

\section{K-mer Tokenization: DNABERT's
Approach}\label{k-mer-tokenization-dnaberts-approach}

\subsection{Concept}\label{concept}

K-mer tokenization treats overlapping subsequences of length \(k\) as
tokens, analogous to words in natural language. DNABERT (2021) pioneered
this approach for genomic transformers, using 6-mers (Ji et al. 2021).

For a 6-mer vocabulary: - Vocabulary size: \(4^6 = 4,096\) possible
tokens - Each token represents 6 consecutive nucleotides - Tokens
overlap by \(k-1 = 5\) positions

\subsection{Overlapping
vs.~Non-Overlapping}\label{overlapping-vs.-non-overlapping}

DNABERT used \textbf{overlapping} k-mers: for a sequence ACGTACGT, the
3-mer tokens would be:

\begin{verbatim}
Position:  1   2   3   4   5   6
Sequence:  A   C   G   T   A   C   G   T
3-mers:   ACG CGT GTA TAC ACG CGT
\end{verbatim}

This preserves positional information but creates computational
redundancy---the sequence length in tokens equals the sequence length in
nucleotides (minus \(k-1\)).

\subsection{Problems with K-mer
Tokenization}\label{problems-with-k-mer-tokenization}

DNABERT-2 (2024) identified fundamental limitations of k-mer
tokenization (Z. Zhou et al. 2024):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{No sequence compression}: Overlapping k-mers don't reduce
  sequence length, so context window limitations persist
\item
  \textbf{Tokenization ambiguity}: A single sequence position
  contributes to \(k\) different tokens, complicating variant effect
  interpretation
\item
  \textbf{Sample inefficiency}: The model must learn that overlapping
  tokens share nucleotides, rather than this being encoded in the
  representation
\item
  \textbf{Computational overhead}: Processing \(L\) overlapping tokens
  for an \(L\)-bp sequence is no more efficient than one-hot encoding
\item
  \textbf{Fixed vocabulary}: The \(4^k\) vocabulary doesn't adapt to
  corpus statistics; frequent and rare k-mers receive equal
  representation capacity
\end{enumerate}

\section{Byte Pair Encoding: Learning the
Vocabulary}\label{byte-pair-encoding-learning-the-vocabulary}

\subsection{The BPE Algorithm}\label{the-bpe-algorithm}

Byte Pair Encoding, originally a data compression algorithm, constructs
a vocabulary by iteratively merging the most frequent adjacent token
pairs in the training corpus:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize vocabulary with single nucleotides: \{A, C, G, T\}
\item
  Count all adjacent token pairs in the corpus
\item
  Merge the most frequent pair into a new token
\item
  Repeat until desired vocabulary size is reached
\end{enumerate}

This produces variable-length tokens that capture frequently occurring
sequence patterns, achieving genuine sequence compression.

\subsection{DNABERT-2's BPE
Implementation}\label{dnabert-2s-bpe-implementation}

DNABERT-2 replaced 6-mer tokenization with BPE, demonstrating
substantial improvements (Z. Zhou et al. 2024):

\begin{itemize}
\tightlist
\item
  \textbf{21× fewer parameters} than comparable k-mer models
\item
  \textbf{92× less GPU time} in pretraining
\item
  \textbf{Non-overlapping tokens}: Actual sequence compression, enabling
  longer effective context
\end{itemize}

The BPE vocabulary learns corpus statistics---repetitive elements,
common motifs, and frequent sequence patterns receive dedicated tokens,
while rare sequences are represented as shorter subunits.

\subsection{GROVER's Custom BPE}\label{grovers-custom-bpe}

GROVER (Genome Rules Obtained Via Extracted Representations) trained BPE
specifically on the human genome and selected vocabulary using a custom
next-k-mer prediction task (Sanabria et al. 2024). Analysis revealed
that learned token embeddings encode:

\begin{itemize}
\tightlist
\item
  \textbf{Frequency}: Common tokens cluster separately from rare ones
\item
  \textbf{Sequence content}: GC-rich versus AT-rich tokens segregate
\item
  \textbf{Length}: Token length correlates with embedding dimensions
\item
  \textbf{Genomic localization}: Some tokens appear primarily in
  repeats; others distribute broadly
\end{itemize}

\section{Single-Nucleotide Tokenization:
HyenaDNA}\label{single-nucleotide-tokenization-hyenadna}

\subsection{The Case for Maximum
Resolution}\label{the-case-for-maximum-resolution}

While k-mer and BPE tokenization compress sequences, they sacrifice
single-nucleotide resolution. A single nucleotide polymorphism (SNP) can
completely alter protein function, yet multi-nucleotide tokens obscure
the precise position and identity of variants.

HyenaDNA (2023) took the opposite approach: single-nucleotide tokens
with no compression (Nguyen et al. 2023). Each nucleotide (A, C, G, T)
is a separate token, preserving:

\begin{itemize}
\tightlist
\item
  \textbf{Full resolution}: Every nucleotide is independently
  represented
\item
  \textbf{Variant precision}: SNP effects can be isolated to specific
  tokens
\item
  \textbf{No tokenization artifacts}: No ambiguity about which token
  contains a variant
\end{itemize}

\subsection{Scaling to 1 Million Base
Pairs}\label{scaling-to-1-million-base-pairs}

The challenge with single-nucleotide tokens is sequence length. A 1 Mb
region requires 1 million tokens---far beyond standard transformer
capacity. HyenaDNA addresses this through the Hyena architecture, which
replaces attention with implicit convolutions that scale
sub-quadratically:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Model & Architecture & Max Context & Complexity \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
DNABERT & Transformer & 512 bp & \(O(L^2)\) \\
Nucleotide Transformer & Transformer & 6 kb & \(O(L^2)\) \\
HyenaDNA & Hyena & 1 Mb & \(O(L \log L)\) \\
\end{longtable}

HyenaDNA achieved a 500× increase in context length over dense attention
models while maintaining single-nucleotide resolution.

\subsection{Performance
Characteristics}\label{performance-characteristics}

On Nucleotide Transformer benchmarks, HyenaDNA reached state-of-the-art
on 12 of 18 datasets with orders of magnitude fewer parameters and less
pretraining data. On GenomicBenchmarks, it surpassed prior
state-of-the-art on 7 of 8 datasets by an average of +10 accuracy
points.

Notably, HyenaDNA demonstrated the first use of \textbf{in-context
learning} in genomics---performing tasks based on examples provided in
the context window without fine-tuning.

\section{Biologically-Informed
Tokenization}\label{biologically-informed-tokenization}

\subsection{The Central Dogma as Tokenization Guide:
Life-Code}\label{the-central-dogma-as-tokenization-guide-life-code}

Standard tokenization treats DNA as a homogeneous string, ignoring the
biological reality that different genomic regions serve different
functions. Coding sequences follow codon structure (3-nucleotide units
encoding amino acids), while noncoding regions have no such constraint.

Life-Code (2025) proposed codon-aware tokenization that respects the
central dogma of molecular biology (Liu et al. 2025):

\begin{itemize}
\tightlist
\item
  \textbf{Coding regions}: Tokenized by codons (3-mers in reading frame)
\item
  \textbf{Noncoding regions}: Tokenized by learned patterns
\item
  \textbf{Integration}: Unified framework spanning DNA, RNA, and protein
\end{itemize}

This approach enables Life-Code to: - Learn protein structure through
knowledge distillation from protein language models - Capture
interactions between coding and noncoding regions - Achieve
state-of-the-art results across DNA, RNA, and protein tasks

\subsection{BioToken: Encoding Genomic
Annotations}\label{biotoken-encoding-genomic-annotations}

BioToken (2025) extends tokenization beyond sequence content to include
genomic structural annotations (Medvedev et al. 2025):

\begin{itemize}
\tightlist
\item
  \textbf{Variant encoding}: Tokens that explicitly represent SNPs,
  insertions, and deletions
\item
  \textbf{Regulatory annotations}: Encoding of known regulatory elements
\item
  \textbf{Functional context}: Integration of gene structure, chromatin
  state, and other annotations
\end{itemize}

By incorporating biological inductive biases directly into the token
representation, BioToken's associated model (BioFM) achieves competitive
or superior performance to specialized models (Enformer, SpliceAI) with
significantly fewer parameters (265M).

\section{The Context Length
Evolution}\label{the-context-length-evolution}

The history of genomic deep learning shows a consistent trend toward
longer sequence context:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Era & Representative Models & Max Context & Tokenization \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2015--2017 & DeepSEA, DeepBind & 1 kb & One-hot \\
2018--2020 & ExPecto, SpliceAI & 10--40 kb & One-hot \\
2021 & DNABERT, Enformer & 512 bp -- 200 kb & K-mer / One-hot \\
2022--2023 & Nucleotide Transformer & 6 kb & K-mer \\
2023--2024 & HyenaDNA, Caduceus & 1 Mb & Single-nucleotide \\
2025 & Evo 2 & 1 Mb & Single-nucleotide (BPE) \\
\end{longtable}

This expansion reflects biological reality: regulatory elements can
influence genes from hundreds of kilobases away, and understanding
genome function requires integrating information across these distances.

\section{Trade-offs in Tokenization
Design}\label{trade-offs-in-tokenization-design}

\subsection{Compression
vs.~Resolution}\label{compression-vs.-resolution}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Strategy & Compression & Resolution & Variant Handling \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-hot & None & Single-nucleotide & Precise \\
Overlapping k-mer & None & K-nucleotide & Ambiguous \\
Non-overlapping k-mer & \textasciitilde K× & K-nucleotide &
Frame-dependent \\
BPE & Variable & Variable & Context-dependent \\
Single-nucleotide & None & Single-nucleotide & Precise \\
\end{longtable}

Higher compression enables longer context but loses precision for
variant effects. BPE offers a middle ground with adaptive compression,
but variant positions relative to token boundaries can affect
predictions.

\subsection{Vocabulary Size
Considerations}\label{vocabulary-size-considerations}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Tokenization & Typical Vocabulary Size \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-hot / Single-nucleotide & 4 (+ special tokens) \\
6-mer & 4,096 \\
BPE (DNABERT-2) & 4,096--32,000 \\
Codon-aware & \textasciitilde64 (codons) + noncoding \\
\end{longtable}

Larger vocabularies increase embedding table size but may capture more
complex patterns. Smaller vocabularies are parameter-efficient but
require the model to learn compositional structure.

\subsection{Computational Efficiency}\label{computational-efficiency}

For a sequence of length \(L\) bp:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Tokenization & Tokens & Attention Cost \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
One-hot & \(L\) & \(O(L^2)\) \\
Non-overlapping k-mer & \(L/k\) & \(O(L^2/k^2)\) \\
BPE (average compression \(c\)) & \(L/c\) & \(O(L^2/c^2)\) \\
\end{longtable}

BPE's variable compression can achieve substantial speedups, but the
benefit depends on corpus statistics and vocabulary size.

\section{Implications for Variant Effect
Prediction}\label{implications-for-variant-effect-prediction}

Tokenization choice directly affects variant effect prediction:

\subsection{Single-Nucleotide Tokens (HyenaDNA, Evo
2)}\label{single-nucleotide-tokens-hyenadna-evo-2}

\begin{itemize}
\tightlist
\item
  Reference and alternate alleles occupy the same token position
\item
  Effects are precisely localized
\item
  No tokenization artifacts
\end{itemize}

\subsection{K-mer Tokens}\label{k-mer-tokens}

\begin{itemize}
\tightlist
\item
  A single SNP changes \(k\) overlapping tokens
\item
  Must aggregate effects across affected tokens
\item
  Boundary effects if variant is at token junction
\end{itemize}

\subsection{BPE Tokens}\label{bpe-tokens}

\begin{itemize}
\tightlist
\item
  Variant may fall within a token or at token boundary
\item
  Effect interpretation depends on token segmentation
\item
  Re-tokenization may be needed for alternate allele
\end{itemize}

For clinical variant interpretation, single-nucleotide resolution is
often preferred despite computational costs, as subtle genetic
variations can have major phenotypic consequences.

\section{The Emerging Consensus}\label{the-emerging-consensus}

Recent developments suggest convergence toward:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Single-nucleotide resolution} for maximum precision, enabled
  by sub-quadratic architectures (Hyena, Mamba, state space models)
\item
  \textbf{Learned embeddings} rather than fixed one-hot vectors,
  allowing the model to discover meaningful nucleotide representations
\item
  \textbf{Biologically-informed augmentation} where
  appropriate---encoding codons in coding regions, incorporating
  annotations, or using species-specific vocabularies
\item
  \textbf{Hybrid approaches} combining the efficiency of compression
  with resolution where needed
\end{enumerate}

The choice ultimately depends on the task: variant effect prediction
demands high resolution, while tasks like species classification or
repeat annotation may benefit from compression.

\section{References in Context}\label{references-in-context}

The models discussed in this chapter set the stage for the genomic
language models covered in Chapter 10. Understanding tokenization
choices clarifies why models like the Nucleotide Transformer use 6-mers
(Dalla-Torre et al. 2023), why DNABERT-2 switched to BPE, and why
HyenaDNA's single-nucleotide approach enabled unprecedented context
lengths. The hybrid architectures of Chapter 11 (Enformer, Borzoi)
largely retained one-hot encoding for its precision, while the
long-range models of Chapter 12 explore how sub-quadratic architectures
enable single-nucleotide tokenization at genomic scale.

\chapter{Protein Language Models}\label{protein-language-models}

\section{Evolutionary Sequences as Natural
Language}\label{evolutionary-sequences-as-natural-language}

Before transformers revolutionized genomic sequence modeling, they first
transformed our ability to model proteins. The success of protein
language models (PLMs) established a paradigm that would later inspire
genomic foundation models: treat biological sequences as a form of
natural language, train large transformer models on massive unlabeled
sequence databases, and extract functional knowledge through
self-supervised learning.

The analogy between protein sequences and natural language runs deeper
than mere metaphor. Both encode complex information in linear strings of
discrete tokens (amino acids or words). Both exhibit hierarchical
structure---motifs combine into domains as words combine into phrases.
Both have syntax (structural constraints) and semantics (functional
meaning). And crucially, both are shaped by evolutionary pressure:
natural selection filters protein sequences just as cultural selection
shapes language.

This chapter examines how protein language models pioneered biological
foundation modeling, from the ESM family's demonstration that
transformers can learn protein structure and function from sequence
alone, to their application in variant effect prediction and structure
determination. Understanding PLMs provides essential context for the
genomic language models covered in subsequent chapters, as many
architectural choices and training strategies transfer directly from
proteins to DNA.

\section{The ESM Model Family}\label{the-esm-model-family}

\subsection{ESM-1b: Establishing the
Paradigm}\label{esm-1b-establishing-the-paradigm}

The Evolutionary Scale Modeling (ESM) project, developed at Meta AI
Research, demonstrated that transformer language models trained on
protein sequences learn biologically meaningful representations without
explicit supervision (Rives et al. 2021).

\textbf{Training data}: ESM-1b was trained on UniRef50, a clustered
database of \textasciitilde33 million protein sequences covering the
known diversity of protein families. UniRef50 clusters sequences at 50\%
identity, providing broad coverage while reducing redundancy.

\textbf{Architecture}: ESM-1b uses a BERT-style bidirectional
transformer with 650 million parameters:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Component & Specification \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Layers & 33 \\
Hidden dimension & 1,280 \\
Attention heads & 20 \\
Parameters & 650M \\
Max sequence length & 1,024 amino acids \\
\end{longtable}

\textbf{Training objective}: Masked language modeling (MLM)---the model
learns to predict randomly masked amino acids given surrounding context.
This is analogous to BERT's masked token prediction, but operating on
amino acids rather than words.

\subsection{What ESM Learns}\label{what-esm-learns}

Despite never seeing structural or functional labels during training,
ESM learns representations that capture:

\textbf{Secondary structure}: Attention patterns in ESM correlate with
alpha helices and beta sheets. The model implicitly learns that certain
amino acid patterns form specific structural elements.

\textbf{Contact prediction}: ESM's attention heads capture
residue-residue contacts---amino acids that are distant in sequence but
close in 3D space. This emergent capability suggests the model learns
aspects of protein folding from sequence statistics alone.

\textbf{Evolutionary conservation}: Masked token predictions correlate
with position-specific conservation scores from multiple sequence
alignments. ESM effectively learns which positions tolerate variation
and which are constrained.

\textbf{Functional sites}: Attention concentrates on catalytic residues,
binding sites, and other functionally important positions, even without
explicit functional annotation.

\subsection{ESM-2: Scaling Up}\label{esm-2-scaling-up}

ESM-2 extended the ESM approach with larger models and improved training
(Z. Lin et al. 2022):

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Model & Parameters & Layers & Performance \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ESM-2 (8M) & 8M & 6 & Baseline \\
ESM-2 (35M) & 35M & 12 & +5\% contact prediction \\
ESM-2 (150M) & 150M & 30 & +8\% contact prediction \\
ESM-2 (650M) & 650M & 33 & +12\% contact prediction \\
ESM-2 (3B) & 3B & 36 & +15\% contact prediction \\
ESM-2 (15B) & 15B & 48 & State-of-the-art \\
\end{longtable}

Performance scales smoothly with model size across structure prediction,
contact prediction, and variant effect tasks---a phenomenon mirroring
the scaling laws observed in natural language processing.

\section{ProtTrans: Alternative
Architectures}\label{prottrans-alternative-architectures}

The ProtTrans family explored multiple transformer architectures for
protein sequences:

\textbf{ProtBERT}: BERT-style bidirectional encoder trained on BFD (Big
Fantastic Database), comprising \textasciitilde2.1 billion protein
sequences.

\textbf{ProtT5}: Encoder-decoder architecture based on T5, enabling both
understanding and generation tasks.

\textbf{ProtXLNet}: XLNet-style permutation language modeling, capturing
bidirectional context without the {[}MASK{]} token artifact.

ProtTrans models demonstrated that the protein language modeling
paradigm generalizes across architectures. The choice between
encoder-only (BERT-style) and encoder-decoder (T5-style) models depends
on the downstream application: encoders excel at classification and
embedding tasks, while encoder-decoders enable sequence generation.

\section{ESM-1v: Zero-Shot Variant Effect
Prediction}\label{esm-1v-zero-shot-variant-effect-prediction}

A critical application of protein language models is predicting the
effects of amino acid substitutions---missense variants that are the
most common type of protein-coding mutation.

\subsection{The Zero-Shot Approach}\label{the-zero-shot-approach}

ESM-1v (2021) demonstrated that PLMs can predict variant effects without
any training on variant labels. The approach exploits masked language
modeling: for a variant at position \(i\) changing amino acid \(a\) to
\(b\), compute:

\[\Delta \text{score} = \log P(b | \text{context}) - \log P(a | \text{context})\]

If the model assigns higher probability to the mutant amino acid than
the wild-type, the variant is predicted benign; if lower, deleterious.
This ``zero-shot'' prediction requires no labeled training data---the
model's evolutionary knowledge, learned from sequence databases,
directly informs variant interpretation.

\subsection{Genome-Wide Prediction}\label{genome-wide-prediction}

Brandes et al.~(2023) applied ESM-1b to predict effects for all
\textasciitilde450 million possible missense variants in the human
genome (Brandes et al. 2023):

\textbf{Scale}: Every position × every possible substitution across all
human proteins

\textbf{Performance on ClinVar}: ESM-1b outperformed existing methods in
classifying \textasciitilde150,000 ClinVar/HGMD missense variants as
pathogenic or benign

\textbf{Deep mutational scanning}: Strong correlation with experimental
measurements across 28 DMS datasets

\textbf{Isoform-specific effects}: \textasciitilde2 million variants
annotated as damaging only in specific protein isoforms, highlighting
the importance of considering alternative splicing

This work established PLMs as practical tools for clinical variant
interpretation, capable of scoring variants that lack experimental
characterization or evolutionary homologs.

\subsection{Benchmarking on
ProteinGym}\label{benchmarking-on-proteingym}

ProteinGym provides a comprehensive benchmark for variant effect
predictors, aggregating 217 deep mutational scanning assays covering
diverse proteins (Notin et al. 2023):

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Method & Mean Spearman ρ \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ESM-1v & 0.48 \\
EVE (evolutionary model) & 0.46 \\
DeepSequence & 0.44 \\
PolyPhen-2 & 0.32 \\
SIFT & 0.30 \\
\end{longtable}

PLMs achieve competitive or superior performance to methods that
explicitly model evolutionary conservation from multiple sequence
alignments, despite using only single sequences as input.

\section{ESMFold: Structure from
Sequence}\label{esmfold-structure-from-sequence}

\subsection{From Language Model to Structure
Predictor}\label{from-language-model-to-structure-predictor}

The most dramatic demonstration of PLM capabilities came with ESMFold,
which predicts protein 3D structure directly from ESM-2 embeddings (Z.
Lin et al. 2022).

Traditional structure prediction (including AlphaFold2) relies heavily
on multiple sequence alignments (MSAs)---computationally expensive
searches against sequence databases that can take hours per protein.
ESMFold eliminates this requirement:

\textbf{Architecture}: ESMFold couples ESM-2 (15B parameters) with a
structure module adapted from AlphaFold2. The language model embeddings
replace MSA-derived features.

\textbf{Speed}: \textasciitilde60× faster than AlphaFold2 for typical
proteins, enabling metagenomic-scale structure prediction

\textbf{Accuracy}: Achieves atomic-level accuracy for many proteins,
though slightly below AlphaFold2 for proteins that benefit from MSA
information

\subsection{What This Reveals About
PLMs}\label{what-this-reveals-about-plms}

ESMFold's success demonstrates that ESM-2's internal representations
encode sufficient information to determine 3D structure. The language
model has learned not just local sequence patterns but global folding
principles---what makes a sequence fold into a particular shape.

This has profound implications: the ``attention'' that transformers pay
to distant sequence positions during masked prediction is, in some
sense, learning the physics of protein folding. Residues that need to be
close in 3D space attend to each other in the transformer's attention
matrices.

\section{Transfer to Genomics: CADD and
AlphaMissense}\label{transfer-to-genomics-cadd-and-alphamissense}

\subsection{CADD v1.7: PLM Features for Variant
Prioritization}\label{cadd-v1.7-plm-features-for-variant-prioritization}

The Combined Annotation Dependent Depletion (CADD) framework integrates
diverse annotations to score variant deleteriousness (Chapter 3). CADD
v1.7 incorporated ESM-1v predictions as features (Schubach et al. 2024):

\textbf{Integration approach}: ESM-1v scores are computed for all
missense variants and included alongside conservation scores, functional
annotations, and regulatory predictions.

\textbf{Performance gains}:

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Benchmark & CADD v1.6 & CADD v1.7 & Improvement \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ClinVar pathogenic vs.~common & 0.94 & 0.95 & +1\% \\
Deep mutational scanning (31 datasets) & 0.78 & 0.81 & +3\% \\
\end{longtable}

The PLM features particularly improve scoring for variants in regions
with limited evolutionary conservation data, where traditional methods
struggle.

\subsection{AlphaMissense: Combining PLM and
Structure}\label{alphamissense-combining-plm-and-structure}

AlphaMissense represents the state-of-the-art in missense variant effect
prediction, combining PLM representations with structural context (Cheng
et al. 2023):

\textbf{Architecture}: AlphaMissense adapts AlphaFold's architecture,
fine-tuning on human and primate variant population frequency databases.
The model learns to predict pathogenicity by combining:

\begin{itemize}
\tightlist
\item
  Sequence embeddings from ESM-style language modeling
\item
  Structural context from predicted protein structures
\item
  Evolutionary information from cross-species comparisons
\end{itemize}

\textbf{Training data}: Population frequency databases (gnomAD) provide
weak labels---common variants are likely benign, absent variants may be
deleterious. Critically, AlphaMissense never trains on clinical
pathogenicity labels (ClinVar), yet achieves state-of-the-art
performance on clinical benchmarks.

\textbf{Scale}: Predictions for all \textasciitilde71 million possible
single amino acid substitutions across the human proteome

\textbf{Classification}: 89\% of missense variants classified as either
likely benign or likely pathogenic, providing actionable predictions for
the vast majority of possible variants

\subsection{Performance Comparison}\label{performance-comparison}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
Method & ClinVar AUC & DMS Correlation & Training Data \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SIFT & 0.78 & 0.30 & Conservation \\
PolyPhen-2 & 0.82 & 0.32 & Conservation + structure \\
CADD v1.7 & 0.95 & 0.81 & Multi-feature integration \\
ESM-1v & 0.89 & 0.48 & Sequence only (zero-shot) \\
AlphaMissense & 0.94 & 0.52 & PLM + structure + population \\
\end{longtable}

AlphaMissense achieves top performance by integrating the strengths of
multiple approaches: PLM-derived sequence understanding,
AlphaFold-derived structural context, and population genetics-derived
evolutionary constraint signals.

\section{Lessons for Genomic Language
Models}\label{lessons-for-genomic-language-models}

The success of protein language models established several principles
that inform genomic foundation modeling:

\subsection{Self-Supervision Works}\label{self-supervision-works}

PLMs demonstrated that massive amounts of biological knowledge can be
learned from unlabeled sequences. The same evolutionary pressures that
shape proteins also shape DNA---purifying selection removes deleterious
variants, leaving statistical signatures in sequence databases.

\subsection{Scale Matters}\label{scale-matters}

Performance improves predictably with model size, motivating the
development of larger genomic models. The 8M → 15B parameter progression
in ESM-2 showed consistent gains across tasks.

\subsection{Transfer Learning is
Effective}\label{transfer-learning-is-effective}

Representations learned for one task (masked token prediction) transfer
to other tasks (structure prediction, variant effects). This suggests
that self-supervised pretraining captures fundamental biological
knowledge rather than task-specific shortcuts.

\subsection{Architecture Choices}\label{architecture-choices}

The BERT-style bidirectional encoder proved highly effective for
proteins, where the entire sequence context is available. However,
genomic sequences present different challenges: much longer lengths
(genes span kilobases, genomes span gigabases), different information
density (proteins are information-dense, intergenic regions less so),
and different symmetries (DNA has reverse-complement structure absent in
proteins).

\subsection{Integration with Other
Modalities}\label{integration-with-other-modalities}

AlphaMissense showed that PLM embeddings combine effectively with
structural information. Similarly, genomic models benefit from
integration with epigenomic data, gene annotations, and other biological
context.

\section{Limitations and Ongoing
Challenges}\label{limitations-and-ongoing-challenges}

\subsection{Sequence Length}\label{sequence-length}

Most PLMs handle sequences up to \textasciitilde1,000--2,000 amino
acids. While sufficient for most individual proteins, this limits
modeling of large protein complexes and doesn't directly transfer to the
much longer sequences in genomics.

\subsection{Orphan Proteins}\label{orphan-proteins}

PLMs struggle with proteins that have few homologs in training
databases. ``Orphan'' or ``dark'' proteins---those unique to specific
lineages---lack the evolutionary signal that PLMs exploit.

\subsection{Epistasis}\label{epistasis}

Most variant effect predictions assume independence---the effect of
mutation A doesn't depend on whether mutation B is present. Real
proteins exhibit epistasis, where variant effects depend on sequence
context.

\subsection{Interpretability}\label{interpretability}

While attention patterns correlate with biological features,
understanding exactly what PLMs learn remains challenging. The field is
developing interpretation methods (Chapter 15), but PLMs remain
partially ``black box.''

\section{Significance}\label{significance}

Protein language models established that transformer architectures can
learn deep biological knowledge from sequence data alone. ESM's ability
to predict structure, function, and variant effects without explicit
labels demonstrated the power of self-supervised learning on
evolutionary data.

This success directly motivated the development of genomic language
models. If proteins are a language that transformers can learn, perhaps
DNA is too. The genomic language models covered in Chapter 10 adapt PLM
architectures and training strategies to the distinct challenges of DNA
sequences---longer contexts, different alphabets, and the full
complexity of gene regulation.

The integration path also continues: just as CADD v1.7 and AlphaMissense
incorporate PLM predictions, future models will integrate genomic and
proteomic language models into unified frameworks (Chapter 13, Chapter
16). The central dogma of molecular biology---DNA → RNA →
protein---suggests that models capturing all three modalities may
achieve the deepest understanding of how genomes encode life.

\chapter{DNA Foundation Models}\label{dna-foundation-models}

Genomic language models extend the ideas of protein language models
(Chapter 9) to the DNA level: they treat genomes themselves as a
``corpus,'' learn statistical regularities through self-supervision, and
reuse those representations for many downstream tasks.

Where Chapters 5--7 focused on supervised sequence-to-function CNNs and
specialized architectures, and Chapter 8 focused on representation and
tokenization, this chapter turns to \textbf{DNA foundation
models}---large, often transformer-based or hybrid architectures trained
on unlabeled genomic sequence at scale.

These models aim to provide a single, reusable backbone for tasks
ranging from regulatory annotation and variant effect prediction to
cross-species transfer and clinical prioritization. They mark the
transition from ``a model per dataset'' to \textbf{general-purpose
genomic backbones} analogous to BERT, GPT, and ESM in natural and
protein language modeling.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Supervised CNNs to Self-Supervised Genomic
LMs}\label{from-supervised-cnns-to-self-supervised-genomic-lms}

The CNN era (DeepSEA, ExPecto, SpliceAI; Chapters 5--7) shared a common
pattern:

\begin{itemize}
\tightlist
\item
  \textbf{Inputs:} One-hot encoded DNA sequence around a locus\\
\item
  \textbf{Targets:} Task-specific labels (chromatin marks, expression,
  splice junctions)\\
\item
  \textbf{Objective:} Predict those labels using supervised loss
  functions
\end{itemize}

This approach achieved remarkable performance but suffers from three
fundamental constraints:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Label dependence} -- Every new assay, cell type, or phenotype
  requires new labeled data.
\item
  \textbf{Task coupling} -- Model design is tightly coupled to the task
  (e.g., splice-aware architectures, expression-distance kernels).
\item
  \textbf{Limited reuse} -- Features learned for one problem do not
  automatically transfer to others.
\end{enumerate}

Protein language models (Chapter 9) showed a different route:
\textbf{self-supervised learning on unlabeled sequences}, with
downstream tasks solved by probing or fine-tuning. Genomic language
models import this recipe to DNA:

\begin{itemize}
\tightlist
\item
  \textbf{Data:} Large collections of genomic sequences across species,
  individuals, or functional regions.
\item
  \textbf{Objectives:}

  \begin{itemize}
  \tightlist
  \item
    Masked language modeling (MLM): predict masked bases or tokens.
  \item
    Next-token or sequence modeling: predict the next token in a
    sequence.
  \item
    Hybrid tasks: combine MLM with auxiliary objectives (e.g.,
    predicting annotations).
  \end{itemize}
\item
  \textbf{Usage modes:}

  \begin{itemize}
  \tightlist
  \item
    Freeze the model and train light-weight \textbf{probes} for specific
    tasks.
  \item
    Fine-tune the entire model (or adapters) for specialized downstream
    tasks.
  \item
    Use \textbf{zero-shot} or \textbf{few-shot} scoring by comparing
    log-likelihoods of alternative sequences or alleles.
  \end{itemize}
\end{itemize}

The promise is that \textbf{once} a sufficiently powerful backbone is
trained, it becomes the default starting point for nearly any DNA-level
prediction problem.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Early Genomic Transformers: DNABERT and
DNABERT-2}\label{early-genomic-transformers-dnabert-and-dnabert-2}

\subsection{DNABERT --- BERT for k-merized
DNA}\label{dnabert-bert-for-k-merized-dna}

DNABERT applied the BERT masked language modeling framework to genomic
sequences, using overlapping k-mers (e.g., 6-mers) as tokens and
training on human reference sequences (Ji et al. 2021). As discussed in
Chapter 8, this design had several defining characteristics:

\begin{itemize}
\tightlist
\item
  \textbf{Tokenization:} Overlapping k-mers created a discrete
  vocabulary of size (4\^{}k).
\item
  \textbf{Objective:} Masked token prediction, exactly as in BERT.
\item
  \textbf{Input scale:} Context windows of a few hundred base pairs
  (e.g., 512 tokens).
\item
  \textbf{Downstream evaluation:} Fine-tuned on tasks such as promoter
  classification, splice site prediction, and transcription factor
  binding.
\end{itemize}

DNABERT provided \emph{proof of concept} that:

\begin{itemize}
\tightlist
\item
  Self-supervised pretraining on raw DNA can improve performance over
  training from scratch.
\item
  Learned embeddings capture biologically meaningful regularities, even
  when trained only on the reference genome.
\item
  BERT-style architectures can be re-used across multiple downstream
  tasks.
\end{itemize}

However, the k-mer design also introduced the limitations detailed in
Chapter 8:

\begin{itemize}
\tightlist
\item
  No true sequence compression---overlapping k-mers do not reduce
  sequence length.
\item
  Ambiguity in positional interpretation---each nucleotide participates
  in multiple tokens.
\item
  Limited context and scaling, due to quadratic attention and redundant
  overlapping tokens.
\end{itemize}

\subsection{DNABERT-2 --- Toward Better Tokenization and
Efficiency}\label{dnabert-2-toward-better-tokenization-and-efficiency}

DNABERT-2 revisited both tokenization and architecture, highlighting how
much \textbf{representation} matters for genomic LMs (Z. Zhou et al.
2024).

Key differences relative to DNABERT:

\begin{itemize}
\tightlist
\item
  \textbf{Tokenization:} Improved schemes (e.g., BPE-style merges) that
  better compress redundancies and reduce sequence length.
\item
  \textbf{Efficiency:} Models that scale to larger contexts and corpora
  without prohibitive memory costs.
\item
  \textbf{Performance:} Consistent gains on a range of seq2label
  genomics benchmarks over both DNABERT and non-pretrained baselines.
\end{itemize}

DNABERT and DNABERT-2 collectively established that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Self-supervision on DNA works and is competitive with hand-engineered
  pipelines.
\item
  Tokenization choices (Chapter 8) have large practical consequences.
\item
  Masked LM training can produce reusable representations for diverse
  sequence tasks.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Scaling Context and Diversity: Nucleotide
Transformer}\label{scaling-context-and-diversity-nucleotide-transformer}

DNABERT showed feasibility, but its context windows and training data
were modest relative to the scale of genomes. \textbf{Nucleotide
Transformer} pushed much further, emphasizing \textbf{scale and
diversity} (Dalla-Torre et al. 2023):

\begin{itemize}
\tightlist
\item
  \textbf{Corpus:} Genomic data spanning multiple species and
  populations.
\item
  \textbf{Models:} Transformer encoders of various sizes, from moderate
  to very large parameter counts.
\item
  \textbf{Context length:} Up to \textasciitilde6 kb per input
  sequence---an order-of-magnitude jump over DNABERT while still using
  dense attention (Chapter 8).
\item
  \textbf{Training objective:} Masked language modeling on subsequences
  sampled from genomes.
\end{itemize}

The Nucleotide Transformer work contributed several important ideas:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Cross-species pretraining}\\
  Training on many genomes (rather than a single reference) exposes the
  model to:

  \begin{itemize}
  \tightlist
  \item
    Diverse sequence patterns and GC content.
  \item
    Different regulatory architectures.
  \item
    Evolutionary constraints that recur across lineages.
  \end{itemize}

  This mirrors the use of large multi-species multiple sequence
  alignments in protein LMs (Chapter 9) but operates at the raw DNA
  level.
\item
  \textbf{Benchmark suite}\\
  To quantify representation quality, Nucleotide Transformer introduced
  a \textbf{benchmark panel} of genomic tasks, commonly referred to in
  later work as the Nucleotide Transformer benchmarks (Dalla-Torre et
  al. 2023). Typical tasks include:

  \begin{itemize}
  \tightlist
  \item
    Promoter and enhancer classification.
  \item
    Histone mark and chromatin accessibility prediction.
  \item
    Variant/pathogenicity proxies.
  \item
    Regulatory element type classification.
  \end{itemize}

  Models are evaluated via linear probes, shallow classifiers, or light
  fine-tuning, providing a standard yardstick for later DNA LMs.
\item
  \textbf{Scaling trends}\\
  As with protein and natural-language models, performance improves
  predictably with:

  \begin{itemize}
  \tightlist
  \item
    Larger models.
  \item
    More pretraining data.
  \item
    Longer context windows.
  \end{itemize}

  These scaling curves help forecast the returns from investing in even
  larger genomic LMs.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Beyond Dense Attention: HyenaDNA and 1 Mb
Context}\label{beyond-dense-attention-hyenadna-and-1-mb-context}

Quadratic attention limits transformer context length to tens of
kilobases at best, even with aggressive engineering. \textbf{HyenaDNA}
replaces attention with a \textbf{Hyena} long-convolution / state-space
architecture that scales sub-quadratically and can process sequences up
to 1 Mb (Nguyen et al. 2023).

As summarized in Chapter 8:

\begin{longtable}[]{@{}llrl@{}}
\toprule\noalign{}
Model & Architecture & Max context & Complexity \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
DNABERT & Transformer & 512 bp & (O(L\^{}2)) \\
Nucleotide Transformer & Transformer & 6 kb & (O(L\^{}2)) \\
HyenaDNA & Hyena & 1 Mb & (O(L \log L)) \\
\end{longtable}

HyenaDNA introduced several qualitative advances:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Megabase-scale context}\\
  Processing 1 Mb windows allows the model to ``see'':

  \begin{itemize}
  \tightlist
  \item
    Entire gene bodies plus flanking regulatory regions.
  \item
    Long-range enhancer--promoter interactions.
  \item
    Topologically associating domain (TAD)-scale structure.
  \end{itemize}

  This aligns better with biological reality, where regulatory
  interactions often span tens to hundreds of kilobases.
\item
  \textbf{Single-nucleotide resolution}\\
  Despite its long context, HyenaDNA maintains \textbf{base-level
  resolution}, so single-nucleotide variants can be evaluated in the
  context of megabases of surrounding sequence.
\item
  \textbf{In-context learning signals}\\
  On Nucleotide Transformer benchmarks and additional tasks, HyenaDNA
  shows \textbf{in-context learning} behaviors---performance improves
  when examples are included in the input context without updating model
  weights (Nguyen et al. 2023). This suggests that at sufficient scale,
  DNA models can adapt to new tasks or distributions via prompts rather
  than fine-tuning, mirroring phenomena seen in large language models.
\item
  \textbf{State-of-the-art performance}\\
  On GenomicBenchmarks and related evaluations, HyenaDNA achieves
  state-of-the-art results on the majority of tasks, often by large
  margins, illustrating that architectural innovations can yield both
  \emph{longer context} and \emph{better predictive accuracy} (Nguyen et
  al. 2023).
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Generative Regulatory Foundation Models:
GROVER}\label{generative-regulatory-foundation-models-grover}

Most models above focus on sequence-level objectives (masked or
next-token). \textbf{GROVER} shifts attention from sequence to
\textbf{regulatory tracks} (Sanabria et al. 2024):

\begin{itemize}
\tightlist
\item
  \textbf{Inputs/outputs:} GROVER is trained on multi-track functional
  genomics signals (e.g., ATAC-seq, histone marks) across many cell
  types and tissues instead of raw sequence alone.
\item
  \textbf{Objective:} Predict masked or held-out regulatory profiles
  conditioned on neighboring tracks, cell-type embeddings, or limited
  sequence context.
\item
  \textbf{Architecture:} A transformer-style backbone tailored to
  \textbf{spatiotemporal grids} of genomic positions × assays × cell
  types.
\end{itemize}

GROVER plays a role analogous to self-supervised vision models for
images:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It treats regulatory profiles as a high-dimensional ``image'' over the
  genome.
\item
  It learns rich representations of \textbf{regulatory states} at each
  position.
\item
  It supports tasks like imputation of missing assays, denoising, and
  cell-type-specific activity prediction.
\end{enumerate}

While not a pure DNA language model, GROVER-style systems complement
sequence-based LMs:

\begin{itemize}
\tightlist
\item
  DNA LMs capture \textbf{what the genome can do} (the encoded
  potential).
\item
  Regulatory LMs like GROVER capture \textbf{what the genome is actually
  doing} in specific contexts (cell types, conditions).
\end{itemize}

Later chapters (Part IV) explore how sequence-based and regulatory
foundation models can be combined---e.g., using DNA LMs to parameterize
sequence priors and regulatory LMs for context-specific readouts
(Sanabria et al. 2024).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Central-Dogma-Aware and Annotation-Enriched
Models}\label{central-dogma-aware-and-annotation-enriched-models}

Chapter 8 discussed how tokenization can encode biological structure.
Some recent models push this further by integrating \textbf{central
dogma} and \textbf{genomic annotations} directly into the modeling
framework.

\subsection{Life-Code: Central Dogma as an Inductive
Bias}\label{life-code-central-dogma-as-an-inductive-bias}

\textbf{Life-Code} proposes codon-aware, central-dogma-informed
tokenization to bridge DNA, RNA, and protein within a single
language-modeling framework (Liu et al. 2025):

\begin{itemize}
\tightlist
\item
  \textbf{Coding regions:} Tokenized as codons (3-mers in frame),
  reflecting the genetic code.
\item
  \textbf{Noncoding regions:} Tokenized via learned subword units.
\item
  \textbf{Integration:} Unified representations span DNA → RNA →
  protein, enabling knowledge sharing across modalities.
\end{itemize}

Life-Code uses distillation from protein LMs (Chapter 9) to:

\begin{itemize}
\tightlist
\item
  Import protein-level structural knowledge into DNA/RNA sequence
  representations.
\item
  Improve performance on tasks involving coding sequence, such as
  predicting missense effects or expression changes.
\item
  Achieve competitive or state-of-the-art results on tasks across the
  full central dogma (DNA, RNA, protein) (Liu et al. 2025).
\end{itemize}

\subsection{BioToken: Encoding Variants and Structure as
Tokens}\label{biotoken-encoding-variants-and-structure-as-tokens}

\textbf{BioToken} extends tokenization beyond nucleotide content to
include \textbf{explicit genomic annotations} (Medvedev et al. 2025):

\begin{itemize}
\tightlist
\item
  \textbf{Variant-aware tokens:} Encode SNPs, insertions, and deletions
  as distinct tokens rather than implicit changes in sequence.
\item
  \textbf{Structural annotations:} Incorporate information about exons,
  introns, UTRs, promoters, enhancers, and other regulatory elements.
\item
  \textbf{Functional context:} Include signals such as chromatin state,
  conservation scores, or known regulatory motifs.
\end{itemize}

This design moves toward \textbf{fully structured genomic LMs}, where:

\begin{itemize}
\tightlist
\item
  The input ``sentence'' is not only DNA bases but also
  position-specific metadata.
\item
  Representations can directly integrate sequence, structure, and
  functional annotations.
\end{itemize}

Life-Code and BioToken foreshadow the \textbf{multi-modal, multi-omic
foundation models} discussed in Part IV, where sequence is only one of
many integrated information streams.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Using Genomic LMs in
Practice}\label{using-genomic-lms-in-practice}

Just as protein LMs can be used in different modes (frozen embeddings,
fine-tuning, zero-shot scoring; Chapter 9), genomic LMs have multiple
usage patterns.

\subsection{Embeddings as Universal
Features}\label{embeddings-as-universal-features}

The simplest way to use a genomic LM is to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Extract embeddings for windows around loci of interest (e.g., 1--6 kb
  segments).
\item
  Pool or select positions relevant to the task (e.g., promoters,
  candidate enhancers, variant sites).
\item
  Train a light-weight downstream model (linear layer, small MLP, or
  logistic regression).
\end{enumerate}

Applications include:

\begin{itemize}
\tightlist
\item
  \textbf{Regulatory element classification:} Distinguishing promoters,
  enhancers, silencers, and insulators.
\item
  \textbf{Chromatin state prediction:} Predicting ATAC-seq or histone
  mark presence from sequence alone, as an alternative to models like
  DeepSEA (Chapter 5).
\item
  \textbf{Variant effect scoring:} Replacing or augmenting hand-crafted
  features in frameworks like CADD with LM-derived features (analogous
  to CADD v1.7's use of PLM features; Chapter 9; Schubach et al.
  (2024)).
\item
  \textbf{Splicing and transcript modeling:} Combining LM embeddings
  with splice-aware architectures like SpliceAI (Chapter 7).
\end{itemize}

Because the LM is frozen, this approach is computationally efficient and
avoids catastrophic forgetting when new tasks are added.

\subsection{Fine-Tuning and Task-Specific
Heads}\label{fine-tuning-and-task-specific-heads}

When more labels are available, \textbf{fine-tuning} can significantly
improve performance:

\begin{itemize}
\tightlist
\item
  \textbf{Full fine-tuning:} Update all LM parameters for a specific
  task.
\item
  \textbf{Adapter-based tuning:} Insert small bottleneck modules into
  each layer and update only those, keeping the backbone mostly frozen.
\item
  \textbf{Prompt-based tuning:} Learn task-specific prompts or prefix
  embeddings that steer the model's behavior without changing its core
  weights.
\end{itemize}

Fine-tuning is especially valuable for:

\begin{itemize}
\tightlist
\item
  High-stakes clinical tasks where every percentage point matters.
\item
  Tasks that probe very specific sequence-function relationships (e.g.,
  particular TF binding specificities).
\item
  Scenarios where domain shift is large (e.g., applying a cross-species
  LM to a previously unseen clade).
\end{itemize}

\subsection{Zero-Shot and Few-Shot Variant
Scoring}\label{zero-shot-and-few-shot-variant-scoring}

Analogous to protein models like ESM-1v and AlphaMissense (Chapter 9;
Brandes et al. (2023); Cheng et al. (2023)), genomic LMs can be used to
compute \textbf{zero-shot variant scores} by:

\begin{itemize}
\tightlist
\item
  Comparing the log-likelihood (or pseudo-likelihood) of sequences with
  reference vs alternate alleles.
\item
  Measuring changes in masked-token prediction probabilities at variant
  positions.
\item
  Evaluating the impact of a variant on internal representations (e.g.,
  vector distances between reference and variant embeddings).
\end{itemize}

These approaches can:

\begin{itemize}
\tightlist
\item
  Provide rapid prioritization of novel variants without task-specific
  training.
\item
  Complement supervised classifiers trained on clinical or functional
  labels (e.g., ClinVar, curated datasets).
\item
  Offer a starting point for more specialized models (e.g.,
  exon-specific splice models, enhancer-specific expression predictors).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Evaluation, Benchmarks, and
Pitfalls}\label{evaluation-benchmarks-and-pitfalls}

As genomic LMs proliferate, evaluation practices become crucial.

\subsection{Benchmark Suites}\label{benchmark-suites}

Nucleotide Transformer introduced a widely used benchmark panel
(Dalla-Torre et al. 2023), and later work, including HyenaDNA and
Life-Code, also reports results on \textbf{GenomicBenchmarks} and
related collections (Nguyen et al. 2023). Common traits of these suites
include:

\begin{itemize}
\tightlist
\item
  \textbf{Multiple task families:}

  \begin{itemize}
  \tightlist
  \item
    Promoter/enhancer classification.
  \item
    TF binding prediction.
  \item
    Chromatin accessibility and histone marks.
  \item
    Splicing, TSS/TES prediction, or other sequence-label tasks.
  \end{itemize}
\item
  \textbf{Standardized splits:}

  \begin{itemize}
  \tightlist
  \item
    Train/validation/test partitions.
  \item
    Consistent evaluation metrics (AUROC, AUPRC, accuracy).
  \end{itemize}
\item
  \textbf{Baseline comparisons:}

  \begin{itemize}
  \tightlist
  \item
    Non-pretrained CNNs and transformers.
  \item
    Earlier models like DeepSEA, ExPecto, and SpliceAI.
  \item
    Previously published genomic LMs.
  \end{itemize}
\end{itemize}

These benchmarks help separate \textbf{true representational gains} from
gains due to dataset choice or training tricks.

\subsection{Data Leakage and
Overfitting}\label{data-leakage-and-overfitting}

Genomic data pose unique leakage challenges:

\begin{itemize}
\tightlist
\item
  \textbf{Nearby windows:} Overlapping or adjacent windows from the same
  locus can leak between train and test.
\item
  \textbf{Reverse-complement duplicates:} Genomic sequences and their
  reverse complements may appear in both splits.
\item
  \textbf{Homologous regions:} Repeats, segmental duplications, and
  conserved elements across chromosomes or species can create hidden
  redundancy.
\end{itemize}

To mitigate this, researchers increasingly use:

\begin{itemize}
\tightlist
\item
  \textbf{Chromosome-level splits:} Holding out entire chromosomes for
  testing.
\item
  \textbf{Species-level splits:} Training on some species and testing on
  others.
\item
  \textbf{Cell-type and tissue splits:} Ensuring that specific
  biological contexts are unseen during training of downstream probes.
\end{itemize}

Even with these precautions, interpreting benchmark gains requires
caution: some tasks are easier than others, and improvements may reflect
distributional quirks rather than truly deeper understanding of genomic
function.

\subsection{Distribution Shift and Clinical
Utility}\label{distribution-shift-and-clinical-utility}

A model that performs well on curated benchmarks may still struggle in
real-world scenarios:

\begin{itemize}
\tightlist
\item
  \textbf{Population diversity:} Training corpora may underrepresent
  certain ancestries, leading to biased variant scoring (Chapter 2).
\item
  \textbf{Assay heterogeneity:} Experimental conditions, labs, and
  technologies differ from the curated datasets used in training.
\item
  \textbf{Phenotypic complexity:} Many clinically relevant phenotypes
  involve long causal chains---from variant to molecular consequence to
  tissue-level effect to disease.
\end{itemize}

Thus, genomic LM evaluation increasingly includes:

\begin{itemize}
\tightlist
\item
  Cross-population robustness.
\item
  Out-of-distribution testing (new tissues, cell types, or species).
\item
  End-to-end evaluations on clinically relevant endpoints (e.g., disease
  risk prediction, rare disease diagnosis), often in combination with
  traditional statistical genetics tools.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Lessons and Outlook}\label{lessons-and-outlook}

DNA language models bring the ``foundation model'' paradigm to the
genome. Several themes emerge:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Representation is central}\\
  Tokenization and context length (Chapter 8) are not superficial
  implementation details---they determine what patterns a model can see
  and how efficiently it can learn. Life-Code and BioToken show that
  \textbf{biologically informed tokenization} and annotations can serve
  as powerful inductive biases (Liu et al. 2025; Medvedev et al. 2025).
\item
  \textbf{Scale and diversity matter}\\
  Nucleotide Transformer and HyenaDNA demonstrate that performance
  improves with both \textbf{model size} and \textbf{training data
  diversity} (Dalla-Torre et al. 2023; Nguyen et al. 2023). Including
  multiple species, populations, and genomic contexts yields more robust
  representations.
\item
  \textbf{Long-range context is biologically necessary}\\
  Many regulatory phenomena operate at tens to hundreds of kilobases.
  Megabase-scale models like HyenaDNA show that we can finally begin to
  model these interactions at single-base resolution in a single forward
  pass.
\item
  \textbf{Self-supervision and supervision are complementary}\\
  Self-supervised LMs excel at learning broad, reusable features, but
  they do not automatically solve every downstream problem. Specialized
  architectures and supervised objectives (e.g., Enformer and related
  models in Chapter 11) remain crucial for accurate quantitative
  prediction of complex genomic readouts.
\item
  \textbf{Integration with other modalities is the next frontier}\\
  Models like GROVER, Life-Code, and BioToken hint at a future where DNA
  LMs are one part of larger \textbf{multi-modal genomic foundation
  models} that integrate:

  \begin{itemize}
  \tightlist
  \item
    Sequence (DNA, RNA, protein).
  \item
    Regulatory profiles (chromatin, expression).
  \item
    3D genome organization.
  \item
    Population genetics, phenotypes, and clinical data.
  \end{itemize}
\end{enumerate}

This chapter has focused on \textbf{sequence-centric DNA LMs} and their
immediate extensions. In Chapter 11, we turn to Enformer and related
long-range sequence-to-function models that explicitly predict molecular
readouts from sequence, closing the loop between \textbf{self-supervised
sequence understanding} and \textbf{supervised functional prediction}.

\chapter{Long-range Hybrid Models}\label{long-range-hybrid-models}

\section{Why Expression Needs Long-Range
Models}\label{why-expression-needs-long-range-models}

ExPecto (Chapter 6) showed that gene expression can be predicted
\emph{ab initio} from sequence by combining a CNN-based chromatin model
(Beluga) with a separate regression layer mapping chromatin features to
expression across tissues (J. Zhou et al. 2018). This modular strategy
worked surprisingly well, but it inherited two key limitations from its
DeepSEA-style backbone (Chapter 5):

\begin{itemize}
\tightlist
\item
  \textbf{Restricted context}: A 40 kb input window captures proximal
  promoters and some nearby enhancers, but many regulatory interactions
  span 100 kb or more.
\item
  \textbf{Two-stage learning}: Chromatin prediction and expression
  prediction are trained separately, leaving no opportunity for the
  expression objective to shape the representation of sequence.
\end{itemize}

As genomic datasets grew (ENCODE, Roadmap, FANTOM, GTEx, and others;
Chapter 4), it became clear that:

\begin{itemize}
\tightlist
\item
  Enhancers can regulate genes \textbf{hundreds of kilobases away}.
\item
  eQTLs often sit \textbf{outside promoter windows} traditionally used
  for expression models.
\item
  Chromatin conformation (loops, TADs) introduces \textbf{non-local
  dependencies} between DNA segments.
\end{itemize}

Pure CNN architectures can expand their receptive field using dilated
convolutions and pooling, but doing so at single-nucleotide resolution
quickly becomes parameter- and memory-intensive. On the other hand,
classic transformer architectures can model long-range dependencies via
attention, but their \textbf{\(O(L^2)\) runtime and memory} makes naïve
application to 200 kb sequences infeasible (Chapter 10).

Hybrid architectures like \textbf{Enformer} and \textbf{Borzoi} emerged
as a compromise:

\begin{itemize}
\tightlist
\item
  Use \textbf{convolutions} to extract local motif features and
  progressively downsample the sequence into a manageable number of
  latent positions.
\item
  Apply \textbf{self-attention} over this compressed representation to
  capture long-range regulatory interactions across
  \textasciitilde100--200 kb.
\item
  Predict \textbf{many signals at once} (chromatin profiles,
  transcription start site activity, RNA-seq coverage), enabling
  multi-task learning and rich variant effect prediction.
\end{itemize}

This chapter focuses on these hybrid designs---particularly Enformer (Ž.
Avsec et al. 2021) and Borzoi (Linder et al. 2025)---and how they
changed what ``sequence-to-expression'' models can do.

\section{Problem Setting: Sequence-to-Expression at
Scale}\label{problem-setting-sequence-to-expression-at-scale}

The models in this chapter tackle a demanding version of the classic
problem:

\begin{quote}
Given a long DNA sequence window around a genomic locus, predict a rich
set of regulatory and transcriptional readouts across many cell types.
\end{quote}

\subsection{Inputs}\label{inputs}

\begin{itemize}
\tightlist
\item
  \textbf{DNA sequence}: One-hot encoded sequence:

  \begin{itemize}
  \tightlist
  \item
    Length: typically \textbf{\textasciitilde200 kb} centered on a
    candidate promoter or gene.
  \item
    Alphabet: A/C/G/T (N masked or handled by special channels).
  \end{itemize}
\item
  \textbf{Positional indexing}:

  \begin{itemize}
  \tightlist
  \item
    The model must know \emph{where} promoter-proximal bases and distal
    elements are, relative to each other.
  \item
    Positional information is encoded via \textbf{convolutional
    receptive fields} and/or explicit \textbf{positional embeddings} for
    the attention layers.
  \end{itemize}
\end{itemize}

\subsection{Outputs}\label{outputs}

Enformer and Borzoi are both \textbf{multi-task, multi-position}
sequence-to-signal models:

\begin{itemize}
\tightlist
\item
  \textbf{Multiple assays}:

  \begin{itemize}
  \tightlist
  \item
    DNase/ATAC-seq (chromatin accessibility)
  \item
    Histone marks (e.g., H3K4me3, H3K27ac, etc.)
  \item
    CAGE or RNA-seq signals related to transcription and expression.
  \end{itemize}
\item
  \textbf{Multiple cell types / conditions}:

  \begin{itemize}
  \tightlist
  \item
    Hundreds of tracks, each representing a signal in a particular cell
    type or experimental context.
  \end{itemize}
\item
  \textbf{Multiple positions along the window}:

  \begin{itemize}
  \tightlist
  \item
    Predictions are made at fixed strides across the input window (e.g.,
    every 128 or 256 bp), yielding a \textbf{coverage track} rather than
    a single scalar.
  \end{itemize}
\end{itemize}

\subsection{Loss Functions}\label{loss-functions}

Typical objective:

\begin{itemize}
\tightlist
\item
  \textbf{Per-track, per-position regression}:

  \begin{itemize}
  \tightlist
  \item
    Often a \textbf{Poisson} or \textbf{negative binomial} likelihood on
    read counts.
  \item
    Sometimes log-transformed counts with a mean-squared error loss.
  \end{itemize}
\item
  \textbf{Multi-task weighting}:

  \begin{itemize}
  \tightlist
  \item
    All tracks contribute to the loss.
  \item
    Some models tune weights to prevent abundant assays (e.g., DNase)
    from dominating scarce but important ones (e.g., rare histone
    marks).
  \end{itemize}
\end{itemize}

The learning problem is thus:

\[
f_\theta: \text{DNA sequence (≈200 kb)} \rightarrow \text{[Tracks × Positions] continuous outputs}
\]

with \(\theta\) shared across assays, cell types, and genomic loci.

\section{Enformer: CNN + Attention for 200 kb
Context}\label{enformer-cnn-attention-for-200-kb-context}

Enformer (Ž. Avsec et al. 2021) is a landmark model that directly
integrates \textbf{long-range sequence context} with
\textbf{cell-type-specific expression prediction}, using a hybrid
CNN--transformer architecture.

\subsection{Architectural Overview}\label{architectural-overview}

Conceptually, Enformer has three stages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Convolutional stem}:\\
  Extract local motifs and progressively downsample the sequence.
\item
  \textbf{Transformer trunk}:\\
  Apply self-attention to model long-range dependencies between
  downsampled positions.
\item
  \textbf{Heads for multi-task outputs}:\\
  Decode the attended representation into assay- and cell-type-specific
  coverage tracks.
\end{enumerate}

A high-level architecture table is:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2115}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.4038}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3846}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Characteristics
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
CNN stem & Local motif extraction, downsampling & Residual + dilated
convs, pooling \\
Transformer blocks & Long-range dependency modeling & Multi-head
self-attention, MLPs \\
Output heads & Predict assays across positions \& cells & Task-specific
linear projections \\
\end{longtable}

\subsubsection{1. Convolutional Stem}\label{convolutional-stem}

The convolutional front-end:

\begin{itemize}
\tightlist
\item
  Takes \textbf{\textasciitilde200 kb one-hot sequence} as input.
\item
  Applies stacked \textbf{conv--norm--nonlinearity--pooling} layers.
\item
  Expands receptive field while downsampling length by a large factor
  (e.g., 128--256×).
\end{itemize}

The resulting representation can be viewed as:

\begin{itemize}
\tightlist
\item
  A sequence of \textbf{\(L'\) latent tokens} (\(L' \ll L\)), each
  summarizing a multi-kilobase region.
\item
  Each token encodes local motif configurations and short-range
  regulatory patterns.
\end{itemize}

This step solves the ``attention on raw nucleotides'' problem by:

\begin{itemize}
\tightlist
\item
  Reducing a 200,000 bp sequence into, say,
  \textbf{\textasciitilde1,000--2,000 tokens}.
\item
  Allowing attention to operate at a much lower effective resolution.
\end{itemize}

\subsubsection{2. Transformer Trunk}\label{transformer-trunk}

Enformer then applies several \textbf{transformer blocks} over the
compressed sequence:

\begin{itemize}
\tightlist
\item
  \textbf{Multi-head self-attention}:

  \begin{itemize}
  \tightlist
  \item
    Every downsampled position can attend to every other position.
  \item
    Captures relationships between distant enhancers and promoters, or
    between multiple regulatory elements.
  \end{itemize}
\item
  \textbf{Feed-forward networks (MLPs)}:

  \begin{itemize}
  \tightlist
  \item
    Nonlinear mixing of information at each position.
  \end{itemize}
\item
  \textbf{Residual connections and normalization}:

  \begin{itemize}
  \tightlist
  \item
    Stabilize training and enable deep stacks.
  \end{itemize}
\end{itemize}

Intuitively:

\begin{itemize}
\tightlist
\item
  Convolution layers answer:\\
  \emph{``What motifs and local patterns exist in this region?''}
\item
  Attention layers answer:\\
  \emph{``How do these regions interact across the 200 kb window to
  shape regulatory activity?''}
\end{itemize}

\subsubsection{3. Multi-Task Output
Heads}\label{multi-task-output-heads}

After attention, Enformer:

\begin{itemize}
\tightlist
\item
  Applies task-specific heads to each position in the latent sequence.
\item
  Produces coverage predictions for each \textbf{assay × cell type}
  combination.
\end{itemize}

For CAGE-based transcription start site (TSS) activity:

\begin{itemize}
\tightlist
\item
  The model predicts coverage around TSS positions.
\item
  Gene-level expression metrics can be obtained by \textbf{aggregating}
  predictions at positions near annotated TSSs (e.g., summing or
  averaging log counts across a small window).
\end{itemize}

\subsection{Training Data and
Objective}\label{training-data-and-objective}

Enformer is trained on a large collection of human and mouse regulatory
datasets:

\begin{itemize}
\tightlist
\item
  \textbf{Human}:

  \begin{itemize}
  \tightlist
  \item
    DNase, histone ChIP-seq, and CAGE across many cell types.
  \end{itemize}
\item
  \textbf{Mouse}:

  \begin{itemize}
  \tightlist
  \item
    Analogous assays used for \textbf{cross-species learning}.
  \end{itemize}
\end{itemize}

Key design choices:

\begin{itemize}
\tightlist
\item
  \textbf{Joint human--mouse training}:

  \begin{itemize}
  \tightlist
  \item
    Encourages the model to learn regulatory principles conserved across
    mammals.
  \item
    Enables zero-shot transfer between species for some tasks.
  \end{itemize}
\item
  \textbf{Chromosome holdout}:

  \begin{itemize}
  \tightlist
  \item
    Entire chromosomes held out for evaluation to avoid overly
    optimistic performance via local sequence similarity.
  \end{itemize}
\end{itemize}

The loss aggregates over:

\begin{itemize}
\tightlist
\item
  All targets (tracks).
\item
  All positions in the output window.
\item
  All training loci.
\end{itemize}

\subsection{Enformer as a Variant Effect
Predictor}\label{enformer-as-a-variant-effect-predictor}

Like DeepSEA, Enformer can be used for \textbf{in silico variant effect
prediction}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Extract a \textbf{200 kb window} around a locus from the reference
  genome.
\item
  Run Enformer to obtain predicted coverage tracks.
\item
  Introduce an alternative allele (e.g., SNP) into the window.
\item
  Re-predict coverage and compute \textbf{Δ-prediction}:

  \[
  \Delta \text{signal} = f_\theta(\text{alt sequence}) - f_\theta(\text{ref sequence})
  \]
\item
  Aggregate Δ-predictions around TSSs to quantify \textbf{predicted
  expression change} for genes in each cell type.
\end{enumerate}

This approach allows:

\begin{itemize}
\tightlist
\item
  Fine-grained assessment of how a variant might alter promoter-proximal
  signals and distal enhancer contributions.
\item
  Integration into downstream tools (e.g., fine-mapping pipelines) that
  require variant-level scores.
\end{itemize}

\subsection{eQTL Validation via GTEx}\label{eqtl-validation-via-gtex}

Enformer's variant effect predictions were systematically evaluated
using GTEx eQTL data (Chapter 4):

\begin{itemize}
\tightlist
\item
  For each gene--tissue pair:

  \begin{itemize}
  \tightlist
  \item
    Known eQTLs (lead variants) and non-eQTL variants in LD were
    compared.
  \end{itemize}
\item
  \textbf{Signed LD profile (SLDP) regression}:

  \begin{itemize}
  \tightlist
  \item
    Correlates predicted expression effects with observed eQTL effect
    sizes, accounting for LD structure.
  \end{itemize}
\item
  Findings (Ž. Avsec et al. 2021):

  \begin{itemize}
  \tightlist
  \item
    Enformer's predictions showed \textbf{stronger alignment with
    observed eQTLs} than prior models like Basenji2 (a purely
    convolutional long-range model).
  \item
    Improvement was especially notable at \textbf{distal regulatory
    variants}, where long-range attention is crucial.
  \end{itemize}
\end{itemize}

In practice, this means Enformer:

\begin{itemize}
\tightlist
\item
  Can prioritize variants likely to be causal eQTLs.
\item
  Provides \textbf{cell-type-specific effect predictions}, which are
  critical for interpreting tissues with sparse experimental data.
\end{itemize}

\subsection{Interpretation and Mechanistic
Insight}\label{interpretation-and-mechanistic-insight}

While Enformer is a complex model, several interpretation strategies
provide mechanistic insight:

\begin{itemize}
\tightlist
\item
  \textbf{Gradient-based attribution}:

  \begin{itemize}
  \tightlist
  \item
    Compute gradients of gene-level expression predictions with respect
    to input sequence.
  \item
    Highlight bases or motifs that drive the predicted expression of a
    gene in a specific cell type.
  \end{itemize}
\item
  \textbf{In silico mutagenesis}:

  \begin{itemize}
  \tightlist
  \item
    Systematically mutate bases to estimate their impact on a target
    gene or track.
  \item
    Identify enhancers and key transcription factor binding sites
    controlling expression.
  \end{itemize}
\item
  \textbf{Perturbation of attention}:

  \begin{itemize}
  \tightlist
  \item
    Analyze which positions attend most strongly to a promoter,
    revealing candidate long-range enhancers.
  \end{itemize}
\end{itemize}

These tools have been used to:

\begin{itemize}
\tightlist
\item
  Map \textbf{promoter--enhancer interactions} directly from sequence.
\item
  Suggest \textbf{causal regulatory elements} for disease-associated
  variants.
\end{itemize}

\section{Borzoi: Transcriptome-Centric Hybrid
Modeling}\label{borzoi-transcriptome-centric-hybrid-modeling}

Enformer is primarily trained on \textbf{chromatin and CAGE} profiles.
Borzoi (Linder et al. 2025) extends the hybrid architecture paradigm to
model the \textbf{RNA transcriptome itself}, with an emphasis on
finer-grained transcriptional features.

\subsection{Motivation}\label{motivation}

RNA-seq data carries richer information than a single expression scalar
per gene:

\begin{itemize}
\tightlist
\item
  \textbf{Coverage along exons and introns}:

  \begin{itemize}
  \tightlist
  \item
    Reflects transcription initiation, elongation, and termination.
  \end{itemize}
\item
  \textbf{Splice junction usage}:

  \begin{itemize}
  \tightlist
  \item
    Reveals alternative splicing patterns (complementing Chapter 7's
    SpliceAI).
  \end{itemize}
\item
  \textbf{Polyadenylation and 3′ UTR usage}:

  \begin{itemize}
  \tightlist
  \item
    Impacts mRNA stability, localization, and translation.
  \end{itemize}
\end{itemize}

A general-purpose model that predicts \textbf{base-level RNA-seq read
coverage} from DNA sequence could:

\begin{itemize}
\tightlist
\item
  Provide a \textbf{unified framework} for transcript-level variant
  effect prediction (transcription, splicing, polyadenylation).
\item
  Offer mechanistic insight into how regulatory sequence features shape
  the full life cycle of transcripts.
\end{itemize}

\subsection{Architectural Highlights}\label{architectural-highlights}

Borzoi builds on the Enformer-style backbone:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Convolutional front-end}:

  \begin{itemize}
  \tightlist
  \item
    Processes long DNA windows (on the order of \textasciitilde100--200
    kb).
  \item
    Learns local motifs and regulatory patterns at single-nucleotide or
    modestly downsampled resolution.
  \end{itemize}
\item
  \textbf{Hybrid long-range module}:

  \begin{itemize}
  \tightlist
  \item
    Uses attention and/or long-range convolutions to integrate
    information across the entire context.
  \item
    Explicitly designed to capture relationships between promoters,
    internal exons, and distal elements.
  \end{itemize}
\item
  \textbf{Multi-layer output heads}:

  \begin{itemize}
  \tightlist
  \item
    Predict \textbf{RNA-seq coverage tracks} across the window.
  \item
    Output separate tracks for:

    \begin{itemize}
    \tightlist
    \item
      Sense vs antisense transcription.
    \item
      Splice junction signals.
    \item
      PolyA-related coverage around 3′ ends.
    \end{itemize}
  \end{itemize}
\end{enumerate}

Like Enformer, Borzoi is trained in a \textbf{multi-task} regime, but
with a stronger emphasis on \textbf{RNA-related readouts}.

\subsection{From Chromatin Signals to RNA
Readouts}\label{from-chromatin-signals-to-rna-readouts}

Conceptually, Borzoi closes the loop:

\begin{itemize}
\tightlist
\item
  DeepSEA/Beluga/Enformer:\\
  \textbf{Sequence → chromatin + transcription start activity}
\item
  Borzoi:\\
  \textbf{Sequence → full transcriptome coverage}
\end{itemize}

This supports several analyses:

\begin{itemize}
\tightlist
\item
  \textbf{Promoter usage}:

  \begin{itemize}
  \tightlist
  \item
    Distinguish alternative promoter TSSs based on coverage patterns.
  \end{itemize}
\item
  \textbf{Alternative splicing}:

  \begin{itemize}
  \tightlist
  \item
    Predict differential exon inclusion or skipping, complementing
    specialized models like SpliceAI.
  \end{itemize}
\item
  \textbf{3′ UTR and polyA site choice}:

  \begin{itemize}
  \tightlist
  \item
    Model coverage drop-offs and polyA-linked patterns.
  \end{itemize}
\end{itemize}

Variant effect prediction follows similar steps as with Enformer:

\begin{itemize}
\tightlist
\item
  Predict transcriptome outputs for reference and alternate sequences.
\item
  Compute Δ-coverage at exons, splice junctions, and 3′ ends.
\item
  Aggregate into variant-level scores for tasks like eQTL or sQTL
  prioritization.
\end{itemize}

\section{What Hybrid Models Changed}\label{what-hybrid-models-changed}

Hybrid CNN--transformer sequence models like Enformer and Borzoi
introduced several conceptual advances over earlier architectures.

\subsection{1. Explicit Long-Range
Modeling}\label{explicit-long-range-modeling}

By combining convolutional downsampling with attention over latent
tokens, these models:

\begin{itemize}
\tightlist
\item
  Achieve \textbf{hundreds of kilobases of effective context} with
  manageable compute.
\item
  Allow \textbf{all positions in the compressed representation to
  interact}, approximating many possible promoter--enhancer
  relationships.
\end{itemize}

This is crucial for:

\begin{itemize}
\tightlist
\item
  Capturing \textbf{distal enhancers} that sit far from genes.
\item
  Modeling \textbf{complex regulatory architectures} where multiple
  enhancers and silencers integrate to control expression.
\end{itemize}

\subsection{2. Unified Multi-Task Learning Across
Modalities}\label{unified-multi-task-learning-across-modalities}

Hybrid models jointly predict:

\begin{itemize}
\tightlist
\item
  Chromatin accessibility.
\item
  Histone marks.
\item
  Transcriptional activity (CAGE, RNA-seq).
\end{itemize}

The result:

\begin{itemize}
\tightlist
\item
  \textbf{Shared representations} that capture general regulatory logic.
\item
  \textbf{Regularization} across assays and cell types, reducing
  overfitting to any single dataset.
\item
  A pathway to \textbf{transfer learning}, where a single pretrained
  model can be adapted to downstream tasks.
\end{itemize}

\subsection{3. Improved Variant Effect Prediction for
Expression}\label{improved-variant-effect-prediction-for-expression}

Compared to earlier CNN-only models (DeepSEA, Beluga, ExPecto,
Basenji2):

\begin{itemize}
\tightlist
\item
  Enformer demonstrated stronger \textbf{eQTL concordance} and better
  performance on expression-related benchmarks (Ž. Avsec et al. 2021).
\item
  Hybrid designs can identify \textbf{distal causal variants} more
  reliably, because their architecture naturally encodes long-range
  dependencies.
\end{itemize}

Borzoi takes this further by providing detailed transcriptome-level
readouts, enabling:

\begin{itemize}
\tightlist
\item
  Combined assessment of \textbf{transcription, splicing, and
  polyadenylation} for each variant.
\item
  A richer mechanistic understanding of how sequence variation impacts
  the full RNA life cycle.
\end{itemize}

\section{Limitations and Failure
Modes}\label{limitations-and-failure-modes}

Despite their power, hybrid long-range models are not omniscient and
introduce new challenges.

\subsection{Data and Label
Limitations}\label{data-and-label-limitations}

\begin{itemize}
\tightlist
\item
  \textbf{Biased training data}:

  \begin{itemize}
  \tightlist
  \item
    ENCODE/Roadmap assays focus on specific cell types, conditions, and
    regions.
  \item
    GTEx eQTLs are enriched for certain ancestries (Chapter 2).
  \end{itemize}
\item
  \textbf{Missing modalities}:

  \begin{itemize}
  \tightlist
  \item
    Many regulatory phenomena (e.g., RNA binding protein effects, 3D
    structure beyond contact frequency) are only partially captured by
    the available assays.
  \end{itemize}
\end{itemize}

As a result, the models may:

\begin{itemize}
\tightlist
\item
  Underperform in \textbf{cell types or ancestries not well represented}
  in the training data.
\item
  Misinterpret patterns that are confounded by \textbf{technical
  artifacts} (batch effects, mapping biases).
\end{itemize}

\subsection{Sequence Context and
Generalization}\label{sequence-context-and-generalization}

\begin{itemize}
\tightlist
\item
  Enformer and Borzoi are trained on \textbf{fixed window sizes} around
  annotated loci:

  \begin{itemize}
  \tightlist
  \item
    Behavior outside those canonical windows may be less reliable.
  \end{itemize}
\item
  Training focuses on \textbf{reference genome context}:

  \begin{itemize}
  \tightlist
  \item
    Large indels, structural variants, or rearrangements may be poorly
    modeled.
  \end{itemize}
\item
  The models assume \textbf{linear genomic context}:

  \begin{itemize}
  \tightlist
  \item
    3D chromatin architecture is only indirectly captured via sequence
    patterns correlated with looping; explicit Hi-C or Micro-C
    integration is limited.
  \end{itemize}
\end{itemize}

\subsection{Interpretability and
Trust}\label{interpretability-and-trust}

Although attribution methods exist:

\begin{itemize}
\tightlist
\item
  Attention weights and gradient-based scores are \textbf{not direct
  causal evidence}.
\item
  Attributions can be \textbf{noisy} and sensitive to how targets are
  aggregated.
\item
  For clinical use, predictions often require \textbf{orthogonal
  validation}, e.g., CRISPR perturbation or allele-specific expression
  assays.
\end{itemize}

These issues are part of the broader interpretability challenges
discussed in later chapters on evaluation and confounders.

\section{Role in the GFM Landscape}\label{role-in-the-gfm-landscape}

Hybrid architectures like Enformer and Borzoi occupy an interesting
middle ground between \textbf{task-specific CNNs} and
\textbf{general-purpose genomic foundation models}:

\begin{itemize}
\tightlist
\item
  Compared to earlier CNN systems:

  \begin{itemize}
  \tightlist
  \item
    They model \textbf{much longer context} and support richer
    multi-modal outputs.
  \item
    They offer significantly improved \textbf{expression-related variant
    effect prediction}.
  \end{itemize}
\item
  Compared to modern GFMs (Chapters 12--13):

  \begin{itemize}
  \tightlist
  \item
    They are \textbf{specialized} and \textbf{supervised} on particular
    assays, not trained with broad self-supervision on raw genomes.
  \item
    Their architecture is \textbf{hand-crafted} for specific tasks
    (chromatin + expression), rather than serving as a universal
    pretraining backbone.
  \end{itemize}
\end{itemize}

In practice, they serve as:

\begin{itemize}
\tightlist
\item
  \textbf{High-performance baselines} for variant effect prediction
  tasks, especially when expression or RNA readouts are primary
  endpoints.
\item
  \textbf{Pretraining sources}: Representations learned by Enformer-like
  trunks can be adapted for downstream tasks or combined with pretrained
  language models over DNA.
\item
  \textbf{Design templates}: Many newer architectures borrow the ``conv
  stem + long-range module + multi-task heads'' pattern, swapping
  attention for alternative long-range mechanisms (e.g., state space
  models, Hyena, Mamba; Chapter 12).
\end{itemize}

As the field moves toward large, multi-modal genomic foundation models
that integrate sequence, chromatin, expression, and 3D structure,
\textbf{Enformer and Borzoi represent key waypoints}---demonstrating
that:

\begin{itemize}
\tightlist
\item
  Long-range context is essential for accurate expression prediction.
\item
  Hybrid architectures can make such context computationally tractable.
\item
  Multi-task supervision across regulatory layers is an effective path
  from raw DNA to clinically relevant variant effect predictions.
\end{itemize}

\part{Part IV: GFMs \& Multi-omics}

\chapter{Genomic FMs: Principles \&
Practice}\label{genomic-fms-principles-practice}

Genomic foundation models (GFMs) are the culmination of several threads
developed across the earlier parts of this book: high-fidelity variant
calling, regulatory sequence-to-function prediction, protein language
models, and long-context transformers for DNA. They extend these ideas
into models that are \emph{general-purpose}, \emph{pretrained at scale},
and \emph{reusable} across a wide range of genomic and genetic tasks.

This chapter steps back from individual architectures to define what it
means for a model to be a genomic foundation model, organizes the
emerging ecosystem into a practical taxonomy, and distills design
principles that will guide the rest of Part IV.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Task-Specific Models to Genomic Foundation
Models}\label{from-task-specific-models-to-genomic-foundation-models}

The earlier chapters traced a fairly linear progression:

\begin{itemize}
\tightlist
\item
  \textbf{Hand-crafted scores and shallow models} such as CADD and early
  pathogenicity predictors (Rentzsch et al. 2019; Schubach et al. 2024).
\item
  \textbf{Task-specific deep models} such as DeepSEA, ExPecto, Sei,
  Enformer and SpliceAI, which learn regulatory or splicing effects
  directly from sequence (J. Zhou and Troyanskaya 2015; J. Zhou et al.
  2018; Chen et al. 2022; Ž. Avsec et al. 2021; Jaganathan et al. 2019).
\item
  \textbf{Sequence language models} over proteins and DNA (ESM, DNABERT,
  Nucleotide Transformer, HyenaDNA, GROVER) that learn general sequence
  representations via self-supervision (Rives et al. 2021; Z. Lin et al.
  2022; Brandes et al. 2023; Ji et al. 2021; Dalla-Torre et al. 2023;
  Nguyen et al. 2023; Sanabria et al. 2024).
\end{itemize}

Foundation models build on these ingredients but change the
\emph{contract}:

\begin{quote}
The primary ``product'' of a GFM is not a task-specific prediction head,
but a reusable representation (and sometimes a general interface) that
can be adapted to many downstream tasks with modest additional
supervision.
\end{quote}

HyenaDNA is a canonical example: a genomic foundation model pretrained
on the human reference genome with context lengths up to 1M tokens at
single-nucleotide resolution using a Hyena-based long-range
architecture. DNABERT-2, Nucleotide Transformer V2, Caduceus-Ph, GROVER
and related models form a parallel family of transformer-style DNA FMs.
A recent benchmark comparing these five models across diverse tasks
(classification, gene expression prediction, variant effect
quantification, TAD recognition) illustrates both the promise and the
limitations of current DNA FMs (Manzo, Borkowski, and Ovcharenko 2025).

At a high level, we can view GFMs as extending the ``pretrain →
finetune'' paradigm from natural language and protein modeling into
genomics, but with domain-specific constraints (extreme context lengths,
single-nucleotide sensitivity, strong mechanistic priors).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{\texorpdfstring{What Makes a Model a \emph{Genomic Foundation
Model}?}{What Makes a Model a Genomic Foundation Model?}}\label{what-makes-a-model-a-genomic-foundation-model}

The term ``foundation model'' is sometimes used loosely in the genomics
literature. For practical purposes, it is useful to define working
criteria that separate GFMs from ordinary deep models.

\subsection{Working definition}\label{working-definition}

A \textbf{genomic foundation model} is a pre-trained model that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Learns from large-scale genomic data with minimal
  task-specific supervision}

  \begin{itemize}
  \tightlist
  \item
    Pretraining on entire genomes (or large portions) across species or
    populations.
  \item
    Objectives such as masked language modeling, next-token prediction,
    denoising, or multi-task sequence-to-function prediction.
  \end{itemize}
\item
  \textbf{Produces general-purpose representations}

  \begin{itemize}
  \tightlist
  \item
    Embeddings of sequences, variants, loci, or genes that are useful
    across many downstream tasks.
  \item
    Representations can be extracted and reused with light adapters or
    linear probes.
  \end{itemize}
\item
  \textbf{Is designed for broad transfer}

  \begin{itemize}
  \tightlist
  \item
    Supports many downstream tasks \emph{without} retraining the full
    model.
  \item
    Transfer across assays (e.g., from chromatin marks to gene
    expression), tissues, species, or variant types.
  \end{itemize}
\item
  \textbf{Scales along at least one dimension}

  \begin{itemize}
  \tightlist
  \item
    Context length (e.g., HyenaDNA's million-token window).
  \item
    Parameter count (e.g., ESM and Nucleotide Transformer families).
  \item
    Data diversity (e.g., pan-genomic pretraining, cross-species
    corpora).
  \end{itemize}
\item
  \textbf{Exposes a relatively standardized interface}

  \begin{itemize}
  \tightlist
  \item
    A common API for embeddings, sequence scoring, and mask-based
    perturbation.
  \item
    Often distributed via model hubs (e.g., Hugging Face) with
    documented downstream recipes.
  \end{itemize}
\end{enumerate}

Many excellent deep models for genomics (e.g., early DeepSEA or
SpliceAI) fail one or more of these criteria: they were trained for a
specific assay or task, use narrowly scoped inputs/outputs, and are not
designed for broad reuse.

\subsection{GFMs vs ``just big models''}\label{gfms-vs-just-big-models}

Scale alone does not make a model a foundation model. A very large
Enformer-like model trained solely on human chromatin tracks is powerful
but still strongly bound to a specific prediction interface (e.g.,
sequence → fixed set of chromatin tracks). By contrast, a DNA LM like
HyenaDNA or DNABERT-2 is explicitly trained to model \emph{raw sequence}
using a general objective, and is naturally repurposed as an embedding
engine.

The distinction matters because it affects:

\begin{itemize}
\tightlist
\item
  \textbf{Evaluation}: GFMs must be assessed across \emph{families} of
  tasks (e.g., TraitGym, ProteinGym) (Benegas, Eraslan, and Song 2025;
  Notin et al. 2023).
\item
  \textbf{Deployment}: GFMs are infrastructure that many downstream
  teams can reuse; task-specific models are closer to ``applications.''
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Taxonomy of Genomic Foundation
Models}\label{taxonomy-of-genomic-foundation-models}

We will use a pragmatic taxonomy based on \emph{input modality} and
\emph{pretraining objective} rather than architecture alone.

\subsection{DNA language models}\label{dna-language-models}

These models treat DNA as a ``language'' and learn to predict masked or
next tokens. Representative examples include:

\begin{itemize}
\tightlist
\item
  \textbf{DNABERT / DNABERT-2}: k-mer and nucleotide-level transformers
  trained with masked language modeling on large genomic corpora (Ji et
  al. 2021; Z. Zhou et al. 2024).
\item
  \textbf{Nucleotide Transformer}: large-scale transformer LMs trained
  across multiple species, with variants V1/V2 differing in context
  length and pretraining data (Dalla-Torre et al. 2023).
\item
  \textbf{HyenaDNA}: a long-range genomic FM using Hyena operators
  (implicit convolutions) with sub-quadratic scaling, trained on human
  reference with up to 1M-token contexts and single-nucleotide
  vocabulary (Nguyen et al. 2023).
\item
  \textbf{GROVER}: an autoregressive DNA LM that learns rich sequence
  context and shows strong performance on annotation and variant tasks
  (Sanabria et al. 2024).
\end{itemize}

\textbf{Strengths}

\begin{itemize}
\tightlist
\item
  Natural fit for \emph{representation learning}: the main output is a
  contextual embedding for each nucleotide or token.
\item
  Flexible adaptation: any task that can be phrased as ``score a
  sequence or variant'' can be built on top.
\item
  Compatible with in-context learning and soft prompting (see HyenaDNA)
  for some tasks.
\end{itemize}

\textbf{Limitations}

\begin{itemize}
\tightlist
\item
  Indirect modeling of quantitative functional readouts (e.g.,
  expression, epigenetic signal).
\item
  Difficult to interpret mechanistically compared to
  sequence-to-function models that predict explicit assays.
\end{itemize}

\subsection{Regulatory sequence-to-function
GFMs}\label{regulatory-sequence-to-function-gfms}

Building on DeepSEA, ExPecto, Sei, and Enformer (J. Zhou and Troyanskaya
2015; J. Zhou et al. 2018; Chen et al. 2022; Ž. Avsec et al. 2021),
newer models aim to:

\begin{itemize}
\tightlist
\item
  Predict hundreds to thousands of chromatin marks, TF binding profiles,
  and accessibility tracks from raw sequence.
\item
  Operate over longer context windows (100 kb or more).
\item
  Provide variant effect scores by computing \(\Delta\)-predictions
  between reference and alternative alleles.
\end{itemize}

While some of these models were originally trained for specific assays,
they approximate GFMs when:

\begin{itemize}
\tightlist
\item
  The \emph{output space} is sufficiently broad (e.g., a panel of assays
  spanning many cell types).
\item
  Their internal representations are reused for tasks beyond the
  original assay set, such as gene expression prediction,
  enhancer--promoter linking, or variant prioritization.
\end{itemize}

Enformer is a prototypical example of a sequence-to-function model that
has been widely reused as a feature extractor for downstream tasks,
including gene expression prediction and fine-mapping of regulatory
variants (Ž. Avsec et al. 2021).

\subsection{Variant-centric GFMs and trait
models}\label{variant-centric-gfms-and-trait-models}

A third class of GFMs focuses not on raw sequence but on \textbf{genetic
variants} as the fundamental unit. These models often:

\begin{itemize}
\tightlist
\item
  Embed variants using contextual information from local sequence, gene
  structure, and external annotations.
\item
  Predict variant pathogenicity, molecular consequences, or trait-level
  effect sizes.
\end{itemize}

Examples in this space include:

\begin{itemize}
\tightlist
\item
  \textbf{CADD} and its deep-learning-enhanced successor models, which
  integrate annotations and sequence features for broad variant
  pathogenicity scoring (Rentzsch et al. 2019; Schubach et al. 2024).
\item
  \textbf{AlphaMissense}, which repurposes ESM-style protein LMs to
  predict missense pathogenicity at scale (Cheng et al. 2023).
\item
  \textbf{Delphi}, \textbf{MIFM}, and related models that couple GFMs
  with polygenic risk score (PRS) estimation for complex traits
  (Georgantas, Kutalik, and Richiardi 2024; Rakowski and Lippert 2025;
  Wu et al. 2024).
\item
  Emerging variant representation learning datasets and benchmarks
  (e.g., GV-Rep) that explicitly probe how well GFMs represent genetic
  variants and clinical annotations.
\end{itemize}

Variant-centric GFMs blur the line between \emph{feature extractors} and
\emph{trait models}: their predictions can be plugged directly into PRS
pipelines, risk stratification tools, or rare disease interpretation
workflows.

\subsection{Multi-omic and cross-modal
GFMs}\label{multi-omic-and-cross-modal-gfms}

Finally, a growing set of models aim to natively integrate multiple
modalities:

\begin{itemize}
\tightlist
\item
  DNA sequence, chromatin state, and gene expression.
\item
  Sequence and 3D genome structure (Hi-C, Micro-C).
\item
  DNA with non-sequence modalities such as images or free text.
\end{itemize}

Recent work (e.g., Omni-DNA) explores transformer-based auto-regressive
models that jointly handle DNA and task-specific tokens, enabling
multi-task learning over sequence, epigenetic marks, and even textual
descriptions of function. These models move GFMs closer to a
\emph{unified interface} for genome biology, at the cost of more complex
training objectives and data engineering.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Design Dimensions of Genomic Foundation
Models}\label{design-dimensions-of-genomic-foundation-models}

When designing or choosing a GFM, it is helpful to think in terms of
several orthogonal design dimensions.

\subsection{Data: what does the model
``see''?}\label{data-what-does-the-model-see}

Key data decisions include:

\begin{itemize}
\item
  \textbf{Species coverage}

  \begin{itemize}
  \tightlist
  \item
    \emph{Human-only}: focused on clinical and human genetics
    applications.
  \item
    \emph{Cross-species}: pretraining on dozens or hundreds of species
    (as in Nucleotide Transformer and many protein LMs) encourages
    discovery of conserved regulatory code and better out-of-domain
    generalization (Dalla-Torre et al. 2023; Rives et al. 2021).
  \end{itemize}
\item
  \textbf{Assay diversity}

  \begin{itemize}
  \tightlist
  \item
    For sequence-to-function GFMs: which epigenomic assays, cell types,
    and perturbation datasets are included (e.g., Cistrome-like
    collections (Zheng et al. 2019)).
  \item
    For variant-centric GFMs: which clinical databases, experimental
    screens, and population cohorts are integrated.
  \end{itemize}
\item
  \textbf{Population diversity}

  \begin{itemize}
  \tightlist
  \item
    Inclusion of genomes from diverse ancestries is crucial to avoid
    embedding population-specific biases into GFMs and downstream risk
    scores.
  \item
    Early deep PRS models such as Delphi and MIFM explicitly tackle
    ancestry-aware evaluation (Georgantas, Kutalik, and Richiardi 2024;
    Rakowski and Lippert 2025; Wu et al. 2024).
  \end{itemize}
\item
  \textbf{Context length and sampling}

  \begin{itemize}
  \tightlist
  \item
    Random slicing of long chromosomes into training windows (HyenaDNA).
  \item
    Targeted sampling around genes, enhancers, or known variants.
  \item
    Warm-up schedules that gradually increase context length to
    stabilize training.
  \end{itemize}
\end{itemize}

\subsection{Architecture: how does the model process
sequence?}\label{architecture-how-does-the-model-process-sequence}

Common architectural families include:

\begin{itemize}
\item
  \textbf{Transformers}

  \begin{itemize}
  \tightlist
  \item
    Encoder-only (BERT-style; DNABERT, Nucleotide Transformer).
  \item
    Decoder-only (GPT-style; GROVER, some Omni-DNA models).
  \item
    Encoder--decoder hybrids for tasks requiring explicit outputs (e.g.,
    sequence→text explanations).
  \end{itemize}
\item
  \textbf{Attention-free long-range models}

  \begin{itemize}
  \tightlist
  \item
    Hyena-based models (HyenaDNA): implicit convolutions with
    sub-quadratic complexity.
  \item
    State space models and related architectures that trade exact
    attention for scalable long-range interactions.
  \end{itemize}
\item
  \textbf{Dense-attention long-range transformers}

  \begin{itemize}
  \tightlist
  \item
    Models like Gene42 show that with careful engineering and context
    extension schedules, dense-attention transformers can also reach
    \textasciitilde200 kb contexts at single-nucleotide resolution.
  \end{itemize}
\item
  \textbf{Hybrid architectures}

  \begin{itemize}
  \tightlist
  \item
    CNN + transformer stacks (e.g., local convolutions followed by
    global attention, as seen in some Enformer-like models (Ž. Avsec et
    al. 2021)).
  \item
    Cross-attention between DNA and auxiliary modalities (e.g.,
    chromatin, 3D contacts).
  \end{itemize}
\end{itemize}

Architecture choices primarily determine:

\begin{itemize}
\tightlist
\item
  Maximum practical context length.
\item
  Memory and compute requirements.
\item
  Ease of adaptation (e.g., decoder-only models are often easier to use
  for generative tasks, transformers easier for cross-modal fusion).
\end{itemize}

\subsection{Objectives: what does the model learn to
predict?}\label{objectives-what-does-the-model-learn-to-predict}

Typical pretraining objectives include:

\begin{itemize}
\item
  \textbf{Masked token prediction}

  \begin{itemize}
  \tightlist
  \item
    Randomly mask nucleotides or k-mers and predict them given context
    (DNABERT, DNABERT-2, many transformers) (Ji et al. 2021; Z. Zhou et
    al. 2024).
  \item
    Encourages the model to capture local and medium-range dependencies.
  \end{itemize}
\item
  \textbf{Next-token prediction}

  \begin{itemize}
  \tightlist
  \item
    Autoregressive LM objective (GROVER, HyenaDNA).
  \item
    Naturally aligns with generative tasks and in-context learning, and
    leverages techniques from large language models.
  \end{itemize}
\item
  \textbf{Denoising and span corruptions}

  \begin{itemize}
  \tightlist
  \item
    Replace or permute spans of sequence and train the model to
    reconstruct them.
  \item
    Encourages robustness to small perturbations and focus on long-range
    structure.
  \end{itemize}
\item
  \textbf{Multi-task sequence-to-function prediction}

  \begin{itemize}
  \tightlist
  \item
    Predict chromatin profiles, TF binding, accessibility, expression,
    etc., directly from sequence (DeepSEA, Enformer, Sei) (J. Zhou and
    Troyanskaya 2015; Ž. Avsec et al. 2021; Chen et al. 2022).
  \item
    Functions as a powerful regularizer and a direct bridge between
    sequence patterns and molecular readouts.
  \end{itemize}
\item
  \textbf{Cross-modal objectives}

  \begin{itemize}
  \tightlist
  \item
    Jointly predict sequence, epigenetic tracks, and textual/function
    labels (e.g., in Omni-DNA-like architectures).
  \item
    Contrastive alignment between DNA slices and other modalities (e.g.,
    3D contacts, histone marks).
  \end{itemize}
\end{itemize}

\subsection{Tokenization and
representations}\label{tokenization-and-representations}

Tokenization is non-trivial for DNA:

\begin{itemize}
\tightlist
\item
  \textbf{Character-level (single nucleotide)}: simplest and compatible
  with single-nucleotide resolution, used by HyenaDNA and many
  sequence-to-function models (Nguyen et al. 2023).
\item
  \textbf{k-mer tokenization} (e.g., 3--6-mers) reduces sequence length
  and helps transformers reach longer effective contexts, at the cost of
  some resolution (Ji et al. 2021).
\item
  \textbf{Learned tokenization} (e.g., BioToken-style approaches) which
  discover sub-sequence units optimized for downstream performance
  (Medvedev et al. 2025).
\end{itemize}

Internally, GFMs typically produce:

\begin{itemize}
\tightlist
\item
  \textbf{Per-position embeddings} \(h_i \in \mathbb{R}^d\) for each
  nucleotide or token.
\item
  \textbf{Pooled sequence embeddings} (mean, CLS token, learned pooling)
  that summarize an entire region.
\item
  \textbf{Variant embeddings}, constructed by contrasting reference vs
  alternative alleles, sometimes augmented with structural context.
\end{itemize}

The choice of pooling strategy can significantly influence downstream
performance; benchmarking studies have found that simple mean pooling of
per-token embeddings often outperforms more elaborate strategies across
many tasks (Manzo, Borkowski, and Ovcharenko 2025).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Evaluating Genomic Foundation
Models}\label{evaluating-genomic-foundation-models}

Because GFMs are meant to be \emph{foundations}, evaluation must be
broader than single-task metrics.

\subsection{Downstream task suites and
benchmarks}\label{downstream-task-suites-and-benchmarks}

Emerging benchmark suites provide structured evaluations:

\begin{itemize}
\tightlist
\item
  \textbf{ProteinGym}: variant effect prediction across many proteins
  for protein LMs (Notin et al. 2023).
\item
  \textbf{TraitGym}: trait-level performance of regulatory and genomic
  models across complex trait prediction tasks (Benegas, Eraslan, and
  Song 2025).
\item
  \textbf{Comparative evaluations} of DNA LMs and regulatory models,
  such as the work by Manzo et al.~comparing sequence models across
  regulatory genomics tasks (Manzo, Borkowski, and Ovcharenko 2025).
\item
  \textbf{DNA FM benchmarks} that systematically compare models like
  DNABERT-2, Nucleotide Transformer V2, HyenaDNA, Caduceus-Ph, and
  GROVER across classification, variant effect, and TAD tasks.
\item
  \textbf{Variant-centric benchmarks} like GV-Rep, probing GFMs' ability
  to represent clinical variants and their contexts.
\end{itemize}

A key lesson from these benchmarks is that \textbf{no single model
dominates all tasks}: general-purpose DNA FMs often perform well but may
lag specialized architectures for gene expression and QTL prediction,
while excelling for variant prioritization and regulatory element
annotation.

\subsection{Evaluation modes: zero-shot, linear probe,
fine-tune}\label{evaluation-modes-zero-shot-linear-probe-fine-tune}

GFMs can be evaluated in several regimes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Zero-shot evaluation}

  \begin{itemize}
  \tightlist
  \item
    Use frozen embeddings with simple operations (similarity,
    clustering) or predefined scoring rules.
  \item
    Example: using HyenaDNA embeddings directly for in-context learning
    on simple motif tasks.
  \end{itemize}
\item
  \textbf{Linear probes}

  \begin{itemize}
  \tightlist
  \item
    Train shallow linear or logistic regression heads on top of frozen
    embeddings.
  \item
    Provides a quick measure of how easily information is linearly
    decodable from GFM representations.
  \end{itemize}
\item
  \textbf{Light-weight adaptation}

  \begin{itemize}
  \tightlist
  \item
    Low-rank adaptation (LoRA), prompt tuning, or small MLP heads
    fine-tuned on specific tasks.
  \item
    Balances performance with computational cost and stability.
  \end{itemize}
\item
  \textbf{Full-model finetuning}

  \begin{itemize}
  \tightlist
  \item
    Finetune all parameters for high-stakes tasks where maximal
    performance is critical and data is abundant.
  \item
    Risk of catastrophic forgetting or overfitting, especially when
    downstream data is limited.
  \end{itemize}
\end{enumerate}

The right regime depends on data size, computational budget, and the
sensitivity of the application (e.g., rare disease diagnosis vs
exploratory motif discovery).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Using GFMs in Practice}\label{using-gfms-in-practice}

From the vantage point of a working computational biologist, the most
pressing questions are ``Which model should I use?'' and ``How do I plug
it into my pipeline?''

\subsection{Typical usage patterns}\label{typical-usage-patterns}

Common ways to use GFMs include:

\begin{itemize}
\item
  \textbf{Embedding-based pipelines}

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Extract per-base or pooled embeddings for loci of interest.
  \item
    Train simple downstream models (e.g., gradient-boosted trees, small
    neural nets) on these embeddings.
  \item
    Evaluate on held-out datasets or across cohorts.
  \end{enumerate}
\item
  \textbf{Variant effect scoring}

  \begin{itemize}
  \tightlist
  \item
    Use sequence-to-function GFMs (Enformer-like) to compute
    \(\Delta\)-predictions between reference and alternate alleles.
  \item
    Feed variant-level scores into downstream calibration layers or PRS
    models (Ž. Avsec et al. 2021; Georgantas, Kutalik, and Richiardi
    2024; Rakowski and Lippert 2025).
  \end{itemize}
\item
  \textbf{Feature augmentation}

  \begin{itemize}
  \tightlist
  \item
    Combine GFM-derived features with classical annotations
    (conservation, CADD scores, functional genomics tracks) (Rentzsch et
    al. 2019; Schubach et al. 2024).
  \item
    Particularly useful for rare variant interpretation where each
    evidence source is sparse.
  \end{itemize}
\item
  \textbf{Cross-modal linking}

  \begin{itemize}
  \tightlist
  \item
    Use GFMs as common embedding spaces linking sequence with
    expression, imaging, or textual annotations (e.g., variant→phenotype
    descriptions).
  \end{itemize}
\end{itemize}

\subsection{Choosing a model for your use
case}\label{choosing-a-model-for-your-use-case}

A simple decision guide:

\begin{itemize}
\item
  \textbf{Need long-range context (\textgreater100 kb)?}

  \begin{itemize}
  \tightlist
  \item
    Consider models like HyenaDNA or long-context dense-attention models
    such as Gene42.
  \end{itemize}
\item
  \textbf{Focus on regulatory variant interpretation near genes?}

  \begin{itemize}
  \tightlist
  \item
    Start with Enformer-like or DeepSEA-like GFMs and compare against
    DNA LMs working via embeddings (Ž. Avsec et al. 2021; J. Zhou and
    Troyanskaya 2015; Chen et al. 2022; Ji et al. 2021).
  \end{itemize}
\item
  \textbf{Trait-level prediction with large cohorts?}

  \begin{itemize}
  \tightlist
  \item
    Explore PRS pipelines that incorporate GFM-based variant priors such
    as Delphi or MIFM (Georgantas, Kutalik, and Richiardi 2024; Rakowski
    and Lippert 2025; Wu et al. 2024).
  \end{itemize}
\item
  \textbf{Method development / benchmarking?}

  \begin{itemize}
  \tightlist
  \item
    Use standardized benchmarks (TraitGym, ProteinGym, GV-Rep, DNA FM
    suites) to ensure your comparisons are meaningful (Benegas, Eraslan,
    and Song 2025; Notin et al. 2023; Manzo, Borkowski, and Ovcharenko
    2025).
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Safety, Robustness, and Responsible
Use}\label{safety-robustness-and-responsible-use}

As GFMs become infrastructure for clinical and research pipelines,
safety and robustness are not optional extras.

\subsection{Robustness and adversarial
sensitivity}\label{robustness-and-adversarial-sensitivity}

Recent work such as SafeGenes highlights that genomic FMs (including
ESM1b-like and other GFMs) can be surprisingly sensitive to adversarial
perturbations---both at the input sequence level and through soft
prompts in embedding space. Even when perturbations are hardly
biologically plausible, they reveal:

\begin{itemize}
\tightlist
\item
  Fragility of decision boundaries in high-dimensional representation
  space.
\item
  Potential failure modes where small spurious changes strongly impact
  pathogenicity or variant effect predictions.
\end{itemize}

This suggests that:

\begin{itemize}
\tightlist
\item
  \textbf{Adversarial testing} should become part of GFM validation,
  especially for clinical use cases.
\item
  \textbf{Robust training} (e.g., via data augmentation, adversarial
  objectives, or distributionally robust optimization) may be needed for
  high-stakes tasks.
\end{itemize}

\subsection{12.7.2 Bias, fairness, and
ancestry}\label{bias-fairness-and-ancestry}

GFMs trained predominantly on reference genomes or Euro-centric cohorts
risk encoding biased priors:

\begin{itemize}
\tightlist
\item
  Underestimation of risk in underrepresented ancestries.
\item
  Misclassification of benign variants that are common in certain
  populations but rare in training data.
\end{itemize}

Deep PRS and variant interpretation pipelines that incorporate GFMs
should:

\begin{itemize}
\tightlist
\item
  Perform ancestry-stratified evaluation (Georgantas, Kutalik, and
  Richiardi 2024; Rakowski and Lippert 2025; Wu et al. 2024).
\item
  Consider explicit debiasing (e.g., reweighting) and careful
  calibration.
\end{itemize}

\subsection{Data governance and
privacy}\label{data-governance-and-privacy}

Because GFMs are often trained on large collections of genomic
sequences:

\begin{itemize}
\tightlist
\item
  Data use agreements and privacy protections must be respected; some
  cohort-level datasets cannot be used for unrestricted pretraining.
\item
  Even when training on reference genomes, leakage from labeled clinical
  datasets into training may complicate downstream evaluation.
\end{itemize}

To date, most published GFMs emphasize training on public reference
genomes or synthetic benchmarks, but clinical deployment will require
stronger guarantees.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Open Challenges and Future
Directions}\label{open-challenges-and-future-directions}

Genomic foundation models are still in their early days. Several open
challenges stand out.

\subsection{Toward unified multi-omic
GFMs}\label{toward-unified-multi-omic-gfms}

Current GFMs are still fragmented:

\begin{itemize}
\tightlist
\item
  DNA-only LMs.
\item
  Sequence-to-function models tied to specific assays.
\item
  Variant-centric pathogenicity models.
\item
  Protein and RNA LMs.
\end{itemize}

A major frontier is \textbf{unified multi-omic GFMs} that:

\begin{itemize}
\tightlist
\item
  Jointly model DNA, RNA, protein, chromatin, and 3D genome structure.
\item
  Support cross-modal queries such as ``given this variant, what is the
  likely impact on TF binding, chromatin accessibility, and gene
  expression in a given cell type?''
\item
  Provide interpretable pathways connecting sequence variation to
  phenotypes.
\end{itemize}

Models such as Omni-DNA are first steps in this direction, showing that
multi-task, cross-modal training is feasible at scale.

\subsection{Integrating causal and mechanistic
structure}\label{integrating-causal-and-mechanistic-structure}

Most GFMs are trained with purely predictive objectives. Incorporating
more \textbf{causal structure} could:

\begin{itemize}
\tightlist
\item
  Improve robustness to distribution shift (e.g., between cell types or
  interventions).
\item
  Enable counterfactual reasoning (``what if we knock out this
  enhancer?'').
\end{itemize}

Potential routes include:

\begin{itemize}
\tightlist
\item
  Causal representation learning on top of GFM embeddings.
\item
  Mechanistic constraints derived from gene regulatory networks or
  biochemical kinetics.
\item
  Joint modeling of perturbation data (CRISPR screens, gene knockouts)
  with observational genomics.
\end{itemize}

\subsection{Efficient and accessible
deployment}\label{efficient-and-accessible-deployment}

Even if GFMs train on large clusters, their deployment should be
feasible in typical research labs and clinical environments:

\begin{itemize}
\tightlist
\item
  Distillation into smaller student models.
\item
  Efficient inference via sparsity, quantization, and hardware-aware
  architectures.
\item
  Task-specific adapters that keep the frozen backbone small enough for
  on-premise use.
\end{itemize}

The long-range efficiency of architectures like HyenaDNA and the
emergence of dense-attention models like Gene42 suggest multiple viable
paths to deployable GFMs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Summary}\label{summary}

In this chapter, we:

\begin{itemize}
\tightlist
\item
  Defined what it means for a model to be a \emph{genomic foundation
  model}, emphasizing scale, generality, and reusability.
\item
  Proposed a practical taxonomy: DNA language models,
  sequence-to-function GFMs, variant-centric GFMs, and emerging
  multi-omic models.
\item
  Surveyed core design dimensions: data, architecture, objectives, and
  tokenization.
\item
  Discussed evaluation regimes and benchmark suites that assess GFMs
  across diverse tasks.
\item
  Outlined how practitioners can integrate GFMs into variant
  interpretation, regulatory genomics, and trait prediction pipelines.
\item
  Highlighted emerging concerns around robustness, bias, and responsible
  deployment.
\end{itemize}

The remaining chapters of Part IV will dive deeper into specific
application domains---clinical interpretation, population-scale trait
modeling, and multi-omics integration---using the conceptual framework
established here to organize a rapidly evolving ecosystem of genomic
foundation models.

\chapter{Variant Effect Prediction}\label{variant-effect-prediction-2}

\section{From Handcrafted Scores to Foundation
Models}\label{from-handcrafted-scores-to-foundation-models}

Variant effect prediction (VEP) sits at the heart of modern genomics.
Most variants discovered in clinical sequencing are rare and lack direct
experimental evidence; yet clinicians still need to decide whether
they're benign, pathogenic, or somewhere in between. Earlier in this
book we saw:

\begin{itemize}
\tightlist
\item
  \textbf{Conservation and heuristic scores} (e.g., traditional tools
  like SIFT, PolyPhen, CADD), which combine evolutionary constraint and
  manually engineered features.
\item
  \textbf{Sequence-to-function CNNs} like DeepSEA and ExPecto (Chapters
  5--6), which predict chromatin and expression to estimate regulatory
  effects.
\item
  \textbf{Specialized architectures} like SpliceAI (Chapter 7), which
  target specific mechanisms such as splicing.
\item
  \textbf{Protein language models} (Chapter 9), which learn rich
  representations from large-scale protein sequences and can be adapted
  for missense VEP.
\end{itemize}

The frontier today is shaped by \textbf{foundation models} that combine:

\begin{itemize}
\tightlist
\item
  \textbf{Massive pretraining} (proteome- or genome-scale),
\item
  \textbf{Long-range context} (from kilobases to megabases),
\item
  \textbf{Multiple sources of information} (sequence, structure,
  multi-species alignments, multi-omic outputs).
\end{itemize}

This chapter surveys four landmark systems:

\begin{itemize}
\tightlist
\item
  \textbf{AlphaMissense} -- proteome-wide missense pathogenicity
  predictions.
\item
  \textbf{GPN-MSA} -- a DNA language model over multi-species alignments
  for genome-wide VEP.
\item
  \textbf{Evo 2} -- a generalist genomic language model spanning all
  domains of life.
\item
  \textbf{AlphaGenome} -- a unified, megabase-scale sequence-to-function
  model with state-of-the-art regulatory VEP.
\end{itemize}

Together, they preview what ``Genomic Foundation Models'' look like when
specialized for variant interpretation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{AlphaMissense: Proteome-Wide Missense
Pathogenicity}\label{alphamissense-proteome-wide-missense-pathogenicity}

AlphaMissense, developed by DeepMind, provides \textbf{precomputed
pathogenicity scores for \textasciitilde71 million possible human
missense variants}, covering almost every single--amino acid change in
the proteome.

\subsection{Inputs: Combining Sequence and
Structure}\label{inputs-combining-sequence-and-structure}

AlphaMissense builds on two pillars:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Protein language modeling}

  \begin{itemize}
  \tightlist
  \item
    A transformer-based model is trained on massive multiple sequence
    alignments (MSAs), learning which amino acids tend to appear at each
    position across evolution.\\
  \item
    From this, the model infers how ``surprising'' a given amino acid
    substitution is in its evolutionary context.
  \end{itemize}
\item
  \textbf{Predicted 3D structure from AlphaFold2}

  \begin{itemize}
  \tightlist
  \item
    Structural context (packing, secondary structure, local
    interactions) helps distinguish tolerated changes (e.g., on
    solvent-exposed loops) from disruptive ones (e.g., in tightly packed
    cores or active sites).
  \end{itemize}
\end{enumerate}

For each variant, AlphaMissense ingests:

\begin{itemize}
\tightlist
\item
  The \textbf{wild-type sequence},
\item
  The \textbf{substitution position} and amino-acid change,
\item
  \textbf{Sequence context} from the MSA,
\item
  \textbf{Structural environment} derived from AlphaFold2.
\end{itemize}

These features are fed into a neural network that outputs a
\textbf{pathogenicity probability} between 0 and 1.

\subsection{Training and Calibration}\label{training-and-calibration}

AlphaMissense's training is hybrid:

\begin{itemize}
\tightlist
\item
  \textbf{Self-supervised pretraining} learns general sequence and
  structural representations from evolutionary data.
\item
  \textbf{Supervised calibration} uses:

  \begin{itemize}
  \tightlist
  \item
    \textbf{ClinVar} and similar databases for labeled pathogenic/benign
    variants,
  \item
    \textbf{Population frequencies} (e.g., gnomAD) under the assumption
    that common variants are more likely benign.
  \end{itemize}
\end{itemize}

The model's raw scores are calibrated so that:

\begin{itemize}
\tightlist
\item
  Scores near \textbf{0} behave like ``likely benign,''\\
\item
  Scores near \textbf{1} behave like ``likely pathogenic,''\\
\item
  Intermediate scores capture uncertainty and ambiguous cases.
\end{itemize}

In practice, AlphaMissense adopts score cutoffs that approximately map
to \textbf{``likely benign,'' ``uncertain,'' and ``likely pathogenic''}
categories used in clinical interpretation frameworks.

\subsection{Performance and Clinical
Utility}\label{performance-and-clinical-utility}

Across diverse benchmarks---ClinVar, curated expert panels, and
multiplexed assays of variant effect (MAVEs)---AlphaMissense:

\begin{itemize}
\tightlist
\item
  Achieves \textbf{state-of-the-art AUROC and AUPRC} for missense VEP.
\item
  Generalizes across many genes, including those with little prior
  annotation.
\item
  Produces scores that are \textbf{more consistent with experimental
  functional readouts} than many earlier predictors.
\end{itemize}

As a result, AlphaMissense scores have already been integrated into:

\begin{itemize}
\tightlist
\item
  Clinical re-annotation of exomes,\\
\item
  Reclassification of variants of uncertain significance (VUS),\\
\item
  Gene-specific studies where high-throughput functional assays are
  impractical.
\end{itemize}

\subsection{Limitations and Caveats}\label{limitations-and-caveats}

Despite its impressive performance, AlphaMissense has important
limitations:

\begin{itemize}
\tightlist
\item
  \textbf{Missense-only}: It does not natively handle nonsense,
  frameshift, regulatory, or deep intronic variants.
\item
  \textbf{Single-variant focus}: It scores one substitution at a time,
  ignoring \textbf{combinations of variants} (compound heterozygosity,
  epistasis).
\item
  \textbf{Dependent on training labels}: Any biases in ClinVar or
  population data (e.g., ancestry representation) can propagate into
  scores.
\item
  \textbf{Interpretability}: While attention maps and feature
  attributions can be examined, the reasoning for a particular score is
  often opaque.
\end{itemize}

For these reasons, guidelines recommend treating AlphaMissense as
\textbf{supporting evidence} to be combined with segregation, functional
data, and population frequencies---not as a standalone decision-maker.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{GPN-MSA: Genome-Wide Variant Effect Prediction from
MSAs}\label{gpn-msa-genome-wide-variant-effect-prediction-from-msas}

While AlphaMissense focuses on proteins, \textbf{GPN-MSA} tackles the
harder problem of \textbf{genome-wide variant effect prediction in
complex genomes such as human}, directly at the DNA level.

\subsection{Alignment-Based DNA Language
Model}\label{alignment-based-dna-language-model}

GPN-MSA extends earlier Genomic Pre-trained Network (GPN) models by
operating on \textbf{multi-species genome alignments}:

\begin{itemize}
\tightlist
\item
  Input: a \textbf{stack of aligned sequences} from multiple species
  (e.g., human plus dozens of mammals).
\item
  Representation: the model sees both:

  \begin{itemize}
  \tightlist
  \item
    The \textbf{reference sequence} (e.g., human), and\\
  \item
    \textbf{Auxiliary features} encoding how each aligned species
    matches, mismatches, or gaps at each base.
  \end{itemize}
\end{itemize}

The model is trained with a \textbf{masked language modeling (MLM)
objective}:

\begin{itemize}
\tightlist
\item
  Randomly mask nucleotides in the reference sequence,
\item
  Predict the masked base given the surrounding context and the aligned
  sequences.
\end{itemize}

This encourages the model to learn \textbf{evolutionary constraints}:
positions where substitutions are strongly disfavored across species get
very confident predictions; unconstrained positions allow more
flexibility.

\subsection{Variant Scoring
Strategies}\label{variant-scoring-strategies}

GPN-MSA supports several ways to derive variant effect scores:

\begin{itemize}
\tightlist
\item
  \textbf{Likelihood-based scoring}: compare the model's log-likelihood
  (or probability) of the reference vs.~alternate allele at the variant
  position.
\item
  \textbf{Embedding distance}: compute embeddings for reference and
  alternate sequences and use their difference (e.g., Euclidean
  distance) as an effect magnitude.
\item
  \textbf{Influence scores}: quantify how much a variant perturbs the
  model's outputs across the surrounding genomic context.
\end{itemize}

Because the model operates on \textbf{whole-genome alignments}, it can
score:

\begin{itemize}
\tightlist
\item
  Coding and noncoding variants,
\item
  Regulatory elements, introns, UTRs, and intergenic regions,
\item
  Variants in regions with \textbf{complex conservation patterns}, where
  simple phyloP-like scores struggle.
\end{itemize}

\subsection{Benchmarking and
Applications}\label{benchmarking-and-applications}

GPN-MSA demonstrates strong performance on:

\begin{itemize}
\tightlist
\item
  Genome-wide pathogenic vs.~benign classification datasets,
\item
  Variant sets from genome-wide association studies,
\item
  Functional readouts from high-throughput reporter assays.
\end{itemize}

Practically, GPN-MSA is useful for:

\begin{itemize}
\tightlist
\item
  \textbf{Genome-wide prefiltering}: prioritizing candidate causal
  variants in regulatory regions.
\item
  \textbf{Complementing protein-focused tools}: supplying information
  where AlphaMissense is blind (deep noncoding, intronic, intergenic).
\end{itemize}

Its key limitation is dependency on \textbf{high-quality multi-species
alignments}; coverage and quality drop in repetitive, structurally
complex, or poorly aligned regions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Evo 2: A Generalist Genomic Language
Model}\label{evo-2-a-generalist-genomic-language-model}

\textbf{Evo 2} pushes the foundation-model paradigm to the extreme: it
is a \textbf{genome-scale language model} trained across all domains of
life---bacteria, archaea, eukaryotes, and phages---on
\textbf{\textgreater9 trillion DNA tokens}.

\subsection{Scale and Architecture}\label{scale-and-architecture}

Key features of Evo 2 include:

\begin{itemize}
\tightlist
\item
  \textbf{Autoregressive training} on DNA: predict the next base given
  the preceding context, analogous to next-token prediction in text
  LLMs.
\item
  A \textbf{StripedHyena 2} architecture, blending convolutional and
  attention mechanisms to support:

  \begin{itemize}
  \tightlist
  \item
    Context windows up to \textbf{1 million base pairs},\\
  \item
    Efficient long-range modeling.
  \end{itemize}
\item
  Multiple model sizes (e.g., 7B and 40B parameters) with open-source
  weights, training code, and the OpenGenome2 dataset.
\end{itemize}

Evo 2 is designed as a \textbf{generalist}: it is not trained
specifically for VEP, but rather to model genomic sequences broadly.

\subsection{Zero-Shot Variant Effect
Scoring}\label{zero-shot-variant-effect-scoring}

Remarkably, Evo 2 can be used for \textbf{zero-shot variant
interpretation}:

\begin{itemize}
\tightlist
\item
  For a given locus, compute the model's sequence likelihood (or
  log-probability) for the \textbf{reference} allele.
\item
  Then compute the likelihood for the \textbf{alternate} allele (or
  sequence containing it).
\item
  The difference in likelihood provides a \textbf{variant effect
  score}---variants that strongly reduce probability are inferred to be
  more disruptive.
\end{itemize}

In benchmarks reported in the preprint and follow-up analyses:

\begin{itemize}
\tightlist
\item
  Evo 2 achieves \textbf{competitive or state-of-the-art accuracy} for
  pathogenic vs.~benign classification across multiple variant types
  (coding and noncoding), even without variant-specific supervised
  training.
\item
  A simple supervised classifier built on Evo 2 embeddings reaches
  \textbf{state-of-the-art performance} on tasks like BRCA1 VUS
  classification.
\end{itemize}

\subsection{Cross-Species Variant
Interpretation}\label{cross-species-variant-interpretation}

Because Evo 2 is trained across diverse species:

\begin{itemize}
\tightlist
\item
  It naturally supports \textbf{variant effect prediction in non-model
  organisms} (e.g., livestock, crops).
\item
  It can help quantify \textbf{mutation load}, prioritize variants for
  breeding programs, and guide \textbf{genome editing} designs across
  species.
\end{itemize}

However, its generality comes with trade-offs:

\begin{itemize}
\tightlist
\item
  Domain-specific models (like AlphaMissense for human missense or
  AlphaGenome for regulatory variants) may still outperform Evo 2 on
  certain human-centric tasks.
\item
  Careful calibration and benchmarking are required before clinical use.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{AlphaGenome: Unified Megabase-Scale Regulatory
Modeling}\label{alphagenome-unified-megabase-scale-regulatory-modeling}

Where Evo 2 is generalist and sequence-only, \textbf{AlphaGenome} is
explicitly designed as a \textbf{multimodal regulatory model} of the
human genome, with a focus on \textbf{variant effect prediction across
many functional readouts}.

\subsection{Architecture: CNNs + Transformers over 1
Mbp}\label{architecture-cnns-transformers-over-1-mbp}

AlphaGenome takes as input \textbf{1 megabase (1 Mb) of DNA sequence}
and produces predictions at \textbf{single-base resolution} for a large
set of genomic ``tracks,'' including:

\begin{itemize}
\tightlist
\item
  Chromatin accessibility and histone marks,
\item
  Transcription factor binding,
\item
  Gene expression (e.g., CAGE-like signals),
\item
  3D genome contacts,
\item
  Splicing features (junctions and splice-site usage).
\end{itemize}

Architecturally:

\begin{itemize}
\tightlist
\item
  \textbf{Convolutional layers} detect local sequence motifs.
\item
  \textbf{Transformer blocks} propagate information across the full
  megabase context.
\item
  \textbf{Task-specific heads} output different experimental modalities
  across many tissues and cell types.
\end{itemize}

This design generalizes earlier models like Basenji/Enformer (for
regulatory tracks) and SpliceAI (for splicing) into a \textbf{single,
unified model}.

\subsection{Variant Effect Prediction Across
Modalities}\label{variant-effect-prediction-across-modalities}

Given a reference sequence and a candidate variant, AlphaGenome scores
variant effects by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Predicting genome-wide functional tracks for the reference sequence.
\item
  Predicting the same tracks for the sequence bearing the variant.
\item
  Comparing predictions to obtain \textbf{delta signals} across:

  \begin{itemize}
  \tightlist
  \item
    Regulatory elements (promoters, enhancers, insulators),
  \item
    Splicing patterns (gain/loss of splice junctions),
  \item
    Gene expression levels,
  \item
    3D contact maps affecting enhancer--promoter communication.
  \end{itemize}
\end{enumerate}

On extensive benchmarks, AlphaGenome:

\begin{itemize}
\tightlist
\item
  Achieves \textbf{state-of-the-art accuracy} in predicting unseen
  functional genomics tracks.
\item
  Shows strong performance on diverse variant effect tasks (e.g.,
  noncoding disease variants, splicing disruptions, regulatory MPRA
  data).
\item
  Provides \textbf{mechanistic hypotheses} (which tracks/tissues are
  disrupted) rather than only a single scalar risk score.
\end{itemize}

An API makes AlphaGenome accessible to the research community, enabling
large-scale variant scoring without local training infrastructure.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Comparing Design Choices Across Modern VEP
Models}\label{comparing-design-choices-across-modern-vep-models}

The models in this chapter span different points in the design space:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0875}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1875}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1312}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2312}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1312}}
  >{\raggedright\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2312}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Input Modality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Context Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pretraining Data
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variant Types
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Primary Outputs
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
AlphaMissense & Protein sequence + structure & Protein-length & MSAs +
structural environment & Missense only & Pathogenicity probability \\
GPN-MSA & Multi-species DNA alignments & kb-scale windows & Whole-genome
MSAs (multiple species) & Coding + noncoding & Likelihood /
embedding-based scores \\
Evo 2 & Raw DNA sequence & Up to \textasciitilde1 Mb & OpenGenome2 (all
domains of life) & All variant types & Zero-shot likelihood-based
scores \\
AlphaGenome & Raw DNA sequence & 1 Mb & Human genome + multi-omic tracks
& All variant types & Multi-omic tracks + delta effects \\
\end{longtable}

Key contrasts:

\begin{itemize}
\tightlist
\item
  \textbf{Scope}

  \begin{itemize}
  \tightlist
  \item
    AlphaMissense is \textbf{human-missense-specific}, with deep
    clinical calibration.\\
  \item
    GPN-MSA and AlphaGenome are \textbf{human-genome-centric}, spanning
    coding and regulatory variants.\\
  \item
    Evo 2 is \textbf{cross-species and general-purpose}.
  \end{itemize}
\item
  \textbf{Context and long-range effects}

  \begin{itemize}
  \tightlist
  \item
    AlphaMissense operates at \textbf{protein scale}.\\
  \item
    GPN-MSA uses modest windows centered on the variant.\\
  \item
    Evo 2 and AlphaGenome support \textbf{megabase-scale context},
    capturing long-range regulatory interactions.
  \end{itemize}
\item
  \textbf{Outputs}

  \begin{itemize}
  \tightlist
  \item
    AlphaMissense and GPN-MSA primarily output \textbf{scalar scores}.\\
  \item
    Evo 2 outputs \textbf{likelihoods/embeddings} that require
    task-specific postprocessing.\\
  \item
    AlphaGenome outputs \textbf{rich functional profiles}, enabling
    mechanistic hypotheses.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Practical Use: Choosing and Interpreting Modern VEP
Tools}\label{practical-use-choosing-and-interpreting-modern-vep-tools}

In realistic workflows, these models are complementary rather than
competing.

\subsection{Coding Missense Variants}\label{coding-missense-variants}

For human missense variants:

\begin{itemize}
\tightlist
\item
  Use \textbf{AlphaMissense} as a high-coverage, clinically calibrated
  score.
\item
  Complement with:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Protein language model embeddings} (Chapter 9) for gene- or
    domain-specific modeling,
  \item
    \textbf{Conservation and population data} (e.g., GPN-MSA in coding
    regions, gnomAD frequencies),
  \item
    \textbf{Gene-level context} (constraint metrics, disease
    association).
  \end{itemize}
\end{itemize}

\subsection{Noncoding and Regulatory
Variants}\label{noncoding-and-regulatory-variants}

For regulatory variation (promoters, enhancers, introns, intergenic):

\begin{itemize}
\tightlist
\item
  Use \textbf{AlphaGenome} to obtain:

  \begin{itemize}
  \tightlist
  \item
    Tissue-specific changes in chromatin and expression,
  \item
    Splicing consequences (especially for intronic and exonic variants),
  \item
    Potential disruption of long-range enhancer--promoter interactions.
  \end{itemize}
\item
  Use \textbf{GPN-MSA} when:

  \begin{itemize}
  \tightlist
  \item
    You want a \textbf{conservation-grounded score},
  \item
    High-quality multi-species alignments are available,
  \item
    You're scanning broad regions genome-wide.
  \end{itemize}
\end{itemize}

\subsection{Cross-Species and Large-Scale
Modeling}\label{cross-species-and-large-scale-modeling}

For non-human organisms, or when building general-purpose genomic tools:

\begin{itemize}
\tightlist
\item
  Leverage \textbf{Evo 2} for:

  \begin{itemize}
  \tightlist
  \item
    Zero-shot variant scoring in poorly annotated species,
  \item
    Designing or screening edits (e.g., CRISPR designs),
  \item
    Serving as a feature extractor feeding downstream supervised models.
  \end{itemize}
\end{itemize}

\subsection{Score Interpretation and
Calibration}\label{score-interpretation-and-calibration}

Regardless of the model:

\begin{itemize}
\tightlist
\item
  Treat scores as \textbf{probabilistic evidence}, not binary labels.
\item
  Consider:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Calibration} (does a score of 0.9 truly correspond to
    \textasciitilde90\% pathogenic variants?),
  \item
    \textbf{Distribution of scores within a gene} (outliers are more
    suspect),
  \item
    \textbf{Consistency across tools} (agreement between AlphaMissense,
    GPN-MSA, AlphaGenome, Evo 2, and simpler conservation metrics
    strengthens confidence).
  \end{itemize}
\end{itemize}

Where possible, tie predictions back to:

\begin{itemize}
\tightlist
\item
  \textbf{Mechanistic hypotheses} (splice site disruption,
  enhancer--promoter rewiring),
\item
  \textbf{Experimental follow-up} (targeted assays, MPRA, CRISPR
  screens).
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Open Challenges and Future
Directions}\label{open-challenges-and-future-directions-1}

Even these state-of-the-art systems leave major gaps:

\begin{itemize}
\item
  \textbf{Ancestry and population bias}\\
  Training data and labels remain skewed toward certain ancestries,
  raising concerns about performance and calibration in underrepresented
  populations.
\item
  \textbf{Complex variant patterns}\\
  Most models focus on \textbf{single-base or single-amino-acid
  changes}. Systematic handling of:

  \begin{itemize}
  \tightlist
  \item
    Haplotypes,\\
  \item
    Indels and structural variants,\\
  \item
    Epistatic interactions across distant loci\\
    is still in its infancy.
  \end{itemize}
\item
  \textbf{Integrating multi-omics and longitudinal data}\\
  AlphaGenome marks a step toward unified multi-omic prediction, but
  dynamic phenomena (developmental trajectories, environment,
  time-series responses) are only lightly modeled.
\item
  \textbf{Interpretability and clinical communication}\\
  Translating high-dimensional predictions into explanations that
  clinicians and patients can understand---and that map onto emerging
  guidelines for AI-assisted variant interpretation---remains a
  human-factor challenge.
\item
  \textbf{Safe deployment and continual learning}\\
  As more functional datasets and clinical labels accumulate, models
  will need \textbf{continual updating} without catastrophic forgetting,
  along with governance frameworks to track model versions and
  provenance.
\end{itemize}

In the next chapters, we will connect these VEP systems to broader
issues in evaluation, bias, and multi-omics integration, positioning
them within the broader landscape of \textbf{Genomic Foundation Models}.
This chapter's models illustrate how the building blocks from earlier
chapters---NGS, functional genomics, CNNs, transformers, protein and DNA
language models---coalesce into powerful, end-to-end systems for variant
interpretation.

\chapter{Multi-omics and Systems
Context}\label{multi-omics-and-systems-context}

Modern genomic foundation models (GFMs) excel at learning from
sequences, structures, or single-omic profiles in isolation. Yet most
complex traits arise from \emph{systems-level} interactions: genetic
variants perturb molecular networks; networks span multiple omics
layers; and these layers interact with environment, development, and
clinical context. A model that sees only one layer rarely captures the
full story.

This chapter surveys how deep learning extends beyond single-omics to
integrate methylation, chromatin, expression, protein, and clinical data
into unified representations. Within the book structure defined by the
Quarto project, this is the final chapter of Part IV and serves as a
bridge from model-centric architecture design to systems-level,
clinically grounded applications.

We focus on several archetypal systems:

\begin{itemize}
\tightlist
\item
  \textbf{CpGPT}: a foundation model for DNA methylation\\
\item
  \textbf{GLUE}: a graph-linked framework for single-cell multi-omics
  integration (e.g., SCGLUE)\\
\item
  \textbf{GNN-based cancer subtyping} with models such as MoGCN and
  CGMega\\
\item
  \textbf{Rare variants and epistasis} with DeepRVAT, NeEDL, and the
  Genotype-to-Phenotype Transformer (G2PT)\\
\item
  \textbf{DL-enhanced polygenic risk and fine-mapping}, including Delphi
  and MIFM
\end{itemize}

Together, these approaches illustrate emerging design patterns for
\emph{systems-aware} GFMs that move from single sequences to
whole-patient representations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Why Single-omics Models Are Not
Enough}\label{why-single-omics-models-are-not-enough}

Earlier chapters emphasized how sequence-based models can predict
variant effects from local DNA or protein context. These models already
improve causal variant prioritization and polygenic risk scoring.
However, they typically assume a narrow view of biology:

\begin{itemize}
\tightlist
\item
  \textbf{Single layer}: A CNN or transformer may see only DNA sequence
  or only expression.\\
\item
  \textbf{Additive effects}: Many downstream uses still treat variant
  effects as additively summing across loci.\\
\item
  \textbf{Static context}: Models rarely account for dynamic state (cell
  type, developmental stage, environment).
\end{itemize}

Real diseases violate all three assumptions:

\begin{itemize}
\tightlist
\item
  \emph{Regulation is multi-layered}: genetic variants alter chromatin
  accessibility and DNA methylation, which modulate transcription,
  splicing, translation, and protein modification.\\
\item
  \emph{Effects are context-dependent}: the same variant can be benign
  in one tissue and pathogenic in another.\\
\item
  \emph{Risk is combinatorial}: epistasis and pathway-level
  perturbations play a significant role in many complex traits.
\end{itemize}

Chapter 2 highlighted the ``missing heritability'' and limited
portability of traditional GWAS and linear PRS, motivating
sequence-based deep learning. Here we take the next step: combining
sequence-derived features with \textbf{multi-omics} and
\textbf{systems-level models} that better reflect biological
organization.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Foundations of Multi-omics
Integration}\label{foundations-of-multi-omics-integration}

Multi-omics data come in several flavors:

\begin{itemize}
\tightlist
\item
  \textbf{Bulk-level} profiles (e.g., GWAS variants, bulk RNA-seq, bulk
  proteomics)\\
\item
  \textbf{Single-cell} modalities (scRNA-seq, scATAC-seq, multiome,
  spatial omics)\\
\item
  \textbf{Epigenetic} readouts (DNA methylation, histone marks,
  chromatin conformation)\\
\item
  \textbf{Clinical and environmental} covariates (EHR, labs, lifestyle)
\end{itemize}

Integration strategies typically fall into three categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Early fusion (feature-level)}

  \begin{itemize}
  \tightlist
  \item
    Concatenate normalized features from multiple omics and feed them
    into a single model.\\
  \item
    Straightforward but sensitive to scaling, missing data, and modality
    imbalance.
  \end{itemize}
\item
  \textbf{Intermediate fusion (shared latent space)}

  \begin{itemize}
  \tightlist
  \item
    Learn modality-specific encoders that map each omic into a common
    latent space.\\
  \item
    Align latent spaces via reconstruction losses, contrastive terms, or
    graph constraints.\\
  \item
    This is the dominant design in modern multi-omics deep learning.
  \end{itemize}
\item
  \textbf{Late fusion (prediction-level)}

  \begin{itemize}
  \tightlist
  \item
    Train separate models per modality; combine outputs via ensemble or
    meta-model.\\
  \item
    Robust to missing modalities but may underutilize cross-omic
    structure.
  \end{itemize}
\end{enumerate}

Modern frameworks like GLUE and multi-omics GNNs adopt
\textbf{intermediate fusion}, often with \textbf{graphs} encoding known
or inferred relationships (e.g., gene--peak, gene--TF, protein--protein,
or sample similarity networks). The rest of this chapter traces how
these design choices implement systems-level reasoning in practice.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{CpGPT: A Foundation Model for DNA
Methylation}\label{cpgpt-a-foundation-model-for-dna-methylation}

\subsection{Motivation: Methylation as a Systems
Hub}\label{motivation-methylation-as-a-systems-hub}

DNA methylation sits at a crucial junction between genotype,
environment, and phenotype:

\begin{itemize}
\tightlist
\item
  It integrates genetic, developmental, and environmental influences.\\
\item
  It encodes \emph{cell type} and \emph{cell state} information.\\
\item
  It is predictive of aging, mortality, and disease risk.
\end{itemize}

Traditional methylation models are task-specific (e.g., age clocks,
mortality predictors). CpGPT reframes methylation as a
\textbf{foundation modeling problem}, using large-scale pretraining to
unlock downstream tasks.

\subsection{Architecture and
Pretraining}\label{architecture-and-pretraining}

CpGPT (Cytosine-phosphate-Guanine Pretrained Transformer) is trained on
large-scale collections of whole-genome and array-based methylation
profiles. Conceptually, CpGPT treats methylomes as sequences or sets of
CpG sites, and uses transformer-style self-attention to model:

\begin{itemize}
\tightlist
\item
  Local CpG correlations (e.g., CpG islands)\\
\item
  Long-range coordination across genomic regions\\
\item
  Global sample-level variation (e.g., age, disease status)
\end{itemize}

Key aspects:

\begin{itemize}
\tightlist
\item
  \textbf{Masked modeling objectives}: Learn to reconstruct held-out CpG
  values from context.\\
\item
  \textbf{Multi-task pretraining}: Auxiliary tasks like array conversion
  or reference mapping encourage robust representations.\\
\item
  \textbf{Sample embeddings}: The {[}CLS{]}-like embedding for each
  sample acts as a compact, task-agnostic representation of its
  methylome.
\end{itemize}

\subsection{Zero-shot and Fine-tuned
Tasks}\label{zero-shot-and-fine-tuned-tasks}

Because CpGPT is trained on diverse cohorts, it exhibits
\emph{zero-shot} or few-shot generalization to new tasks:

\begin{itemize}
\tightlist
\item
  \textbf{Imputation and array conversion}: Fill in missing CpGs or
  harmonize different methylation platforms.\\
\item
  \textbf{Chronological age and mortality prediction}: Yield clocks that
  match or exceed specialized models.\\
\item
  \textbf{Sample classification}: Distinguish tissues, disease states,
  or exposure profiles.
\end{itemize}

In a multi-omics context, CpGPT-derived embeddings can serve as:

\begin{itemize}
\tightlist
\item
  Inputs to \emph{downstream} predictors (e.g., risk scores, prognosis
  models).\\
\item
  One modality in a shared latent space (with expression, proteomics,
  etc.).\\
\item
  A way to inject epigenetic state into otherwise sequence-centric GFMs.
\end{itemize}

Conceptually, CpGPT is an example of a \textbf{single-omic foundation
model} that is \emph{designed} to plug into multi-omics architectures.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{GLUE: Graph-linked Unified Embedding for Single-cell
Multi-omics}\label{glue-graph-linked-unified-embedding-for-single-cell-multi-omics}

\subsection{The Unpaired Single-cell Integration
Problem}\label{the-unpaired-single-cell-integration-problem}

Single-cell experiments often profile different modalities in different
cells---for instance:

\begin{itemize}
\tightlist
\item
  Some cells with \textbf{scRNA-seq} only\\
\item
  Other cells with \textbf{scATAC-seq} only\\
\item
  Sometimes a small subset with both (multiome) or additional modalities
  (e.g., protein, methylation)
\end{itemize}

The central challenge: build a unified atlas that aligns these cells in
a common space, recovers cell types and trajectories, and infers
regulatory networks.

GLUE (Graph-Linked Unified Embedding) addresses this by combining
modality-specific encoders with a \textbf{graph of biological prior
knowledge} linking features across omics.

\subsection{Architecture}\label{architecture-2}

GLUE consists of three key components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Modality-specific variational autoencoders (VAEs)}

  \begin{itemize}
  \tightlist
  \item
    Each omic (e.g., RNA, ATAC) has its own encoder--decoder pair.\\
  \item
    Encoders map cells to a low-dimensional latent embedding; decoders
    reconstruct modality-specific features.
  \end{itemize}
\item
  \textbf{Feature graph and SCGLUE}

  \begin{itemize}
  \tightlist
  \item
    Features (genes, peaks, motifs) form a graph whose edges capture
    biological relationships: e.g., a peak linked to a gene's promoter
    or enhancer, or TF binding motifs affecting genes.\\
  \item
    A graph neural network (GNN) learns feature embeddings consistent
    with this graph.
  \end{itemize}
\item
  \textbf{Alignment objectives}

  \begin{itemize}
  \tightlist
  \item
    Loss terms encourage the \emph{cell} latent spaces to align (so
    RNA-only and ATAC-only cells with similar biology end up near each
    other).\\
  \item
    The feature embeddings are tied to the cell latents via generative
    decoders, enforcing consistency between data and prior graph.
  \end{itemize}
\end{enumerate}

The result is a \textbf{unified embedding} in which cells from multiple
modalities can be jointly clustered, visualized, and used for downstream
tasks.

\subsection{Applications}\label{applications}

The GLUE framework has demonstrated:

\begin{itemize}
\tightlist
\item
  \textbf{Multi-omics integration} (RNA, ATAC, methylation or protein)
  at single-cell resolution.\\
\item
  \textbf{Regulatory network inference} by linking chromatin features to
  gene expression through the feature graph.\\
\item
  \textbf{Atlas construction} over large cohorts, correcting earlier
  annotation errors and unifying datasets across labs.
\end{itemize}

From the perspective of GFMs, GLUE exemplifies \textbf{graph-guided
multi-modal pretraining}: modality-specific encoders learn a shared
latent space regularized by biological networks, enabling reuse across
tasks and tissues.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{GNN-based Multi-omics Cancer Subtyping: MoGCN, CGMega, and
Beyond}\label{gnn-based-multi-omics-cancer-subtyping-mogcn-cgmega-and-beyond}

Cancer is inherently multi-omic: driver mutations, copy number changes,
epigenetic reprogramming, and transcriptional rewiring jointly define
tumor subtypes. Multi-omics cancer subtyping models increasingly rely on
\textbf{graph neural networks} to capture this complexity.

\subsection{MoGCN: Patient Graphs from
Multi-omics}\label{mogcn-patient-graphs-from-multi-omics}

MoGCN is a graph-convolutional framework for cancer subtype
classification that integrates genomics, transcriptomics, and
proteomics.

Design:

\begin{itemize}
\tightlist
\item
  Each \textbf{patient} is a node in a graph; edges encode similarity
  (e.g., based on expression or multi-omics features).\\
\item
  For each omic, a GCN learns modality-specific latent
  representations.\\
\item
  These representations are \textbf{concatenated} into a joint embedding
  per patient.\\
\item
  A classifier operating on node embeddings predicts cancer subtypes
  (e.g., BRCA subtypes).
\end{itemize}

Benefits:

\begin{itemize}
\tightlist
\item
  Captures non-linear relationships between patients in a data-driven
  graph.\\
\item
  Naturally integrates multiple omics via multi-view GCNs.\\
\item
  Enables subtype discovery and interpretation via graph structure and
  learned embeddings.
\end{itemize}

\subsection{CGMega: Multi-omics Cancer Gene
Modules}\label{cgmega-multi-omics-cancer-gene-modules}

Where MoGCN focuses on \emph{patient-level} graphs, CGMega operates on
\textbf{gene-level} graphs:

\begin{itemize}
\tightlist
\item
  Nodes represent genes; edges capture multi-omics relationships
  (expression, copy number, methylation, 3D genome contacts, etc.).\\
\item
  A graph attention network learns \textbf{cancer gene
  modules}---subsets of genes that co-vary across omics and are
  associated with phenotypes.
\end{itemize}

This module-centric view aligns with systems biology: instead of
single-gene markers, CGMega identifies network-level signatures that
better reflect pathway dysregulation.

\subsection{Design Patterns and
Alternatives}\label{design-patterns-and-alternatives}

A growing ecosystem of multi-omics subtyping methods uses related
patterns:

\begin{itemize}
\tightlist
\item
  \textbf{Contrastive learning} for multi-omics sample embeddings.\\
\item
  \textbf{Generative models} (e.g., GAN-based subtyping) that jointly
  model multiple omics for unsupervised clustering.\\
\item
  \textbf{Transformer-based hybrids} that blend MLPs and transformer
  blocks for high-dimensional omics.
\end{itemize}

Common themes:

\begin{itemize}
\tightlist
\item
  Modality-specific encoders with shared latent spaces\\
\item
  Graphs capturing patient--patient or gene--gene relationships\\
\item
  Emphasis on \emph{interpretability} via clusters, modules, or
  attention over features
\end{itemize}

These cancer models illustrate how multi-omics integration naturally
leads to \textbf{graph-structured GFMs}, where sequences, epigenetics,
and expression are all nodes in a learned biological network.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Rare Variants and Epistasis in Systems
Context}\label{rare-variants-and-epistasis-in-systems-context}

Chapter 2 discussed how standard PRS methods often ignore \textbf{rare
variants} and \textbf{epistasis}, despite their importance for
individual-level risk and disease mechanism. Multi-omics and systems
models offer a framework to incorporate these effects more effectively.

\subsection{DeepRVAT: Set-based Rare Variant Burden
Modeling}\label{deeprvat-set-based-rare-variant-burden-modeling}

DeepRVAT (Deep Rare Variant Association Testing) learns gene-level
impairment scores from rare variant annotations and genotypes using
\textbf{set neural networks}.

Key properties:

\begin{itemize}
\tightlist
\item
  Treats each gene's rare variants as an unordered set.\\
\item
  Learns a \emph{trait-agnostic} gene impairment score that generalizes
  across traits.\\
\item
  Improves both gene discovery and detection of high-risk individuals
  across many complex traits.
\end{itemize}

Conceptually, DeepRVAT bridges the gap between variant-level annotations
(e.g., VEP, conservation, structure-based predictions) and gene-level
burden, making it naturally compatible with sequence-based variant
effect models introduced earlier in the book.

\subsection{NeEDL: Network-based Epistasis
Detection}\label{needl-network-based-epistasis-detection}

NeEDL (Network-based Epistasis Detection via Local search) uses
\textbf{network medicine and quantum-inspired optimization} to identify
epistatic interactions among SNPs.

Core ideas:

\begin{itemize}
\tightlist
\item
  Build a network of SNPs and genes based on biological priors and GWAS
  signals.\\
\item
  Use local search strategies to explore combinations of variants that
  jointly influence disease.\\
\item
  Prioritize \emph{interpretable} epistatic modules that map onto
  pathways and cellular processes.
\end{itemize}

NeEDL does not yet operate as a full GFM, but it points toward
\textbf{systems-level combinatorial reasoning} that future GFMs will
need to support.

\subsection{G2PT: Hierarchical Genotype-to-Phenotype
Transformers}\label{g2pt-hierarchical-genotype-to-phenotype-transformers}

G2PT (Genotype-to-Phenotype Transformer) explicitly models hierarchical
structure:

\begin{itemize}
\tightlist
\item
  Variant-level signals aggregate into \textbf{genes}.\\
\item
  Genes aggregate into \textbf{systems} (e.g., pathways, tissues).\\
\item
  Systems collectively determine phenotypes and polygenic risk.
\end{itemize}

Architecturally:

\begin{itemize}
\tightlist
\item
  Uses transformer blocks to model interactions at each level.\\
\item
  Incorporates prior knowledge (e.g., gene--pathway membership) to
  structure attention patterns.\\
\item
  Provides explanations by attributing risk to specific variants, genes,
  and systems.
\end{itemize}

G2PT can be viewed as an early example of a \textbf{systems-aware GFM}
for genotype data, unifying additive and interaction effects within a
single deep model.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Deep Learning-enhanced Polygenic Risk and
Fine-mapping}\label{deep-learning-enhanced-polygenic-risk-and-fine-mapping}

Chapter 2 framed PRS as linear weighted sums of SNP effects. Deep
learning extends this paradigm by:

\begin{itemize}
\tightlist
\item
  Modeling non-linear interactions and context dependence\\
\item
  Integrating multi-omics features as priors or inputs\\
\item
  Sharing information across ancestries and cohorts
\end{itemize}

\subsection{Deep-learning PRS (e.g., Delphi-like
frameworks)}\label{deep-learning-prs-e.g.-delphi-like-frameworks}

Deep-learning PRS frameworks learn complex functions of genotype and
covariates, rather than relying on additive SNP weights.

Key contributions:

\begin{itemize}
\tightlist
\item
  Incorporate \textbf{non-genetic risk factors} alongside genome-wide
  variants.\\
\item
  Learn non-linear functions that can capture dominance, epistasis, and
  interactions with covariates.\\
\item
  Demonstrate improved discrimination over traditional PRS across
  several traits.
\end{itemize}

From a systems perspective, these models represent a move toward
\textbf{whole-patient risk modeling}, albeit still primarily from
genotype + covariates, without explicit multi-omics integration.

\subsection{MIFM and Multi-ancestry
Fine-mapping}\label{mifm-and-multi-ancestry-fine-mapping}

Multiple-instance fine-mapping frameworks (MIFM-like methods) address a
key bottleneck: lack of per-variant labels. Instead, we often know only
that \emph{some} variant(s) in a locus are causal. This is formulated as
a \textbf{multiple-instance learning} problem:

\begin{itemize}
\tightlist
\item
  Each locus is a ``bag'' of variants.\\
\item
  Loci with significant GWAS signals form positive bags; others form
  negative bags.\\
\item
  A deep model learns to assign high scores to causal variants within
  positive bags.
\end{itemize}

Related methods in multi-ancestry contexts combine signals across
cohorts and ancestries, leveraging divergent LD patterns to refine
causal inference.

Connections to earlier chapters:

\begin{itemize}
\tightlist
\item
  Variant effect predictors (Chapters 5--7, 13) can supply per-variant
  features.\\
\item
  Multi-omics models (this chapter) provide functional priors (e.g.,
  regulatory activity, methylation, chromatin accessibility).\\
\item
  MIFM-type frameworks integrate these priors with GWAS evidence to
  produce more accurate, ancestry-aware fine-mapping.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Design Patterns for Multi-omics and Systems
GFMs}\label{design-patterns-for-multi-omics-and-systems-gfms}

Pulling these examples together, several design patterns emerge for
systems-level GFMs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Modality-specific encoders + shared latent space}

  \begin{itemize}
  \tightlist
  \item
    CpGPT, GLUE, and many multi-omics subtyping models use separate
    encoders for each omic, aligned in a common embedding space.\\
  \item
    This design supports flexible inference with missing modalities and
    incremental addition of new data types.
  \end{itemize}
\item
  \textbf{Graph-guided integration}

  \begin{itemize}
  \tightlist
  \item
    GLUE's feature graph, CGMega's gene modules, and NeEDL's epistasis
    networks all use prior or learned graphs to structure learning.\\
  \item
    GNNs, graph transformers, and attention over graph edges are natural
    tools for encoding biological networks.
  \end{itemize}
\item
  \textbf{Hierarchical modeling}

  \begin{itemize}
  \tightlist
  \item
    G2PT formalizes the hierarchy from variants → genes → systems →
    phenotypes.\\
  \item
    Similar hierarchies can be defined for omics layers: sequence →
    chromatin → methylation → expression → protein → clinical traits.
  \end{itemize}
\item
  \textbf{Set- and bag-based learning}

  \begin{itemize}
  \tightlist
  \item
    DeepRVAT and MIFM treat variants or loci as sets/bags with
    permutation-invariant architectures.\\
  \item
    This is crucial when sample sizes are large, labels are sparse, and
    order is biologically meaningless.
  \end{itemize}
\item
  \textbf{Foundation pretraining + task-specific adaptation}

  \begin{itemize}
  \tightlist
  \item
    CpGPT is pretrained on massive methylation datasets and then adapted
    to tasks like aging clocks, mortality prediction, or disease
    classification.\\
  \item
    Future models may pretrain jointly on sequence, chromatin,
    methylation, expression, and clinical data, then specialize for
    specific traits.
  \end{itemize}
\end{enumerate}

These patterns collectively point toward \textbf{general-purpose systems
GFMs} that can ingest heterogeneous biological data and output risk
predictions, mechanistic hypotheses, or treatment recommendations.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Practical Pitfalls and
Considerations}\label{practical-pitfalls-and-considerations}

Despite impressive progress, multi-omics and systems GFMs are especially
vulnerable to confounding and overinterpretation---issues examined in
depth in Chapter 14. Key challenges include:

\begin{itemize}
\tightlist
\item
  \textbf{Batch effects and platform heterogeneity}

  \begin{itemize}
  \tightlist
  \item
    Different omics layers often come from different assays, labs, or
    time points.\\
  \item
    Integration methods can inadvertently encode batch structure rather
    than biology if not properly corrected.
  \end{itemize}
\item
  \textbf{Sample size and missingness}

  \begin{itemize}
  \tightlist
  \item
    Multi-omics datasets are typically smaller than single-omic
    datasets.\\
  \item
    Many samples lack certain modalities, requiring robust handling of
    missing data.
  \end{itemize}
\item
  \textbf{Population diversity and fairness}

  \begin{itemize}
  \tightlist
  \item
    As highlighted for PRS, representation of diverse ancestries is
    essential.\\
  \item
    Multi-omics GFMs risk amplifying disparities if trained primarily on
    European-ancestry or high-resource cohorts.
  \end{itemize}
\item
  \textbf{Evaluation complexity}

  \begin{itemize}
  \tightlist
  \item
    Multi-omics models can be evaluated at many levels: predictive
    performance, biological consistency, network plausibility, and
    clinical utility.\\
  \item
    Overfitting to proxy metrics (e.g., clustering quality) may not
    translate to actionable biology.
  \end{itemize}
\item
  \textbf{Interpretability and causal inference}

  \begin{itemize}
  \tightlist
  \item
    Attention or feature importance scores are not guarantees of causal
    mechanism.\\
  \item
    Integrating deep models with perturbation data (e.g., CRISPR
    screens) and robust causal frameworks remains an open frontier.
  \end{itemize}
\end{itemize}

Careful experimental design, thoughtful validation, and transparent
reporting are therefore especially crucial for multi-omics GFMs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Outlook: Toward Whole-patient Foundation
Models}\label{outlook-toward-whole-patient-foundation-models}

The methods in this chapter sketch an endgame for genomic deep learning:

\begin{itemize}
\tightlist
\item
  \textbf{Genome-wide variant and sequence representation} via hybrid
  CNN/transformer/SSM architectures (Chapters 10--13).\\
\item
  \textbf{Multi-omics integration} through graph-guided latent spaces
  (CpGPT, GLUE, MoGCN, CGMega).\\
\item
  \textbf{Systems-level reasoning} about rare variants and epistasis
  (DeepRVAT, NeEDL, G2PT).\\
\item
  \textbf{Clinically oriented risk modeling} with deep PRS and
  fine-mapping (Delphi-like and MIFM-like frameworks).
\end{itemize}

A future \textbf{whole-patient foundation model} might:

\begin{itemize}
\tightlist
\item
  Jointly encode genotype, methylome, chromatin state, expression,
  proteomics, imaging, and EHR data.\\
\item
  Provide unified representations across tissues, cell types, and time
  points.\\
\item
  Offer calibrated, equitable predictions of disease risk and treatment
  response.\\
\item
  Support mechanistic queries like ``which pathways mediate this
  variant's effect in this tissue?'' or ``which interventions counteract
  this rare variant burden in this patient?''
\end{itemize}

Realizing this vision will require advances in data sharing,
privacy-preserving learning, scalable architecture design, and causal
validation. But the methods surveyed here show that moving beyond
single-omics is not just incremental---it fundamentally changes what
kinds of questions genomic models can answer, bringing us closer to
truly systems-level, clinically actionable genomics.

\chapter{Confounders in Model
Training}\label{confounders-in-model-training}

In previous chapters, we treated model performance curves and ROC--AUC
numbers as if they transparently reflected how well a model learns
biology. In practice, genomic data is riddled with structure that makes
it dangerously easy for models---especially large, overparameterized
ones---to exploit shortcuts.

Population structure, technical batch effects, benchmark leakage, and
label noise can all inflate headline metrics while leaving real-world
performance and clinical reliability largely unchanged. These issues are
not unique to deep learning; they affect traditional statistics and GWAS
as well. But the scale, flexibility, and opacity of modern genomic
foundation models (GFMs) make them particularly susceptible.

This chapter surveys the main confounders that arise when training and
evaluating genomic models, and outlines practical strategies to detect,
mitigate, and transparently report them. We focus on five recurring
themes:

\begin{itemize}
\tightlist
\item
  Ancestry stratification and population bias\\
\item
  Benchmark leakage and train/test overlap\\
\item
  Technical artifacts and batch effects\\
\item
  Label noise and ground-truth uncertainty\\
\item
  Cross-ancestry transferability of polygenic risk scores (PRS) and
  other models
\end{itemize}

Throughout, the key message is simple: \textbf{architecture advances are
only as meaningful as the datasets and evaluation protocols that support
them}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Why Confounders Are Ubiquitous in Genomic
ML}\label{why-confounders-are-ubiquitous-in-genomic-ml}

A confounder is a variable that influences both the features (e.g.,
genotypes, readouts) and the labels (e.g., case/control status,
functional effect), creating spurious associations. In genomics,
confounders abound because:

\begin{itemize}
\tightlist
\item
  \textbf{Data are observational, not randomized.} Disease labels,
  population sampling, and technical pipelines are all determined by
  real-world constraints and historical biases.\\
\item
  \textbf{Population structure is strong and multi-layered.} Ancestry,
  relatedness, and local adaptation affect allele frequencies throughout
  the genome.\\
\item
  \textbf{Technical pipelines are complex.} Each step---sample
  collection, library prep, sequencing, alignment, variant calling,
  QC---can introduce systematic differences between cohorts.\\
\item
  \textbf{Labels are noisy.} Clinical databases (e.g., ClinVar) and
  high-throughput assays contain uncertain and sometimes incorrect
  annotations.
\end{itemize}

Deep models are powerful pattern detectors. If confounders produce
consistent patterns that correlate with labels, models will happily
learn those shortcuts instead of the causal biology we care about. The
result is impressive performance on held-out data that share the same
hidden structure, but brittle behavior as soon as we change ancestry,
institution, assay, or time period.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Ancestry Stratification and Population
Bias}\label{ancestry-stratification-and-population-bias}

\subsection{How ancestry becomes a
shortcut}\label{how-ancestry-becomes-a-shortcut}

Human genetic variation is structured by ancestry: allele frequencies
and haplotype patterns differ across populations due to demographic
history, drift, and selection. Disease prevalence, environmental
exposures, and health-care access are also ancestry- and
region-dependent.

This creates a classic confounding scenario:

\begin{itemize}
\tightlist
\item
  \textbf{Features:} Genotypes or sequence variants reflect ancestry.\\
\item
  \textbf{Labels:} Case/control status, disease subtype, or even
  ``pathogenic vs.~benign'' annotations can vary with ancestry.
\end{itemize}

If a case cohort is primarily of one ancestry and controls are primarily
of another, a model can achieve high predictive performance by acting as
an ancestry classifier rather than a disease predictor. The same issue
arises for variant effect prediction: variants common in one ancestry
but rare in another can be spuriously tagged as pathogenic or benign
because of how databases were curated.

\subsection{Manifestations in genomic
models}\label{manifestations-in-genomic-models}

Some common ways ancestry confounding shows up:

\begin{itemize}
\tightlist
\item
  \textbf{Case/control imbalance across ancestries.} For example, cases
  over-representing individuals of European ancestry, controls
  over-representing other groups.\\
\item
  \textbf{Reference database bias.} Variant annotations derived mostly
  from European-ancestry cohorts; ``benign'' often means ``common in
  Europeans''.\\
\item
  \textbf{Implicit ancestry markers.} Cryptic relatedness, shared
  haplotypes, and local LD patterns let models recover ancestry even
  when explicit labels are removed.
\end{itemize}

For high-capacity models such as transformer-based GFMs, even subtle
ancestry differences are enough to support a shortcut.

\subsection{Detecting ancestry
confounding}\label{detecting-ancestry-confounding}

Practical diagnostics include:

\begin{itemize}
\tightlist
\item
  \textbf{PCA or UMAP of genotypes/embeddings.} If cases and controls
  cluster by ancestry, that's a red flag.\\
\item
  \textbf{Stratified performance.} Evaluate metrics separately within
  each ancestry group; large performance drops or reversals across
  groups suggest confounding.\\
\item
  \textbf{Ancestry-only baselines.} Fit a simple classifier on ancestry
  PCs or self-identified ancestry alone. If this baseline approaches
  your model's performance, your model is likely exploiting similar
  information.\\
\item
  \textbf{Permutation tests within ancestry strata.} Shuffling labels
  within ancestry groups should destroy performance for a truly
  disease-specific signal, but not for models relying on cross-ancestry
  differences.
\end{itemize}

\subsection{Mitigating ancestry bias}\label{mitigating-ancestry-bias}

Mitigation is imperfect, but several strategies help:

\begin{itemize}
\tightlist
\item
  \textbf{Balanced study design.} Wherever possible, recruit cases and
  controls with similar ancestry distributions, or match controls to
  cases.\\
\item
  \textbf{Within-ancestry evaluation.} Report metrics for each ancestry
  separately; use training--validation splits that preserve within-group
  structure.\\
\item
  \textbf{Covariate adjustment.} Include ancestry PCs, kinship matrices,
  or mixed-model random effects in simpler models; for deep models,
  condition on or adversarially remove ancestry signals from learned
  embeddings.\\
\item
  \textbf{Multi-ancestry training.} Train on diverse populations rather
  than restricting to a single ancestry, and explicitly model ancestry
  as a domain variable.\\
\item
  \textbf{Fairness-aware objectives.} Introduce regularizers or
  constraints that penalize performance disparities across ancestry
  groups, especially in clinical deployment contexts.
\end{itemize}

In later chapters on PRS and multi-omics, careful ancestry handling will
be essential for equitable risk prediction.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Benchmark Leakage and Train/Test
Overlap}\label{benchmark-leakage-and-traintest-overlap}

Even with perfectly balanced ancestries, evaluation can be misleading if
information ``leaks'' from training to test sets. Leakage is especially
insidious in genomics because:

\begin{itemize}
\tightlist
\item
  The genome is highly structured and redundant.\\
\item
  Public datasets and benchmarks are heavily reused.\\
\item
  Many papers do not fully specify how splits were constructed.
\end{itemize}

\subsection{Forms of leakage}\label{forms-of-leakage}

Common leakage patterns include:

\begin{itemize}
\tightlist
\item
  \textbf{Individual overlap.} The same person (or close relative)
  appears in both train and test sets, directly or via related
  cohorts.\\
\item
  \textbf{Variant overlap.} Exact variants, or near-identical ones at
  the same locus, appear in both splits; this can happen when different
  datasets are merged.\\
\item
  \textbf{Locus-level overlap.} Variants in the same gene, regulatory
  element, or LD block are split between train and test. A model may
  learn locus-specific idiosyncrasies instead of general rules.\\
\item
  \textbf{Database reuse leakage.} Benchmarks constructed from ClinVar,
  gnomAD, or other public databases but evaluated against an external
  set that partially overlaps those sources.\\
\item
  \textbf{Time-based leakage.} Models trained on data that include later
  submissions of the same variants or patients that are used as
  ``future'' test examples.
\end{itemize}

For large models, even very small overlaps can inflate metrics,
particularly when test sets are small.

\subsection{Safer splitting
strategies}\label{safer-splitting-strategies}

To reduce leakage:

\begin{itemize}
\tightlist
\item
  \textbf{Individual-level splits.} Ensure that no individual (or
  closely related individuals, if kinship is known) appears in both
  train and test sets.\\
\item
  \textbf{Locus- or gene-level splits.} For variant effect prediction,
  split at the gene, enhancer, or genomic region level so that test loci
  are unseen.\\
\item
  \textbf{Chromosome-based splits.} For genome-wide tasks, hold out
  entire chromosomes or chromosome arms. This is not perfect but greatly
  reduces local dependency leakage.\\
\item
  \textbf{Time-based splits.} Train on data up to a cutoff date and test
  on later data, mimicking realistic deployment.\\
\item
  \textbf{Transparent data provenance.} Track the origin of each sample
  and variant (e.g., database version, submission ID) to avoid
  accidental reuse.
\end{itemize}

\subsection{Evaluation design and
reporting}\label{evaluation-design-and-reporting}

Beyond the split itself, \textbf{evaluation design} matters:

\begin{itemize}
\tightlist
\item
  Report both \textbf{in-distribution} performance (same cohort) and
  \textbf{out-of-distribution} performance (new cohorts, ancestries, or
  technical pipelines).\\
\item
  Whenever possible, include \textbf{cross-cohort benchmarks}: train on
  one cohort, test on another with different recruitment or sequencing
  characteristics.\\
\item
  Share code and detailed recipes for dataset construction so that
  others can reproduce and critique splitting choices.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Technical Artifacts: Batch Effects and Platform
Differences}\label{technical-artifacts-batch-effects-and-platform-differences}

While ancestry and population structure reflect biological reality,
\textbf{batch effects} are artifacts of the measurement process. In
genomics, differences in:

\begin{itemize}
\tightlist
\item
  Sample collection protocols\\
\item
  Library preparation kits\\
\item
  Sequencing platforms and chemistry versions\\
\item
  Read length, depth, and coverage\\
\item
  Alignment and variant calling pipelines
\end{itemize}

can all introduce systematic shifts in feature distributions.

\subsection{How batch effects confound
models}\label{how-batch-effects-confound-models}

Technical batches often correlate with labels:

\begin{itemize}
\tightlist
\item
  A case cohort may be sequenced at one institution on one platform,
  while controls are sequenced elsewhere with different protocols.\\
\item
  A longitudinal study might switch from one capture kit or sequencer to
  another halfway through, coinciding with changes in enrollment
  criteria.\\
\item
  Public datasets may aggregate studies with very different technical
  characteristics.
\end{itemize}

In such settings, a model can achieve high accuracy by recognizing
\textbf{batch signatures} (e.g., patterns of missingness, depth, noise
spectra) rather than bona fide biological signals.

\subsection{Diagnosing technical
confounders}\label{diagnosing-technical-confounders}

Common diagnostics include:

\begin{itemize}
\tightlist
\item
  \textbf{Embedding visualization by batch.} Project learned embeddings
  or expression/coverage profiles via PCA or UMAP, then color points by
  batch, platform, or institution. Strong clustering by these variables
  suggests technical structure.\\
\item
  \textbf{Batch-only baselines.} Train a classifier using only batch
  labels or simple technical covariates (e.g., read depth, platform
  indicators). High baseline performance is a warning sign.\\
\item
  \textbf{Negative controls.} Evaluate models on samples where labels
  should be uncorrelated with batch (e.g., technical replicates,
  randomized subsets).\\
\item
  \textbf{Replicate consistency.} Examine consistency of predictions
  across technical replicates processed in different batches.
\end{itemize}

\subsection{Mitigating batch effects}\label{mitigating-batch-effects}

Mitigation is an active research area; common approaches include:

\begin{itemize}
\tightlist
\item
  \textbf{Careful study design.} Randomize cases and controls across
  batches whenever possible; avoid systematic alignment between batch
  and outcome.\\
\item
  \textbf{Preprocessing harmonization.} Use standardized pipelines for
  alignment and variant calling; reprocess raw data when feasible to
  reduce inter-study differences.\\
\item
  \textbf{Statistical batch correction.} Methods such as ComBat,
  Harmony, and related approaches can reduce batch effects in expression
  or chromatin data; similar ideas can be applied to embeddings from
  GFMs.\\
\item
  \textbf{Domain adaptation and adversarial training.} Train
  representations that are predictive of labels while being invariant to
  batch or platform (e.g., via gradient reversal layers or distribution
  matching objectives).\\
\item
  \textbf{Explicit multi-domain modeling.} Treat each batch or platform
  as a domain and learn domain-conditional parameters or
  mixture-of-experts models.
\end{itemize}

Even with aggressive correction, residual batch structure typically
remains; transparent reporting and robustness checks are essential.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Label Noise and Ground-Truth
Uncertainty}\label{label-noise-and-ground-truth-uncertainty}

Large-scale genomic models rely on labels from:

\begin{itemize}
\tightlist
\item
  Clinical variant interpretation databases (e.g., pathogenic
  vs.~benign)\\
\item
  GWAS-derived case/control status\\
\item
  High-throughput functional screens (e.g., MPRA, saturation
  mutagenesis, CRISPR screens)\\
\item
  Curated ``gold-standard'' sets for VEP, splicing predictions, or PRS
\end{itemize}

These labels are \textbf{not} error-free. Sources of label noise
include:

\begin{itemize}
\tightlist
\item
  \textbf{Conflicting annotations.} ClinVar often contains variants with
  conflicting interpretations or uncertain significance; criteria for
  pathogenicity change over time.\\
\item
  \textbf{Ascertainment bias.} Variants labeled as ``benign'' may simply
  be common in some populations; variants labeled as ``pathogenic'' may
  be enriched in clinically ascertained cohorts.\\
\item
  \textbf{Measurement noise in functional assays.} High-throughput
  experiments have variable reproducibility across labs, conditions, and
  replicates. Thresholding continuous scores into discrete classes
  compounds the issue.\\
\item
  \textbf{Phenotyping noise.} Clinical case/control labels may be
  inaccurate due to misdiagnosis, incomplete records, or heterogeneous
  disease definitions.
\end{itemize}

\subsection{Consequences for models}\label{consequences-for-models}

Label noise can:

\begin{itemize}
\tightlist
\item
  Limit achievable performance, especially for tasks with overlapping
  phenotype definitions.\\
\item
  Encourage models to learn spurious proxies that correlate with
  annotation errors.\\
\item
  Bias calibration and decision thresholds, particularly in imbalanced
  settings.
\end{itemize}

In some scenarios, training on noisy labels still improves performance
if noise is roughly symmetric or if the dataset is very large. However,
for rare disease variants and high-stakes predictions, even small
fractions of mislabeled examples can be problematic.

\subsection{Strategies for robust learning with noisy
labels}\label{strategies-for-robust-learning-with-noisy-labels}

Approaches to deal with label noise include:

\begin{itemize}
\tightlist
\item
  \textbf{Curated subsets.} Restrict training and evaluation to
  high-confidence annotations (e.g., ClinVar ``Pathogenic'' and
  ``Benign'' with multiple submitters and no conflicts), even at the
  cost of reduced size.\\
\item
  \textbf{Soft labels and uncertainty modeling.} Use probabilistic
  labels derived from inter-rater disagreement, confidence scores, or
  continuous assay measurements rather than hard 0/1 labels.\\
\item
  \textbf{Robust losses.} Employ loss functions less sensitive to
  mislabeled points (e.g., label smoothing, margin-based losses, or
  methods that down-weight high-loss outliers).\\
\item
  \textbf{Noise-aware training.} Explicitly model label noise (e.g., via
  a noise transition matrix or latent variable models) and jointly infer
  true labels.\\
\item
  \textbf{Consensus across modalities.} Combine evidence from protein
  structure, evolutionary conservation, regulatory context, and clinical
  data; treat disagreements as signals of uncertainty.
\end{itemize}

Mechanistic interpretability can also help flag model predictions that
disagree with known biology, potentially identifying mislabeled
examples.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Cross-Ancestry PRS Transferability and Model
Fairness}\label{cross-ancestry-prs-transferability-and-model-fairness}

Polygenic risk scores and other genome-wide predictors have gained
traction as potential tools for early disease risk stratification.
However, many PRS have been developed primarily in individuals of
European ancestry, raising concerns about:

\begin{itemize}
\tightlist
\item
  \textbf{Reduced predictive accuracy} in underrepresented ancestries.\\
\item
  \textbf{Biased calibration,} where risk is systematically over- or
  under-estimated in certain groups.\\
\item
  \textbf{Downstream disparities} if PRS-informed clinical decisions
  (e.g., screening recommendations) are applied uniformly.
\end{itemize}

\subsection{Why transferability fails}\label{why-transferability-fails}

Reasons for poor cross-ancestry transfer include:

\begin{itemize}
\tightlist
\item
  \textbf{Allele frequency differences.} Effect estimates calibrated in
  one population may not generalize when allele frequencies change.\\
\item
  \textbf{LD pattern differences.} Tagging SNPs used in PRS may capture
  causal variants in one ancestry but not another.\\
\item
  \textbf{Gene--environment interaction.} Environmental exposures and
  lifestyle factors that interact with genetic risk differ across
  populations.\\
\item
  \textbf{Ascertainment and recruitment biases.} Early GWAS datasets
  often oversampled certain ancestries, clinical populations, or
  socioeconomic strata.
\end{itemize}

These issues carry over to deep learning--based PRS and GFMs fine-tuned
for disease prediction. Even if the underlying model is trained on
diverse genomes in a self-supervised fashion, the \textbf{supervised
fine-tuning and evaluation data} can reintroduce bias.

\subsection{Towards more equitable
models}\label{towards-more-equitable-models}

Approaches to improve cross-ancestry performance and fairness include:

\begin{itemize}
\tightlist
\item
  \textbf{Multi-ancestry GWAS and training data.} Include diverse
  cohorts at the design stage rather than as an afterthought.\\
\item
  \textbf{Ancestry-aware modeling.} Condition effect sizes or model
  parameters on ancestry, or learn ancestry-invariant representations
  coupled with ancestry-specific calibration.\\
\item
  \textbf{Transfer learning and fine-tuning.} Adapt models from
  ancestries with large datasets to those with smaller datasets using
  domain adaptation techniques.\\
\item
  \textbf{Fairness metrics.} Report group-wise calibration, sensitivity,
  specificity, and decision-curve analyses, not just overall AUC.\\
\item
  \textbf{Stakeholder engagement.} Work with clinicians, ethicists, and
  affected communities to decide when and how PRS should be used, and
  what constitutes acceptable performance gaps.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Cautionary Tales to Best
Practices}\label{from-cautionary-tales-to-best-practices}

Modern genomic foundation models promise impressive capabilities:
genome-scale variant effect prediction, cross-species transfer,
multi-omics integration, and clinically actionable risk scores. Yet
without rigorous attention to confounders, these capabilities can be
overstated or misapplied.

Emerging work on genomic evaluation frameworks emphasizes:

\begin{itemize}
\tightlist
\item
  \textbf{Data documentation.} Detailed datasheets for datasets and
  benchmarks, including recruitment, ancestry composition, technical
  pipelines, and label provenance.\\
\item
  \textbf{Robust evaluation protocols.} Cross-cohort, cross-ancestry,
  and time-split evaluations that stress-test models beyond their
  training distribution.\\
\item
  \textbf{Confounder-aware training.} Explicit modeling of ancestry,
  batch, and label uncertainty, and the use of adversarial or
  domain-adaptation techniques.\\
\item
  \textbf{Transparent reporting.} Clear communication of limitations,
  potential failure modes, and groups for whom the model has not been
  validated.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{A Practical Checklist for Confounder-Resilient Genomic
Modeling}\label{a-practical-checklist-for-confounder-resilient-genomic-modeling}

To close, here is a concise checklist you can apply when designing,
training, and evaluating genomic models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Population structure}

  \begin{itemize}
  \tightlist
  \item
    Have you quantified ancestry and relatedness (e.g., via PCs or
    kinship)?\\
  \item
    Are cases and controls balanced within ancestry groups?\\
  \item
    Do you report performance stratified by ancestry?
  \end{itemize}
\item
  \textbf{Data splits and leakage}

  \begin{itemize}
  \tightlist
  \item
    Are individuals, families, and closely related samples confined to a
    single split?\\
  \item
    Do you split at the locus, gene, or chromosome level where
    appropriate?\\
  \item
    Have you checked for overlap with external databases used in
    evaluation?
  \end{itemize}
\item
  \textbf{Batch and platform effects}

  \begin{itemize}
  \tightlist
  \item
    Are technical variables (batch, platform, institution) correlated
    with labels?\\
  \item
    Have you visualized embeddings colored by batch?\\
  \item
    Do you use harmonization, batch correction, or domain adaptation as
    needed?
  \end{itemize}
\item
  \textbf{Label quality}

  \begin{itemize}
  \tightlist
  \item
    How are labels defined, and what is their uncertainty?\\
  \item
    Do you filter to high-confidence subsets for primary evaluation?\\
  \item
    Do you employ robust training strategies to handle label noise?
  \end{itemize}
\item
  \textbf{Cross-group performance and fairness}

  \begin{itemize}
  \tightlist
  \item
    Do you report metrics for each ancestry and relevant subgroup?\\
  \item
    Are risk scores calibrated across groups, or is group-specific
    calibration required?\\
  \item
    Have you considered the ethical and clinical implications of
    residual performance gaps?
  \end{itemize}
\item
  \textbf{Reproducibility and transparency}

  \begin{itemize}
  \tightlist
  \item
    Are dataset construction and splitting procedures fully documented
    and shareable?\\
  \item
    Are code and evaluation pipelines available for independent
    verification?
  \end{itemize}
\end{enumerate}

By systematically addressing these points, we can ensure that the gains
from modern architectures---transformers, SSMs, and GFMs---translate
into \textbf{trustworthy} advances in genomic science and medicine,
rather than brittle models that merely reflect quirks of our data and
history.

\chapter{Interpretability \&
Mechanisms}\label{interpretability-mechanisms}

\section{Why Interpretability Matters for Genomic
Models}\label{why-interpretability-matters-for-genomic-models}

Deep learning models in genomics increasingly operate as
\textbf{systems-level surrogates} for biology: they predict chromatin
features, gene expression, or variant effects directly from sequence.
When such models drive mechanistic hypotheses or clinical decisions,
\emph{how} they make predictions becomes as important as \emph{how well}
they perform.

Interpretability in this context serves at least four roles:

\begin{itemize}
\tightlist
\item
  \textbf{Mechanistic insight}

  \begin{itemize}
  \tightlist
  \item
    Extract sequence motifs (putative TF binding sites), regulatory
    grammars, and long-range interaction patterns directly from trained
    models.\\
  \item
    Turn ``black-box'' predictions into candidate mechanisms that can be
    tested experimentally.
  \end{itemize}
\item
  \textbf{Model debugging and confounder detection}

  \begin{itemize}
  \tightlist
  \item
    Reveal when models rely on artifacts (e.g., GC content, mappability,
    batch-specific motifs) instead of bona fide regulatory signals.\\
  \item
    Complement Chapter 14's focus on data and evaluation confounders by
    interrogating \emph{model internals}.
  \end{itemize}
\item
  \textbf{Clinical and translational trust}

  \begin{itemize}
  \tightlist
  \item
    Support variant interpretation workflows by explaining why specific
    rare or de novo variants are predicted to be damaging.\\
  \item
    Provide interpretable axes of variation (e.g., motif disruptions,
    regulatory ``sequence classes'') that can be combined with
    orthogonal evidence.
  \end{itemize}
\item
  \textbf{Scientific communication}

  \begin{itemize}
  \tightlist
  \item
    Condense high-dimensional latent representations into human-readable
    abstractions---motifs, regulatory classes, or interaction
    graphs---that can be shared across labs and applications.
  \end{itemize}
\end{itemize}

This chapter surveys the main interpretability tools developed for
genomic models, from convolutional filters and saliency maps to global
regulatory vocabularies and attention patterns in genomic language
models (gLMs) and transformer-based regulatory models. Throughout, the
emphasis is on \textbf{mechanistic} interpretability: moving from ``what
correlates with the prediction?'' to ``what regulatory hypothesis does
the model imply?''

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Interpreting Convolutional Filters as
Motifs}\label{interpreting-convolutional-filters-as-motifs}

Convolutional neural networks (CNNs) remain a workhorse for modeling
cis-regulatory sequence (Chapters 5--7). In many of these models,
\textbf{first-layer convolutional filters} act as \emph{motif
detectors}:

\begin{itemize}
\tightlist
\item
  A filter slides along the one-hot encoded sequence (Chapter 8).\\
\item
  At each position, it computes a dot product between its weights and
  the local sequence window.\\
\item
  High activation indicates that the subsequence closely matches the
  filter's preferred pattern.
\end{itemize}

\subsection{From Filters to Motif
Logos}\label{from-filters-to-motif-logos}

A common workflow to interpret filters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Collect high-activation instances}

  \begin{itemize}
  \tightlist
  \item
    Run the trained model on a large sequence set (e.g., training data
    or genome tiles).\\
  \item
    For each filter, record positions where its activation exceeds a
    threshold.
  \end{itemize}
\item
  \textbf{Extract and align subsequences}

  \begin{itemize}
  \tightlist
  \item
    Pull out fixed-length windows around those positions.\\
  \item
    Align them and compute base frequencies at each position.
  \end{itemize}
\item
  \textbf{Build a position weight matrix (PWM)}

  \begin{itemize}
  \tightlist
  \item
    Convert base frequencies to log-odds scores relative to a background
    distribution.\\
  \item
    Visualize as a sequence logo.
  \end{itemize}
\item
  \textbf{Match to known motif databases}

  \begin{itemize}
  \tightlist
  \item
    Compare PWMs to JASPAR or HOCOMOCO TF motif libraries using
    similarity scores.\\
  \item
    Annotate filters with candidate TF identities (``this filter
    resembles CTCF'').
  \end{itemize}
\end{enumerate}

This procedure has been applied to models like DeepSEA and its
successors to demonstrate that early layers learn motifs for canonical
TFs and chromatin-associated patterns, validating that models are
discovering biologically meaningful sequence features rather than
arbitrary patterns.

\subsection{Beyond First-Layer
Filters}\label{beyond-first-layer-filters}

Deeper convolutional layers aggregate lower-level motifs:

\begin{itemize}
\tightlist
\item
  \textbf{Combinatorial motifs}: Filters that respond to pairs or
  clusters of TF motifs.\\
\item
  \textbf{Grammar patterns}: Distance or orientation constraints (e.g.,
  ``ETS motif \textasciitilde10 bp upstream of GATA motif'').\\
\item
  \textbf{Contextual preferences}: Filters that fire only in particular
  GC contexts or nucleosome positioning patterns.
\end{itemize}

However, directly interpreting deeper layers becomes challenging because
receptive fields expand and nonlinearities accumulate. This motivates
\textbf{attribution-based} approaches that connect predictions back to
individual bases.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Attribution Methods: Connecting Bases to
Predictions}\label{attribution-methods-connecting-bases-to-predictions}

Attribution methods assign an ``importance score'' to each input base
(or k-mer), reflecting how much it contributes to a prediction for a
specific task and sequence.

Let ( f(x) ) be a model predicting some output (e.g., chromatin
accessibility, gene expression, or variant effect) from sequence ( x ).
Attribution methods estimate the contribution of each base ( x\_i ) to (
f(x) ), often for a specific output neuron (e.g., a particular cell
type).

\subsection{In Silico Mutagenesis
(ISM)}\label{in-silico-mutagenesis-ism}

\textbf{In silico mutagenesis} is conceptually straightforward and
model-agnostic:

\begin{itemize}
\tightlist
\item
  For each position ( i ) and base ( b ), create a mutated sequence (
  x\^{}\{(i \rightarrow b)\} ).\\
\item
  Compute the change in prediction\\
  {[} \Delta f\_\{i,b\} = f(x\^{}\{(i \rightarrow b)\}) - f(x). {]}
\item
  Aggregate these changes (e.g., max across non-reference alleles) to
  obtain a per-base importance score.
\end{itemize}

Variants:

\begin{itemize}
\tightlist
\item
  \textbf{Single-nucleotide ISM}: Mutate each base individually;
  expensive but faithful.\\
\item
  \textbf{Saturation mutagenesis}: Explore all possible oligos in a
  window to probe combinatorial effects and grammar.\\
\item
  \textbf{Variant-specific scoring}: Evaluate ( f(\text{alt}) -
  f(\text{ref}) ) for a particular SNV or indel.
\end{itemize}

Strengths:

\begin{itemize}
\tightlist
\item
  True ``what-if'' causal perturbations under the model.\\
\item
  Works for any differentiable or non-differentiable model (including
  ensembles and post-processed scores).
\end{itemize}

Limitations:

\begin{itemize}
\tightlist
\item
  Computationally expensive: ( O(L \times \textbar{}\mathcal{A}\textbar)
  ) forward passes for sequence length ( L ) and alphabet size (
  \textbar{}\mathcal{A}\textbar{} ).\\
\item
  Captures local effects; may miss distributed interactions if not
  designed carefully.
\end{itemize}

\subsection{Gradient-Based Methods}\label{gradient-based-methods}

Gradient-based methods approximate ``how much would the prediction
change if we nudged this base?'' via backpropagation.

\subsubsection{Vanilla Gradient /
Saliency}\label{vanilla-gradient-saliency}

Compute the gradient of the output with respect to the input:

{[} s\_i = \frac{\partial f(x)}{\partial x_i}. {]}

With one-hot encoding, this gradient can be interpreted as the
sensitivity to changing the nucleotide at position ( i ). A common
variant multiplies the gradient by the input (``gradient × input'').

Pros:

\begin{itemize}
\tightlist
\item
  Requires a single backward pass per sequence.\\
\item
  Easy to implement and integrate into training workflows.
\end{itemize}

Cons:

\begin{itemize}
\tightlist
\item
  Susceptible to gradient saturation (zero gradients in regions where
  the model is already confident).\\
\item
  Noisy saliency maps often require smoothing or aggregation across
  multiple noisy inputs.
\end{itemize}

\subsubsection{DeepLIFT}\label{deeplift}

\textbf{DeepLIFT} (Deep Learning Important FeaTures) compares neuron
activations between an input and a \emph{reference} (or baseline)
sequence, distributing differences back to inputs using layer-wise rules
rather than raw gradients. It aims to:

\begin{itemize}
\tightlist
\item
  Avoid gradient saturation.\\
\item
  Enforce a consistency constraint: the sum of input contributions
  matches the difference in output between input and reference.
\end{itemize}

DeepLIFT has been widely used for genomic models, particularly in
conjunction with TF-MoDISco (next section), where its base-level
importance scores serve as inputs for motif discovery.

\subsubsection{Integrated Gradients (IG)}\label{integrated-gradients-ig}

\textbf{Integrated Gradients} compute the path integral of gradients
along a linear interpolation from a reference ( x' ) to the input ( x ):

{[} \text{IG}\_i(x) = (x\_i - x'\emph{i) \int}\{\alpha=0\}\^{}1
\frac{\partial f\left(x' + \alpha(x - x')\right)}{\partial x_i} d\alpha.
{]}

In practice, this integral is approximated via a Riemann sum over
discrete steps. IG satisfies desirable axioms (e.g., sensitivity,
implementation invariance) and tends to be less noisy than raw
gradients.

Key design considerations for all gradient-based methods:

\begin{itemize}
\tightlist
\item
  \textbf{Choice of reference}:

  \begin{itemize}
  \tightlist
  \item
    Random genomic background, dinucleotide-shuffled sequence, or an
    ``average'' non-functional sequence.\\
  \item
    Different references emphasize different aspects of the signal.
  \end{itemize}
\item
  \textbf{Output selection}:

  \begin{itemize}
  \tightlist
  \item
    Single-task models: directly attribute the scalar output.\\
  \item
    Multi-task models: choose a specific track (e.g., H3K27ac in one
    cell type) or aggregate across tasks.
  \end{itemize}
\item
  \textbf{Post-processing}:

  \begin{itemize}
  \tightlist
  \item
    Smooth along the sequence (e.g., average in sliding windows).\\
  \item
    Aggregate over channels or strands.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Attributions to Motifs:
TF-MoDISco}\label{from-attributions-to-motifs-tf-modisco}

Attribution maps highlight \emph{where} the model focuses, but they do
not automatically yield \textbf{consistent motifs} or \textbf{regulatory
grammars}. TF-MoDISco (Transcription Factor Motif Discovery from
Importance Scores) was developed to bridge this gap.

\subsection{Core Idea}\label{core-idea}

Rather than performing motif discovery on raw sequences, TF-MoDISco
operates on \textbf{base-level importance scores}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Compute importance scores}

  \begin{itemize}
  \tightlist
  \item
    Use DeepLIFT, ISM, IG, or similar methods on many sequences.\\
  \item
    Obtain an importance score for each base and strand.
  \end{itemize}
\item
  \textbf{Extract ``seqlets''}

  \begin{itemize}
  \tightlist
  \item
    Identify local windows where the \emph{total} importance exceeds a
    threshold.\\
  \item
    Treat each window (seqlet) as a candidate motif instance.
  \end{itemize}
\item
  \textbf{Cluster seqlets}

  \begin{itemize}
  \tightlist
  \item
    Compare seqlets using similarity metrics that consider both sequence
    and importance scores.\\
  \item
    Cluster into groups corresponding to putative motifs.
  \end{itemize}
\item
  \textbf{Build consolidated motifs}

  \begin{itemize}
  \tightlist
  \item
    Align seqlets within each cluster.\\
  \item
    Construct PWMs and importance-weighted logos.\\
  \item
    Optionally match to known TF motifs.
  \end{itemize}
\item
  \textbf{Report motif instances and grammar}

  \begin{itemize}
  \tightlist
  \item
    Map motifs back onto the genome.\\
  \item
    Analyze co-occurrence, spacing, and orientation rules.
  \end{itemize}
\end{enumerate}

When applied to models like BPNet, TF-MoDISco has recovered known TF
motifs, discovered novel variants, and revealed grammars (e.g.,
directional spacing constraints) that can be validated with synthetic
reporter assays.

In the context of genomic foundation models, an analogous workflow can
be applied:

\begin{itemize}
\tightlist
\item
  Use a GFM or transformer-based model to produce base-level
  attributions for a specific downstream task (e.g., chromatin
  accessibility).\\
\item
  Run TF-MoDISco to extract a \textbf{task-specific motif vocabulary}.\\
\item
  Analyze how motif usage changes across cell types, conditions, or
  species.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Interpreting Attention and Long-Range
Context}\label{interpreting-attention-and-long-range-context}

Transformer-based models (Chapters 8--11) use \textbf{self-attention} to
mix information across long genomic contexts, enabling them to capture
distal regulatory interactions and genomic organization.
Interpretability here often centers on \textbf{attention patterns} and
\textbf{long-range attribution}.

\subsection{Genomic Language Models and Operon Structure
(gLM)}\label{genomic-language-models-and-operon-structure-glm}

Genomic language models (gLMs) treat genes or genomic tokens as a
sequence and train transformers to predict masked tokens, analogous to
protein or text LMs. Work on gLMs trained on millions of metagenomic
scaffolds shows that these models learn non-trivial genomic structure:

\begin{itemize}
\tightlist
\item
  \textbf{Attention heads mark operons and co-regulated modules}

  \begin{itemize}
  \tightlist
  \item
    Certain heads specialize in connecting genes that are part of the
    same operon or functional module.\\
  \item
    Attention maps reveal networks of co-regulated genes, often aligning
    with known operon boundaries.
  \end{itemize}
\item
  \textbf{Functional semantics and taxonomic signals}

  \begin{itemize}
  \tightlist
  \item
    Latent representations cluster by enzymatic function and gene
    ontology.\\
  \item
    Attention patterns can separate clades and capture clade-specific
    gene neighborhoods.
  \end{itemize}
\item
  \textbf{Mechanistic interpretation}

  \begin{itemize}
  \tightlist
  \item
    These patterns suggest the model has inferred a ``syntax'' of gene
    neighborhoods: which genes tend to co-occur and in what order,
    conditioned on phylogenetic context.
  \end{itemize}
\end{itemize}

While attention is not universally a faithful explanation of model
decisions, attention analysis in gLM reveals \textbf{emergent
mechanistic structure} that is consistent with biological organization.

\subsection{Distal Regulatory Elements in Enformer-Like
Models}\label{distal-regulatory-elements-in-enformer-like-models}

Enformer and related models predict chromatin features and gene
expression from large genomic windows (e.g., 100 kb+) by combining
convolutional layers with transformer blocks.

Key interpretability questions:

\begin{itemize}
\tightlist
\item
  Which distal enhancers drive the predicted expression at a given
  transcription start site (TSS)?\\
\item
  How do variants in distal elements propagate to gene-level outputs?
\end{itemize}

Interpretability strategies include:

\begin{itemize}
\tightlist
\item
  \textbf{Gradient-based attributions over long windows}

  \begin{itemize}
  \tightlist
  \item
    Compute attributions of a gene's expression output with respect to
    input bases across the entire window.\\
  \item
    Visualize importance tracks to highlight putative enhancers and
    silencers.
  \end{itemize}
\item
  \textbf{Attention pattern analysis}

  \begin{itemize}
  \tightlist
  \item
    Identify attention heads that consistently link distal positions to
    TSS regions.\\
  \item
    Relate high-attention edges to Hi-C contact maps or chromatin
    interaction data.
  \end{itemize}
\item
  \textbf{In silico perturbation of regulatory elements}

  \begin{itemize}
  \tightlist
  \item
    Delete or scramble candidate enhancers and recompute gene expression
    predictions.\\
  \item
    Insert synthetic motifs or enhance motif scores to gauge
    dose--response relationships.
  \end{itemize}
\end{itemize}

These analyses can reveal \emph{candidate enhancer--promoter links} and
TF motifs that the model deems critical for gene regulation, helping
translate raw attention weights and attributions into mechanistic
hypotheses.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Global Regulatory Vocabularies: Sei Sequence
Classes}\label{global-regulatory-vocabularies-sei-sequence-classes}

Most motif-based interpretation operates at the \textbf{local} level.
Sei takes a complementary \textbf{global} approach by learning a
vocabulary of regulatory sequence \emph{classes} that summarize a vast
array of chromatin profiles.

\subsection{The Sei Framework}\label{the-sei-framework}

Sei trains a deep sequence model to predict \textbf{tens of thousands of
chromatin profiles} (TF binding, histone marks, accessibility) across
many cell types directly from DNA sequence. The key interpretability
step is to compress these thousands of outputs into \textbf{a few dozen
``sequence classes''}, each representing a characteristic regulatory
activity pattern:

\begin{itemize}
\tightlist
\item
  Promoter-like classes (e.g., H3K4me3-rich, TSS-proximal).\\
\item
  Enhancer-like classes (H3K27ac, H3K4me1).\\
\item
  Repressive classes (H3K27me3, H3K9me3).\\
\item
  Cell-type- or lineage-specific modules (e.g., neuronal, immune).
\end{itemize}

Each input sequence (or variant) is assigned a score for each sequence
class, effectively mapping it to a point in a \textbf{low-dimensional
``regulatory activity space''}.

\subsection{Interpretation and
Applications}\label{interpretation-and-applications}

A regulatory vocabulary like Sei's supports several interpretability
goals:

\begin{itemize}
\tightlist
\item
  \textbf{Intermediate, human-interpretable features}

  \begin{itemize}
  \tightlist
  \item
    Instead of raw high-dimensional outputs, one can reason in terms of
    ``promoter-like,'' ``B-cell enhancer,'' or ``polycomb-repressed''
    scores.
  \end{itemize}
\item
  \textbf{Variant interpretation}

  \begin{itemize}
  \tightlist
  \item
    Variants can be summarized by their shifts in sequence-class scores,
    yielding concise descriptions like ``increases neuronal enhancer
    activity while decreasing repressive marks.''
  \end{itemize}
\item
  \textbf{Trait and disease enrichment}

  \begin{itemize}
  \tightlist
  \item
    GWAS loci can be enriched for specific sequence classes, revealing
    tissues and regulatory programs most relevant to disease.
  \end{itemize}
\end{itemize}

This notion of a \textbf{regulatory vocabulary} parallels word
embeddings or topics in NLP and provides a bridge between highly
multivariate model outputs and mechanistically interpretable axes of
variation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Case Study: From Base-Pair Attributions to Regulatory
Grammar}\label{case-study-from-base-pair-attributions-to-regulatory-grammar}

Putting the pieces together, a typical mechanistic interpretability
pipeline for a CNN or transformer-based regulatory model might look
like:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Train a predictive model}

  \begin{itemize}
  \tightlist
  \item
    For example, predict chromatin accessibility or TF ChIP-seq tracks
    from sequence.
  \end{itemize}
\item
  \textbf{Compute base-level attributions}

  \begin{itemize}
  \tightlist
  \item
    Use DeepLIFT or IG for positive predictions in a target cell type.
  \end{itemize}
\item
  \textbf{Discover motifs with TF-MoDISco}

  \begin{itemize}
  \tightlist
  \item
    Extract seqlets from high-attribution regions, cluster, and derive
    motifs.\\
  \item
    Match motifs to known TFs and identify novel ones.
  \end{itemize}
\item
  \textbf{Infer grammar from motif instances}

  \begin{itemize}
  \tightlist
  \item
    Analyze motif co-occurrence, spacing, and orientation in
    high-scoring sequences.\\
  \item
    Use knock-in/knock-out \emph{in silico} experiments to confirm
    dependencies (e.g., both motifs needed, order matters).
  \end{itemize}
\item
  \textbf{Relate motifs to sequence classes or attention patterns}

  \begin{itemize}
  \tightlist
  \item
    Map motif-rich regions to Sei sequence classes or Enformer
    attributions.\\
  \item
    Connect local motif grammar to global regulatory context (e.g.,
    distal enhancer--promoter linkages, cell-type specificity).
  \end{itemize}
\item
  \textbf{Validate with experiments or external datasets}

  \begin{itemize}
  \tightlist
  \item
    Check whether motif disruptions align with reporter assay effects or
    allelic imbalance.\\
  \item
    Compare inferred enhancer--promoter links to Hi-C or CRISPR
    perturbation screens.
  \end{itemize}
\end{enumerate}

This integrated approach moves beyond ``pretty saliency maps'' toward
\textbf{testable hypotheses} about regulatory logic.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Evaluating Interpretations: Faithfulness vs
Plausibility}\label{evaluating-interpretations-faithfulness-vs-plausibility}

Not all explanations are equally trustworthy. Effective interpretability
work must grapple with the distinction between:

\begin{itemize}
\tightlist
\item
  \textbf{Plausibility}: Does the explanation ``look'' biological (e.g.,
  known motifs, enhancer marks)?\\
\item
  \textbf{Faithfulness}: Does the explanation accurately reflect the
  internal computation of the model?
\end{itemize}

Potential pitfalls:

\begin{itemize}
\tightlist
\item
  \textbf{Attention as explanation}

  \begin{itemize}
  \tightlist
  \item
    High attention weights need not correspond to large changes in
    output; they may reflect information routing rather than causal
    influence.\\
  \item
    Combining attention with attribution or perturbation analyses yields
    more reliable insights.
  \end{itemize}
\item
  \textbf{Attribution noise and saturation}

  \begin{itemize}
  \tightlist
  \item
    Gradient-based methods can produce noisy maps or miss important
    features in saturated regions.\\
  \item
    Use multiple methods (ISM, DeepLIFT, IG) and check for consistency.
  \end{itemize}
\item
  \textbf{Shortcut features}

  \begin{itemize}
  \tightlist
  \item
    Models may rely on dataset-specific artifacts (e.g., barcode k-mers,
    GC content) that produce clean motifs but are not mechanistically
    meaningful.
  \end{itemize}
\end{itemize}

Recommended practices:

\begin{itemize}
\tightlist
\item
  \textbf{Sanity checks}

  \begin{itemize}
  \tightlist
  \item
    Randomize model weights: attributions should degrade to noise.\\
  \item
    Randomize labels: derived motifs should disappear or lose predictive
    power.
  \end{itemize}
\item
  \textbf{Counterfactual tests}

  \begin{itemize}
  \tightlist
  \item
    Delete or scramble high-attribution regions and confirm that
    predictions drop accordingly.\\
  \item
    Insert discovered motifs into neutral backgrounds to test
    gain-of-function effects.
  \end{itemize}
\item
  \textbf{Benchmarking interpretability methods}

  \begin{itemize}
  \tightlist
  \item
    Use synthetic datasets with known ground-truth grammar.\\
  \item
    Compare methods on their ability to recover planted motifs and
    interactions.
  \end{itemize}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{A Practical Interpretability Toolbox for Genomic Foundation
Models}\label{a-practical-interpretability-toolbox-for-genomic-foundation-models}

For practitioners working with genomic foundation models (GFMs) and
their fine-tuned derivatives, a practical toolbox might include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Local effect estimation}

  \begin{itemize}
  \tightlist
  \item
    For variant effect prediction: use \textbf{ref/alt scoring} and
    small-window ISM around variants.\\
  \item
    Aggregate per-base attributions into per-variant or per-motif
    scores.
  \end{itemize}
\item
  \textbf{Motif and grammar discovery}

  \begin{itemize}
  \tightlist
  \item
    Compute base-level attributions for high-confidence predictions.\\
  \item
    Run TF-MoDISco or similar algorithms to build a motif vocabulary.\\
  \item
    Analyze motif grammars across tasks (e.g., multiple cell types or
    assays).
  \end{itemize}
\item
  \textbf{Global context visualization}

  \begin{itemize}
  \tightlist
  \item
    For transformer-based GFMs: inspect attention patterns to identify
    heads that track operons, gene neighborhoods, or enhancer--promoter
    loops.\\
  \item
    For models like Enformer: combine long-range attributions with
    contact maps to hypothesize regulatory architectures.
  \end{itemize}
\item
  \textbf{Regulatory vocabularies and embeddings}

  \begin{itemize}
  \tightlist
  \item
    Use frameworks like Sei to project sequences into a low-dimensional
    regulatory activity space.\\
  \item
    Cluster variants, enhancers, or genomic regions by their
    sequence-class profiles to reveal shared regulatory programs.
  \end{itemize}
\item
  \textbf{Model and dataset auditing}

  \begin{itemize}
  \tightlist
  \item
    Use interpretability tools to identify reliance on confounded or
    undesirable features.\\
  \item
    Cross-reference with Chapter 14's confounder taxonomy (ancestry
    stratification, batch effects) to design deconfounded training and
    evaluation.
  \end{itemize}
\item
  \textbf{Human-in-the-loop analysis}

  \begin{itemize}
  \tightlist
  \item
    Integrate motif and sequence-class outputs into visualization tools
    (e.g., genome browsers with attribution tracks, motif tracks, and
    class scores).\\
  \item
    Enable domain experts to iteratively refine hypotheses.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Outlook: From Explanations to Mechanistic
Models}\label{outlook-from-explanations-to-mechanistic-models}

Interpretability in genomic deep learning is evolving from \textbf{post
hoc explanation} toward \textbf{model-assisted mechanistic discovery}:

\begin{itemize}
\tightlist
\item
  Foundation models provide rich latent spaces and long-range context.\\
\item
  Attribution and motif discovery tools translate those representations
  into candidate regulatory grammars.\\
\item
  Global vocabularies like Sei's sequence classes offer interpretable
  axes spanning thousands of assays.\\
\item
  Attention analysis in genomic language models reveals emergent
  gene-level organization, hinting at scalable ways to capture
  systems-level biology.
\end{itemize}

The next frontier is to \textbf{close the loop}:

\begin{itemize}
\tightlist
\item
  Use insights from interpretability (motifs, grammars, sequence
  classes) to design better architectures and training objectives.\\
\item
  Feed experimentally validated grammars back into models as inductive
  biases.\\
\item
  Develop evaluation frameworks where success is measured not only by
  predictive accuracy but also by \textbf{mechanistic fidelity}---how
  well model-derived hypotheses align with the causal structure of
  regulatory biology.
\end{itemize}

In this sense, interpretability is not just a diagnostic for black-box
models. It is a central tool for turning genomic foundation models into
\textbf{engines of biological discovery}, capable of bridging the gap
between sequence-level predictions and the mechanistic understanding
that underpins robust clinical translation.

\part{Part V: Applications}

\chapter{Clinical Risk Prediction}\label{clinical-risk-prediction}

Modern genomic foundation models (GFMs) give us increasingly rich
representations of DNA, RNA, proteins, and multi-omic context (Parts
II--IV). The natural next question is: \textbf{how do we turn these
representations into actionable predictions for individual patients?}

This chapter focuses on \textbf{clinical risk prediction and decision
support}---models that estimate the probability, timing, or trajectory
of outcomes such as incident disease, progression, recurrence, or
adverse drug reactions. We emphasize how GFMs and related deep models:

\begin{itemize}
\tightlist
\item
  Extend \textbf{polygenic risk scores (PRS)} with richer sequence-based
  features and epistatic structure (e.g., Delphi, G2PT, MIFM)
  (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025; Rakowski
  and Lippert 2025).\\
\item
  Combine \textbf{genomic features with EHR and multi-omics} to produce
  holistic patient-level risk representations, building on systems from
  @sec-multiomics (e.g., GLUE, CpGPT, DeepRVAT) (Cao and Gao 2022; Camillo et
  al. 2024; Clarke et al. 2024).\\
\item
  Are wrapped in \textbf{evaluation, calibration, uncertainty, and
  fairness} pipelines suitable for high-stakes clinical decisions.
\end{itemize}

We end with case studies in \textbf{cardiometabolic risk},
\textbf{oncology risk and recurrence}, and \textbf{pharmacogenomics /
adverse drug reactions}, illustrating how GFMs move from bench to
bedside.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Problem Framing: What Is Clinical Risk
Prediction?}\label{problem-framing-what-is-clinical-risk-prediction}

Clinical risk prediction is the task of mapping \textbf{patient
data}---including genotypes, family history, clinical measurements,
imaging, and environmental factors---to \textbf{probabilistic statements
about future outcomes}. Concretely, a model might answer questions like:

\begin{itemize}
\tightlist
\item
  What is this patient's \textbf{10-year risk of coronary artery
  disease} if treated with standard of care?\\
\item
  Given current tumor characteristics and therapy, what is the
  \textbf{hazard of recurrence} within 2 years?\\
\item
  If we start this medication, what is the \textbf{probability of a
  severe adverse drug reaction (ADR)} in the next 6 months?
\end{itemize}

In practice, these tasks fall into a few archetypes:

\begin{itemize}
\tightlist
\item
  \textbf{Individual-level incident risk}

  \begin{itemize}
  \tightlist
  \item
    Will a currently disease-free individual develop disease
    \emph{within a specified time window} (e.g., 10-year type 2 diabetes
    risk)?\\
  \end{itemize}
\item
  \textbf{Progression and complication risk}

  \begin{itemize}
  \tightlist
  \item
    Among individuals with an existing condition, who will develop
    complications (e.g., nephropathy in diabetes, heart failure after
    myocardial infarction)?\\
  \end{itemize}
\item
  \textbf{Prognosis and survival}

  \begin{itemize}
  \tightlist
  \item
    Time-from-baseline to events such as death, recurrence, or
    transplant, often with censoring and competing risks.\\
  \end{itemize}
\item
  \textbf{Treatment response and toxicity}

  \begin{itemize}
  \tightlist
  \item
    Will a patient benefit from therapy A vs B, and what is their risk
    of severe toxicity or ADR?
  \end{itemize}
\end{itemize}

GFMs enter these problems as \textbf{feature generators}: they transform
raw genomic and multi-omic data into structured embeddings, variant
effect scores, or region-level functional annotations. These
representations then feed classic supervised learning
tasks---classification, regression, and survival modeling---alongside
clinical covariates.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Task Types and Loss
Functions}\label{task-types-and-loss-functions}

Although GFMs provide sophisticated inputs, the \textbf{prediction
tasks} themselves often re-use well-understood statistical frameworks.

\subsection{Binary and Multi-label
Classification}\label{binary-and-multi-label-classification}

For many screening or triage problems, risk prediction is posed as:

\begin{quote}
Will outcome \emph{Y} occur within horizon \emph{T}?
\end{quote}

Examples include incident atrial fibrillation within 5 years, or
hospitalization for heart failure in the next 12 months. Models output a
\textbf{risk score} ( \hat{p} = P(Y=1 \mid x) ), trained with
cross-entropy or focal losses.

Extensions:

\begin{itemize}
\tightlist
\item
  \textbf{Multi-label classification}: Predict multiple outcomes (e.g.,
  myocardial infarction, stroke, heart failure) simultaneously; share a
  common representation but separate output heads.\\
\item
  \textbf{Ordinal endpoints}: Disease stages or severity scores modeled
  with ordinal losses instead of strictly binary outcomes.
\end{itemize}

GFMs contribute by providing \textbf{richer genetic features} than
traditional hand-crafted burden scores or PRS (e.g., variant-level
embeddings from Nucleotide Transformer, GPN, or regulatory LMs)
(Dalla-Torre et al. 2023; Benegas, Batra, and Song 2023).

\subsection{Survival and Time-to-Event
Modeling}\label{survival-and-time-to-event-modeling}

Risk is often more naturally expressed as \textbf{time-to-event}:

\begin{itemize}
\tightlist
\item
  Time from baseline to myocardial infarction or revascularization.\\
\item
  Time from surgery to cancer recurrence.\\
\item
  Time from first exposure to drug to severe toxicity.
\end{itemize}

These require models that handle \textbf{censoring} (patients lost to
follow-up or event-free at study end). Approaches include:

\begin{itemize}
\tightlist
\item
  \textbf{Cox proportional hazards models} with genomic and GFM-derived
  features as covariates.\\
\item
  \textbf{Deep survival models} that use neural networks to parameterize
  hazard functions, survival curves, or discrete-time hazards.\\
\item
  \textbf{Competing risks models} for mutually exclusive outcomes (e.g.,
  cancer-specific vs non-cancer mortality).
\end{itemize}

GFMs naturally provide \textbf{high-dimensional, possibly non-linear
features}; deep survival architectures can exploit these features while
learning flexible hazard structures.

\subsection{Multi-task Risk and Shared
Representations}\label{multi-task-risk-and-shared-representations}

Large health systems increasingly estimate risk for \textbf{dozens of
outcomes} simultaneously (e.g., hospital readmission, multiple
cardiovascular endpoints, medication-specific ADRs). This motivates
\textbf{multi-task frameworks}:

\begin{itemize}
\tightlist
\item
  A shared encoder (combining EHR, genomic, and multi-omic encoders)
  produces a \textbf{patient-level embedding}.\\
\item
  Multiple output heads estimate risks for different endpoints or time
  horizons.
\end{itemize}

Such models can exploit \textbf{cross-task correlations} and share
statistical strength (e.g., overlapping genetic architectures between
lipids, CAD, and stroke). Deep polygenic architectures like Delphi and
G2PT already adopt multi-trait ideas for genomic risk representations
(Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From PRS to GFM-Enabled Risk
Scores}\label{from-prs-to-gfm-enabled-risk-scores}

Polygenic risk scores (PRS) are a natural starting point for genomically
informed clinical prediction.

\subsection{Classical PRS: Strengths and
Limitations}\label{classical-prs-strengths-and-limitations}

Traditional PRS typically:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use GWAS summary statistics to estimate per-variant weights.\\
\item
  Construct a score ( S = \sum\_j w\_j g\_j ), where ( g\_j ) is
  genotype at variant ( j ) and ( w\_j ) is its estimated effect size.\\
\item
  Plug PRS into regression or survival models alongside clinical
  covariates (age, sex, BMI, labs).
\end{enumerate}

Despite many successes, classical PRS have well-known limitations:

\begin{itemize}
\tightlist
\item
  \textbf{Limited modeling of epistasis and non-linearities}: Additive
  models struggle with higher-order interactions and context-dependent
  effects.\\
\item
  \textbf{Challenge in integrating functional priors}: Annotation-aware
  methods exist, but rarely leverage full GFMs.\\
\item
  \textbf{Portability gaps}: Performance often drops in
  under-represented ancestries due to LD structure and GWAS
  ascertainment.
\end{itemize}

These limitations motivate \textbf{deep learning-based PRS} that better
exploit structure in both genotype and functional annotation space.

\subsection{Deep Polygenic Risk: Delphi and
G2PT}\label{deep-polygenic-risk-delphi-and-g2pt}

Recent methods push beyond additive scores by using \textbf{deep
sequence and genotype models}:

\begin{itemize}
\tightlist
\item
  \textbf{Delphi}: A deep-learning method for polygenic risk prediction
  that jointly models variant-level features and higher-order patterns
  across the genome (Georgantas, Kutalik, and Richiardi 2024).

  \begin{itemize}
  \tightlist
  \item
    Can incorporate \textbf{variant annotations and linkage structure}
    more flexibly than linear PRS.\\
  \item
    Supports multi-phenotype prediction, effectively performing
    \textbf{task-conditioned PRS}.
  \end{itemize}
\item
  \textbf{G2PT (Genotype-to-Phenotype Transformer)}: A transformer-based
  architecture that treats an individual's genotype as a sequence of
  variant ``tokens'' and learns \textbf{polygenic risk representations}
  with attention-based context (Lee et al. 2025).

  \begin{itemize}
  \tightlist
  \item
    Naturally captures \textbf{epistatic interactions} via attention,
    not just additive effects.\\
  \item
    Emphasizes interpretability by tying attention patterns back to loci
    and pathways.
  \end{itemize}
\end{itemize}

Both systems can optionally use \textbf{GFM-derived variant features}
(e.g., scores from sequence-level LMs such as Nucleotide Transformer,
HyenaDNA, or GPN) (Dalla-Torre et al. 2023; Nguyen et al. 2023; Benegas,
Batra, and Song 2023). In this view:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A GFM maps variant and local sequence context to \textbf{variant
  effect features} (e.g., predicted impact on chromatin, expression,
  motifs).\\
\item
  A \textbf{polygenic risk model} (Delphi, G2PT, or related) aggregates
  these features across the genome to produce a \textbf{patient-level
  risk embedding}.\\
\item
  A \textbf{clinical head} uses this embedding plus EHR covariates to
  output risk for specific outcomes or time horizons.
\end{enumerate}

\subsection{Fine-mapping and Causal Variants:
MIFM}\label{fine-mapping-and-causal-variants-mifm}

Polygenic risk ultimately hinges on \textbf{causal variants}, not just
associated markers. MIFM (Multiple Instance Fine-Mapping) exemplifies
how deep sequence models can refine the link between variant effects and
risk:

\begin{itemize}
\tightlist
\item
  \textbf{MIFM} uses a deep sequence model in a
  \textbf{multiple-instance learning} framework to identify causal
  regulatory variants within associated loci (Rakowski and Lippert
  2025).\\
\item
  By modeling sets (bags) of variants per locus, it distinguishes
  \textbf{causal variants from passengers} in tight LD blocks.\\
\item
  The outputs---posterior probabilities or importance scores for
  candidate variants---can inform both mechanistic studies and more
  \textbf{parsimonious, interpretable PRS}.
\end{itemize}

Together, Delphi, G2PT, and MIFM illustrate a pattern that recurs
throughout this chapter:

\begin{quote}
Use GFMs and deep sequence models to \textbf{transform raw genotype into
rich, structured features}, then plug those features into
\textbf{prediction and decision-support architectures} that live closer
to the clinic.
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Beyond Genotype: Fusing GFMs with EHR and
Multi-omics}\label{beyond-genotype-fusing-gfms-with-ehr-and-multi-omics}

Clinical risk prediction rarely depends on genetics alone. Real-world
deployment typically requires \textbf{fusing genomic features with EHR,
imaging, and other omics}, mirroring the multi-omics integration
strategies from @sec-multiomics (Cao and Gao 2022; Camillo et al. 2024;
Clarke et al. 2024).

\subsection{Feature Sources}\label{feature-sources}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Genomics and regulatory features}

  \begin{itemize}
  \tightlist
  \item
    Zero-shot variant scores from DNA GFMs (e.g., Nucleotide
    Transformer, HyenaDNA, GPN, Grover) (Dalla-Torre et al. 2023; Nguyen
    et al. 2023; Benegas, Batra, and Song 2023).\\
  \item
    Coding variant scores from protein LMs (e.g., AlphaMissense-like
    systems; see earlier chapters).\\
  \item
    Fine-mapped causal variant probabilities from MIFM or similar
    (Rakowski and Lippert 2025).
  \end{itemize}
\item
  \textbf{Multi-omics and systems context}

  \begin{itemize}
  \tightlist
  \item
    Cell-type--resolved epigenomic and transcriptomic embeddings from
    GLUE/SCGLUE and CpGPT (Cao and Gao 2022; Camillo et al. 2024).\\
  \item
    Rare-variant burden and pathway-level representations from DeepRVAT
    and related models (Clarke et al. 2024).\\
  \item
    Tumor-level representations from models such as SetQuence/SetOmic or
    GNN-based cancer subtypers (Jurenaite et al. 2024; X. Li et al.
    2022; H. Li et al. 2024).
  \end{itemize}
\item
  \textbf{Clinical covariates and EHR}

  \begin{itemize}
  \tightlist
  \item
    Demographics, vitals, lab results, medication history.\\
  \item
    Problem lists, procedures, imaging-derived features.\\
  \item
    Time-varying trajectories of biomarkers (e.g., eGFR, HbA1c, tumor
    markers).
  \end{itemize}
\end{enumerate}

\subsection{Fusion Patterns}\label{fusion-patterns}

Architecturally, risk models usually adopt one of the fusion strategies
echoed from @sec-multiomics:

\begin{itemize}
\tightlist
\item
  \textbf{Early fusion}

  \begin{itemize}
  \tightlist
  \item
    Concatenate GFM-derived genomic embeddings with static clinical
    covariates and feed into a single MLP or survival model.\\
  \item
    Simple but sensitive to scaling, missingness, and modality
    imbalance.
  \end{itemize}
\item
  \textbf{Intermediate fusion}

  \begin{itemize}
  \tightlist
  \item
    Separate encoders for genomics, EHR, and multi-omics produce
    modality-specific embeddings.\\
  \item
    A fusion layer (attention, cross-modal transformer, or graph-based
    integration) combines them into a \textbf{patient embedding}, which
    downstream heads use for risk prediction.
  \end{itemize}
\item
  \textbf{Late fusion / ensembling}

  \begin{itemize}
  \tightlist
  \item
    Independent models per modality (e.g., a PRS-only model, an EHR-only
    model).\\
  \item
    Meta-model or decision rule combines predictions (e.g., ``treat if
    either PRS or EHR risk is high'').
  \end{itemize}
\end{itemize}

From a practical standpoint, \textbf{intermediate fusion} is often most
attractive: it allows modularity (swap encoders as GFMs improve) while
enabling cross-modal interactions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Evaluation: Discrimination, Calibration, and Clinical
Utility}\label{evaluation-discrimination-calibration-and-clinical-utility}

High performance on test sets is not enough for clinical deployment.
Risk models must be \textbf{discriminative, well-calibrated, robust, and
clinically useful}.

\subsection{Discrimination}\label{discrimination}

\textbf{Discrimination} measures how well the model ranks individuals by
risk:

\begin{itemize}
\tightlist
\item
  \textbf{AUROC (AUC)} for binary endpoints.\\
\item
  \textbf{AUPRC} when outcomes are rare (e.g., severe ADRs).\\
\item
  \textbf{C-index} and time-dependent AUC for survival tasks.
\end{itemize}

Strong discrimination is necessary but not sufficient; poorly calibrated
models can still achieve high AUROC.

\subsection{Calibration and Risk
Stratification}\label{calibration-and-risk-stratification}

\textbf{Calibration} asks whether predicted probabilities match observed
frequencies:

\begin{itemize}
\tightlist
\item
  If a group of patients is assigned \textasciitilde20\% risk of an
  event, do \textasciitilde20\% actually experience it?\\
\item
  Calibration is assessed with \textbf{calibration plots},
  \textbf{Hosmer--Lemeshow tests}, and \textbf{Brier scores}, often
  stratified by subgroups (e.g., ancestry, sex, age).
\end{itemize}

For PRS-informed models, calibration is especially important because:

\begin{itemize}
\tightlist
\item
  Raw PRS are often \textbf{centered and scaled} rather than calibrated;
  mapping PRS to absolute risk usually requires \textbf{post-hoc models}
  that incorporate baseline incidence and covariates.\\
\item
  GFMs can shift score distributions as architectures evolve;
  recalibration may be required when swapping or updating encoders.
\end{itemize}

\subsection{Uncertainty Estimation and ``When Not to
Predict''}\label{uncertainty-estimation-and-when-not-to-predict}

In high-stakes settings, models should know when they \textbf{do not
know}. Common strategies include:

\begin{itemize}
\tightlist
\item
  \textbf{Ensemble variance} or \textbf{Monte Carlo dropout} as
  uncertainty proxies.\\
\item
  \textbf{Conformal prediction} to output \textbf{risk intervals} or
  \textbf{prediction sets} with guaranteed coverage.\\
\item
  \textbf{Selective prediction / abstention}: allow models to
  \textbf{abstain} on cases where uncertainty is high or inputs are
  out-of-distribution (e.g., rare ancestries missing from training,
  novel tumor subtypes).
\end{itemize}

For GFM-based systems, uncertainty can be decomposed:

\begin{itemize}
\tightlist
\item
  \textbf{Genomic uncertainty}: confidence in variant effect predictions
  or fine-mapping (e.g., MIFM probabilities).\\
\item
  \textbf{Clinical uncertainty}: extrapolation to new care settings,
  practice patterns, or patient populations.
\end{itemize}

Communicating uncertainty transparently is a core part of decision
support.

\subsection{Fairness, Bias, and Health
Equity}\label{fairness-bias-and-health-equity}

Many genomic and EHR datasets reflect \textbf{historical and structural
inequities}. Risk models can \textbf{amplify} these biases if not
carefully evaluated.

Key considerations:

\begin{itemize}
\tightlist
\item
  \textbf{Ancestry and PRS portability}: Classical PRS underperform in
  under-represented ancestries due to GWAS design; GFM-based methods
  such as Delphi and G2PT have the opportunity---but not the
  guarantee---to improve this by leveraging functional priors and
  cross-ancestry information (Georgantas, Kutalik, and Richiardi 2024;
  Lee et al. 2025).\\
\item
  \textbf{Measurement and access bias}: EHR-derived features may differ
  systematically across groups (e.g., who gets genotyped, which labs are
  ordered).\\
\item
  \textbf{Group-wise calibration}: Evaluate calibration and
  discrimination separately by ancestry, sex, socio-economic proxies,
  and care site.\\
\item
  \textbf{Fairness metrics and constraints}: When necessary, enforce
  group-level constraints (e.g., equalized odds) or design
  \textbf{affirmative} models targeting historically disadvantaged
  groups.
\end{itemize}

Equity is not an afterthought; for GFMs, it should inform \textbf{what
data to pretrain on}, which \textbf{benchmarks} to report, and how to
deploy models in practice.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Prospective Validation, Trials, and
Regulation}\label{prospective-validation-trials-and-regulation}

Retrospective AUCs are not enough to justify clinical use. Clinical risk
models typically require:

\begin{itemize}
\tightlist
\item
  \textbf{Prospective validation}: Evaluate model performance in a
  temporally held-out cohort, ideally in multiple health systems with
  different population structures and practice patterns.\\
\item
  \textbf{Impact studies}: Measure whether using the model actually
  \textbf{changes clinician behavior} and \textbf{improves outcomes}
  (e.g., better statin targeting, fewer ADRs, reduced unnecessary
  imaging).\\
\item
  \textbf{Randomized or pragmatic trials} when models materially
  influence treatment decisions, to guard against hidden confounding in
  observational evaluations.
\end{itemize}

Regulatory landscapes (e.g., device approvals,
software-as-a-medical-device frameworks) increasingly recognize
\textbf{learning systems} and \textbf{continuous updates}. GFMs
complicate this further:

\begin{itemize}
\tightlist
\item
  A ``fixed'' risk model may rely on a GFM backbone that improves over
  time; updates may change risk rankings and calibration.\\
\item
  Regulatory strategies include \textbf{locked models with explicit
  versions}, \textbf{change control plans}, or \textbf{adaptive
  approvals} for constrained forms of continual learning.
\end{itemize}

Regardless of the framework, clear documentation of \textbf{data
provenance}, \textbf{GFM versions}, \textbf{training procedures}, and
\textbf{validation results} is essential.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Monitoring, Drift, and Continual
Learning}\label{monitoring-drift-and-continual-learning}

Once deployed, GFMs and downstream risk models operate in
\textbf{non-stationary environments}:

\begin{itemize}
\tightlist
\item
  Clinical practice patterns change (new treatments, guidelines).\\
\item
  Patient populations drift (e.g., new screening programs).\\
\item
  Lab assays and sequencing pipelines evolve.
\end{itemize}

Monitoring should track:

\begin{itemize}
\tightlist
\item
  \textbf{Input distributions} (e.g., genotype frequencies, EHR feature
  patterns).\\
\item
  \textbf{Output distributions} (risk score histograms, fraction of
  patients above decision thresholds).\\
\item
  \textbf{Performance over time} (calibration, discrimination), often
  via rolling windows or periodic audits.
\end{itemize}

When drift is detected:

\begin{itemize}
\tightlist
\item
  \textbf{Recalibration} may suffice (e.g., refitting a calibration
  layer to current data).\\
\item
  \textbf{Partial retraining} of heads or fusion layers can adapt to new
  environments while keeping GFM weights fixed.\\
\item
  \textbf{Full continual learning}---including updating GFM
  backbones---requires careful safeguards to avoid catastrophic
  forgetting and maintain regulatory compliance.
\end{itemize}

Design patterns from @sec-multiomics's systems models (e.g., modular
encoders, robust interfaces between GFMs and clinical layers) are
crucial for maintainable, updatable decision support.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Case Studies}\label{case-studies}

To make these ideas concrete, we outline three stylized case studies
that build on models and concepts from earlier chapters.

\subsection{Cardiometabolic Risk
Stratification}\label{cardiometabolic-risk-stratification}

\textbf{Goal:} Identify individuals at high risk of major adverse
cardiovascular events (MACE)---e.g., myocardial infarction, stroke,
cardiovascular death---over a 10-year horizon.

\textbf{Inputs:}

\begin{itemize}
\tightlist
\item
  \textbf{Genotype}: Biobank-scale genotyping or WGS data.\\
\item
  \textbf{GFM features}: Variant effect scores from DNA GFMs (Nucleotide
  Transformer, HyenaDNA, GPN) (Dalla-Torre et al. 2023; Nguyen et al.
  2023; Benegas, Batra, and Song 2023).\\
\item
  \textbf{Polygenic model}: Delphi or G2PT to produce a
  \textbf{polygenic risk embedding} for cardiometabolic traits
  (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025).\\
\item
  \textbf{Clinical data}: Age, sex, BMI, blood pressure, lipids,
  smoking, diabetes status, medications (e.g., statins,
  antihypertensives).
\end{itemize}

\textbf{Model design:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use a DNA GFM to compute variant-level annotations (e.g., predicted
  enhancer disruption in cardiomyocyte or hepatocyte contexts).\\
\item
  Feed annotations and genotypes into Delphi or G2PT to obtain a
  \textbf{patient-level genomics embedding} tuned for cardiometabolic
  outcomes.\\
\item
  Fuse the genomics embedding with EHR covariates via an intermediate
  fusion network (e.g., MLP or transformer over structured features).\\
\item
  Train the model to predict \textbf{10-year MACE risk} using survival
  or discrete-time hazard losses.
\end{enumerate}

\textbf{Clinical use:}

\begin{itemize}
\tightlist
\item
  Stratify patients into risk categories (e.g., low, intermediate, high)
  that inform \textbf{statin initiation}, \textbf{PCSK9 inhibitor
  consideration}, or \textbf{intensive lifestyle intervention}.\\
\item
  Provide \textbf{individual-level explanations}: highlight variants and
  pathways (via G2PT attention or Delphi variant contributions) that
  most contributed to risk---bridging Chapters 9 and 15.\\
\item
  Evaluate equity: ensure performance and calibration hold across
  ancestries and care sites.
\end{itemize}

\subsection{Oncology: Risk and Recurrence
Prediction}\label{oncology-risk-and-recurrence-prediction}

\textbf{Goal:} Predict recurrence risk and treatment benefit for
patients with solid tumors after surgery or first-line therapy.

\textbf{Inputs:}

\begin{itemize}
\tightlist
\item
  \textbf{Somatic landscapes} from whole-exome or whole-genome tumor
  sequencing.\\
\item
  \textbf{Tumor representations} from deep set or transformer
  architectures such as SetQuence/SetOmic (Jurenaite et al. 2024).\\
\item
  \textbf{Multi-omics}: tumor expression, methylation, and chromatin
  accessible from integrated frameworks (GLUE, CpGPT) (Cao and Gao 2022;
  Camillo et al. 2024).\\
\item
  \textbf{GNN-based subtyping}: embeddings or cluster assignments from
  cancer subtyping models like MoGCN and CGMega (X. Li et al. 2022; H.
  Li et al. 2024).\\
\item
  \textbf{Clinical features}: stage, grade, performance status,
  treatment regimen.
\end{itemize}

\textbf{Model design:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Encode somatic mutation sets with SetQuence/SetOmic to obtain
  \textbf{tumor-variant embeddings} (Jurenaite et al. 2024).\\
\item
  Integrate transcriptomic and epigenomic profiles via GLUE-like latent
  spaces and CpGPT methylation embeddings (Cao and Gao 2022; Camillo et
  al. 2024).\\
\item
  Combine these with \textbf{GNN-based subtype embeddings}
  (MoGCN/CGMega) to capture tumor--microenvironment and
  histopathological context (X. Li et al. 2022; H. Li et al. 2024).\\
\item
  Fuse tumor-level representations with clinical features in a
  \textbf{time-to-recurrence model} (e.g., flexible deep survival
  network).
\end{enumerate}

\textbf{Clinical use:}

\begin{itemize}
\tightlist
\item
  Provide risk estimates that guide \textbf{adjuvant therapy decisions}
  (e.g., intensifying chemotherapy or adding targeted agents for
  high-risk patients).\\
\item
  Suggest \textbf{candidate biomarkers} or pathways for trial
  stratification, based on GFM-derived importance scores and attention
  maps.\\
\item
  Monitor drift as treatment standards evolve; update models to reflect
  new targeted therapies and immune checkpoint inhibitors.
\end{itemize}

\subsection{Pharmacogenomics and Adverse Drug Reaction
Risk}\label{pharmacogenomics-and-adverse-drug-reaction-risk}

\textbf{Goal:} Predict which patients are at high risk of \textbf{severe
ADRs} (e.g., myopathy on statins, severe cutaneous reactions to certain
drugs, cardiotoxicity of oncology agents).

\textbf{Inputs:}

\begin{itemize}
\tightlist
\item
  \textbf{Germline variation} in pharmacogenes (e.g., CYP family, HLA
  alleles) and broader genome.\\
\item
  \textbf{Variant effect scores} from both DNA and protein LMs for
  coding and regulatory variants in drug metabolism and immune genes
  (see Chapters 2--3, 9--10).\\
\item
  \textbf{Clinical context}: co-medications, comorbidities, organ
  function (liver, kidney), prior adverse reactions.
\end{itemize}

\textbf{Model design:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use GFMs to derive \textbf{mechanistically meaningful features} for
  variants in pharmacogenes (e.g., predicted impact on protein
  stability, binding, or gene regulation).\\
\item
  Aggregate these features across loci into a \textbf{pharmacogenomic
  risk embedding}, possibly using a G2PT-style transformer restricted to
  relevant genes (Lee et al. 2025).\\
\item
  Combine this with EHR data in a multi-task classification model that
  predicts ADR risk for multiple drugs or drug classes.
\end{enumerate}

\textbf{Clinical use:}

\begin{itemize}
\tightlist
\item
  Flag patients at high risk \textbf{before initiating therapy},
  prompting genotype-guided drug choice or dose adjustment.\\
\item
  Generate reports that tie risk back to \textbf{specific variants and
  pharmacogenes}, aligned with existing clinical pharmacogenomics
  guidelines.\\
\item
  Evaluate performance across ancestries to avoid \textbf{exacerbating
  disparities} in access to safe and effective therapy.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Practical Design Patterns and
Outlook}\label{practical-design-patterns-and-outlook}

Across these examples, several design patterns for GFM-enabled clinical
prediction recur:

\begin{itemize}
\item
  Treat GFMs as \textbf{modular feature extractors}:

  \begin{itemize}
  \tightlist
  \item
    Keep a clear separation between \textbf{foundation encoders} and
    \textbf{clinical prediction heads}, easing updates and regulatory
    management.
  \end{itemize}
\item
  Embrace \textbf{multi-modal fusion}:

  \begin{itemize}
  \tightlist
  \item
    Combine genotype, multi-omics, and EHR, taking advantage of
    architectures discussed in Chapters 10 and 16 (Dalla-Torre et al.
    2023; Cao and Gao 2022).
  \end{itemize}
\item
  Prioritize \textbf{calibration, uncertainty, and fairness} as
  first-class citizens, not post-hoc add-ons.
\item
  Bridge \textbf{interpretability and mechanism}:

  \begin{itemize}
  \tightlist
  \item
    Use tools from Chapter 15 to connect individual risk predictions to
    variants, regions, and pathways, enabling mechanistic hypotheses and
    clinician trust.
  \end{itemize}
\item
  Design for \textbf{continual learning and monitoring}:

  \begin{itemize}
  \tightlist
  \item
    Assume that clinical practice and data distributions will change;
    build pipelines that can adapt responsibly.
  \end{itemize}
\end{itemize}

In the broader story of this book, clinical risk prediction and decision
support represent a key \textbf{translation layer}: they connect the
representational gains of genomic foundation models to the realities of
patient care. The next chapters will extend these ideas to other
application domains (e.g., rare disease diagnosis, discovery of
pathogenic variants, and drug/biotech innovation), further exploring how
GFMs reshape translational genomics.

\chapter{Pathogenic Variant
Discovery}\label{pathogenic-variant-discovery}

Clinical genetics ultimately cares about \textbf{specific variants and
genes}: which changes in a patient's genome plausibly explain their
phenotype, and which loci are compelling targets for follow-up in the
lab. The previous chapters focused on \textbf{foundation models for
variant effect prediction} (Chapter 13), \textbf{multi-omics
integration} (@sec-multiomics), and \textbf{clinical risk prediction}
(Chapter 17). This chapter shifts the emphasis from prediction to
\textbf{discovery workflows}.

The central question is:

\begin{quote}
Given a huge space of possible variants and genes, how can genomic
foundation models (GFMs) help us efficiently home in on those most
likely to be causal?
\end{quote}

We will treat ``pathogenic'' broadly---covering both \textbf{Mendelian
variants} with large effects and \textbf{complex trait variants} that
modulate risk more subtly. GFMs appear at multiple stages of these
pipelines:

\begin{itemize}
\tightlist
\item
  As \textbf{variant-level effect predictors} (e.g., AlphaMissense,
  GPN-MSA, Evo 2, AlphaGenome) that score coding and noncoding changes
  Cheng et al. (2023) Benegas, Albors, et al. (2024) Brixi et al. (2025)
  Z. Avsec, Latysheva, and Cheng (2025).\\
\item
  As \textbf{inputs or priors for fine-mapping} and rare variant
  association tests Wu et al. (2024) Rakowski and Lippert (2025) Clarke
  et al. (2024).\\
\item
  As \textbf{node features in gene and network models}, including graph
  neural networks (GNNs) over multi-omics and knowledge graphs Cao and
  Gao (2022) X. Li et al. (2022) H. Li et al. (2024) Chandak, Huang, and
  Zitnik (2023).\\
\item
  As guides for \textbf{CRISPR, MPRA, and other functional assays},
  closing the loop between in silico prediction and experimental
  validation Ž. Avsec et al. (2021) Linder et al. (2025).
\end{itemize}

We will walk through these roles from locus-level variant ranking, to
Mendelian disease diagnostics, to graph-based gene prioritization, and
finally to \textbf{closed-loop ``hypothesis factory'' workflows} that
blend GFMs with systematic perturbation experiments.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Variant Effect Prediction to
Prioritization}\label{from-variant-effect-prediction-to-prioritization}

Chapter 13 surveyed state-of-the-art \textbf{variant effect prediction
(VEP)} systems. Models such as AlphaMissense, GPN-MSA, Evo 2, and
AlphaGenome assign each variant a score reflecting predicted impact on
protein function, regulatory activity, or multi-omic phenotypes Cheng et
al. (2023) Benegas, Albors, et al. (2024) Brixi et al. (2025) Z. Avsec,
Latysheva, and Cheng (2025). In isolation, these scores are powerful but
\textbf{not yet a full prioritization pipeline}.

In practice, discovery workflows require several additional steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Contextualizing the score}\\
  A raw VEP score has different implications depending on:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Variant class} (missense, splice, promoter, enhancer, UTR,
    intronic).\\
  \item
    \textbf{Gene context} (constraint, tissue-specific expression,
    pathway membership).\\
  \item
    \textbf{Clinical or experimental question} (dominant Mendelian
    disease, recessive disease, modifier of complex trait).
  \end{itemize}

  For example, a moderately damaging missense variant in a
  \textbf{highly constrained gene} expressed in the relevant tissue may
  be more compelling than a strongly damaging variant in a gene with no
  supporting biology.
\item
  \textbf{Aggregation from variants to loci and genes}\\
  Discovery problems often operate at \textbf{locus or gene level},
  requiring some aggregation of variant scores. Common strategies
  include:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Max or top-k pooling} -- Focus on the worst predicted
    variant per gene or locus.\\
  \item
    \textbf{Burden-style aggregation} -- Sum or average the predicted
    impact of all rare variants in a gene, possibly weighted by allele
    frequency and predicted effect.\\
  \item
    \textbf{Mechanism-aware aggregation} -- Separate coding vs
    regulatory, or promoter vs distal enhancer contributions, using
    tissue-specific scores from models like Enformer or AlphaGenome Ž.
    Avsec et al. (2021) Z. Avsec, Latysheva, and Cheng (2025).
  \end{itemize}
\item
  \textbf{Combining VEP with orthogonal evidence}\\
  VEP is rarely used alone. Modern pipelines combine:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Population data} -- Allele frequency and constraint (pLI,
    LOEUF, missense and LoF intolerance).\\
  \item
    \textbf{Clinical databases} -- ClinVar classifications, disease-gene
    catalogs (OMIM, HGMD).\\
  \item
    \textbf{Functional annotations} -- Chromatin state, conservation
    (PhyloP, PhastCons), known regulatory elements.\\
  \item
    \textbf{Pathway and network context} -- Membership in pathways
    enriched for the trait, or centrality in relevant biological
    networks.
  \end{itemize}

  GFMs enter as \textbf{feature providers} in this stack, often
  replacing or augmenting hand-crafted features.
\item
  \textbf{Calibration and interpretability}\\
  For prioritization, ranking may matter more than perfectly calibrated
  probabilities, but \textbf{interpretable risk categories} are crucial
  in clinical and experimental settings. This pushes towards:

  \begin{itemize}
  \tightlist
  \item
    Score thresholds with empirical \textbf{positive predictive value
    (PPV)} estimates.\\
  \item
    Qualitative explanations (e.g., ``strong disruption of a conserved
    splice donor in a haploinsufficient gene'').\\
  \item
    Visualizations of attention maps, saliency, or motif-level
    contributions (Chapter 15).
  \end{itemize}
\end{enumerate}

In other words, GFMs provide \textbf{high-resolution local perturbation
scores}, but the art of discovery is in \textbf{wiring those scores into
larger decision frameworks}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Integrating VEP with GWAS, Fine-Mapping, and Burden
Tests}\label{integrating-vep-with-gwas-fine-mapping-and-burden-tests}

Genome-wide association studies (GWAS) identify \textbf{statistical
associations} between variants and traits. However, GWAS hits are often:

\begin{itemize}
\tightlist
\item
  \textbf{Noncoding} -- Located in enhancers or other regulatory
  elements.\\
\item
  \textbf{In linkage disequilibrium (LD)} -- Dozens of variants in a
  region share similar association statistics.\\
\item
  \textbf{Mechanistically opaque} -- Even the top GWAS SNP may not be
  truly causal.
\end{itemize}

\subsection{VEP as a prior for
fine-mapping}\label{vep-as-a-prior-for-fine-mapping}

Fine-mapping methods aim to assign each variant in a locus a
\textbf{posterior probability of causality}, usually by combining LD
patterns, effect-size estimates, and sometimes functional annotations Wu
et al. (2024). GFMs naturally provide \textbf{functional priors}:

\begin{itemize}
\tightlist
\item
  Regulatory sequence models such as Enformer and AlphaGenome predict
  how a variant perturbs gene expression or chromatin landscapes Ž.
  Avsec et al. (2021) Z. Avsec, Latysheva, and Cheng (2025).\\
\item
  Genome-scale LMs like GPN-MSA and Evo 2 estimate the likelihood or
  impact of nucleotide substitutions in their genomic context Benegas,
  Albors, et al. (2024) Brixi et al. (2025).\\
\item
  Specialized models like TREDNet and MIFM directly target
  \textbf{causal variant prediction} at GWAS loci Hudaiberdiev et al.
  (2023) Rakowski and Lippert (2025).
\end{itemize}

From a Bayesian perspective, these models provide a \textbf{functional
prior} ( \pi\_j ) for each variant ( j ) in the locus. Fine-mapping
frameworks can then:

\begin{itemize}
\tightlist
\item
  Upweight variants predicted to have large regulatory or coding
  effects.\\
\item
  Downweight variants with benign or neutral predictions.\\
\item
  Support \textbf{multi-variant configurations}, where multiple causal
  variants exist at the same locus.
\end{itemize}

Recent benchmarks like TraitGym systematically evaluate how well various
genomic LMs and VEP models serve as fine-mapping priors across traits
and tissues Benegas, Eraslan, and Song (2025).

\subsection{Rare variant association and DeepRVAT-style
models}\label{rare-variant-association-and-deeprvat-style-models}

For rare variants, single-variant tests have limited power. Instead,
\textbf{gene- or region-based burden tests} aggregate rare variants
across individuals to detect association. Here, VEP plays two key roles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Variant weighting and filtering}\\
  Classical burden tests often restrict to ``damaging'' variants using
  simple filters (e.g., predicted LoF, CADD \textgreater{} threshold).
  GFMs provide richer filters and weights, enabling:

  \begin{itemize}
  \tightlist
  \item
    Fine-grained distinctions among missense variants (e.g., using
    AlphaMissense scores Cheng et al. (2023)).\\
  \item
    Inclusion of regulatory variants predicted to modulate gene
    expression.\\
  \item
    Continuous weights reflecting predicted effect size, rather than
    binary include/exclude decisions.
  \end{itemize}
\item
  \textbf{End-to-end deep set models}\\
  DeepRVAT exemplifies a newer paradigm: instead of hand-engineered
  burden summaries, a \textbf{deep set network} ingests per-variant
  features (including GFM-derived VEP scores) and learns to aggregate
  them into a gene-level risk signal Clarke et al. (2024). This
  approach:

  \begin{itemize}
  \tightlist
  \item
    Supports \textbf{heterogeneous variant classes} within a gene.\\
  \item
    Learns flexible aggregation functions (e.g., non-additive
    interactions) while preserving permutation invariance.\\
  \item
    Accommodates multiple phenotypes and covariates within a single
    model.
  \end{itemize}
\end{enumerate}

As more cohorts with whole-exome or whole-genome sequencing become
available, these \textbf{GFM-enhanced burden frameworks} blur the line
between GWAS and rare variant analysis, providing a continuum of variant
discovery tools.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Mendelian Disease Gene and Variant
Discovery}\label{mendelian-disease-gene-and-variant-discovery}

In Mendelian disease genetics, the questions tend to be more concrete:
\textbf{Which variant explains this patient's phenotype? Which gene is
implicated?} WES/WGS of trios and families produces thousands of
variants per individual. The standard pipeline includes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Quality control and filtering}

  \begin{itemize}
  \tightlist
  \item
    Remove low-quality calls and technical artifacts.\\
  \item
    Filter by allele frequency (e.g., \textless0.1\% in population
    databases), inheritance mode (de novo, recessive, X-linked), and
    variant type (LoF, missense, splice, structural).
  \end{itemize}
\item
  \textbf{Gene-centric ranking}

  \begin{itemize}
  \tightlist
  \item
    Aggregate candidate variants per gene, using constraint metrics and
    known disease-gene catalogs.\\
  \item
    Integrate phenotype similarity (e.g., HPO-based matching between
    patient and known gene syndromes).
  \end{itemize}
\item
  \textbf{Manual curation}

  \begin{itemize}
  \tightlist
  \item
    Expert review of gene function, expression patterns, animal models,
    and literature.\\
  \item
    Assessment of segregation in the family, de novo status, and
    evidence of pathogenic mechanism.
  \end{itemize}
\end{enumerate}

\subsection{GFMs in Mendelian variant
prioritization}\label{gfms-in-mendelian-variant-prioritization}

GFMs reshape several stages of this process:

\begin{itemize}
\item
  \textbf{Richer coding impact scores}\\
  AlphaMissense provides proteome-wide missense pathogenicity estimates
  with continuous scores that often outperform traditional tools Cheng
  et al. (2023). Coding-aware foundation models (cdsFM and related
  systems) further capture codon-level context and co-evolutionary
  patterns Naghipourfar et al. (2024).
\item
  \textbf{Regulatory and splice prediction}\\
  Genome-wide models like GPN-MSA, Evo 2, and AlphaGenome estimate the
  effect of noncoding and splice-proximal variants, filling a gap for
  Mendelian variants outside exons Benegas, Albors, et al. (2024) Brixi
  et al. (2025) Z. Avsec, Latysheva, and Cheng (2025).
\item
  \textbf{Combined variant--gene scoring}\\
  For each gene, we can aggregate:

  \begin{itemize}
  \tightlist
  \item
    Max or weighted VEP score across all candidate variants.\\
  \item
    Separate tallies for LoF, missense, regulatory, and splice
    variants.\\
  \item
    Gene-level features (constraint, expression, pathways) and phenotype
    similarity.
  \end{itemize}

  A simple model might compute a \textbf{composite gene score} as a
  learned function of these features, trained on cohorts with labeled
  diagnoses.
\end{itemize}

\subsection{Rare disease association at
scale}\label{rare-disease-association-at-scale}

Beyond single-family diagnostics, large consortia collect \textbf{rare
disease cohorts} where the goal is to discover new gene--disease
associations. DeepRVAT-style models provide one blueprint:

\begin{itemize}
\tightlist
\item
  Represent each individual as a set of rare variants with
  \textbf{multi-dimensional VEP features} (from GFMs and traditional
  tools).\\
\item
  Use deep set networks to map from per-variant features to
  \textbf{individual-level phenotype predictions} or \textbf{gene-level
  association signals} Clarke et al. (2024).\\
\item
  Incorporate multi-omics context (e.g., tissue-specific expression,
  chromatin accessibility from GLUE-like models) as additional features
  Cao and Gao (2022).
\end{itemize}

This pushes Mendelian discovery closer to the \textbf{foundation model
paradigm}: instead of hand-designed burden statistics, we train flexible
architectures that learn how to combine variant-level representations
into gene- and phenotype-level insights.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Graph-Based Prioritization of Disease
Genes}\label{graph-based-prioritization-of-disease-genes}

Many discovery problems are inherently \textbf{network-structured}.
Genes interact through pathways, protein--protein interaction (PPI)
networks, co-expression modules, regulatory networks, and knowledge
graphs. GNNs offer a natural way to fuse:

\begin{itemize}
\tightlist
\item
  \textbf{Node features} from GFMs (e.g., aggregated VEP scores,
  expression profiles).\\
\item
  \textbf{Graph structure} capturing biological relationships.\\
\item
  \textbf{Labels} such as disease associations, essentiality, or cancer
  driver status.
\end{itemize}

\subsection{Multi-omics and cancer gene
modules}\label{multi-omics-and-cancer-gene-modules}

GLUE (and SCGLUE) frame multi-omics integration as a
\textbf{graph-linked embedding problem}, connecting cells and features
across modalities Cao and Gao (2022). Inspired by this, GNN frameworks
like MoGCN and CGMega build:

\begin{itemize}
\tightlist
\item
  \textbf{Gene-level graphs} combining expression, methylation, copy
  number, and other omics layers X. Li et al. (2022) H. Li et al.
  (2024).\\
\item
  \textbf{Attention mechanisms} to highlight important neighbors and
  pathways in cancer gene modules.\\
\item
  Predictive models for \textbf{cancer subtypes, driver genes, and
  prognostic signatures}.
\end{itemize}

GFMs can enhance these systems by supplying:

\begin{itemize}
\tightlist
\item
  \textbf{Variant-aware gene features} (e.g., aggregated predicted
  impact of observed somatic mutations).\\
\item
  \textbf{Regulatory context} via sequence-based predictions of
  expression and chromatin (Enformer, Borzoi, AlphaGenome) Ž. Avsec et
  al. (2021) Linder et al. (2025) Z. Avsec, Latysheva, and Cheng (2025).
\end{itemize}

\subsection{Knowledge graphs and essential gene
prediction}\label{knowledge-graphs-and-essential-gene-prediction}

Knowledge graphs like PrimeKG aggregate \textbf{heterogeneous biomedical
entities}---genes, diseases, drugs, pathways, and phenotypes---into a
unified relational structure Chandak, Huang, and Zitnik (2023). GNNs on
such graphs can be trained to:

\begin{itemize}
\tightlist
\item
  Prioritize \textbf{disease genes} based on graph proximity to known
  genes.\\
\item
  Suggest \textbf{drug repurposing candidates} by connecting genetic
  evidence to drug targets.\\
\item
  Discover \textbf{modules} linked to therapeutic response or adverse
  effects.
\end{itemize}

Bingo provides a related example, combining a \textbf{large language
model (LLM)} with GNNs to predict essential genes from protein-level
data Ma et al. (2023). In principle, the node features in such systems
could incorporate:

\begin{itemize}
\tightlist
\item
  Gene-level embeddings derived from protein LMs (Chapter 9).\\
\item
  Aggregated variant effect embeddings from genomic LMs (Chapter 10 and
  13).\\
\item
  Multi-omic signatures from GLUE-like integrative models Cao and Gao
  (2022).
\end{itemize}

Together, these approaches illustrate a broader trend: \textbf{GFMs
rarely act alone}. Instead, they supply dense, information-rich features
to graph-based models that reason over the \textbf{network context}
where disease mechanisms actually play out.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Experimental Follow-Up and Closed-Loop
Refinement}\label{experimental-follow-up-and-closed-loop-refinement}

Computational prioritization is only half of discovery. Ultimately, we
need \textbf{experimental validation}: does perturbing a candidate
variant or gene alter the relevant molecular or cellular phenotype?

\subsection{Designing CRISPR and MPRA experiments with
GFMs}\label{designing-crispr-and-mpra-experiments-with-gfms}

High-throughput perturbation assays such as:

\begin{itemize}
\tightlist
\item
  \textbf{Massively parallel reporter assays (MPRAs)} targeting many
  regulatory variants.\\
\item
  \textbf{CRISPR tiling and base editing screens} across enhancers,
  promoters, and coding regions.\\
\item
  \textbf{Perturb-seq} linking genetic perturbations to single-cell
  transcriptomes.
\end{itemize}

are expensive and capacity-limited. GFMs help \textbf{prioritize and
design} these experiments:

\begin{itemize}
\tightlist
\item
  Sequence-to-expression models like Enformer and Borzoi can identify
  \textbf{regions and variants with large predicted regulatory effects},
  guiding where to tile and which alleles to test Ž. Avsec et al. (2021)
  Linder et al. (2025).\\
\item
  Genome-scale generative models like Evo 2 can propose
  \textbf{counterfactual edits} that maximize predicted effect, enabling
  focused exploration of regulatory landscapes Brixi et al. (2025).\\
\item
  Variant effect models can suggest \textbf{multiplexed libraries} that
  systematically probe key motifs, splice sites, or codon usage
  patterns.
\end{itemize}

Instead of brute-force tiling every base pair, we can use GFMs to
\textbf{bias the library toward informative perturbations}, effectively
turning them into \textbf{experiment design engines}.

\subsection{Using functional data to retrain and recalibrate
models}\label{using-functional-data-to-retrain-and-recalibrate-models}

The feedback loop goes in the other direction as well. Functional
genomics screens produce \textbf{rich labeled datasets}:

\begin{itemize}
\tightlist
\item
  MPRA readouts of allele-specific regulatory activity.\\
\item
  CRISPR screen scores for gene essentiality or drug sensitivity.\\
\item
  Single-cell perturbation responses across cell states.
\end{itemize}

These can be used to:

\begin{itemize}
\tightlist
\item
  \textbf{Refine model heads} for specific tasks (e.g., fine-tune a GFM
  to predict MPRA outcomes in a particular cell type).\\
\item
  \textbf{Calibrate scores} so that predicted effect magnitudes align
  with measured changes.\\
\item
  \textbf{Discover failure modes}, such as motifs or chromatin contexts
  where current models systematically mispredict.
\end{itemize}

Some recent systems explicitly design \textbf{closed-loop pipelines},
where model predictions drive experiments, which then feed back to
improve the model and inform the next round of design Benegas, Eraslan,
and Song (2025) Rakowski and Lippert (2025). In the limit, we approach a
semi-automated \textbf{``hypothesis factory''}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start from GWAS, rare variant, or tumor sequencing data.\\
\item
  Use GFMs plus graphs to prioritize candidate variants and genes.\\
\item
  Design perturbation experiments guided by model predictions.\\
\item
  Update the models with new functional data.\\
\item
  Iterate, progressively sharpening our understanding of the underlying
  mechanisms.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Case Studies and Practical
Considerations}\label{case-studies-and-practical-considerations}

To ground these ideas, consider two representative application areas.

\subsection{Rare disease diagnosis pipelines leveraging VEP
scores}\label{rare-disease-diagnosis-pipelines-leveraging-vep-scores}

Modern rare disease centers increasingly adopt \textbf{GFM-enhanced
diagnostic workflows}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Variant filtering and annotation}

  \begin{itemize}
  \tightlist
  \item
    Standard QC and frequency filters.\\
  \item
    Annotation with GFM-based VEP scores (coding, regulatory, splice),
    constraint, and ClinVar evidence.
  \end{itemize}
\item
  \textbf{Gene-ranking model}

  \begin{itemize}
  \tightlist
  \item
    Per-gene aggregation of variant scores and features.\\
  \item
    A trained model that predicts the likelihood of each gene being
    causal, based on retrospective cohorts with known diagnoses.
  \end{itemize}
\item
  \textbf{Phenotype integration}

  \begin{itemize}
  \tightlist
  \item
    HPO-based similarity to known gene syndromes.\\
  \item
    Network-based propagation of phenotype associations using knowledge
    graphs like PrimeKG Chandak, Huang, and Zitnik (2023).
  \end{itemize}
\item
  \textbf{Expert review}

  \begin{itemize}
  \tightlist
  \item
    Geneticists and clinicians inspect the top-ranked genes and
    variants, cross-checking against patient phenotypes, family
    segregation, and literature.
  \end{itemize}
\end{enumerate}

Compared to traditional pipelines, the GFM-enhanced version tends to:

\begin{itemize}
\tightlist
\item
  \textbf{Surface non-obvious candidates}, such as noncoding or splice
  variants with strong predicted functional effects.\\
\item
  Provide \textbf{more nuanced prioritization} among multiple missense
  variants in the same gene.\\
\item
  Offer \textbf{richer mechanistic hypotheses} to guide follow-up
  experiments.
\end{itemize}

\subsection{Cancer driver mutation discovery (coding and
noncoding)}\label{cancer-driver-mutation-discovery-coding-and-noncoding}

In cancer genomics, the goal is to distinguish \textbf{driver mutations}
from a large background of passenger mutations. GFMs and graph-based
models contribute at multiple levels:

\begin{itemize}
\tightlist
\item
  \textbf{Variant-level scoring}

  \begin{itemize}
  \tightlist
  \item
    Use coding VEP (e.g., AlphaMissense, cdsFM-like models) for missense
    drivers Cheng et al. (2023) Naghipourfar et al. (2024).\\
  \item
    Use regulatory sequence models (Enformer, AlphaGenome, TREDNet) to
    evaluate noncoding mutations in promoters and enhancers Ž. Avsec et
    al. (2021) Z. Avsec, Latysheva, and Cheng (2025) Hudaiberdiev et al.
    (2023).
  \end{itemize}
\item
  \textbf{Gene- and module-level aggregation}

  \begin{itemize}
  \tightlist
  \item
    Aggregate somatic variants per gene, weighted by predicted
    functional impact.\\
  \item
    Apply GNNs such as MoGCN and CGMega to identify \textbf{driver gene
    modules} that are recurrently perturbed across patients X. Li et al.
    (2022) H. Li et al. (2024).\\
  \item
    Use set-based models (akin to DeepRVAT) to relate patient-specific
    variant sets to tumor subtypes or outcomes Clarke et al. (2024).
  \end{itemize}
\item
  \textbf{Functional follow-up}

  \begin{itemize}
  \tightlist
  \item
    Design focused CRISPR tiling screens around candidate regulatory
    elements, prioritized by GFMs.\\
  \item
    Validate predicted driver genes in cell line or organoid models,
    integrating transcriptional responses with multi-omic readouts
    (Chapter 16).
  \end{itemize}
\end{itemize}

These pipelines exemplify \textbf{multi-scale integration}: GFMs for
variant-level effects, GNNs for network-level reasoning, and
high-throughput perturbations for experimental validation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Outlook: Towards End-to-End Discovery
Systems}\label{outlook-towards-end-to-end-discovery-systems}

Biomedical discovery of pathogenic variants is moving from
\textbf{manual, hypothesis-driven workflows} toward \textbf{data- and
model-driven pipelines} where GFMs act as a central substrate:

\begin{itemize}
\tightlist
\item
  They turn raw sequence variation into \textbf{rich, context-aware
  variant embeddings}.\\
\item
  They provide \textbf{priors and features} for fine-mapping, rare
  variant association, and gene prioritization.\\
\item
  They guide the design of \textbf{targeted perturbation experiments},
  which in turn provide new data to refine the models.
\end{itemize}

At the same time, several challenges remain:

\begin{itemize}
\tightlist
\item
  \textbf{Robustness and generalization} across ancestries, tissues, and
  disease cohorts.\\
\item
  \textbf{Calibration and interpretability} suitable for clinical and
  experimental decision-making.\\
\item
  \textbf{Evaluation frameworks} (like TraitGym) that fairly compare
  models and reveal domain gaps Benegas, Eraslan, and Song (2025).\\
\item
  \textbf{Ethical and regulatory considerations} around automated
  variant classification and gene discovery in sensitive contexts.
\end{itemize}

In the next chapter, we zoom out to the broader \textbf{drug discovery
and biotech landscape} (Chapter 19), where many of these discovery
building blocks are embedded in industrial-scale pipelines that span
from \textbf{genetic association} to \textbf{target validation},
\textbf{biomarker discovery}, and eventually \textbf{clinical
translation}.

\chapter{Drug Discovery \& Biotech}\label{drug-discovery-biotech}

Genomic foundation models (GFMs) are built to turn raw sequence and
multi-omic data into \emph{reusable biological representations} and
\emph{fine-grained predictions} (Chapter 12). In previous chapters you
saw how these models improve variant effect prediction (Chapters 10, 11,
13), long-range regulatory modeling (Chapters 8, 11, 12), and disease
genetics workflows (Chapters 14--16).

This chapter zooms out to ask a more translational question:

\begin{quote}
How do genomic foundation models actually plug into \textbf{drug
discovery and biotech workflows}?
\end{quote}

Rather than walking step-by-step through a single therapeutic program,
this chapter offers a compact, high-level map of where GFMs are already
useful---or plausibly soon will be. The focus is on three broad roles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Target discovery and genetic validation}:\\
  Using human genetics, variant-level scores, and gene-level evidence to
  prioritize safer, more effective targets.
\item
  \textbf{Functional genomics and perturbation screens}:\\
  Designing, interpreting, and iteratively improving large-scale
  CRISPR/perturb-seq/MPRA screens with help from GFMs.
\item
  \textbf{Biomarkers, patient stratification, and biotech
  infrastructure}:\\
  Turning model outputs into biomarkers for trial design and integrating
  GFMs into the industrial MLOps stack.
\end{enumerate}

Throughout, the aim is not to promise ``end-to-end AI drug discovery,''
but to show pragmatic ways that genomic foundation models can
\textbf{reduce risk, prioritize hypotheses, and make experiments more
informative}, especially when coupled to high-quality human data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Where Genomics Touches the Drug Discovery
Pipeline}\label{where-genomics-touches-the-drug-discovery-pipeline}

The canonical small-molecule or biologics pipeline is often summarized
as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Target identification and validation}\\
\item
  \textbf{Hit finding and lead optimization}\\
\item
  \textbf{Preclinical characterization (safety, PK/PD, tox)}\\
\item
  \textbf{Clinical trials (Phase I--III) and post-marketing}
\end{enumerate}

Genomics most directly enters at three points:

\begin{itemize}
\tightlist
\item
  \textbf{Early-stage target discovery and validation}

  \begin{itemize}
  \tightlist
  \item
    Human genetic associations (GWAS, rare-variant burden, somatic
    mutation landscapes) point to potential targets.\\
  \item
    Variant-level effect predictions and gene-level constraint metrics
    help de-prioritize potentially unsafe or non-causal signals.
  \end{itemize}
\item
  \textbf{Biomarker discovery and patient stratification}

  \begin{itemize}
  \tightlist
  \item
    Genetic risk scores, regulatory embeddings, and multi-omic
    signatures define patient subgroups and endpoints for trials.\\
  \item
    Embeddings from GFMs make it easier to find \emph{molecularly
    coherent} patient strata beyond traditional clinical labels.
  \end{itemize}
\item
  \textbf{Mechanism-of-action (MoA) and resistance}

  \begin{itemize}
  \tightlist
  \item
    Functional genomics screens and perturbation assays help dissect how
    a compound perturbs cellular networks.\\
  \item
    GFMs can predict which perturbations matter and suggest follow-up
    experiments.
  \end{itemize}
\end{itemize}

Other AI-for-drug-discovery efforts focus on molecular design, docking,
or protein structure; those are largely beyond the scope of this book.
Here we stay close to the \textbf{DNA- and RNA-centric} capabilities
you've seen earlier: variant effect prediction, regulatory modeling, and
multi-omics integration.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Target Discovery and Genetic
Validation}\label{target-discovery-and-genetic-validation}

Human genetics provides some of the strongest evidence that modulating a
particular target can safely change disease risk. GFMs don't replace
classical statistical genetics, but they provide \emph{richer priors}
and \emph{more mechanistic features} for identifying and validating
targets.

\subsection{From variant-level scores to gene-level
targets}\label{from-variant-level-scores-to-gene-level-targets}

Variant effect prediction (VEP) models provide a natural starting point.
Earlier chapters introduced:

\begin{itemize}
\tightlist
\item
  \textbf{Genome-wide deleteriousness scores} such as CADD, which
  integrate diverse annotations and---more recently---deep and
  foundation-model features (Rentzsch et al. 2019; Schubach et al.
  2024).
\item
  \textbf{Protein-centric VEP GFMs}, including AlphaMissense, GPN-MSA,
  and AlphaGenome, which combine protein language models, structure, and
  long-range context to score coding variants (Cheng et al. 2023;
  Benegas, Albors, et al. 2024; Z. Avsec, Latysheva, and Cheng 2025;
  Brandes et al. 2023).
\item
  \textbf{Sequence-to-function models} such as Enformer and long-context
  DNA LMs (e.g., Nucleic Transformer, HyenaDNA), which predict
  regulatory outputs from large genomic windows (Ž. Avsec et al. 2021;
  He et al. 2023; Nguyen et al. 2023; Trop et al. 2024).
\end{itemize}

Drug target teams rarely care about \emph{individual variants} per se;
they care about \textbf{genes and pathways}. The key move is therefore
to aggregate variant-level information into \textbf{gene-level
evidence}:

\begin{itemize}
\tightlist
\item
  \textbf{Coding variant aggregation}

  \begin{itemize}
  \tightlist
  \item
    Summarize missense and predicted loss-of-function (pLoF) variants in
    each gene using VEP scores.\\
  \item
    Partition variants by predicted functional category (e.g.~likely
    loss-of-function vs.~benign missense) and by allele frequency.\\
  \item
    Derive gene-level metrics such as ``burden of predicted damaging
    variants in cases vs controls.''
  \end{itemize}
\item
  \textbf{Noncoding and regulatory evidence}

  \begin{itemize}
  \tightlist
  \item
    Aggregate variant effect predictions on enhancers, promoters, and
    splice sites that link (via chromatin interaction maps or models
    like Enformer) to a candidate gene (Ž. Avsec et al. 2021; He et al.
    2023).\\
  \item
    Use long-range GFMs to connect distal regulatory elements to target
    loci across 100 kb--1 Mb.
  \end{itemize}
\item
  \textbf{Constraint and intolerance}

  \begin{itemize}
  \tightlist
  \item
    Combine VEP-informed burden with gene constraint measures (as used
    implicitly in CADD and downstream tools) to identify genes that are
    highly intolerant to damaging variation (Rentzsch et al. 2019;
    Schubach et al. 2024).\\
  \item
    Extremely constrained genes may be risky targets
    (essentiality/toxicity), while ``dose-sensitive'' but not lethal
    genes may present more attractive opportunities.
  \end{itemize}
\end{itemize}

From a GFM perspective, the core idea is to \textbf{treat gene-level
evidence as an aggregation problem over high-dimensional variant
embeddings}. Instead of manually defining a handful of summary
statistics, teams can feed variant embeddings or predicted functional
profiles into downstream models that learn which patterns matter most
for disease.

\subsection{Linking genetic evidence to target safety and
efficacy}\label{linking-genetic-evidence-to-target-safety-and-efficacy}

Classical human genetics has established several now-standard heuristics
for target selection:

\begin{itemize}
\tightlist
\item
  \textbf{``Human knockout'' individuals} (carrying biallelic LoF
  variants) provide a natural experiment on what happens when a gene is
  effectively inactivated.\\
\item
  \textbf{Protective variants} that reduce disease risk suggest
  \emph{directionality} of effect (e.g.~partial inhibition of a protein
  is beneficial rather than harmful).\\
\item
  \textbf{Pleiotropy}---associations with many unrelated traits---may
  signal safety liabilities.
\end{itemize}

GFMs reinforce and extend these ideas by:

\begin{itemize}
\tightlist
\item
  \textbf{Improving causal variant identification}

  \begin{itemize}
  \tightlist
  \item
    Fine-mapping methods and multiple-instance models like MIFM can
    distinguish truly causal regulatory variants from correlated
    passengers (Wu et al. 2024; Rakowski and Lippert 2025).\\
  \item
    Combining these with regulatory GFMs tightens the map from GWAS
    locus → variant → target gene.
  \end{itemize}
\item
  \textbf{Refining effect direction and magnitude}

  \begin{itemize}
  \tightlist
  \item
    VEP scores from protein and regulatory GFMs can approximate effect
    sizes (e.g.~how ``severe'' a missense change is, or how strongly a
    regulatory variant alters expression) (Cheng et al. 2023; Benegas,
    Albors, et al. 2024; Z. Avsec, Latysheva, and Cheng 2025).\\
  \item
    This can help differentiate subtle modulators from catastrophic LoF.
  \end{itemize}
\item
  \textbf{Highlighting mechanism-enriched loci}

  \begin{itemize}
  \tightlist
  \item
    GFMs provide multi-task predictions (chromatin marks, TF binding,
    expression, splicing) that make it easier to interpret \emph{how} a
    risk locus affects biology (Ž. Avsec et al. 2021; Benegas, Ye, et
    al. 2024).
  \end{itemize}
\end{itemize}

In practice, a target discovery workflow might:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start from GWAS summary statistics or rare variant analyses.\\
\item
  Apply fine-mapping (e.g.~MIFM) to identify candidate causal variants
  (Wu et al. 2024; Rakowski and Lippert 2025).\\
\item
  Score candidate variants with VEP GFMs (both protein and
  regulatory).\\
\item
  Map variants to genes using long-range regulatory models (Enformer,
  Nucleic Transformer, HyenaDNA) (Ž. Avsec et al. 2021; He et al. 2023;
  Nguyen et al. 2023).\\
\item
  Aggregate signals into gene-level ``genetic support'' scores,
  incorporating constraint and pleiotropy information.
\end{enumerate}

The result is a \textbf{ranked list of candidate targets} with
structured evidence that can be compared across diseases and programs.

\subsection{Evolving from hand-curated to model-centric target
triage}\label{evolving-from-hand-curated-to-model-centric-target-triage}

Historically, target triage relied heavily on \emph{manual curation}:

\begin{itemize}
\tightlist
\item
  Experts would review GWAS hits, literature, and pathway diagrams.\\
\item
  Limited quantitative information was available for most genes,
  especially in non-classical pathways.
\end{itemize}

GFMs shift this towards a \textbf{model-centric, continuously updated
view}:

\begin{itemize}
\tightlist
\item
  New data (e.g.~biobank sequencing, single-cell atlases) can be fed
  through trained GFMs to update variant and gene evidence.\\
\item
  The same underlying model suite can support many disease programs,
  enabling consistent cross-portfolio comparisons.\\
\item
  Benchmark frameworks like TraitGym emphasize standardized evaluation
  of genotype-phenotype modeling, helping teams choose appropriate model
  stacks for a given trait (Benegas, Eraslan, and Song 2025).
\end{itemize}

The limiting factor becomes less ``do we have an annotation?'' and more
``can we interpret the model's representation and connect it to
biological plausibility and druggability?''---a theme echoed in Chapters
13 and 15.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Functional Genomics Screens in Drug
Discovery}\label{functional-genomics-screens-in-drug-discovery}

While human genetics offers \emph{observational} evidence, drug
discovery also relies heavily on \textbf{perturbation experiments}:

\begin{itemize}
\tightlist
\item
  CRISPR knockout/knockdown/activation screens.\\
\item
  Base-editing or saturation mutagenesis around key domains.\\
\item
  MPRA and massively parallel promoter/enhancer assays.\\
\item
  Perturb-seq and other high-throughput transcriptomic readouts.
\end{itemize}

Genomic foundation models are well positioned to both \textbf{design}
and \textbf{interpret} such screens.

\subsection{Designing smarter perturbation
libraries}\label{designing-smarter-perturbation-libraries}

Traditional pooled screens often rely on simple design rules (e.g.~one
sgRNA per exon, or tiling a region at fixed spacing). GFMs enable more
\emph{information-dense} designs:

\begin{itemize}
\tightlist
\item
  \textbf{Sequence-to-function priors}

  \begin{itemize}
  \tightlist
  \item
    Models like DeepSEA, Enformer, and related CNN/transformer
    architectures predict which bases are most functionally critical for
    regulatory outputs (J. Zhou and Troyanskaya 2015; Ž. Avsec et al.
    2021; Benegas, Ye, et al. 2024).\\
  \item
    Library design can focus perturbations on high-sensitivity
    sites---predicted TF motifs, splice junctions, or enhancer
    ``hotspots.''
  \end{itemize}
\item
  \textbf{Variant prioritization for saturation mutagenesis}

  \begin{itemize}
  \tightlist
  \item
    Protein and DNA GFMs can prioritize substitutions expected to span a
    wide range of predicted fitness, enabling better estimation of
    quantitative genotype--phenotype maps (Cheng et al. 2023; Marquet et
    al. 2024).\\
  \item
    This is especially useful for deep mutational scanning near active
    sites or in regulatory domains.
  \end{itemize}
\item
  \textbf{Off-target and safety considerations}

  \begin{itemize}
  \tightlist
  \item
    Sequence models can help filter sgRNA designs with high predicted
    off-target binding, or prioritize guide positions that minimize
    unintended regulatory disruption.
  \end{itemize}
\end{itemize}

The overarching idea is to \textbf{maximize the information gained per
experimental budget} by letting GFMs suggest \emph{where} to perturb in
sequence space.

\subsection{Interpreting screen readouts with
GFMs}\label{interpreting-screen-readouts-with-gfms}

Once a screen has been run, GFMs can assist in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Embedding perturbations and outcomes}

  \begin{itemize}
  \tightlist
  \item
    Encode each perturbed sequence (e.g.~enhancer variant, gene
    knockout) using a DNA or protein GFM, and represent each
    experimental condition as the combination of its embedding and
    observed phenotype (e.g.~expression profile).\\
  \item
    This enables manifold learning over perturbations, in which clusters
    correspond to shared mechanism-of-action.
  \end{itemize}
\item
  \textbf{Mapping hits back to pathways}

  \begin{itemize}
  \tightlist
  \item
    Combine GFMs with graph-based models over protein--protein
    interaction networks and regulatory networks to identify enriched
    pathways (Gao et al. 2023; Yuan and Duren 2025).\\
  \item
    Learned embeddings help propagate signal to weakly observed genes or
    variants.
  \end{itemize}
\item
  \textbf{Closing the loop with model retraining}

  \begin{itemize}
  \tightlist
  \item
    Use screen outcomes as labeled examples to fine-tune
    sequence-to-function models in the relevant cell type or context.\\
  \item
    This ``lab-in-the-loop'' refinement turns generic GFMs into highly
    tuned models for the cell system of interest.
  \end{itemize}
\end{itemize}

For example, an MPRA that assays thousands of enhancer variants can
yield sequence--activity pairs that dramatically improve
expression-prediction GFMs in that locus or tissue. Conversely, model
predictions can suggest follow-up experiments (additional variants, cell
types, or perturbation strengths) that would be maximally informative
given previous data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Biomarker Discovery, Patient Stratification, and Trial
Design}\label{biomarker-discovery-patient-stratification-and-trial-design}

Even when a target is well validated, many programs fail in late-stage
trials because \textbf{the right patients, endpoints, or biomarkers were
not selected}. GFMs, combined with large cohorts, offer new tools for
defining and validating biomarkers.

\subsection{From polygenic risk scores to GFM-informed
biomarkers}\label{from-polygenic-risk-scores-to-gfm-informed-biomarkers}

Classical \textbf{polygenic risk scores (PRS)} summarize the additive
effect of many common variants on disease risk. Deep learning methods
such as Delphi extend this idea by learning non-linear
genotype--phenotype mappings directly from genome-wide data (Georgantas,
Kutalik, and Richiardi 2024).

GFMs can enhance these approaches by:

\begin{itemize}
\tightlist
\item
  \textbf{Providing richer genetic features}

  \begin{itemize}
  \tightlist
  \item
    Instead of raw genotypes, models can use VEP-derived scores, variant
    embeddings, or gene-level features produced by GFMs.\\
  \item
    This can capture non-additive effects, regulatory architecture, and
    variant-level biology in a more compact representation.
  \end{itemize}
\item
  \textbf{Transferring knowledge across traits and ancestries}

  \begin{itemize}
  \tightlist
  \item
    Foundation models trained across diverse genomes (e.g.~Nucleotide
    Transformer, GENA-LM, HyenaDNA) provide features that may generalize
    more robustly across populations than trait-specific models
    (Dalla-Torre et al. 2023; Fishman et al. 2025; Nguyen et al.
    2023).\\
  \item
    Fine-mapping--aware approaches like MIFM further reduce dependence
    on linkage disequilibrium patterns (Wu et al. 2024; Rakowski and
    Lippert 2025).
  \end{itemize}
\item
  \textbf{Distinguishing risk and progression}

  \begin{itemize}
  \tightlist
  \item
    By integrating regulatory and expression predictions, risk models
    can differentiate genetic influences on \emph{disease onset} vs
    \emph{progression}, enabling more targeted enrichment strategies.
  \end{itemize}
\end{itemize}

In trial design, such models can be used to:

\begin{itemize}
\tightlist
\item
  Enrich for high-risk individuals (in prevention trials).\\
\item
  Define genetic subtypes that may respond differently to the same
  mechanism.\\
\item
  Construct composite biomarkers that mix genetics with conventional
  clinical features.
\end{itemize}

\subsection{Multi-omic and single-cell biomarker
discovery}\label{multi-omic-and-single-cell-biomarker-discovery}

Beyond DNA variation, drug development increasingly leverages
\textbf{multi-omic and single-cell readouts}:

\begin{itemize}
\tightlist
\item
  Whole-genome/exome tumor sequencing combined with expression,
  methylation, and copy-number profiling.\\
\item
  Single-cell multiome datasets (RNA + ATAC) that characterize
  cell-state landscapes in disease (Jurenaite et al. 2024; Yuan and
  Duren 2025).\\
\item
  Microbiome sequencing for host--microbe interplay and response to
  therapy (Yan et al. 2025).
\end{itemize}

GFMs and related architectures can help here in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Set-based and graph-based encoders}

  \begin{itemize}
  \tightlist
  \item
    Models like SetQuence/SetOmic treat heterogeneous genomic features
    for each tumor as a set, using deep set transformers to extract
    predictive representations (Jurenaite et al. 2024).\\
  \item
    GRN inference models such as LINGER leverage atlas-scale multiome
    data to infer regulatory networks that can serve as biomarkers of
    pathway activity (Yuan and Duren 2025).
  \end{itemize}
\item
  \textbf{Multi-scale integration}

  \begin{itemize}
  \tightlist
  \item
    DNA and RNA GFMs can be combined with graph neural networks over
    gene and protein networks to build end-to-end predictors that map
    from genotype + cell state to clinical endpoints (Gao et al. 2023;
    Benegas, Ye, et al. 2024).\\
  \item
    Embeddings from protein LMs (e.g.~ESM-2-based variant models)
    provide additional structure for coding variants (Brandes et al.
    2023; Marquet et al. 2024).
  \end{itemize}
\item
  \textbf{Biomarker discovery workflows}

  \begin{itemize}
  \tightlist
  \item
    Use GFMs to generate rich embeddings for patients (e.g.~from tumor
    genomes, germline variation, or multi-omic profiles).\\
  \item
    Cluster or perform supervised learning to identify molecular
    subgroups with differential prognosis or treatment response.\\
  \item
    Validate candidate biomarkers on held-out cohorts or external
    datasets before deploying them in a trial.
  \end{itemize}
\end{itemize}

The key shift is that biomarkers are no longer limited to a handful of
hand-picked variants or expression markers: they become
\textbf{functions over high-dimensional genomic and multi-omic
embeddings}, learned in a data-driven way yet grounded in biological
priors from GFMs.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Biotech Workflows and Infrastructure for
GFMs}\label{biotech-workflows-and-infrastructure-for-gfms}

For pharma and biotech organizations, the primary challenge is not ``can
we train a big model?'' so much as \textbf{``how do we integrate GFMs
into existing data platforms, governance, and decision-making?''}

\subsection{GFMs as shared
infrastructure}\label{gfms-as-shared-infrastructure}

In a mature organization, GFMs should be treated as \textbf{shared
infrastructure}, not ad hoc scripts:

\begin{itemize}
\tightlist
\item
  \textbf{Model catalog}

  \begin{itemize}
  \tightlist
  \item
    DNA LMs (e.g.~Nucleic Transformer, HyenaDNA, GENA-LM) (He et al.
    2023; Nguyen et al. 2023; Fishman et al. 2025).\\
  \item
    Sequence-to-function models (e.g.~Enformer, Genomic Interpreter) (Ž.
    Avsec et al. 2021; Z. Li et al. 2023).\\
  \item
    Variant effect predictors (AlphaMissense, GPN-MSA, AlphaGenome, CADD
    v1.7) (Rentzsch et al. 2019; Schubach et al. 2024; Cheng et al.
    2023; Benegas, Albors, et al. 2024; Z. Avsec, Latysheva, and Cheng
    2025).
  \end{itemize}
\item
  \textbf{Feature services}

  \begin{itemize}
  \tightlist
  \item
    Centralized APIs that take as input variants, genomic intervals, or
    genes and return embeddings, predicted functional profiles, or risk
    features.\\
  \item
    Logging and versioning so that analyses can be reproduced even as
    models and data evolve.
  \end{itemize}
\item
  \textbf{Data governance}

  \begin{itemize}
  \tightlist
  \item
    Clear separation between models trained on public data vs.~sensitive
    internal cohorts.\\
  \item
    Guardrails around where internal data can be used for fine-tuning
    and how resulting models can be shared.
  \end{itemize}
\end{itemize}

Embedding GFMs in this way allows multiple teams---target ID, biomarker
discovery, clinical genetics---to reuse the same core representations
rather than each building bespoke models.

\subsection{Build vs buy vs fine-tune}\label{build-vs-buy-vs-fine-tune}

Organizations face three strategic options:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Use external GFMs ``as-is''}

  \begin{itemize}
  \tightlist
  \item
    Pros: Low up-front cost; benefits from community benchmarking
    (e.g.~TraitGym for genotype--phenotype modeling (Benegas, Eraslan,
    and Song 2025)).\\
  \item
    Cons: May not capture organization-specific populations, assays, or
    traits.
  \end{itemize}
\item
  \textbf{Fine-tune open-source GFMs on internal data}

  \begin{itemize}
  \tightlist
  \item
    Pros: Retains powerful general representations while adapting to
    local distribution.\\
  \item
    Cons: Requires careful privacy controls and computational
    investment.
  \end{itemize}
\item
  \textbf{Train bespoke internal GFMs}

  \begin{itemize}
  \tightlist
  \item
    Pros: Maximum control; can align pretraining exactly with available
    data and target use cases.\\
  \item
    Cons: Expensive, complex MLOps; risk of overfitting to narrow
    datasets if not complemented by broader pretraining.
  \end{itemize}
\end{enumerate}

In practice, many groups adopt a \textbf{hybrid strategy}:

\begin{itemize}
\tightlist
\item
  Start with public GFMs for early exploration and non-sensitive
  tasks.\\
\item
  Gradually fine-tune on internal biobank or trial data when added value
  is clear.\\
\item
  Maintain lightweight model-serving infrastructure for
  latency-sensitive applications (e.g.~clinical decision support) and
  heavier offline systems for large-scale research workloads.
\end{itemize}

\subsection{IP, collaboration, and regulatory
considerations}\label{ip-collaboration-and-regulatory-considerations}

GFMs also raise new questions around:

\begin{itemize}
\tightlist
\item
  \textbf{Intellectual property}

  \begin{itemize}
  \tightlist
  \item
    Models trained on proprietary data can be valuable IP assets but are
    hard to patent directly.\\
  \item
    Downstream discoveries (targets, biomarkers) derived from GFMs must
    be carefully documented for freedom-to-operate.
  \end{itemize}
\item
  \textbf{Data sharing and federated approaches}

  \begin{itemize}
  \tightlist
  \item
    Joint training or evaluation across institutions may require
    federated learning or model-to-data paradigms, especially for
    patient-level data.
  \end{itemize}
\item
  \textbf{Regulatory expectations}

  \begin{itemize}
  \tightlist
  \item
    For biomarkers used in pivotal trials, regulators will expect
    transparent documentation of model training, validation, and
    performance across subgroups.\\
  \item
    Chapters 14 and 15 highlight confounding and interpretability
    challenges that become even more acute when models inform trial
    inclusion or primary endpoints.
  \end{itemize}
\end{itemize}

Overall, leveraging GFMs in biotech is as much an \textbf{organizational
and regulatory engineering problem} as a technical one.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Forward Look: Toward Lab-in-the-Loop
GFMs}\label{forward-look-toward-lab-in-the-loop-gfms}

A recurring theme across this book is moving from static models to
\textbf{closed loops} that integrate:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Foundational representation learning} on large unlabeled
  datasets (genomes, multi-omics).\\
\item
  \textbf{Task-specific supervision} (disease status, expression,
  variant effects).\\
\item
  \textbf{Experimental feedback} from perturbation assays, functional
  screens, and clinical trials.
\end{enumerate}

In the drug discovery context, this suggests an evolution toward
\textbf{lab-in-the-loop GFMs}:

\begin{itemize}
\tightlist
\item
  \textbf{Hypothesis generation}

  \begin{itemize}
  \tightlist
  \item
    GFMs identify promising targets, variants, and regulatory regions.\\
  \item
    Graph and set-based models suggest network-level interventions
    (Jurenaite et al. 2024; Gao et al. 2023; Yuan and Duren 2025).
  \end{itemize}
\item
  \textbf{Experiment design}

  \begin{itemize}
  \tightlist
  \item
    Models propose perturbation libraries (CRISPR, MPRA) that maximize
    expected information gain.\\
  \item
    Safety and off-target predictions help filter risky designs.
  \end{itemize}
\item
  \textbf{Evidence integration and model refinement}

  \begin{itemize}
  \tightlist
  \item
    Screen results feed back into GFMs, improving their local accuracy
    in disease-relevant regions of sequence space.\\
  \item
    Clinical trial outcomes update biomarker models and risk predictors
    for future trials.
  \end{itemize}
\item
  \textbf{Portfolio-level decision support}

  \begin{itemize}
  \tightlist
  \item
    Genetic and functional evidence from GFMs is combined with classical
    pharmacology to prioritize or deprioritize programs.\\
  \item
    Uncertainty estimates and model critique (Chapter 15) help avoid
    over-confidence in purely model-driven recommendations.
  \end{itemize}
\end{itemize}

Realizing this vision will require:

\begin{itemize}
\tightlist
\item
  Better \textbf{calibration and uncertainty quantification} in GFMs.\\
\item
  Stronger \textbf{causal reasoning} to distinguish correlation from
  intervention-worthiness.\\
\item
  Careful \textbf{ethical and equity considerations}, especially when
  models influence who gets access to trials or targeted therapies
  (Chapter 14).
\end{itemize}

Yet even in the near term, GFMs already offer tangible value in
\textbf{de-risking targets, enriching cohorts, and interpreting complex
functional data}. When combined with rigorous experimental design and
domain expertise, they can act not as oracle decision-makers, but as
\textbf{force multipliers for human scientists and clinicians}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In summary, this chapter has sketched how genomic foundation models
extend beyond academic benchmarks into \textbf{practical levers for drug
discovery and biotech}:

\begin{itemize}
\tightlist
\item
  Turning variant and regulatory predictions into \textbf{target
  discovery and validation pipelines}.\\
\item
  Designing and interpreting \textbf{functional genomics screens} that
  probe mechanism and vulnerability.\\
\item
  Building richer \textbf{biomarkers and patient stratification schemes}
  for trials.\\
\item
  Embedding GFMs into \textbf{industrial data platforms and MLOps}.
\end{itemize}

Subsequent chapters in Part V can zoom into specific application
domains---clinical risk prediction (Chapter 17) and pathogenic variant
discovery (Chapter 18)---using the conceptual toolkit laid out here.

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-avsec_enformer_2021}
Avsec, Žiga, Vikram Agarwal, D. Visentin, J. Ledsam, A.
Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet
Kohli, and David R. Kelley. 2021. {``{[}{Enformer}{]} {Effective} Gene
Expression Prediction from Sequence by Integrating Long-Range
Interactions.''} \emph{Nature Methods} 18 (October): 1196--1203.
\url{https://doi.org/10.1038/s41592-021-01252-x}.

\bibitem[\citeproctext]{ref-avsec_alphagenome_2025}
Avsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. {``{AlphaGenome}:
{AI} for Better Understanding the Genome.''} \emph{Google DeepMind}.
\url{https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/}.

\bibitem[\citeproctext]{ref-benegas_gpn-msa_2024}
Benegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.
Song. 2024. {``{GPN}-{MSA}: An Alignment-Based {DNA} Language Model for
Genome-Wide Variant Effect Prediction.''} \emph{bioRxiv}, April,
2023.10.10.561776. \url{https://doi.org/10.1101/2023.10.10.561776}.

\bibitem[\citeproctext]{ref-benegas_gpn_2023}
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023.
{``{[}{GPN}{]} {DNA} Language Models Are Powerful Predictors of
Genome-Wide Variant Effects.''} \emph{Proceedings of the National
Academy of Sciences} 120 (44): e2311219120.
\url{https://doi.org/10.1073/pnas.2311219120}.

\bibitem[\citeproctext]{ref-benegas_traitgym_2025}
Benegas, Gonzalo, Gökcen Eraslan, and Yun S. Song. 2025.
{``{[}{TraitGym}{]} {Benchmarking} {DNA} {Sequence} {Models} for
{Causal} {Regulatory} {Variant} {Prediction} in {Human} {Genetics}.''}
bioRxiv. \url{https://doi.org/10.1101/2025.02.11.637758}.

\bibitem[\citeproctext]{ref-benegas_genomic_2024}
Benegas, Gonzalo, Chengzhong Ye, Carlos Albors, Jianan Canal Li, and Yun
S. Song. 2024. {``Genomic {Language} {Models}: {Opportunities} and
{Challenges}.''} arXiv. \url{https://doi.org/10.48550/arXiv.2407.11435}.

\bibitem[\citeproctext]{ref-brandes_genome-wide_2023}
Brandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and
Vasilis Ntranos. 2023. {``Genome-Wide Prediction of Disease Variant
Effects with a Deep Protein Language Model.''} \emph{Nature Genetics} 55
(9): 1512--22. \url{https://doi.org/10.1038/s41588-023-01465-0}.

\bibitem[\citeproctext]{ref-brixi_evo_2025}
Brixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg
Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. {``{[}{Evo}
2{]} {Genome} Modeling and Design Across All Domains of Life with {Evo}
2.''} bioRxiv. \url{https://doi.org/10.1101/2025.02.18.638918}.

\bibitem[\citeproctext]{ref-camillo_cpgpt_2024}
Camillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T.
Higgins-Chen, Steve Horvath, and Bo Wang. 2024. {``{CpGPT}: A
{Foundation} {Model} for {DNA} {Methylation}.''} bioRxiv.
\url{https://doi.org/10.1101/2024.10.24.619766}.

\bibitem[\citeproctext]{ref-cao_glue_2022}
Cao, Zhi-Jie, and Ge Gao. 2022. {``{[}{GLUE}{]} {Multi}-Omics
Single-Cell Data Integration and Regulatory Inference with Graph-Linked
Embedding.''} \emph{Nature Biotechnology} 40 (10): 1458--66.
\url{https://doi.org/10.1038/s41587-022-01284-4}.

\bibitem[\citeproctext]{ref-chandak_primekg_2023}
Chandak, Payal, Kexin Huang, and Marinka Zitnik. 2023.
{``{[}{PrimeKG}{]} {Building} a Knowledge Graph to Enable Precision
Medicine.''} \emph{Scientific Data} 10 (1): 67.
\url{https://doi.org/10.1038/s41597-023-01960-3}.

\bibitem[\citeproctext]{ref-chen_deepsea_2022}
Chen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou.
2022. {``{[}{DeepSEA} {Sei}{]} {A} Sequence-Based Global Map of
Regulatory Activity for Deciphering Human Genetics.''} \emph{Nature
Genetics} 54 (7): 940--49.
\url{https://doi.org/10.1038/s41588-022-01102-2}.

\bibitem[\citeproctext]{ref-cheng_alphamissense_2023}
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, Akvilė Žemgulytė,
Taylor Applebaum, Alexander Pritzel, et al. 2023.
{``{[}{AlphaMissense}{]} {Accurate} Proteome-Wide Missense Variant
Effect Prediction with {AlphaMissense}.''} \emph{Science} 381 (6664):
eadg7492. \url{https://doi.org/10.1126/science.adg7492}.

\bibitem[\citeproctext]{ref-clarke_deeprvat_2024}
Clarke, Brian, Eva Holtkamp, Hakime Öztürk, Marcel Mück, Magnus
Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024.
{``{[}{DeepRVAT}{]} {Integration} of Variant Annotations Using Deep Set
Networks Boosts Rare Variant Association Testing.''} \emph{Nature
Genetics} 56 (10): 2271--80.
\url{https://doi.org/10.1038/s41588-024-01919-z}.

\bibitem[\citeproctext]{ref-cornman_glm2_2024}
Cornman, Andre, Jacob West-Roberts, Antonio Pedro Camargo, Simon Roux,
Martin Beracochea, Milot Mirdita, Sergey Ovchinnikov, and Yunha Hwang.
2024. {``{[}{gLM2}{]} {The} {OMG} Dataset: {An} {Open} {MetaGenomic}
Corpus for Mixed-Modality Genomic Language Modeling.''} bioRxiv.
\url{https://doi.org/10.1101/2024.08.14.607850}.

\bibitem[\citeproctext]{ref-dalla-torre_nucleotide_2023}
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez
Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago,
et al. 2023. {``Nucleotide {Transformer}: Building and Evaluating Robust
Foundation Models for Human Genomics.''} \emph{Nature Methods} 22 (2):
287--97. \url{https://doi.org/10.1038/s41592-024-02523-z}.

\bibitem[\citeproctext]{ref-fishman_gena-lm_2025}
Fishman, Veniamin, Yuri Kuratov, Aleksei Shmelev, Maxim Petrov, Dmitry
Penzar, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, and Mikhail
Burtsev. 2025. {``{GENA}-{LM}: A Family of Open-Source Foundational
{DNA} Language Models for Long Sequences.''} \emph{Nucleic Acids
Research} 53 (2): gkae1310. \url{https://doi.org/10.1093/nar/gkae1310}.

\bibitem[\citeproctext]{ref-gao_high-ppi_2023}
Gao, Ziqi, Chenran Jiang, Jiawen Zhang, Xiaosen Jiang, Lanqing Li,
Peilin Zhao, Huanming Yang, Yong Huang, and Jia Li. 2023.
{``{[}{HIGH}-{PPI}{]} {Hierarchical} Graph Learning for Protein--Protein
Interaction.''} \emph{Nature Communications} 14 (1): 1093.
\url{https://doi.org/10.1038/s41467-023-36736-1}.

\bibitem[\citeproctext]{ref-georgantas_delphi_2024}
Georgantas, Costa, Zoltán Kutalik, and Jonas Richiardi. 2024. {``Delphi:
{A} {Deep}-Learning {Method} for {Polygenic} {Risk} {Prediction}.''}
medRxiv. \url{https://doi.org/10.1101/2024.04.19.24306079}.

\bibitem[\citeproctext]{ref-he_nucleic_2023}
He, Shujun, Baizhen Gao, Rushant Sabnis, and Qing Sun. 2023. {``Nucleic
{Transformer}: {Classifying} {DNA} {Sequences} with {Self}-{Attention}
and {Convolutions}.''} \emph{ACS Synthetic Biology} 12 (11): 3205--14.
\url{https://doi.org/10.1021/acssynbio.3c00154}.

\bibitem[\citeproctext]{ref-hudaiberdiev_trednet_2023}
Hudaiberdiev, Sanjarbek, D. Leland Taylor, Wei Song, Narisu Narisu,
Redwan M. Bhuiyan, Henry J. Taylor, Xuming Tang, et al. 2023.
{``{[}{TREDNet}{]} {Modeling} Islet Enhancers Using Deep Learning
Identifies Candidate Causal Variants at Loci Associated with {T2D} and
Glycemic Traits.''} \emph{Proceedings of the National Academy of
Sciences} 120 (35): e2206612120.
\url{https://doi.org/10.1073/pnas.2206612120}.

\bibitem[\citeproctext]{ref-jaganathan_spliceai_2019}
Jaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F.
McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A.
Kosmicki, et al. 2019. {``{[}{SpliceAI}{]} {Predicting} {Splicing} from
{Primary} {Sequence} with {Deep} {Learning}.''} \emph{Cell} 176 (3):
535--548.e24. \url{https://doi.org/10.1016/j.cell.2018.12.015}.

\bibitem[\citeproctext]{ref-ji_dnabert_2021}
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021.
{``{DNABERT}: Pre-Trained {Bidirectional} {Encoder} {Representations}
from {Transformers} Model for {DNA}-Language in Genome.''}
\emph{Bioinformatics} 37 (15): 2112--20.
\url{https://doi.org/10.1093/bioinformatics/btab083}.

\bibitem[\citeproctext]{ref-jurenaite_setquence_2024}
Jurenaite, Neringa, Daniel León-Periñán, Veronika Donath, Sunna Torge,
and René Jäkel. 2024. {``{SetQuence} \& {SetOmic}: {Deep} Set
Transformers for Whole Genome and Exome Tumour Analysis.''}
\emph{BioSystems} 235 (January): 105095.
\url{https://doi.org/10.1016/j.biosystems.2023.105095}.

\bibitem[\citeproctext]{ref-kagda_data_2025}
Kagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A.
Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. {``Data Navigation on
the {ENCODE} Portal.''} \emph{Nature Communications} 16 (1): 9592.
\url{https://doi.org/10.1038/s41467-025-64343-9}.

\bibitem[\citeproctext]{ref-lee_g2pt_2025}
Lee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam,
Amit R. Majithia, and Trey Ideker. 2025. {``{[}{G2PT}{]} {A}
Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.''}
bioRxiv. \url{https://doi.org/10.1101/2024.10.23.619940}.

\bibitem[\citeproctext]{ref-li_cgmega_2024}
Li, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai,
et al. 2024. {``{CGMega}: Explainable Graph Neural Network Framework
with Attention Mechanisms for Cancer Gene Module Dissection.''}
\emph{Nature Communications} 15 (1): 5997.
\url{https://doi.org/10.1038/s41467-024-50426-6}.

\bibitem[\citeproctext]{ref-li_mogcn_2022}
Li, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and
Yunping Zhu. 2022. {``{MoGCN}: {A} {Multi}-{Omics} {Integration}
{Method} {Based} on {Graph} {Convolutional} {Network} for {Cancer}
{Subtype} {Analysis}.''} \emph{Frontiers in Genetics} 13 (February).
\url{https://doi.org/10.3389/fgene.2022.806842}.

\bibitem[\citeproctext]{ref-li_genomic_2023}
Li, Zehui, Akashaditya Das, William A. V. Beardall, Yiren Zhao, and
Guy-Bart Stan. 2023. {``Genomic {Interpreter}: {A} {Hierarchical}
{Genomic} {Deep} {Neural} {Network} with {1D} {Shifted} {Window}
{Transformer}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2306.05143}.

\bibitem[\citeproctext]{ref-lin_deepvariant_2022}
Lin, Yi-Lin, Pi-Chuan Chang, Ching Hsu, Miao-Zi Hung, Yin-Hsiu Chien,
Wuh-Liang Hwu, FeiPei Lai, and Ni-Chung Lee. 2022.
{``{[}{DeepVariant}{]} {Comparison} of {GATK} and {DeepVariant} by Trio
Sequencing.''} \emph{Scientific Reports} 12 (1): 1809.
\url{https://doi.org/10.1038/s41598-022-05833-4}.

\bibitem[\citeproctext]{ref-lin_esm-2_2022}
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting
Lu, Allan dos Santos Costa, et al. 2022. {``{[}{ESM}-2{]} {Language}
Models of Protein Sequences at the Scale of Evolution Enable Accurate
Structure Prediction.''} bioRxiv.
\url{https://doi.org/10.1101/2022.07.20.500902}.

\bibitem[\citeproctext]{ref-linder_borzoi_2025}
Linder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and
David R. Kelley. 2025. {``{[}{Borzoi}{]} {Predicting} {RNA}-Seq Coverage
from {DNA} Sequence as a Unifying Model of Gene Regulation.''}
\emph{Nature Genetics} 57 (4): 949--61.
\url{https://doi.org/10.1038/s41588-024-02053-6}.

\bibitem[\citeproctext]{ref-liu_life-code_2025}
Liu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang,
Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025.
{``Life-{Code}: {Central} {Dogma} {Modeling} with {Multi}-{Omics}
{Sequence} {Unification}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2502.07299}.

\bibitem[\citeproctext]{ref-ma_bingo-large_2023}
Ma, Jiani, Jiangning Song, Neil D. Young, Bill C. H. Chang, Pasi K.
Korhonen, Tulio L. Campos, Hui Liu, and Robin B. Gasser. 2023.
{``'{Bingo}'-a Large Language Model- and Graph Neural Network-Based
Workflow for the Prediction of Essential Genes from Protein Data.''}
\emph{Briefings in Bioinformatics} 25 (1): bbad472.
\url{https://doi.org/10.1093/bib/bbad472}.

\bibitem[\citeproctext]{ref-manzo_comparative_2025}
Manzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025.
{``Comparative {Analysis} of {Deep} {Learning} {Models} for {Predicting}
{Causative} {Regulatory} {Variants}.''} \emph{bioRxiv: The Preprint
Server for Biology}, June, 2025.05.19.654920.
\url{https://doi.org/10.1101/2025.05.19.654920}.

\bibitem[\citeproctext]{ref-marquet_vespag_2024}
Marquet, Céline, Julius Schlensok, Marina Abakarova, Burkhard Rost, and
Elodie Laine. 2024. {``{[}{VespaG}{]} {Expert}-Guided Protein Language
Models Enable Accurate and Blazingly Fast Fitness Prediction.''}
\emph{Bioinformatics} 40 (11): btae621.
\url{https://doi.org/10.1093/bioinformatics/btae621}.

\bibitem[\citeproctext]{ref-medvedev_biotoken_2025}
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill
Vishniakov, Prateek Munjal, Clément Christophe, Marco AF Pimentel,
Ronnie Rajan, and Shadab Khan. 2025. {``{BioToken} and {BioFM} --
{Biologically}-{Informed} {Tokenization} {Enables} {Accurate} and
{Efficient} {Genomic} {Foundation} {Models}.''} bioRxiv.
\url{https://doi.org/10.1101/2025.03.27.645711}.

\bibitem[\citeproctext]{ref-naghipourfar_cdsfm_2024}
Naghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B.
Macdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow
Coyote-Maestas, and Hani Goodarzi. 2024. {``{[}{cdsFM} -
{EnCodon}/{DeCodon}{]} {A} {Suite} of {Foundation} {Models} {Captures}
the {Contextual} {Interplay} {Between} {Codons}.''} bioRxiv.
\url{https://doi.org/10.1101/2024.10.10.617568}.

\bibitem[\citeproctext]{ref-nguyen_hyenadna_2023}
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum
Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. {``{HyenaDNA}:
{Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide}
{Resolution}.''} arXiv. \url{https://doi.org/10.48550/arXiv.2306.15794}.

\bibitem[\citeproctext]{ref-notin_proteingym_2023}
Notin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk,
Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023.
{``{ProteinGym}: {Large}-{Scale} {Benchmarks} for {Protein} {Fitness}
{Prediction} and {Design}.''} \emph{Advances in Neural Information
Processing Systems} 36 (December): 64331--79.
\url{https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html}.

\bibitem[\citeproctext]{ref-poplin_deepvariant_2018}
Poplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas
Colthurst, Alexander Ku, Dan Newburger, et al. 2018.
{``{[}{DeepVariant}{]} {A} Universal {SNP} and Small-Indel Variant
Caller Using Deep Neural Networks.''} \emph{Nature Biotechnology} 36
(10): 983--87. \url{https://doi.org/10.1038/nbt.4235}.

\bibitem[\citeproctext]{ref-rakowski_mifm_2025}
Rakowski, Alexander, and Christoph Lippert. 2025. {``{[}{MIFM}{]}
{Multiple} Instance Fine-Mapping: Predicting Causal Regulatory Variants
with a Deep Sequence Model.''} medRxiv.
\url{https://doi.org/10.1101/2025.06.13.25329551}.

\bibitem[\citeproctext]{ref-rentzsch_cadd_2019}
Rentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and
Martin Kircher. 2019. {``{CADD}: Predicting the Deleteriousness of
Variants Throughout the Human Genome.''} \emph{Nucleic Acids Research}
47 (D1): D886--94. \url{https://doi.org/10.1093/nar/gky1016}.

\bibitem[\citeproctext]{ref-rives_esm_2021}
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,
Jason Liu, Demi Guo, et al. 2021. {``{[}{ESM}-1b{]} {Biological}
Structure and Function Emerge from Scaling Unsupervised Learning to 250
Million Protein Sequences.''} \emph{Proceedings of the National Academy
of Sciences of the United States of America} 118 (15): e2016239118.
\url{https://doi.org/10.1073/pnas.2016239118}.

\bibitem[\citeproctext]{ref-sanabria_grover_2024}
Sanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch.
2024. {``{[}{GROVER}{]} {DNA} Language Model {GROVER} Learns Sequence
Context in the Human Genome.''} \emph{Nature Machine Intelligence} 6
(8): 911--23. \url{https://doi.org/10.1038/s42256-024-00872-0}.

\bibitem[\citeproctext]{ref-schiff_caduceus_2024}
Schiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and
Volodymyr Kuleshov. 2024. {``Caduceus: {Bi}-{Directional} {Equivariant}
{Long}-{Range} {DNA} {Sequence} {Modeling}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2403.03234}.

\bibitem[\citeproctext]{ref-schubach_cadd_2024}
Schubach, Max, Thorben Maass, Lusiné Nazaretyan, Sebastian Röner, and
Martin Kircher. 2024. {``{CADD} V1.7: Using Protein Language Models,
Regulatory {CNNs} and Other Nucleotide-Level Scores to Improve
Genome-Wide Variant Predictions.''} \emph{Nucleic Acids Research} 52
(D1): D1143--54. \url{https://doi.org/10.1093/nar/gkad989}.

\bibitem[\citeproctext]{ref-trop_genomics_2024}
Trop, Evan, Yair Schiff, Edgar Mariano Marroquin, Chia Hsiang Kao, Aaron
Gokaslan, McKinley Polen, Mingyi Shao, et al. 2024. {``The {Genomics}
{Long}-{Range} {Benchmark}: {Advancing} {DNA} {Language} {Models},''}
October. \url{https://openreview.net/forum?id=8O9HLDrmtq}.

\bibitem[\citeproctext]{ref-vishniakov_genomic_2024}
Vishniakov, Kirill, Karthik Viswanathan, Aleksandr Medvedev,
Praveenkumar Kanithi, Marco AF Pimentel, and Shadab Khan. 2024.
{``Genomic {Foundationless} {Models}: {Pretraining} {Does} {Not}
{Promise} {Performance},''} October.
\url{https://openreview.net/forum?id=kDZKEtDnT1}.

\bibitem[\citeproctext]{ref-wu_genome-wide_2024}
Wu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray,
Peter M. Visscher, and Jian Zeng. 2024. {``Genome-Wide Fine-Mapping
Improves Identification of Causal Variants.''} \emph{Research Square},
August, rs.3.rs--4759390.
\url{https://doi.org/10.21203/rs.3.rs-4759390/v1}.

\bibitem[\citeproctext]{ref-yan_recent_2025}
Yan, Binghao, Yunbi Nam, Lingyao Li, Rebecca A. Deek, Hongzhe Li, and
Siyuan Ma. 2025. {``Recent Advances in Deep Learning and Language Models
for Studying the Microbiome.''} \emph{Frontiers in Genetics} 15
(January). \url{https://doi.org/10.3389/fgene.2024.1494474}.

\bibitem[\citeproctext]{ref-yuan_linger_2025}
Yuan, Qiuyue, and Zhana Duren. 2025. {``{[}{LINGER}{]} {Inferring} Gene
Regulatory Networks from Single-Cell Multiome Data Using Atlas-Scale
External Data.''} \emph{Nature Biotechnology} 43 (2): 247--57.
\url{https://doi.org/10.1038/s41587-024-02182-7}.

\bibitem[\citeproctext]{ref-zhang_scientific_2024}
Zhang, Qiang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen
Zhang, Jing Yu, et al. 2024. {``Scientific {Large} {Language} {Models}:
{A} {Survey} on {Biological} \& {Chemical} {Domains}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2401.14656}.

\bibitem[\citeproctext]{ref-zheng_cistrome_2019}
Zheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei
Sun, Chen-Hao Chen, et al. 2019. {``Cistrome {Data} {Browser}: Expanded
Datasets and New Tools for Gene Regulatory Analysis.''} \emph{Nucleic
Acids Research} 47 (D1): D729--35.
\url{https://doi.org/10.1093/nar/gky1094}.

\bibitem[\citeproctext]{ref-zhou_expecto_2018}
Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K.
Wong, and Olga G. Troyanskaya. 2018. {``{[}{ExPecto}{]} {Deep} Learning
Sequence-Based Ab Initio Prediction of Variant Effects on Expression and
Disease Risk.''} \emph{Nature Genetics} 50 (8): 1171--79.
\url{https://doi.org/10.1038/s41588-018-0160-6}.

\bibitem[\citeproctext]{ref-zhou_deepsea_2015}
Zhou, Jian, and Olga G. Troyanskaya. 2015. {``{[}{DeepSEA}{]}
{Predicting} Effects of Noncoding Variants with Deep Learning--Based
Sequence Model.''} \emph{Nature Methods} 12 (10): 931--34.
\url{https://doi.org/10.1038/nmeth.3547}.

\bibitem[\citeproctext]{ref-zhou_dnabert-2_2024}
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and
Han Liu. 2024. {``{DNABERT}-2: {Efficient} {Foundation} {Model} and
{Benchmark} {For} {Multi}-{Species} {Genome}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2306.15006}.

\bibitem[\citeproctext]{ref-zvyagin_genslms_2022}
Zvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang,
Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. {``{GenSLMs}:
{Genome}-Scale Language Models Reveal {SARS}-{CoV}-2 Evolutionary
Dynamics.''} bioRxiv. \url{https://doi.org/10.1101/2022.10.10.511571}.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Deep Learning Primer for
Genomics}\label{deep-learning-primer-for-genomics}

This appendix gives a compact introduction to deep learning for readers
who are comfortable with genomics but less familiar with modern neural
networks. The goal is not to replace a full machine learning textbook,
but to provide enough background to make the models in Chapters 5--19
feel intuitive rather than magical.

We focus on:

\begin{itemize}
\tightlist
\item
  How deep models are structured (layers, parameters, activations)\\
\item
  How they are trained (loss functions, gradients, optimization)\\
\item
  Core architectures that appear throughout the book (CNNs,
  Transformers)\\
\item
  Concepts like self-supervised pretraining and transfer learning
\end{itemize}

Where possible, we connect directly to the genomic case studies in the
main text (DeepSEA, ExPecto, SpliceAI, Enformer, genomic language
models, and GFMs).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Linear Models to Deep
Networks}\label{from-linear-models-to-deep-networks}

\subsection{Models as Functions}\label{models-as-functions}

At its core, a predictive model is just a function:

\begin{equation}\phantomsection\label{eq-model}{
f_\theta: x \mapsto \hat{y}
}\end{equation}

where:

\begin{itemize}
\tightlist
\item
  \(x\) is an input (e.g., a one-hot encoded DNA sequence, variant-level
  features, or a patient feature vector).\\
\item
  \(\hat{y}\) is a prediction (e.g., probability of a histone mark, gene
  expression level, disease risk).\\
\item
  \(\theta\) are the parameters (weights) of the model.
\end{itemize}

In classical genomics workflows, \(f_\theta\) might be:

\begin{itemize}
\tightlist
\item
  \textbf{Logistic regression} (for case--control status)\\
\item
  \textbf{Linear regression} (for quantitative traits)\\
\item
  \textbf{Random forests} or \textbf{gradient boosting} (for variant
  pathogenicity scores)
\end{itemize}

Deep learning keeps the same basic structure but allows \(f_\theta\) to
be a much more flexible, high-capacity function built by composing many
simple operations.

\subsection{Linear Models vs Neural
Networks}\label{linear-models-vs-neural-networks}

A simple linear model for classification looks like:

\[
\hat{y} = \sigma(w^\top x + b),
\]

where \(w\) and \(b\) are parameters and \(\sigma(\cdot)\) is a
squashing nonlinearity (e.g., the logistic function). The model draws a
single separating hyperplane in feature space.

A \textbf{neural network} generalizes this by stacking multiple linear
transformations with nonlinear activation functions:

\[
\begin{aligned}
h_1 &= \phi(W_1 x + b_1) \\
h_2 &= \phi(W_2 h_1 + b_2) \\
&\vdots \\
\hat{y} &= g(W_L h_{L-1} + b_L)
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  Each \(W_\ell, b_\ell\) is a layer's weight matrix and bias.\\
\item
  \(\phi(\cdot)\) is a nonlinear activation (e.g., ReLU).\\
\item
  \(g(\cdot)\) is a final activation (e.g., sigmoid for probabilities,
  identity for regression).
\end{itemize}

The key idea:

\begin{quote}
By composing many simple nonlinear transformations, deep networks can
approximate very complex functions.
\end{quote}

In Chapters 5--7, DeepSEA, ExPecto, and SpliceAI implement exactly this
pattern, but with \textbf{convolutional} layers (Section 4) tailored to
1D DNA sequence instead of dense matrix multiplications (J. Zhou and
Troyanskaya 2015; J. Zhou et al. 2018; Jaganathan et al. 2019).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Training Deep Models}\label{training-deep-models}

\subsection{Data, Labels, and Loss
Functions}\label{data-labels-and-loss-functions}

To train a model, we need:

\begin{itemize}
\tightlist
\item
  A dataset of examples \(\{(x_i, y_i)\}_{i=1}^N\)\\
\item
  A model \(f_\theta\)\\
\item
  A \textbf{loss function} \(L(\hat{y}, y)\) that measures how wrong a
  prediction is
\end{itemize}

Common loss functions:

\begin{itemize}
\tightlist
\item
  \textbf{Binary cross-entropy} (for yes/no labels, e.g., ``is this
  ChIP--seq peak present?''):\\
  \[
  L(\hat{p}, y) = -\big(y \log \hat{p} + (1-y)\log(1-\hat{p})\big)
  \]
\item
  \textbf{Multiclass cross-entropy} (for one-of-K labels)\\
\item
  \textbf{Mean squared error (MSE)} (for continuous outputs, e.g., gene
  expression)
\end{itemize}

The \textbf{training objective} is to find \(\theta\) that minimizes the
average loss:

\[
\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N L\big(f_\theta(x_i), y_i\big).
\]

\subsection{2.2 Gradient-Based
Optimization}\label{gradient-based-optimization}

Deep networks may have millions to billions of parameters. We can't
search over all possibilities, but we can follow the gradient of the
loss with respect to \(\theta\):

\begin{itemize}
\tightlist
\item
  \textbf{Gradient descent} updates: \[
  \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta),
  \] where \(\eta\) is the learning rate.
\end{itemize}

In practice, we use:

\begin{itemize}
\tightlist
\item
  \textbf{Mini-batch stochastic gradient descent (SGD)}: Compute
  gradients on small batches of examples (e.g., 128 sequences at a time)
  for efficiency and better generalization.
\item
  \textbf{Adaptive optimizers} like Adam, which adjust learning rates
  per parameter.
\end{itemize}

You never compute gradients by hand; modern frameworks (PyTorch, JAX,
TensorFlow) use \textbf{automatic differentiation} to efficiently
compute \(\nabla_\theta \mathcal{L}\) even for very complex
architectures.

\subsection{Backpropagation in One
Sentence}\label{backpropagation-in-one-sentence}

\textbf{Backpropagation} is just the chain rule of calculus applied
efficiently through the layers of a network. It propagates ``blame''
from the output back to each weight, telling us how changing that weight
would change the loss.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Generalization, Overfitting, and
Evaluation}\label{generalization-overfitting-and-evaluation}

\subsection{Train / Validation / Test
Splits}\label{train-validation-test-splits}

Deep networks can memorize training data if we're not careful. To
evaluate generalization, we typically split data into:

\begin{itemize}
\tightlist
\item
  \textbf{Training set} -- used to fit parameters\\
\item
  \textbf{Validation set} -- used to tune hyperparameters (learning
  rate, depth, etc.) and perform early stopping\\
\item
  \textbf{Test set} -- held out until the end to estimate performance on
  new data
\end{itemize}

In genomics, \textbf{how we split} matters as much as \textbf{how much
data} we have:

\begin{itemize}
\tightlist
\item
  Splitting by \textbf{locus or chromosome} (to test cross-locus
  generalization)\\
\item
  Splitting by \textbf{individual or cohort} (to avoid leakage between
  related samples)\\
\item
  Splitting by \textbf{species or ancestry} when evaluating transfer
\end{itemize}

These issues are developed in more depth in the evaluation and
confounding chapters (Chapters 12 and 14).

\subsection{Overfitting and
Regularization}\label{overfitting-and-regularization}

Signs of overfitting:

\begin{itemize}
\tightlist
\item
  Training loss keeps decreasing, but validation loss starts
  increasing.\\
\item
  Metrics like AUROC or AUPRC plateau or drop on validation data even as
  they improve on training data.
\end{itemize}

Common regularization techniques:

\begin{itemize}
\tightlist
\item
  \textbf{Weight decay / L2 regularization} -- penalize large weights.\\
\item
  \textbf{Dropout} -- randomly zero out activations during training.\\
\item
  \textbf{Early stopping} -- stop training when validation performance
  stops improving.\\
\item
  \textbf{Data augmentation} -- generate more training examples by
  transforming inputs, e.g.:

  \begin{itemize}
  \tightlist
  \item
    Reverse-complement augmentation for DNA sequences (treat sequence
    and its reverse complement as equivalent).\\
  \item
    Window jittering: randomly shifting the sequence window around a
    target site.
  \end{itemize}
\end{itemize}

\subsection{Basic Metrics}\label{basic-metrics}

You'll encounter metrics such as:

\begin{itemize}
\tightlist
\item
  \textbf{AUROC (Area Under the ROC Curve)} -- how well the model ranks
  positives above negatives.\\
\item
  \textbf{AUPRC (Area Under the Precision--Recall Curve)} -- more
  informative when positives are rare.\\
\item
  \textbf{Calibration metrics} (e.g., Brier score) and reliability
  diagrams -- especially for clinical risk prediction (Chapter 17).
\end{itemize}

The model and application chapters provide details about which metrics
are appropriate for which tasks.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Convolutional Networks for Genomic
Sequences}\label{convolutional-networks-for-genomic-sequences}

Convolutional neural networks (CNNs) are the workhorse architecture in
early genomic deep learning models like DeepSEA, ExPecto, and SpliceAI
(J. Zhou and Troyanskaya 2015; J. Zhou et al. 2018; Jaganathan et al.
2019).

\subsection{1D Convolutions as Motif
Detectors}\label{d-convolutions-as-motif-detectors}

For a 1D DNA sequence encoded as a matrix
\(X \in \mathbb{R}^{L \times 4}\) (length \(L\), 4 nucleotides), a
\textbf{convolutional layer} applies a set of filters (kernels) of width
\(k\):

\begin{itemize}
\tightlist
\item
  Each filter is a small matrix \(K \in \mathbb{R}^{k \times 4}\).\\
\item
  At each position, the filter computes a dot product between \(K\) and
  the corresponding \(k\)-length chunk of \(X\).\\
\item
  Sliding the filter along the sequence creates an activation map that
  is high wherever the motif encoded by \(K\) is present.
\end{itemize}

Intuitively:

\begin{quote}
A 1D convolutional filter learns to recognize sequence motifs (e.g.,
transcription factor binding sites) directly from data.
\end{quote}

\subsection{Stacking Layers and Receptive
Fields}\label{stacking-layers-and-receptive-fields}

Deeper convolutional layers allow the model to ``see'' longer-range
patterns:

\begin{itemize}
\tightlist
\item
  \textbf{First layer}: short motifs (e.g., 8--15 bp).\\
\item
  \textbf{Higher layers}: combinations of motifs, motif spacing, and
  local regulatory grammar.\\
\item
  \textbf{Pooling layers} (e.g., max pooling) reduce spatial resolution
  while aggregating features, increasing the \textbf{receptive field}.
\end{itemize}

In DeepSEA, stacked convolutions and pooling allow the model to use
hundreds of base pairs of context around a locus to predict chromatin
state (J. Zhou and Troyanskaya 2015). ExPecto extends this idea by
mapping sequence to tissue-specific expression predictions (J. Zhou et
al. 2018). SpliceAI uses very deep dilated convolutions to reach
\textasciitilde10 kb of context for splicing (Jaganathan et al. 2019).

\subsection{Multi-Task Learning}\label{multi-task-learning-1}

Early sequence-to-function CNNs are almost always \textbf{multi-task}:

\begin{itemize}
\tightlist
\item
  A single input sequence is used to predict many outputs simultaneously
  (e.g., hundreds of TF ChIP--seq peaks, histone marks, DNase
  hypersensitivity tracks).\\
\item
  Shared convolutional layers learn \textbf{common features}, while the
  final layer has many output units (one per task).
\end{itemize}

Benefits:

\begin{itemize}
\tightlist
\item
  Efficient use of data and compute\\
\item
  Better regularization: related tasks constrain each other\\
\item
  Natural interface for variant effect prediction: you can see how a
  mutation affects many functional readouts at once
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Beyond CNNs: Recurrent Networks
(Briefly)}\label{beyond-cnns-recurrent-networks-briefly}

Before Transformers dominated sequence modeling, \textbf{recurrent
neural networks (RNNs)}---especially LSTMs and GRUs---were the default
architecture for language and time series.

Conceptually:

\begin{itemize}
\tightlist
\item
  An RNN processes a sequence one position at a time.\\
\item
  It maintains a hidden state that is updated as it moves along the
  sequence.\\
\item
  In principle, it can capture arbitrarily long-range dependencies.
\end{itemize}

In practice, for genomic sequences:

\begin{itemize}
\tightlist
\item
  Very long-range dependencies (tens to hundreds of kilobases) are
  difficult to learn with standard RNNs.\\
\item
  Training can be slow and unstable on very long sequences.\\
\item
  CNNs and attention-based models have largely displaced RNNs in genomic
  applications.
\end{itemize}

You may still see RNNs in some multi-modal or temporal settings (e.g.,
modeling longitudinal clinical data), but they are not central to this
book's architectures.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Transformers and
Self-Attention}\label{transformers-and-self-attention}

Transformers, introduced in natural language processing, have become the
dominant architecture for sequence modeling. In this book, they underpin
protein language models, DNA language models (DNABERT and successors),
and long-range models like Enformer (Ji et al. 2021; Ž. Avsec et al.
2021).

\subsection{The Idea of
Self-Attention}\label{the-idea-of-self-attention}

In a \textbf{self-attention} layer, each position in a sequence can
directly ``look at'' and combine information from every other position.

For an input sequence represented as vectors \(\{x_1, \dots, x_L\}\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each position is mapped to \textbf{query} (\(q_i\)), \textbf{key}
  (\(k_i\)), and \textbf{value} (\(v_i\)) vectors via learned linear
  projections.\\
\item
  The attention weight from position \(i\) to position \(j\) is:

  \[
  \alpha_{ij} \propto \exp\left(\frac{q_i^\top k_j}{\sqrt{d}}\right),
  \]

  followed by normalization so that \(\sum_j \alpha_{ij} = 1\).
\item
  The new representation of position \(i\) is a weighted sum of all
  value vectors:

  \[
  z_i = \sum_{j=1}^L \alpha_{ij} v_j.
  \]
\end{enumerate}

Key properties:

\begin{itemize}
\tightlist
\item
  \textbf{Content-based}: Interactions are determined by similarity of
  representations, not just distance.\\
\item
  \textbf{Global context}: Each position can, in principle, attend to
  any other position.\\
\item
  \textbf{Permutation-aware via positional encodings}: Additional
  information (sinusoidal or learned) encodes position so the model
  knows order.
\end{itemize}

\subsection{Multi-Head Attention and Transformer
Blocks}\label{multi-head-attention-and-transformer-blocks}

Real Transformer layers use \textbf{multi-head attention}:

\begin{itemize}
\tightlist
\item
  The model runs self-attention in parallel with multiple sets of
  \((Q,K,V)\) projections (heads).\\
\item
  Different heads can specialize in different patterns (e.g., local
  motif combinations, long-range enhancer--promoter contacts).
\end{itemize}

A typical Transformer block has:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multi-head self-attention\\
\item
  Add \& layer normalization\\
\item
  Position-wise feed-forward network\\
\item
  Another add \& layer normalization
\end{enumerate}

Stacking many blocks yields a deep Transformer.

\subsection{Computational Cost and Long-Range
Genomics}\label{computational-cost-and-long-range-genomics}

Naive self-attention has \(O(L^2)\) cost in sequence length \(L\). For
genomic sequences, where we might want 100 kb--1 Mb contexts, this is
expensive.

Long-range genomic models like Enformer and HyenaDNA address this with:

\begin{itemize}
\tightlist
\item
  \textbf{Hybrid designs} (CNNs + attention) to reduce sequence length
  before applying global attention (Ž. Avsec et al. 2021).\\
\item
  \textbf{Structured state space models (SSMs)} and related
  architectures that scale more gracefully with length (Nguyen et al.
  2023).
\end{itemize}

These details are treated in depth in the long-range modeling chapters;
here it suffices to know that Transformers give flexible global context
at the cost of higher computational complexity.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Self-Supervised Learning and
Pretraining}\label{self-supervised-learning-and-pretraining}

A central theme of this book is \textbf{pretraining}: training a large
model once on a broad, unlabeled or weakly-labeled task, then re-using
it for many downstream problems.

\subsection{Supervised vs
Self-Supervised}\label{supervised-vs-self-supervised}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised learning}: Each input \(x\) comes with a label
  \(y\). Examples:

  \begin{itemize}
  \tightlist
  \item
    Predicting chromatin marks from sequence (DeepSEA).\\
  \item
    Predicting splice junctions (SpliceAI).\\
  \item
    Predicting disease risk from features (Chapter 17).
  \end{itemize}
\item
  \textbf{Self-supervised learning}: The model learns from raw input
  data without explicit labels, using some \textbf{pretext task}
  constructed from the data itself. Examples:

  \begin{itemize}
  \tightlist
  \item
    Masked token prediction (BERT-style): hide some nucleotides and
    train the model to predict them from surrounding context.\\
  \item
    Next-token prediction (GPT-style): predict the next base given
    previous ones.\\
  \item
    Denoising or reconstruction tasks.
  \end{itemize}
\end{itemize}

In genomics, self-supervised models treat DNA sequences as a language
and learn from the vast amount of genomic sequence without needing
curated labels.

\subsection{Masked Language Modeling on
DNA}\label{masked-language-modeling-on-dna}

DNABERT applied BERT-style masked language modeling to DNA sequences
tokenized as overlapping k-mers (Ji et al. 2021). The model:

\begin{itemize}
\tightlist
\item
  Reads sequences as k-mer tokens.\\
\item
  Randomly masks a subset of tokens.\\
\item
  Learns to predict the masked tokens given surrounding context.
\end{itemize}

Benefits:

\begin{itemize}
\tightlist
\item
  Uses essentially unlimited unlabeled genomic data.\\
\item
  Learns rich representations that can be fine-tuned for tasks like
  promoter prediction, splice site detection, and variant effect
  prediction.
\end{itemize}

Chapter 10 generalizes this story to broader DNA foundation models,
including alternative tokenization schemes and architectures.

\subsection{Pretraining, Fine-Tuning, and
Probing}\label{pretraining-fine-tuning-and-probing}

After pretraining, we can use a model in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Fine-tuning}: Initialize with pretrained weights, then
  continue training on a specific downstream task with task-specific
  labels.\\
\item
  \textbf{Linear probing}: Freeze the pretrained model, extract
  embeddings, and train a simple linear classifier on top.\\
\item
  \textbf{Prompting / adapters}: Add small task-specific modules
  (adapters) while keeping most of the model fixed.
\end{itemize}

These patterns reappear across protein LMs, DNA LMs, variant effect
models, and GFMs in Chapters 9--16.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Foundations for Evaluation and
Reliability}\label{foundations-for-evaluation-and-reliability}

While the main book has dedicated chapters for evaluation (Chapter 12),
confounding (Chapter 14), and clinical metrics (Chapter 17), it's useful
to have a few basic concepts in mind.

\subsection{Distribution Shift}\label{distribution-shift}

A model is trained under some data distribution (e.g., certain assays,
cohorts, ancestries) and then deployed under another (e.g., a different
hospital system or population). When these differ, we have
\textbf{distribution shift}, which can degrade performance.

Typical genomic shifts include:

\begin{itemize}
\tightlist
\item
  New sequencing technologies or lab protocols\\
\item
  New ancestries or populations\\
\item
  New tissues, diseases, or phenotypes
\end{itemize}

\subsection{Data Leakage}\label{data-leakage}

\textbf{Data leakage} occurs when information from the test set
``leaks'' into training (e.g., through overlapping loci or related
individuals), leading to overly optimistic estimates of performance.
Chapters 12 and 14 discuss strategies for leak-resistant splits in
detail.

\subsection{Calibration and
Uncertainty}\label{calibration-and-uncertainty}

For many applications, especially in the clinic, we care not just about
whether the model is \emph{correct}, but whether its probabilities are
\textbf{well calibrated} and whether we know when the model is
uncertain. Calibration and uncertainty quantification are covered in
Chapter 17; here, the main takeaway is that \textbf{perfect AUROC does
not imply perfect clinical utility}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{A Minimal Recipe for a Genomic Deep Learning
Project}\label{a-minimal-recipe-for-a-genomic-deep-learning-project}

To make the abstractions more concrete, here is a lightweight ``recipe''
that roughly mirrors what the case-study chapters do.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Define the prediction problem}

  \begin{itemize}
  \tightlist
  \item
    Input: e.g., 1 kb sequence around a variant, or patient-level
    features.\\
  \item
    Output: e.g., presence of a chromatin mark, change in expression,
    disease risk.
  \end{itemize}
\item
  \textbf{Choose an input representation}

  \begin{itemize}
  \tightlist
  \item
    One-hot encoding or tokenization scheme for sequences (see Chapter
    8).\\
  \item
    Encodings for variants, genes, or patients (e.g., aggregate from
    per-variant features).
  \end{itemize}
\item
  \textbf{Pick a model family}

  \begin{itemize}
  \tightlist
  \item
    CNN for local sequence-to-function (Chapters 5--7).\\
  \item
    Transformer or SSM for long-range or language model-style tasks
    (Chapters 8--11).\\
  \item
    Pretrained GFM + small task-specific head (Chapters 12--16).
  \end{itemize}
\item
  \textbf{Specify the loss and metrics}

  \begin{itemize}
  \tightlist
  \item
    Cross-entropy for binary classification, MSE for regression, etc.\\
  \item
    Metrics like AUROC, AUPRC, correlation, calibration.
  \end{itemize}
\item
  \textbf{Set up data splits and evaluation}

  \begin{itemize}
  \tightlist
  \item
    Decide whether to split by locus, individual, cohort, or species.\\
  \item
    Hold out a test set and use validation data to tune hyperparameters.
  \end{itemize}
\item
  \textbf{Train with regularization and monitoring}

  \begin{itemize}
  \tightlist
  \item
    Use an optimizer (SGD or Adam-like) with a learning rate schedule.\\
  \item
    Apply regularization (dropout, weight decay, augmentation).\\
  \item
    Monitor training and validation curves for overfitting.
  \end{itemize}
\item
  \textbf{Inspect and stress-test}

  \begin{itemize}
  \tightlist
  \item
    Check performance across subgroups (e.g., ancestries, assays,
    cohorts).\\
  \item
    Use interpretability tools (Chapter 15) to see what patterns the
    model is using.\\
  \item
    Run robustness checks and ablations.
  \end{itemize}
\item
  \textbf{Iterate}

  \begin{itemize}
  \tightlist
  \item
    Adjust architecture, add more data, refine labels, or incorporate
    pretrained backbones.\\
  \item
    Move from model-centric tuning to system-level considerations (data
    quality, deployment environment, feedback loops).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{How This Primer Connects to the Rest of the
Book}\label{how-this-primer-connects-to-the-rest-of-the-book}

This appendix gives you the minimum vocabulary to navigate the rest of
the text:

\begin{itemize}
\tightlist
\item
  \textbf{Chapters 5--7} show how CNNs on one-hot sequence learn
  regulatory code, expression, and splicing.\\
\item
  \textbf{Chapters 8--11} extend these ideas to richer sequence
  representations, Transformers, and long-range sequence models.\\
\item
  \textbf{Chapters 12--16} frame these models as genomic foundation
  models, introduce evaluation, interpretability, and multi-omics.\\
\item
  \textbf{Chapters 17--19} show how these ingredients are assembled into
  clinical, discovery, and biotech applications.
\end{itemize}

You don't need to internalize every detail here. The goal is simply that
when you see terms like ``convolution,'' ``attention,'' ``pretraining,''
or ``fine-tuning'' in the main chapters, they feel like familiar tools
rather than mysterious jargon.




\end{document}
