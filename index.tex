% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{scrbook}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Genomic Foundation Models},
  pdfauthor={Josh Meehl},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Genomic Foundation Models}
\author{Josh Meehl}
\date{2025-12-01}
\begin{document}
\frontmatter
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Introduction}\label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

A single fertilized egg divides into trillions of cells sharing
essentially the same genome, yet these cells differentiate into over two
hundred distinct types, each with characteristic patterns of gene
expression, chromatin accessibility, and regulatory state. The
instructions for this differentiation are written in the genome itself:
in enhancers and silencers distributed across hundreds of megabases, in
splice sites that determine which exons join to form mature transcripts,
in three-dimensional chromatin contacts that bring distant regulatory
elements together. Reading these instructions requires understanding a
regulatory grammar that evolution wrote over billions of years but never
documented.

Classical computational approaches attacked this problem piecemeal. One
model predicted splice sites from local sequence context. Another
identified transcription factor binding motifs. A third scored variant
pathogenicity using evolutionary conservation. Each required
hand-crafted features, curated training sets, and careful validation
within a narrow domain. Insights rarely transferred: a model trained to
recognize promoters knew nothing about enhancers, and neither could
predict how a single nucleotide change might alter splicing. The result
was a fragmented landscape where each biological question demanded its
own specialized tool.

Foundation models represent a fundamentally different approach. By
training on vast corpora of genomic sequence with self-supervised
objectives, these models learn representations that capture regulatory
logic without explicit supervision on any particular task. The same
model that predicts masked nucleotides can, after minimal adaptation,
predict chromatin accessibility, identify splice sites, score variant
effects, and distinguish pathogenic mutations from benign polymorphisms.
This capacity for transfer learning suggests that foundation models have
learned something general about how genomes encode function.
Understanding what they have learned, how to deploy them effectively,
and where they still fail is the subject of this book.

\section*{Why Foundation Models for
Genomics?}\label{why-foundation-models-for-genomics}
\addcontentsline{toc}{section}{Why Foundation Models for Genomics?}

\markright{Why Foundation Models for Genomics?}

Traditional genomic modeling has been overwhelmingly task-specific. A
variant caller is tuned to distinguish sequencing errors from true
variants in a particular sequencing platform and sample type. A
supervised convolutional network predicts a fixed set of chromatin marks
for a specific cell line. A polygenic risk score is fit for one trait,
in one ancestry group, using data from one biobank. These models can
achieve excellent performance in the settings they were designed for,
but they often transfer poorly to new assays, tissues, ancestries, or
institutions. When the input distribution shifts, whether because of a
new sequencing chemistry, a different population, or a novel cell type,
performance degrades in ways that are difficult to anticipate.

Foundation models address this fragility through three interrelated
strategies. First, they leverage scale: training on massive,
heterogeneous datasets spanning multiple assays, tissues, species, and
cohorts forces the model to learn representations that capture shared
biological structure rather than dataset-specific artifacts. Second,
they employ self-supervised objectives that do not require manual
labels, allowing them to exploit the vast quantities of unlabeled
sequence data, perturbation screens, and population variation that
genomics generates. Third, they are designed for reusability: rather
than training a new model for each task, practitioners probe, adapt, or
fine-tune a shared backbone, amortizing the cost of representation
learning across many downstream applications.

The extent to which this paradigm delivers on its promises in genomics
remains an active research question. Some tasks benefit dramatically
from pretrained representations; others show marginal improvement over
strong classical baselines. Transfer across species, cell types, and
assays works better in some settings than others. The computational
costs of training and deploying large models create practical
constraints that vary across research and clinical environments. This
book does not assume that foundation models are the answer to every
genomic problem. It aims instead to equip readers with the frameworks to
evaluate when these approaches help, when simpler methods suffice, and
how to design analyses that exploit the strengths of modern
architectures while remaining alert to their limitations.

\section*{Recurring Themes}\label{recurring-themes}
\addcontentsline{toc}{section}{Recurring Themes}

\markright{Recurring Themes}

Several threads run through the book, and individual chapters can be
read as different perspectives on the same underlying questions.

The co-evolution of data and architecture is one such thread. Early
variant effect predictors relied on hand-engineered features and shallow
models trained on modest curated datasets. Convolutional networks
enabled direct learning of regulatory motifs and local grammar from raw
sequence, but their fixed receptive fields limited their reach.
Transformers and other long-context architectures opened the door to
capturing broader regulatory neighborhoods and chromatin structure.
Foundation models push toward representations that span multiple assays,
tissues, and organisms. At each stage, the question is not simply
whether the model is more sophisticated, but how the available data
constrain what the model can sensibly learn.

Scaling laws and emergent capabilities represent a related concern. As
models grow larger and train on more data, certain capabilities appear
discontinuously rather than gradually. The relationship between
parameters, training data, and compute follows predictable patterns that
inform practical decisions about model development. Understanding these
scaling dynamics helps practitioners decide when to train larger models,
when existing models suffice, and what capabilities to expect at
different scales.

Context length and genomic geometry form another recurring thread. Many
genomic phenomena are intrinsically non-local: enhancers regulate genes
across hundreds of kilobases, chromatin loops bring distal elements into
contact, and polygenic effects distribute risk across thousands of
variants genome-wide. The book returns repeatedly to how models
represent these long-range dependencies, what architectural choices
enable or constrain their reach, and what is gained or lost as context
windows scale.

The distinction between prediction and design cuts across multiple
chapters. Most current models are used as predictors: given a sequence
and context, what molecular or phenotypic outcome is expected? The same
models can also be embedded in design workflows, from variant
prioritization and library construction to therapeutic sequence
optimization. Foundation models change where the boundary lies between
analysis and experimental planning, and they introduce new failure modes
when generative or optimization objectives are misspecified.

Evaluation connects benchmark performance to real-world decisions.
Benchmark scores are seductive and easy to compare, but biological and
clinical decisions are messy, multi-objective, and constrained by data
drift, confounding, and poorly specified endpoints. A recurring theme is
the gap between state-of-the-art metrics on held-out test sets and
actual impact in research or clinical deployment. Careful evaluation,
confounder analysis, uncertainty quantification, and calibration can
narrow that gap, but only when practitioners understand what their
metrics actually measure.

Interpretability and mechanism form a final thread. The book treats
interpretability not as optional decoration but as a design constraint
that shapes how models should be built and evaluated. Saliency maps,
motif extraction, and mechanistic analyses can deepen understanding of
what a model has learned, but they can also provide false comfort when
applied to confounded or brittle representations. Distinguishing genuine
biological insight from pattern-matching artifacts requires both
technical tools and careful experimental design.

\section*{How the Book Is Organized}\label{how-the-book-is-organized}
\addcontentsline{toc}{section}{How the Book Is Organized}

\markright{How the Book Is Organized}

The book is organized into six parts containing twenty-nine chapters,
plus six appendices. Each part can be read on its own, but the parts are
designed to build on one another.

\textbf{Part I: Genomic Foundations} lays the genomic and statistical
groundwork that later models rest on. Chapter~\ref{sec-ngs} introduces
next-generation sequencing, alignment, and variant calling, highlighting
sources of error and the evolution from hand-crafted pipelines to
learned variant callers. Chapter~\ref{sec-data} surveys the core data
resources that underlie most modern work: reference genomes, population
variation catalogs, clinical variant databases, and functional genomics
consortia such as ENCODE and GTEx. Chapter~\ref{sec-gwas} reviews
genome-wide association studies, linkage disequilibrium, fine-mapping,
and polygenic scores, emphasizing what these variant-to-trait
associations do and do not tell us about mechanism.
Chapter~\ref{sec-vep-classical} covers conservation-based and
machine-learning-based variant effect predictors such as CADD, including
their feature sets, label construction, and issues of circularity and
dataset bias. Together, these chapters answer a foundational question:
what data and pre-deep-learning tools form the backdrop that any genomic
foundation model must respect, integrate with, or improve upon?

\textbf{Part II: Deep Learning for Sequences} introduces the conceptual
and technical foundations of modern sequence modeling.
Chapter~\ref{sec-representations} examines how genomic and protein
sequences are converted into model-compatible representations, covering
one-hot encodings, k-mers, byte-pair encodings, learned embeddings, and
position encodings, showing how these choices shape downstream model
behavior. Chapter~\ref{sec-cnn} examines convolutional approaches that
established the field of genomic deep learning, including DeepSEA,
Basset, and SpliceAI, analyzing what they learn about motifs and
regulatory grammar and where their fixed receptive fields impose
limitations that motivate attention-based architectures.
Chapter~\ref{sec-attention} provides a detailed treatment of attention
mechanisms, position encodings, and transformer architectures, with
emphasis on how these ideas translate from language to biological
sequence. Chapter~\ref{sec-pretraining} covers pretraining objectives,
from masked language modeling and next-token prediction to contrastive
and generative approaches, examining how self-supervision extracts
structure from unlabeled biological data. Chapter~\ref{sec-transfer}
addresses transfer learning, domain adaptation, and few-shot learning,
asking when and how pretrained representations generalize to new tasks,
species, and data modalities.

\textbf{Part III: Foundation Models for Biology} surveys the major
foundation model families, organized by modality, and establishes
variant effect prediction as the integrating application.
Chapter~\ref{sec-fm-principles} develops a working definition and
taxonomy of foundation models in genomics, distinguishing them from
earlier supervised approaches and examining scaling laws that
characterize how model capabilities change with size and data.
Chapter~\ref{sec-dna-lm} covers DNA language models such as DNABERT,
Nucleotide Transformer, HyenaDNA, and Evo, tracing their training
corpora, objectives, evaluation suites, and current capabilities.
Chapter~\ref{sec-protein-lm} describes large protein language models
trained on evolutionary sequence databases, their emergent structure and
function representations, and applications to structure prediction and
design. Chapter~\ref{sec-regulatory} covers hybrid CNN-transformer and
related architectures designed for long genomic contexts, such as
Enformer and Borzoi, which predict regulatory readouts over tens to
hundreds of kilobases. Chapter~\ref{sec-vep-fm} serves as a capstone
that integrates these model families, examining how protein-based
approaches such as AlphaMissense and DNA-based approaches such as
splicing and regulatory models combine to address variant effect
prediction, the central interpretive challenge that motivates the field.

\textbf{Part IV: Multi-Scale and Systems Modeling} examines how
foundation model principles extend beyond one-dimensional sequence to
embrace cellular and systems-level biology. Chapter~\ref{sec-rna}
extends beyond splicing to RNA structure prediction and RNA foundation
models, examining how secondary structure and functional context inform
representation learning. Chapter~\ref{sec-single-cell} covers foundation
models for single-cell transcriptomics and epigenomics, showing how
transformer architectures adapt to the unique characteristics of these
data types. Chapter~\ref{sec-3d-genome} addresses the three-dimensional
organization of the genome, from chromatin loops and TAD boundaries to
emerging spatial transcriptomics foundation models, examining how 3D
structure provides the missing link between sequence and regulatory
function. Chapter~\ref{sec-networks} turns to graph neural networks and
network-based approaches, framing these not as alternatives to sequence
models but as higher-level reasoning systems that consume foundation
model embeddings as node features. Chapter~\ref{sec-multi-omics}
broadens the view to multi-omics integration, exploring how models can
jointly represent genomic, transcriptomic, proteomic, and clinical
information to connect sequence variation to phenotype across multiple
layers of biological organization.

\textbf{Part V: Evaluation and Reliability} develops frameworks for
assessing what models actually learn and how reliably they perform.
Chapter~\ref{sec-benchmarks} surveys existing benchmarks for genomic
foundation models, analyzing their construction, coverage, and
limitations. Chapter~\ref{sec-evaluation} presents evaluation principles
and proper methodology, covering data splitting, metric choice, and the
link between benchmark performance and real-world utility.
Chapter~\ref{sec-confounding} details sources of confounding and data
leakage, from batch effects and ancestry structure to label bias and
covariate shift, offering practical strategies for detection and
mitigation. Chapter~\ref{sec-uncertainty} addresses uncertainty
quantification, examining calibration, epistemic versus aleatoric
uncertainty, and practical methods such as ensembles and conformal
prediction that help models express when they do not know.
Chapter~\ref{sec-interpretability} explores interpretability tools from
classical motif discovery and attribution methods to emerging
mechanistic approaches, asking when these tools reveal genuine
biological mechanisms and when they provide false comfort.

\textbf{Part VI: Translation} moves from methods to end-to-end workflows
in research and clinical practice. Chapter~\ref{sec-clinical-risk}
discusses clinical risk prediction that combines genomic features with
electronic health records and environmental data, focusing on
discrimination, calibration, fairness, and deployment in health systems.
Chapter~\ref{sec-rare-disease} examines how foundation models fit into
rare disease and cancer workflows, including variant prioritization
pipelines, integration with family and tumor-normal data, and laboratory
validation. Chapter~\ref{sec-drug-discovery} looks at how GFMs intersect
with target discovery, functional genomics screens, and biomarker
development in pharmaceutical and biotechnology settings.
Chapter~\ref{sec-design} covers generative applications, from protein
design and therapeutic sequence optimization to synthetic biology and
bioengineering workflows. Chapter~\ref{sec-future} concludes with
regulatory and ethical considerations, open problems, emerging
directions, and considerations for responsible development and
deployment of genomic AI systems.

\textbf{Six appendices} provide supporting material.
Appendix~\ref{sec-apx-dl} offers a compact introduction to neural
networks, CNNs, transformers, training, and evaluation for readers who
want enough machine learning background to engage with the main chapters
without consulting external references. Appendix~\ref{sec-apx-compute}
covers practical considerations for deploying genomic foundation models,
including computational requirements, hardware selection, and
infrastructure concerns. Appendix~\ref{sec-apx-data-curation} provides
guidance on constructing training datasets, covering data sources,
quality filtering, deduplication, and contamination detection.
Appendix~\ref{sec-apx-models} provides a comprehensive reference table
of models discussed throughout the book, with architecture summaries,
training data, and key citations. Appendix~\ref{sec-apx-resources}
offers a curated collection of datasets, software tools, courses, and
papers for deeper exploration. Appendix~\ref{sec-apx-glossary} defines
key terms spanning genomics, machine learning, and clinical
applications.

\section*{A Framework, Not a Snapshot}\label{a-framework-not-a-snapshot}
\addcontentsline{toc}{section}{A Framework, Not a Snapshot}

\markright{A Framework, Not a Snapshot}

Genomic foundation models represent a moving target: architectures
evolve, datasets expand, and evaluation standards shift. A book that
cataloged the state of the art in 2024 would be obsolete before
publication. This book instead provides a framework for reasoning about
new models as they appear, grounding readers in principles stable enough
to outlast any particular architecture or benchmark.

Readers who work through this material should be equipped to place new
models in the landscape of data, architecture, objective, and
application. They should be able to design analyses that use foundation
models as components (whether as feature extractors, priors, or
simulators) without overclaiming what the models can do. They should
recognize pitfalls in training, evaluation, and deployment, especially
in clinical settings where errors have real consequences. And they
should be able to decide where foundation models genuinely add value and
where simpler methods remain sufficient.

The journey begins with foundations: how raw reads become variants, how
variants become the datasets on which all subsequent models depend, and
where errors in this upstream process create systematic challenges that
propagate through everything built upon them.

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

Working on genomic foundation models means context-switching constantly:
debugging data artifacts one week, reproducing a transformer-based
variant effect predictor the next, and arguing about clinical patient
cohorts the week after. The knowledge required is scattered across
textbooks, methods papers, and tribal folklore - genomics on one shelf,
deep learning on another, clinical deployment in someone else's head
entirely.

This book is my attempt to put those pieces in one place: to connect the
mature, statistically grounded tradition of human genetics with the
rapidly changing ecosystem of deep learning and foundation models, and
to make that transition legible for people who live in one corner of the
triangle and are trying to get oriented to the others.

I wrote it first for myself and my collaborators: as a way to organize
wiki pages, markdown files, and half-finished slide decks into something
coherent. Over time it became clear that turning those notes into a book
might be useful to others navigating the same landscape.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{Why I Wrote This Book}\label{why-i-wrote-this-book}
\addcontentsline{toc}{section}{Why I Wrote This Book}

\markright{Why I Wrote This Book}

What I wanted, but could not find, was a \textbf{conceptual
throughline}:

\begin{itemize}
\tightlist
\item
  How do we get from reads to variants in a way that a deep model can
  trust?
\item
  How should we think about polygenic scores, fine-mapping, and
  functional assays in the era of foundation models?
\item
  When we say a model ``understands'' regulatory grammar or protein
  function, what does that actually mean?
\item
  And what does it take to move from a promising preprint to a tool that
  can support decisions about real patients?
\end{itemize}

This book is my best attempt at answering those questions in a way that
is historically grounded, technically honest, and practically oriented.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{How This Book Came
Together}\label{how-this-book-came-together}
\addcontentsline{toc}{section}{How This Book Came Together}

\markright{How This Book Came Together}

The structure of the book reflects the way these ideas evolved in my own
work.

Early sections grew out of teaching and mentoring conversations:
explaining next-generation sequencing, variant calling, and
pre-deep-learning interpretation methods to new team members who were
strong in statistics or ML but new to genomics (and vice versa).

The middle sections emerged from a series of ``journal club +
experiments'' cycles, where we:

\begin{itemize}
\tightlist
\item
  read papers on sequence-to-function CNNs, protein language models, and
  genomic transformers,
\item
  tried to reproduce key results or adapt them to key datasets,
\item
  and documented the pain points---data formats, training instabilities,
  evaluation pitfalls, which never quite fit into a methods section.
\end{itemize}

The later parts were shaped by collaborations around clinical
prediction, variant interpretation pipelines, and larger multi-omic
models. Many of the examples and caveats come directly from these
projects: places where a model that looked excellent on paper behaved in
surprising ways when exposed to real-world data, or where simple
baselines outperformed much fancier architectures once confounding and
distribution shift were handled correctly.

Because of that origin, the book has a particular bias: it is written
from the perspective of someone who spends much of their time trying to
get models to work in messy, high-stakes settings. You will see this in
the emphasis on data quality, evaluation, and clinical translation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{How to Read This Book}\label{how-to-read-this-book}
\addcontentsline{toc}{section}{How to Read This Book}

\markright{How to Read This Book}

This is \textbf{not} a genomics textbook, a complete review of every DNA
or protein model, or a deep-learning-from-scratch course. Instead, it is
meant to be:

\begin{itemize}
\tightlist
\item
  a \textbf{roadmap} to the main kinds of data, models, and objectives
  that matter for genomic foundation models today
\item
  a \textbf{bridge} between classical statistical genetics and modern
  representation learning
\item
  a \textbf{practical guide} to the kinds of failure modes and design
  choices that matter in real applications.
\end{itemize}

You do \textbf{not} need to read the book cover-to-cover in order.

\begin{itemize}
\tightlist
\item
  If your background is in \textbf{genomics or statistical genetics},
  you may want to skim the early deep-learning motivations and focus
  more on the sections that introduce convolutional models,
  transformers, and self-supervision, then move on to evaluation and
  applications.
\item
  If you come from \textbf{machine learning}, it may be more helpful to
  start with the genomic data and pre-deep-learning methods, then dive
  into the sequence-to-function and transformer-based chapters with an
  eye toward how the data and objectives differ from text or images.
\item
  If you are a \textbf{clinician or translational researcher}, you might
  care most about the reliability, confounding, and clinical deployment
  discussions, dipping back into the modeling parts as needed to
  interpret results or communicate with technical collaborators.
\end{itemize}

The book is organized into six parts:

\begin{itemize}
\tightlist
\item
  \textbf{Part I} introduces genomic data and pre-deep-learning
  interpretation methods, from sequencing and variant calling to early
  pathogenicity scores and polygenic models.
\item
  \textbf{Part II} focuses on supervised sequence-to-function models,
  with an emphasis on convolutional architectures, regulatory
  prediction, and splicing.
\item
  \textbf{Part III} turns to transformer-based models and
  self-supervision, covering protein and DNA language models and hybrid
  architectures that combine CNNs and transformers.
\item
  \textbf{Part IV} discusses what makes a model a \emph{foundation
  model} in genomics, including multi-omic architectures, variant effect
  modeling, and emergent capabilities.
\item
  \textbf{Part V} examines reliability, evaluation, confounding, and
  interpretability---how we know whether a model is learning what we
  think it is, and how to detect when it is not.
\item
  \textbf{Part VI} looks at applications: clinical and risk prediction,
  variant interpretation workflows, and early steps toward drug
  discovery and biotech use cases.
\end{itemize}

Within each part, the goal is not to catalogue every paper, but to
highlight representative examples and the design principles they
illustrate. References are there to give you starting points, not to
serve as a comprehensive literature review.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{What This Book Assumes (and What It Does
Not)}\label{what-this-book-assumes-and-what-it-does-not}
\addcontentsline{toc}{section}{What This Book Assumes (and What It Does
Not)}

\markright{What This Book Assumes (and What It Does Not)}

The book assumes:

\begin{itemize}
\tightlist
\item
  basic familiarity with probability and statistics (regression,
  hypothesis testing, effect sizes),
\item
  core genomics concepts (genes, variants, linkage disequilibrium, GWAS
  at a high level),
\item
  and some exposure to machine learning ideas (training versus test
  data, overfitting, loss functions).
\end{itemize}

It \textbf{does not} assume that you have implemented deep learning
models yourself, or that you are fluent in every area. When a chapter
leans heavily on a particular background (for example, causal inference
or modern self-supervised learning), it will either provide a brief
refresher or point you to an appendix or external resource.

If you are missing some of this background, that is fine. The intent is
for you to be able to read actively: to pause, look up side topics, and
then return to the main arc without feeling lost.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{A Note on Scope and
Opinions}\label{a-note-on-scope-and-opinions}
\addcontentsline{toc}{section}{A Note on Scope and Opinions}

\markright{A Note on Scope and Opinions}

Genomic foundation models are evolving quickly. Any snapshot is, by
definition, incomplete and slightly out of date.

Rather than chasing every new architecture or benchmark, the book
focuses on \textbf{durable ideas}:

\begin{itemize}
\tightlist
\item
  how different data types fit together,
\item
  what kinds of objectives encourage useful representations,
\item
  how evaluation can fail in genomics-specific ways,
\item
  and where deep models complement (rather than replace) classical
  approaches.
\end{itemize}

Inevitably, there are judgment calls about which papers, methods, and
perspectives to emphasize. Those choices reflect my own experiences and
biases. They are not an official position of any institution I work
with, and they will certainly differ from other reasonable views in the
field.

You should treat the book as one opinionated map of the landscape, not
the landscape itself.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

This book exists because of many generous people who shared their time,
ideas, and encouragement.

First, I owe a deep debt of gratitude to my colleagues in the
\textbf{Mayo Clinic GenAI} and broader data science community. The
day-to-day conversations, whiteboard sessions, and ``what went wrong
here?'' post-mortems with this group shaped much of the perspective and
many of the examples in the chapters.

I am especially grateful to the \textbf{principal investigators and
clinicians} whose questions kept the focus on real patients and real
decisions:\\
\textbf{Dr.~Shant Ayanian}, \textbf{Dr.~Elena Myasoedova}, and
\textbf{Dr.~Alexander Ryu}.

To \textbf{leadership at Mayo Clinic} who supported the time, computing
resources, and institutional patience needed for both the models and
this book:\\
\textbf{Dr.~Matthew Callstrom}, \textbf{Dr.~Panos Korfiatis}, and
\textbf{Matt Redlon}.

To my \textbf{data science and machine learning engineering colleagues},
whose work and feedback directly shaped many of the workflows and case
studies:\\
\textbf{Bridget Toomey}, \textbf{Carl Molnar}, \textbf{Zach Jensen}, and
\textbf{Marc Blasi}.

I am also grateful for the architectural creativity, hardware insight,
and willingness to experiment from our \textbf{collaborators at
Cerebras}:\\
\textbf{Natalia Vassilieva}, \textbf{Jason Wolfe}, \textbf{Omid Shams
Solari}, \textbf{Vinay Pondenkandath}, \textbf{Bhargav Kanakiya}, and
\textbf{Faisal Al-khateeb}.

And to our \textbf{collaborators at GoodFire}, whose partnership helped
push these ideas toward interpretable and deployable systems:\\
\textbf{Daniel Balsam}, \textbf{Nicholas Wang}, \textbf{Michael Pearce},
and \textbf{Mark Bissell}.

I would also like to thank my former colleagues at \textbf{LGC} for
foundational work and conversations around protein language models and
large-scale representation learning:\\
\textbf{Prasad Siddavatam} and \textbf{Robin Butler}.

Beyond these named groups, I owe a broader debt to the geneticists,
molecular biologists, statisticians, clinicians, and engineers whose
work this book draws on. The field moves forward because people share
code, publish honest benchmarks, and insist that models be connected
back to biologically meaningful questions. Thank you for setting that
standard.

Finally, I am grateful to my wife, Alyssa, and our two kids for their
patience with the evenings and weekends this book consumed. You gave me
the space to finish it and the reasons to step away from it.

If this book helps you connect a new model to a real biological
question, design a more robust evaluation, or communicate more clearly
across disciplinary boundaries then it will have done its job.

--- \emph{Josh Meehl}

\part{Part I: Data Foundations}

Every genomic foundation model inherits the biases of its training data.
A model trained on European-dominated biobanks will miscalibrate
predictions for other populations. A variant effect predictor learning
from ClinVar inherits whatever ascertainment biases clinical
laboratories embedded in those classifications. A regulatory model
trained on ENCODE cell lines may fail on primary tissues absent from the
training compendium. The models examined throughout this book do not
transcend their data sources; they compress and reflect them.
Understanding what data resources contain, what they systematically
miss, and what assumptions they encode is prerequisite to understanding
what foundation models can and cannot accomplish.

Part I establishes this foundation by surveying the technologies,
datasets, and pre-deep-learning methods that genomic foundation models
must respect, integrate with, or improve upon. The chapters trace a
natural arc from data generation to interpretation.
Chapter~\ref{sec-ngs} introduces next-generation sequencing and variant
calling, the processes that transform biological samples into the VCF
files that serve as inputs to nearly all downstream analysis.
Understanding these technologies reveals both their remarkable power and
their systematic blind spots, from reference bias to missing structural
variants, limitations that propagate into every model trained on their
outputs.

Chapter~\ref{sec-data} surveys the public resources that underpin modern
computational genomics: reference genomes, population variation catalogs
like gnomAD, functional genomics consortia such as ENCODE and Roadmap
Epigenomics, and biobank-scale cohorts including the UK Biobank and
GTEx. These resources serve simultaneously as training data, evaluation
benchmarks, and sources of prior biological knowledge.
Chapter~\ref{sec-gwas} introduces the statistical machinery of
genome-wide association studies and polygenic scores, the classical
approach to connecting genotype with phenotype that provides both
baselines against which deep models are measured and conceptual
frameworks that inform their design. Finally,
Chapter~\ref{sec-vep-classical} examines pre-deep-learning variant
effect prediction through CADD and related methods, establishing what
careful feature engineering achieved and where its limitations motivated
the learned representations that subsequent parts develop.

\chapter{From Reads to Variants}\label{sec-ngs}

Every polygenic risk score, every variant pathogenicity prediction,
every clinical interpretation of a patient's genome begins with a prior
assumption: that the variants being analyzed are real. Researchers
download VCF files from biobanks, filter by allele frequency, and feed
variants into predictive models without questioning whether those
variants truly exist in the original biological sample. Yet variant
calling is not observation; it is inference. The sequencer produces
millions of short reads with characteristic error profiles, alignment
algorithms place those reads against a reference genome with varying
confidence, and variant callers integrate the evidence into genotype
calls that may or may not reflect reality. When this inference fails,
every downstream analysis inherits errors that propagate silently
through the entire computational pipeline.

This chapter examines the technical foundations of variant calling, from
raw sequencing signal to the VCF files that serve as input to every
model in this book. The focus is not on operational details of running
pipelines but on the inferential challenges that determine when variant
calls can be trusted and when they cannot. Short-read sequencing
technologies produce reads of 100 to 300 base pairs with error rates
near one percent, creating fundamental ambiguities in repetitive
regions, segmental duplications, and regions of high sequence
complexity. Classical variant callers addressed these challenges through
probabilistic models that integrate multiple evidence types: base
quality scores, mapping quality, strand balance, and population allele
frequencies. Deep learning approaches, exemplified by DeepVariant,
reformulated variant calling as image classification, learning to
distinguish true variants from artifacts in ways that generalize across
sequencing platforms and sample types.

Understanding where variant calling succeeds and fails is essential
context for the foundation models examined throughout this book. The
challenging regions of the genome (tandem repeats, segmental
duplications, the major histocompatibility complex) that confound
variant callers also challenge the models trained on variant calls. The
systematic errors introduced at this stage create systematic blind spots
that no amount of sophisticated downstream modeling can correct.

\section{The Challenge of NGS Data}\label{the-challenge-of-ngs-data}

The human genome contains approximately three billion base pairs, yet no
instrument can read this sequence in one continuous stretch.
\textbf{Next-generation sequencing (NGS)} instead fragments DNA
molecules into short pieces, sequences each fragment independently, and
produces tens to hundreds of gigabases of sequence data per run
(Goodwin, McPherson, and McCombie 2016). The typical output consists of
paired-end Illumina reads spanning 100 to 300 base pairs each, with each
base assigned a quality score reflecting the instrument's confidence in
that call. This abundance comes at a cost: every read carries
non-trivial measurement uncertainty, including substitutions from
miscalled bases, context-specific errors near homopolymers, and quality
degradation toward read ends.

Long-read technologies from Pacific Biosciences and Oxford Nanopore
extend the observable space dramatically, producing reads of 10
kilobases to over a megabase in length (Wenger et al. 2019;
Dabernig-Heinz et al. 2024). These platforms access genomic territory
invisible to short reads, including complex structural variants,
segmental duplications, and repetitive regions. They carry their own
characteristic error profiles, however, and the choice of sequencing
platform fundamentally shapes which variants are discoverable and which
systematic biases enter downstream analyses. A variant residing within a
repetitive element may be invisible to short reads but readily detected
by long reads that span the entire repeat.

The central problem is deceptively simple in statement but profound in
consequence: how do we turn raw reads into a reliable list of genomic
variants? Answering this question requires disentangling three
fundamentally different sources of signal that manifest identically as
mismatches between reads and reference. Sequencing errors arise from
instrument noise and PCR artifacts during library preparation, creating
false variants that never existed in the original DNA. Alignment
artifacts occur when reads are mapped to incorrect genomic locations,
particularly in repetitive regions and paralogous gene families, causing
true variants to appear at wrong positions or disappear entirely.
Genuine biological variation encompasses germline variants inherited
from parents, somatic mutations acquired during cellular division, and
mosaicism where only a fraction of cells carry a particular change.
Historically, complex modular pipelines combining probabilistic models
and hand-crafted heuristics addressed this separation (Nielsen et al.
2011). Deep learning now plays an important role in simplifying and
improving parts of this stack, but understanding the classical pipeline
remains essential for interpreting what downstream models actually
learn.

The scope of this chapter centers on germline variant calling in human
whole-exome sequencing (WES) and whole-genome sequencing (WGS) data, the
core technical challenge underlying most genomic deep learning
applications. Somatic variant calling in cancer and RNA-seq-specific
variant calling share many parallels but require additional
considerations that fall outside Part I of this book.

\section{Targeting Strategies: Panels, Exomes, and
Genomes}\label{targeting-strategies-panels-exomes-and-genomes}

Different clinical and scientific goals demand different sequencing
strategies. A patient presenting with sudden cardiac arrest at age 35
needs deep, reliable coverage of \emph{KCNQ1}, \emph{KCNH2},
\emph{SCN5A}, and other ion channel genes associated with long QT
syndrome; sequencing her entire genome to find these variants would
waste resources and delay clinical decisions. A biobank building
training data for polygenic risk scores across hundreds of thousands of
participants needs genome-wide coverage, even if individual sites have
modest depth. A family searching for the cause of their child's
undiagnosed developmental delay needs comprehensive coverage that leaves
no coding exon unexamined. These competing demands drive the choice
between targeted panels, whole-exome sequencing, and whole-genome
sequencing.

\subsection{Targeted and Panel
Sequencing}\label{targeted-and-panel-sequencing}

When clinicians already know which genes to examine, \textbf{targeted
gene panels} capture tens to hundreds of genes selected for a specific
clinical indication. Panels for cardiomyopathy, hereditary cancer
syndromes, or epilepsy restrict sequencing to regions of known clinical
relevance. By limiting the target to a small number of loci, panels
achieve very deep coverage (often exceeding 500Ã—) at modest cost,
enabling sensitive detection of rare variants and some degree of
mosaicism.

The narrow scope of panels limits their utility for deep learning and
population-scale analysis. Panels miss novel disease genes outside their
predefined targets, cannot be easily repurposed for new traits, and
often have heterogeneous content across laboratories that complicates
data aggregation. For large-scale genomic foundation models, panel data
serve better as richly phenotyped anchors than as primary training
material: they provide clean labels for specific variants but sparse
genomic coverage overall.

\subsection{Whole-Exome Sequencing}\label{whole-exome-sequencing}

Protein-coding sequence represents approximately 1 to 2 percent of the
genome but harbors a disproportionate share of variants with known
functional consequences. \textbf{Whole-exome sequencing (WES)} enriches
coding exons and some splice-adjacent regions through hybridization
probes that pull down targeted DNA, followed by short-read sequencing.
Typical coverage ranges from 80 to 150Ã— for exonic targets, sufficient
for confident heterozygous variant calling in most regions.

WES has driven Mendelian disease gene discovery for over a decade and
powered early biobank-scale efforts, including the exome subsets of
gnomAD and many hospital-based cohorts (Karczewski et al. 2020). The
capture-based approach introduces systematic biases that propagate into
downstream analyses, however. Certain exons consistently fail to capture
efficiently, particularly those with extreme GC content, high repetitive
content, or unusual length. A variant in the first exon of \emph{HTT}
(the Huntington disease gene) might be missed entirely due to extreme GC
richness, and a variant effect predictor trained on WES data will never
encounter variants in poorly captured regions. These blind spots are
invisible in standard benchmarks but can have substantial clinical
consequences. Batch effects tied to reagent lots and evolving panel
designs further complicate multi-cohort analyses.

\subsection{Whole-Genome Sequencing}\label{whole-genome-sequencing}

Noncoding variants contribute substantially to human disease, and
structural variants often span boundaries between exonic and intronic
sequence. \textbf{Whole-genome sequencing (WGS)} samples nearly all
bases in the genome at typical coverage of 30 to 60Ã—, encompassing both
coding and noncoding regions without the biases introduced by capture
chemistry. Because there is no enrichment step, WGS produces more
uniform depth than WES and enables detection of noncoding regulatory
variants, structural variants, and copy-number changes alongside
\textbf{single nucleotide variants (SNVs)} and \textbf{indels}
(insertions and deletions).

WGS has become increasingly favored for new large cohorts and rare
disease studies. The UK Biobank's release of 500,000 whole genomes and
gnomAD's expansion to include diverse populations both rely on WGS as
the primary data type (Bycroft et al. 2018; Karczewski et al. 2020). The
data are reusable for many downstream analyses, including GWAS,
polygenic score development, and rare variant burden tests, and the
simplified pipeline eliminates the need to track changing capture
designs across time and centers. When subsequent chapters refer to
``whole-genome models,'' they implicitly assume access to WGS-based
variant calls, even when actual training sets combine WES and WGS data
for practical reasons.

\subsection{Long-Read Sequencing
Technologies}\label{long-read-sequencing-technologies}

Short reads face a fundamental limitation rooted in information theory:
sequences shorter than local repeats, segmental duplications, or
structural variants cannot unambiguously resolve these features.
Consider the \emph{SMN1} and \emph{SMN2} genes, which differ by only
five nucleotides across their entire coding regions. Distinguishing them
is clinically critical for diagnosing spinal muscular atrophy, yet short
reads routinely fail this task because a 150-bp read maps equally well
to either genomic location.

Pacific Biosciences (PacBio) HiFi sequencing produces reads of 10 to 25
kilobases with per-base accuracy exceeding 99.9\% through circular
consensus sequencing, where the same molecule is read multiple times to
correct random errors (Wenger et al. 2019). Oxford Nanopore Technologies
(ONT) instruments generate reads ranging from a few kilobases to over a
megabase in length, with rapidly improving raw accuracy and unique
capabilities including portable sequencers suitable for field
deployment, direct RNA sequencing without reverse transcription, and
real-time base calling during sequencing (Dabernig-Heinz et al. 2024).
These technologies played central roles in the telomere-to-telomere
(T2T) assembly of a complete human genome and in emerging human
pangenome references that capture population diversity beyond what any
single linear reference can represent (Nurk et al. 2022; Liao et al.
2023).

Long reads transform variant calling by traversing low-complexity and
repetitive regions essentially invisible to short-read technologies.
Dedicated variant callers such as PEPPER-Margin-DeepVariant (Shafin et
al. 2021), Clair3 (Z. Zheng et al. 2022), Sniffles2 (Smolka et al.
2024), \texttt{pbsv} ({``{PacificBiosciences}/Pbsv''} 2025), and cuteSV
(Jiang et al. 2020) exploit read length and alignment patterns to detect
insertions, deletions, inversions, and complex rearrangements. Single
molecules spanning multiple heterozygous sites provide direct phasing
information for \textbf{haplotype} resolution without statistical
inference. Long reads also inform graph-based references and pangenomes
that better represent population diversity than traditional linear
references (Liao et al. 2023).

Short-read pipelines remain the workhorse for large human cohorts due to
cost and throughput advantages that will persist for years. The models
discussed in later chapters must accommodate variants discovered by
either technology and must be evaluated on composite callsets that
integrate short- and long-read information.

\section{Classical Variant Calling
Pipelines}\label{classical-variant-calling-pipelines}

Understanding classical approaches matters not merely for historical
completeness but because deep learning models like DeepVariant still
operate within this overall framework. They replace specific components
rather than rebuilding from scratch. The GATK Best Practices, first
formalized around 2011 and refined continuously since, represent
accumulated wisdom from a decade of methodological development (DePristo
et al. 2011; Van der Auwera et al. 2018). These pipelines encode expert
intuition about which quality metrics matter, how to balance sensitivity
against specificity, and when borderline evidence should be trusted.
Modern deep learning approaches inherit this structure even as they
replace individual components with learned alternatives.

\subsection{From Sequencer to Aligned
Reads}\label{from-sequencer-to-aligned-reads}

The journey from DNA sample to variant calls begins when instrument
software converts fluorescent images or electrical signals to base calls
and quality scores through \textbf{base calling}. Reads are
demultiplexed by sample barcode into \texttt{FASTQ} files, each
containing millions of short sequences with associated Phred-scaled
quality scores. These files serve as the raw material for all subsequent
analysis, encoding both the sequence content and the instrument's
confidence in each base.

\textbf{Read alignment} maps each short read to its most likely position
in a reference genome using seed-and-extend algorithms implemented in
tools such as BWA-MEM and minimap2 (Heng Li 2013, 2018). The current
standard references include GRCh38 (the traditional linear reference)
and T2T-CHM13 (the first complete telomere-to-telomere assembly). The
alignment challenge is substantial: algorithms must cope with mismatches
arising from both true variants and sequencing errors, small indels that
shift the alignment frame, and repetitive sequences where multiple
genomic locations match equally well. When a read could plausibly
originate from several locations, the aligner must either choose one
(potentially incorrectly), report multiple candidates, or assign a
mapping quality score reflecting its uncertainty about the true origin.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-variant-calling-pipeline}{[}Essential{]} End-to-end
schematic showing the journey from DNA sample to VCF file. Major stages:
(1) DNA fragmentation and library preparation, (2) sequencing
(distinguish short-read Illumina vs long-read PacBio/ONT icons), (3)
base calling producing FASTQ, (4) read alignment to reference (show
reads stacking against genome), (5) post-alignment processing (duplicate
marking, BQSR), (6) per-sample variant calling producing gVCF, (7) joint
genotyping across cohort, (8) variant filtering producing final VCF.
Annotate data formats at each arrow (FASTQ, BAM/CRAM, gVCF, VCF).
Include approximate file sizes for a 30Ã— WGS to ground scale.}

\end{figure}%

\textbf{Post-alignment processing} addresses systematic artifacts that
would otherwise corrupt variant calls. PCR duplicates arise when
multiple reads are amplified from the same original DNA fragment during
library preparation; these inflate apparent coverage and can amplify
sequencing errors into false variants that appear well-supported.
Duplicate marking identifies and flags these reads based on identical
alignment coordinates. \textbf{Base quality score recalibration (BQSR)}
models systematic quality score errors by comparing observed mismatches
to databases of known variants, adjusting quality scores to better
reflect true error rates in each sequence context (DePristo et al.
2011). Older pipelines also performed local realignment around indels,
though modern callers have largely internalized this step.

\subsection{Per-Sample Variant
Calling}\label{per-sample-variant-calling}

At each position in the genome, the fundamental question is: given the
reads overlapping this site, what is the most likely genotype? For
diploid humans at biallelic sites, three possibilities exist: homozygous
reference (0/0), heterozygous (0/1), or homozygous alternate (1/1). The
task is to compute \textbf{genotype likelihoods} that quantify the
probability of the observed read data under each possible genotype, then
combine these likelihoods with prior expectations to estimate posterior
probabilities.

GATK HaplotypeCaller approaches this by first identifying regions with
evidence of variation, then locally assembling candidate haplotypes from
the reads spanning that region, and finally computing likelihoods for
each possible diploid genotype. The core calculation uses a \textbf{pair
hidden Markov model (pair-HMM)} to marginalize over possible alignments
between each read and each candidate haplotype, incorporating base
quality scores to weight the contribution of each base (DePristo et al.
2011; Heng Li 2014).

The mathematical framework is Bayesian. At a given site, the posterior
probability of genotype \(G\) given read data \(D\) follows from Bayes'
theorem:

\[
P(G \mid D) \propto P(G) \prod_{r \in \text{reads}} P(r \mid G)
\]

The prior \(P(G)\) often assumes Hardy-Weinberg equilibrium with a
specified allele frequency, while the likelihood \(P(r \mid G)\)
captures the probability of observing read \(r\) given that the true
genotype is \(G\). This formulation assumes conditional independence of
reads given the genotype, an assumption violated in practice by
systematic sequencing errors, read pair correlations, and library-level
artifacts that create dependencies among observations. Classical
pipelines attempt to correct for these violations through BQSR and ad
hoc filters. Deep learning-based callers can learn these dependencies
implicitly by processing entire pileups simultaneously, one of their key
advantages.

The per-read likelihoods aggregate into genotype likelihoods, which
combine with priors to yield posterior probabilities. These posteriors
become the \textbf{genotype quality (GQ)} scores that downstream
analyses often treat as ground truth. Per-sample results are output as
\texttt{gVCF} files encoding both variant calls and ``reference blocks''
with estimated confidence at non-variant positions, enabling later joint
analysis across samples.

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm, toprule=.15mm, toptitle=1mm, breakable, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, arc=.35mm, titlerule=0mm, opacityback=0, bottomrule=.15mm, rightrule=.15mm, colback=white]

The VCF Format The Variant Call Format (VCF) is the standard file format
for storing variant calls. Each variant occupies one row with mandatory
columns: CHROM and POS specify genomic location; REF and ALT give the
reference and alternate alleles; QUAL provides a Phred-scaled quality
score; FILTER indicates whether the variant passed quality filters; and
INFO contains semicolon-delimited annotations such as allele frequency
or functional predictions. For multi-sample files, a FORMAT column
defines per-sample fields (typically GT for genotype, GQ for genotype
quality, DP for depth), followed by one column per sample containing
those values. Genotypes are encoded as allele indices separated by /
(unphased) or \textbar{} (phased), where 0 represents the reference
allele and 1 the first alternate. A heterozygous call appears as 0/1; a
phased heterozygote as 0\textbar1 or 1\textbar0 depending on which
allele was inherited maternally versus paternally.

\end{tcolorbox}

\subsection{Cohort Calling and
Filtering}\label{cohort-calling-and-filtering}

Individual samples rarely provide sufficient information for confident
rare variant calling. A variant observed in only one sample with modest
supporting reads might be a true rare variant or a systematic artifact;
examining a single sample cannot distinguish these possibilities.
Examining the same site across thousands of samples resolves this
ambiguity: true variants appear in multiple individuals following
population genetic expectations, while artifacts show patterns
inconsistent with inheritance and population structure.

Joint genotyping combines gVCFs across many samples to produce a
multi-sample \texttt{VCF}. This process ensures that all samples are
evaluated at the same candidate sites, avoiding the problem of comparing
different variant lists, and pools information across carriers to
improve sensitivity for rare variants. A variant with marginal evidence
in three individuals gains credibility when those individuals share
ancestry and the variant frequency matches population expectations from
external databases.

Filtering strategies separate high-confidence variants from probable
artifacts. Early approaches applied independent thresholds on quality
metrics such as depth, mapping quality, and strand bias, but these hard
filters poorly captured the complex, multivariate patterns
distinguishing true variants from errors. \textbf{Variant Quality Score
Recalibration (VQSR)} instead trains a Gaussian mixture model on known
true positives from validated resources (HapMap, 1000 Genomes) and
likely false positives, learning a composite quality score that
integrates multiple annotation dimensions (DePristo et al. 2011). This
approach dominated large-scale variant calling for a decade before
machine learning methods began to replace it.

\subsection{Sample-Level Quality
Control}\label{sample-level-quality-control}

Before any downstream analysis or model training, variant callsets must
pass through sample-level quality control. Sex checks compare reported
sex to X chromosome heterozygosity and Y chromosome coverage to detect
sample swaps or sex chromosome aneuploidy. Contamination analysis
estimates whether DNA from multiple individuals mixed during sample
preparation, which would create apparent heterozygosity at sites where
the individual is actually homozygous. Relatedness detection identifies
unexpected relatives or duplicate sequencing of the same individual,
both of which confound association analyses and inflate apparent sample
sizes. \textbf{Ancestry inference} estimates genetic ancestry using
principal component analysis or model-based clustering, which matters
for controlling population stratification in downstream analyses
(Chapter~\ref{sec-confounding}).

These QC steps determine which samples enter training sets, how models
are stratified by ancestry, and which samples must be excluded due to
technical artifacts. When subsequent chapters refer to ``a callset,''
they implicitly assume that careful QC has already been applied.

\section{Haplotype Phasing}\label{haplotype-phasing}

The clinical stakes of phasing emerge clearly in compound
heterozygosity. Consider a child who inherits two rare, potentially
pathogenic variants in \emph{CFTR}, the cystic fibrosis gene. If both
variants reside on the chromosome inherited from the mother (in
\emph{cis}), the child retains one functional copy from the father and
may be unaffected or merely a carrier. If the variants are on opposite
chromosomes (in \emph{trans}), no functional copy exists and the child
will develop cystic fibrosis. Standard \texttt{VCF} genotypes cannot
distinguish these scenarios, encoding only that heterozygous genotypes
exist at two positions without specifying which alleles travel together
on the same physical chromosome. The clinical implications are entirely
different, yet the data appear identical without phase information.

Diploid organisms carry two copies of each autosomal chromosome, one
inherited from each parent. \textbf{Haplotype phasing} resolves the
ambiguity in unphased genotype calls by assigning each allele to a
specific parental chromosome, transforming genotypes such as
\texttt{0/1} into phased representations like \texttt{0\textbar{}1} or
\texttt{1\textbar{}0} where the delimiter indicates that phase has been
determined.

\subsection{Clinical and Analytical
Importance}\label{clinical-and-analytical-importance}

The distinction between \emph{cis} and \emph{trans} configurations
drives clinical decisions for recessive conditions across hundreds of
disease genes, from metabolic disorders to hearing loss to retinal
degeneration. Beyond compound heterozygosity, phased haplotypes enable
several critical analyses. \textbf{Haplotype-specific expression}
studies reveal allelic imbalance where one parental copy is
preferentially transcribed, a phenomenon with implications for
imprinting disorders and variable penetrance. Accurate modeling of
\textbf{linkage disequilibrium (LD)} structure in population genetics
depends on knowing which alleles are inherited together. Reference
panels used for genotype imputation are stored as phased haplotypes;
inaccurate phasing in these panels propagates errors to every study that
uses them for imputation.

For deep learning applications, phasing determines whether models
receive unordered genotype pairs or structured, haplotype-resolved
representations. This choice affects model architecture, training
procedures, and ultimately performance. A model that processes phased
haplotypes can learn patterns spanning multiple variant sites that would
appear as noise in unphased data.

\subsection{Phasing Methods}\label{phasing-methods}

Different data types enable different phasing strategies, each with
characteristic strengths and resolution. \textbf{Read-backed phasing}
uses sequencing reads that span multiple heterozygous sites to assign
alleles to the same physical molecule. Short reads typically phase
variants within tens to hundreds of base pairs, limited by fragment
length. Long reads extend this range to tens of kilobases or more,
providing direct physical evidence of haplotype structure.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER A}}
\end{figure}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER B}}
\end{figure}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER C}}
\end{figure}

}

\caption{\label{fig-phasing-compound-het}{[}High{]} Three-panel figure
illustrating the clinical importance of phase information. Panel A:
Maternal and paternal chromosomes carrying the \emph{CFTR} gene, with
two pathogenic variant positions marked. Panel B (\emph{cis}
configuration): Both variants on maternal chromosome; paternal copy
functional; child is carrier (unaffected). Panel C (\emph{trans}
configuration): One variant on each chromosome; no functional copy;
child has cystic fibrosis. Include VCF notation showing how unphased
(0/1) vs phased (0\textbar1, 1\textbar0) genotypes encode this
information.}

\end{figure}%

\textbf{Statistical phasing} tools such as SHAPEIT, Eagle, and Beagle
use reference panels of previously phased haplotypes combined with
linkage disequilibrium patterns to infer phase across much longer
distances (O'Connell et al. 2014; Loh et al. 2016; Browning et al.
2021). When a study sample shares long haplotype segments with
individuals in the reference panel, the algorithm can confidently assign
alleles to chromosomes even without direct read evidence. These methods
work well for common variation where LD is informative but struggle with
rare variants that lack haplotype context in reference panels.

\textbf{Pedigree-based phasing} becomes possible when parent-offspring
trios or larger families are available. Mendelian inheritance rules
resolve phase with high confidence: an allele present in the child and
one parent but absent in the other must have been inherited from that
parent. The deterministic nature of this inference makes it the gold
standard when family data exist.

Modern pipelines often combine these approaches, using statistical
phasing anchored by a large reference panel, augmented by read-backed
evidence where available, and refined by trio data when present. The
resulting phase accuracy varies by variant frequency, local
recombination rate, and representation in reference panels.

\subsection{Genotype Imputation}\label{genotype-imputation}

Sequencing every individual at high coverage is expensive, but
statistical inference from population structure can fill gaps at much
lower cost. \textbf{Genotype imputation} matches a cohort with
incomplete genotype data (from array genotyping or low-coverage
sequencing) against a reference panel of densely phased haplotypes.
Statistical models infer missing genotypes and refine uncertain calls by
leveraging LD patterns and shared haplotype segments with individuals in
the reference (Browning et al. 2021).

Two related processes deserve distinction. \textbf{Genotype refinement}
improves quality at sites where genotypes were already measured but with
uncertainty, particularly useful in low-coverage WGS or WES where
stochastic sampling creates noisy calls. \textbf{Imputation of untyped
variants} infers genotypes at positions not directly observed in the
study cohort but present in the reference panel, dramatically increasing
variant density without additional sequencing.

For downstream deep learning, imputation has several important
consequences. It increases the number of variants available as input
features for genotype-based models. It produces well-calibrated genotype
probabilities (dosages) that probabilistic models can exploit rather
than forcing hard calls. It also ties model performance to the
composition and ancestry representation of the reference panel:
imputation errors are systematically larger when target individuals come
from populations underrepresented in the panel, reinforcing themes of
bias and confounding addressed in Chapter~\ref{sec-confounding}.

\section{Sources of Error and
Uncertainty}\label{sources-of-error-and-uncertainty}

Even with sophisticated pipelines, variant calls remain imperfect
measurements of biological reality. These errors concentrate in specific
genomic contexts and variant types, creating systematic blind spots in
training data that downstream models inherit without warning (Heng Li
2014). Understanding where errors arise, and why they cluster where they
do, is essential for interpreting model performance and designing robust
training strategies.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-error-sources}{[}High{]} Taxonomy diagram organizing
error sources hierarchically. Three major branches: (1) Sequencing
artifacts (subdivide: homopolymer errors, PCR duplicates, index hopping,
strand bias, quality score miscalibration); (2) Alignment artifacts
(subdivide: mapping ambiguity in repeats, reference bias favoring ref
allele, misalignment in segmental duplications); (3) Biological
complexity (subdivide: somatic mosaicism, low allele fraction, complex
variants like MNVs). For each leaf, include 1-sentence description and
indicate whether it creates false positives, false negatives, or both.}

\end{figure}%

\subsection{Mapping Ambiguity and Reference
Bias}\label{mapping-ambiguity-and-reference-bias}

When reads align almost equally well to multiple genomic locations, no
algorithm can confidently determine their true origin. Segmental
duplications, paralogous gene families, and repetitive elements create
these ambiguous contexts throughout the genome. The consequences flow in
both directions: misassigned reads create false positive variants at
incorrect locations, while correctly placed reads may be discarded or
down-weighted due to mapping uncertainty, creating false negatives.

\textbf{Reference bias} compounds these problems by systematically
favoring detection of reference alleles over alternate alleles. A read
carrying a non-reference variant may align slightly worse than an
identical read matching the reference due to the mismatch penalty,
leading to preferential retention of reference-supporting evidence. This
bias causes systematic undercalling of alternate alleles, particularly
in highly polymorphic regions or for variants that substantially alter
local sequence context. Populations divergent from the reference genome
experience more severe reference bias, creating ancestry-correlated
error patterns.

\subsection{Systematic Sequencing
Artifacts}\label{systematic-sequencing-artifacts}

Sequencing chemistry introduces predictable error patterns that differ
qualitatively from random noise. Homopolymer runs (stretches of
identical nucleotides such as AAAAAAA or GGGGGG) cause polymerase
slippage during synthesis, generating false indels at rates far
exceeding substitution errors. Certain sequence motifs, particularly
those with extreme GC content, exhibit systematically elevated error
rates that persist even at high coverage. PCR amplification during
library preparation can introduce errors early in the process; these
errors then propagate into multiple reads, creating correlated false
positives that appear well-supported by independent evidence.

Index hopping occurs when sample barcodes are misassigned during
multiplexed sequencing, causing variants from one sample to appear
spuriously in others sharing the same flow cell. Strand bias, where
variant-supporting reads cluster on one strand orientation, often
indicates systematic artifact rather than true variation. These patterns
create correlated errors that cluster by batch, lane, or library
preparation method, and they are difficult to distinguish from rare true
variants precisely because they can appear in multiple reads with
reasonable quality scores.

\subsection{Coverage Gaps and Allelic
Imbalance}\label{coverage-gaps-and-allelic-imbalance}

Stochastic sampling means some genomic regions receive fewer reads than
average purely by chance, even when capture or sequencing is nominally
uniform. In these low-coverage regions, one or both alleles may be
missed entirely, and allelic balance can deviate substantially from the
expected 50:50 ratio in heterozygotes. A heterozygous site with 20Ã—
coverage and 10 reads supporting each allele is confidently called; the
same site with 4Ã— coverage might show 4 reference reads and 0 alternate
reads by chance alone, leading to a false homozygous reference call with
no indication that an allele was missed.

Somatic mosaic variants present at low allele fractions face similar
detection challenges. A variant present in 10\% of cells produces reads
indistinguishable in individual quality from sequencing errors at
typical coverage depths, requiring specialized statistical methods or
very deep sequencing to detect reliably.

\subsection{Complex Variants and
Representation}\label{complex-variants-and-representation}

Small indels near homopolymers, multi-nucleotide variants (MNVs), and
overlapping indels present representation challenges beyond simple
detection. The same biological event can often be encoded in multiple
equivalent ways depending on alignment and normalization conventions.
The variant \texttt{chr1:100\ AT\textgreater{}A} might alternatively
appear as \texttt{chr1:101\ T\textgreater{}-}, with different callers
and normalization tools potentially choosing different representations
for the identical underlying mutation. These equivalent representations
complicate comparisons across pipelines and benchmarks; two callsets may
disagree on representation while agreeing on biology, or may appear to
agree while representing different events.

The deep learning models discussed in later chapters inherit all these
errors and uncertainties as their input. If a variant never enters the
\texttt{VCF}, no model trained on VCFs can learn its effect. If genotype
qualities are miscalibrated, models trained on hard calls may be
systematically overconfident in regions where input data are
fundamentally noisy. These inherited limitations propagate silently
through the analysis chain.

\section{Difficult Regions: The Limits of Short-Read
Calling}\label{difficult-regions-the-limits-of-short-read-calling}

Certain genomic regions resist accurate variant calling regardless of
algorithmic sophistication, with their difficulty stemming from
fundamental properties of sequence structure that challenge alignment
and assembly. These regions are disproportionately responsible for
discordant calls between pipelines and technologies (Heng Li 2014).
Their clinical importance often exceeds their representation in training
data.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-difficult-regions}{[}Enhancing{]} Ideogram-style
genome overview highlighting regions where short-read variant calling
systematically fails. Mark: (1) Segmental duplications (show
\emph{CYP2D6} region on chr22 as exemplar); (2) HLA complex on chr6p21
(shade entire region); (3) Centromeric and pericentromeric regions; (4)
Subtelomeric regions; (5) Example tandem repeat loci (e.g., \emph{HTT},
\emph{FMR1}). Inset panel shows approximate percentage of genome in each
difficulty class. Second inset shows how long reads resolve a region
invisible to short reads.}

\end{figure}%

\subsection{Segmental Duplications and Gene
Families}\label{segmental-duplications-and-gene-families}

The \emph{CYP2D6} gene illustrates how sequence complexity creates
clinical blind spots. This gene encodes a cytochrome P450 enzyme
responsible for metabolizing approximately 25\% of clinically used
drugs, including codeine, tamoxifen, and many antidepressants. It
resides in a complex genomic region alongside two pseudogenes
(\emph{CYP2D7} and \emph{CYP2D8}) sharing over 90\% sequence identity.
Short reads from one copy map almost equally well to another, producing
ambiguous alignments that either receive arbitrarily assigned positions
or inflated mapping quality scores that mask the underlying uncertainty.

Variant callers operating in this region face an impossible choice
between sensitivity and specificity. Conservative approaches undercall
true variation to avoid false positives; aggressive approaches call
spurious variants in the wrong paralog. A patient's \emph{CYP2D6}
metabolizer status, critical for drug dosing decisions that can mean the
difference between therapeutic efficacy and serious adverse events, may
be incorrectly inferred from short-read data alone.

\subsection{Low-Complexity and Repetitive
Sequence}\label{low-complexity-and-repetitive-sequence}

Homopolymers, short tandem repeats (STRs), and other low-complexity
regions challenge both sequencing chemistry and alignment algorithms.
Indel error rates are especially elevated in these contexts, and many
pipelines mask or flag these regions as low confidence. Yet variation in
repeats can be biologically critical. Triplet repeat expansion disorders
including Huntington disease, fragile X syndrome, and myotonic dystrophy
arise from unstable repeat sequences that standard short-read pipelines
handle poorly. Models trained on callsets that exclude these regions
inherit blind spots at clinically important loci.

\subsection{The HLA Region: A Case Study in
Complexity}\label{the-hla-region-a-case-study-in-complexity}

The \textbf{human leukocyte antigen (HLA)} locus on chromosome 6p21
exemplifies both the biological importance and technical difficulty of
complex genomic regions. HLA genes including \emph{HLA-A}, \emph{HLA-B},
\emph{HLA-C}, and \emph{HLA-DRB1} encode proteins central to immune
recognition and represent some of the most polymorphic sequences in the
human genome. The region spans several megabases of near-identical
sequences interspersed with gene conversions, copy number variation, and
pseudogenes.

Standard reference-based alignment fails in HLA because the extreme
polymorphism means reads carrying common, well-characterized alleles may
match the linear reference genome poorly. A read from the
\emph{HLA-B*57:01} allele (clinically important for predicting abacavir
hypersensitivity in HIV treatment) may fail to align or align with low
mapping quality, causing systematic undercalling of this medically
actionable variant (Mallal et al. 2008). The same problems affect HLA
typing for transplant matching, autoimmune disease association studies,
and pharmacogenomic testing across diverse therapeutic areas (Robinson
et al. 2020; Sakaue et al. 2023).

Specialized tools address these challenges through alternative
strategies. HLA imputation methods use dense reference panels to infer
HLA alleles from array genotypes, enabling large-scale association
studies that would otherwise require expensive targeted sequencing
(Sakaue et al. 2023). Sequence-based typing tools such as T1K perform
HLA and \textbf{KIR (killer immunoglobulin-like receptor)} genotyping
directly from WES, WGS, or RNA-seq data by aligning reads against allele
databases rather than the linear reference (Song et al. 2022).
Graph-based approaches incorporate known HLA alleles as alternate paths
through the region, improving both alignment and variant calling
(Garrison et al. 2018; Liao et al. 2023).

HLA exemplifies a broader principle: regions that are biologically rich
and clinically actionable are often technically difficult. Deep models
trained on callsets that downweight or exclude these regions inherit
their absence, creating blind spots precisely where accurate genotyping
matters most.

\section{Benchmarking and Ground
Truth}\label{benchmarking-and-ground-truth}

Evaluating variant callers requires high-confidence truth sets and
standardized comparison tools. The challenge is that ``ground truth''
for variant calling is not actually true in any absolute sense; it
represents consensus derived from multiple imperfect observations using
different technologies and algorithms. Without careful benchmarking
design, it is easy to overfit to specific datasets, underestimate errors
in difficult regions, or misinterpret the practical significance of
small metric improvements.

\subsection{GIAB Reference Samples}\label{giab-reference-samples}

The \textbf{Genome in a Bottle (GIAB)} Consortium, coordinated by NIST,
provides extensively characterized reference samples with validated
variant calls across most of the genome (Zook et al. 2019). The primary
sample is NA12878 (also known as HG001), a female of European ancestry
from the CEPH/Utah pedigree with the longest history of multi-platform
characterization. Additional samples span ancestral diversity and family
structures: HG002 through HG004 comprise an Ashkenazi Jewish trio
enabling trio-based validation, while HG005 through HG007 provide a Han
Chinese trio.

For each sample, GIAB provides \textbf{high-confidence variant calls}
representing consensus from multiple sequencing technologies and variant
callers that constitute the best current estimate of true genotypes.
Equally important are the \textbf{high-confidence regions}, genomic
intervals where the truth set is believed to be reliable. Performance
outside these regions remains formally unmeasured. Benchmarking tools
such as \texttt{hap.py} and RTG Tools enable standardized comparison of
test callsets against truth, implementing reproducible calculation of
precision, recall, and F1 metrics by variant type (Krusche et al. 2019;
{``{RealTimeGenomics}/Rtg-Core''} 2025).

\subsection{Metrics and Their Meaning}\label{metrics-and-their-meaning}

Standard metrics for variant calling include \textbf{recall
(sensitivity)}, the fraction of true variants in the benchmark
successfully identified by the caller; \textbf{precision (positive
predictive value)}, the fraction of called variants that are present in
the benchmark truth set; and \textbf{F1 score}, the harmonic mean of
precision and recall providing a single summary when both matter
equally. These metrics are typically reported separately for SNVs and
indels and may be stratified by genomic context to reveal where
performance degrades.

Metrics can be defined at different levels: per-variant (did we identify
the correct alternate allele?), per-genotype (did we correctly determine
zygosity?), or per-site (did we recognize variation at this position
regardless of allele?). For downstream models, genotype-level accuracy
and sample-level completeness often matter more than simply counting
variant matches. A model that receives incorrect genotypes at common
regulatory variants will learn corrupted associations even if overall
variant-level metrics appear strong.

\subsection{Limitations of Benchmarks}\label{limitations-of-benchmarks}

GIAB truth sets derive primarily from a small number of deeply sequenced
samples, predominantly of European ancestry in early releases, and
initially focused on genomic regions where high confidence was
achievable. High-confidence regions cover approximately 85 to 90 percent
of the genome, leaving performance in excluded regions formally unknown.
Performance in underrepresented ancestries, in complex structural
variant regions, and for novel variant classes may differ substantially
from headline GIAB metrics (Zook et al. 2019; Liao et al. 2023).

When benchmarks are reused extensively for method development, the risk
of overfitting to benchmark-specific patterns becomes substantial.
Pipelines may be tuned to maximize F1 on GIAB-like samples without
improving performance on real-world cohorts with different ancestry
composition, sequencing protocols, or variant spectra. For deep
learning-based callers with large capacity to absorb quirks in training
data, this risk is especially acute. Later chapters revisit similar
themes for benchmarking and evaluation of deep models more broadly
(Chapter~\ref{sec-benchmarks}, Chapter~\ref{sec-evaluation}).

Ongoing efforts from the T2T Consortium and the Human Pangenome
Reference Consortium are expanding benchmark scope to include complete
genome assemblies and diverse haplotype collections that better
represent human genetic diversity (Nurk et al. 2022; Liao et al. 2023).

\section{DeepVariant: Variant Calling as Image
Classification}\label{sec-deepvar}

Classical variant calling pipelines encode accumulated expert intuition
through hand-crafted features and carefully tuned heuristics developed
over years of experience with sequencing data. \emph{DeepVariant},
introduced by Google in 2018, posed a different question: what if we let
the model learn these patterns directly from data? The key insight was
not better probabilistic modeling of sequencing errors but rather a
reformulation of the problem itself. Variant calling becomes image
classification, and convolutional neural networks learn to distinguish
true variants from artifacts through the same pattern recognition that
enables them to classify natural images (Poplin et al. 2018).

\subsection{Pileup Images as Input}\label{pileup-images-as-input}

Around each candidate variant site, \emph{DeepVariant} constructs a
multi-channel tensor resembling an image. Each row corresponds to a read
overlapping the site, with columns indexing positions relative to the
candidate variant. Channels encode multiple features: match or mismatch
with the reference, Phred-scaled base quality, mapping quality, strand
orientation, support for different alleles, and additional alignment
characteristics. The reference sequence and candidate alleles are
overlaid as additional channels providing context.

This representation transforms the variant calling problem
fundamentally. Rather than computing summary statistics (depth, allelic
balance, strand bias) and feeding them to a classifier with predefined
decision rules, \emph{DeepVariant} presents the raw evidence to a neural
network. The model learns that strand-biased support clustered at read
ends looks different from balanced support distributed across read
positions without anyone explicitly defining these features or their
relative importance. Patterns invisible to hand-crafted heuristics
become learnable.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-deepvariant-pileup}{[}Essential{]} Annotated example
of a DeepVariant-style pileup tensor. Show a candidate variant site
(e.g., a heterozygous SNV) with approximately 30 reads stacked
vertically. Label the multiple channels: (1) read bases with matches in
gray, mismatches in color-coded nucleotides; (2) base quality as
intensity gradient; (3) mapping quality as separate channel; (4) strand
orientation (forward/reverse); (5) reference sequence overlay at top.
Include the candidate alleles. Show corresponding genotype posterior
output (e.g., P(0/0)=0.02, P(0/1)=0.96, P(1/1)=0.02).}

\end{figure}%

\subsection{Architecture and Training}\label{architecture-and-training}

\emph{DeepVariant} uses an Inception-style CNN architecture originally
developed for natural image classification. The convolutional
architecture that makes this possible is examined in detail in
Chapter~\ref{sec-cnn}. The network processes the pileup tensor through
multiple convolutional layers, pooling operations, and nonlinearities,
outputting posterior probabilities over three genotype classes
(homozygous reference, heterozygous, homozygous alternate) for each
candidate site (Poplin et al. 2018).

Training uses high-confidence truth sets such as GIAB genomes. The model
observes many examples of true variants and non-variants along with
their associated pileup images, learning complex decision boundaries
that integrate base quality, mapping quality, local sequence context,
and read-level patterns. Where VQSR fits a separate model on
hand-selected annotations after initial calling, \emph{DeepVariant}
processes raw evidence directly during the primary classification step.

The end-to-end training produces well-calibrated genotype likelihoods
across a range of sequencing chemistries, instruments, and read lengths,
particularly when fine-tuned for specific experimental contexts (Yun et
al. 2021). Once trained, the same architecture generalizes across
whole-genome versus whole-exome data, PCR-free versus PCR-amplified
libraries, and different sequencing instruments. This adaptability
contrasts with classical pipelines where calibration is often a
separate, post hoc step requiring platform-specific tuning by experts.

\subsection{Cohort Calling with
GLnexus}\label{cohort-calling-with-glnexus}

\emph{DeepVariant} operates primarily at the per-sample level, producing
a \texttt{gVCF} of genotype likelihoods for each individual sample. To
generate a multi-sample \texttt{VCF} suitable for population-scale
analysis, these per-sample results must be combined through joint
genotyping.

GLnexus provides this cohort-level integration for \emph{DeepVariant}
gVCFs (Yun et al. 2021). The system merges per-sample likelihoods,
applies cohort-level priors informed by observed allele frequencies, and
performs multi-sample genotype refinement and filtering. Together,
\emph{DeepVariant} and GLnexus form a modular pipeline where deep
learning replaces the per-sample likelihood engine while the overall
architecture (per-sample calls, joint genotyping, cohort filtering)
remains structurally similar to classical approaches.

Joint calling improves sensitivity for rare variants by pooling evidence
across carriers, ensures consistent variant representation across all
samples in a cohort, and enables cohort-level quality filters that
identify systematic artifacts visible only across many samples. This
combination has become a de facto standard for large WES and WGS
projects, including recent releases from gnomAD and the UK Biobank
(Karczewski et al. 2020; Bycroft et al. 2018).

\subsection{Comparison with Classical
Approaches}\label{comparison-with-classical-approaches}

The fundamental difference between \emph{DeepVariant} and classical
pipelines lies in how evidence is combined. HaplotypeCaller uses
pair-HMM models with explicit assumptions about read independence and
then applies VQSR to recalibrate quality scores using hand-selected
annotation features. \emph{DeepVariant} processes entire pileups
simultaneously, implicitly learning correlations among reads that
violate the independence assumptions built into classical probabilistic
models.

This end-to-end approach offers several practical advantages.
Calibration emerges from training rather than requiring separate
recalibration steps with their own parameter tuning. Transfer across
platforms and even species often succeeds with modest fine-tuning rather
than complete redevelopment. The model can detect subtle artifact
patterns that escape hand-crafted filters, learning representations of
error modes that human experts never explicitly described.

Both approaches share important limitations. Neither handles structural
variants well; both focus primarily on SNVs and small indels. Both
operate within the same overall pipeline framework: alignment, duplicate
marking, and joint genotyping remain largely unchanged regardless of the
per-sample caller used. \emph{DeepVariant} is best understood as a
drop-in replacement for the per-sample calling step, not a complete
reimagining of variant discovery from raw data.

\section{Implications for Genomic Deep
Learning}\label{implications-for-genomic-deep-learning}

NGS and variant calling establish the foundation for everything else in
this book. They determine what data downstream models receive, where
coverage exists, and where systematic blind spots remain hidden.
Understanding how variants are called, and where that process fails, is
essential for interpreting the performance and limitations of every
model built on this foundation.

\subsection{Variants as Atomic Units}\label{variants-as-atomic-units}

The output of WES and WGS pipelines (a \texttt{VCF} of SNVs, indels, and
inferred genotypes) defines the atomic units that many downstream models
operate on. Polygenic risk scores treat variants as weighted features
summed across the genome. GWAS summary statistics quantify associations
at individual variant positions. Variant annotation tools classify each
site by predicted functional consequence. Foundation models that operate
on genotypes rather than raw sequence inherit the variant catalog as
their effective vocabulary.

If a variant is never called, it cannot appear in training data, and no
model can learn its effect. False positives introduce noise into labels
and features, teaching models to associate spurious variants with
phenotypes. False negatives create blind spots where models must
extrapolate from incomplete information, often without any indication
that data are missing. Choices about phasing, imputation, and variant
representation determine whether models see haplotype-structured inputs,
unordered genotypes, or scalar dosage summaries. The quality of variant
calls directly limits the quality of everything built upon them.

\subsection{Inherited Biases and Blind
Spots}\label{inherited-biases-and-blind-spots}

Upstream decisions constrain what downstream models can learn. If an
assay rarely observes indels in certain repeat classes, models trained
on those callsets effectively learn a world where such variants do not
exist. If certain ancestries are underrepresented in reference panels or
truth sets, models may perform poorly for those populations while
appearing well-calibrated in benchmarks dominated by European samples.
High-confidence region definitions determine which variants enter
training sets; variants in excluded regions are invisible to models
regardless of their biological importance.

For regulatory sequence models and variant effect predictors
(Chapter~\ref{sec-dna-lm}, Chapter~\ref{sec-regulatory},
Chapter~\ref{sec-cnn}, Chapter~\ref{sec-vep-classical}), upstream
variant calling determines which sites appear as candidates and how
often certain sequence patterns are observed in association with
functional outcomes. The HLA blind spot in short-read calling means that
models trained primarily on short-read callsets will systematically
underperform for immune-related variants despite their substantial
clinical importance for autoimmune disease, transplant rejection, and
drug hypersensitivity. The accuracy ceiling imposed by variant calling
on downstream effect prediction is examined quantitatively in
Chapter~\ref{sec-vep-fm}.

\subsection{Effect Sizes Across the Frequency
Spectrum}\label{effect-sizes-across-the-frequency-spectrum}

Variant calling quality modulates the effective effect sizes that
downstream models can detect, with different dynamics for common and
rare variation. For common variants contributing to highly polygenic
traits, modest genotype error acts as additional measurement noise that
attenuates effect size estimates without creating spurious large
effects. Improving variant calling in already ``easy'' genomic regions
yields diminishing returns compared to simply increasing sample size.

For rare variants with large individual effects, the dynamics change
substantially. Loss-of-function variants, damaging missense mutations,
and splice-altering changes can have substantial effects on disease
risk. Here, false negatives dominate the problem: if the variant is
never called, its effect is invisible to association tests and to models
trained on called genotypes. Small improvements in recall for clinically
important rare variants can have outsized impact on gene discovery and
interpretation.

Imputed variants introduce their own effect size modulation. The squared
correlation between true and imputed genotypes acts as an attenuation
factor: an association with true effect size \(\beta\) behaves as if the
effect were approximately \(r^2 \beta\) in downstream analyses using
imputed dosages. Improvements in imputation quality, particularly for
underrepresented ancestries where current panels perform poorly,
directly scale the effective signals that models can learn.

Clinically critical loci often present the most challenging technical
contexts, creating a systematic mismatch between importance and data
quality. Pharmacogenomic variants in CYP gene families, immune-related
variants in HLA, and many other medically actionable sites reside in
regions where standard pipelines perform poorly. Global accuracy metrics
may change only slightly when these regions improve, but the clinical
impact can be substantial.

\section{The Reliability Landscape}\label{the-reliability-landscape}

Variant calling produces the substrate on which every subsequent model
operates. The quality of that substrate varies systematically:
high-confidence calls in unique sequence with adequate coverage,
uncertain calls in repetitive regions and structural variant
breakpoints, systematic gaps where short reads cannot reach. These
patterns are not random. They concentrate in genomic regions of
particular biological importance: segmental duplications that drive gene
family evolution, tandem repeats that modulate gene expression, HLA
haplotypes that determine immune response.

DeepVariant and related approaches demonstrate that learned
representations can outperform hand-crafted heuristics, at least on
established benchmarks. This paradigm recurs throughout the book:
sequence-to-function models that learn regulatory grammar directly from
data (Chapter~\ref{sec-regulatory}), splice predictors that discover
motifs without explicit encoding (Chapter~\ref{sec-cnn}), language
models that learn evolutionary constraint from sequence alone
(Chapter~\ref{sec-dna-lm}). In each case, the central question is
whether the additional model capacity captures genuine biological signal
or overfits to artifacts in training data.

The models that follow inherit whatever systematic biases exist in
variant calls. A pathogenicity predictor trained on ClinVar labels
inherits the sequencing technologies and population composition that
generated those labels. A fine-mapping method that trusts variant calls
uniformly will miscalibrate its posterior probabilities in difficult
regions. Understanding where variant calling succeeds and fails is
prerequisite to understanding where downstream models can be trusted.

\chapter{The Data Landscape}\label{sec-data}

The models throughout this book learn from labels, and those labels come
from somewhere. A variant effect predictor trained on ClinVar
classifications learns whatever biases clinical laboratories embedded in
those classifications. A chromatin accessibility model trained on ENCODE
cell lines may fail on primary tissues absent from the training
compendium. A constraint metric derived from European-ancestry cohorts
will be poorly calibrated for variants private to other populations.
Every machine learning model in genomics inherits both the signal and
the systematic gaps of its training data. Understanding what genomic
resources contain, and what they systematically miss, is prerequisite to
interpreting what models learn.

No single dataset captures the complexity of genomic function. The field
depends on a mosaic of complementary resources: reference genomes and
gene annotations that define the coordinate system, population variant
catalogs that reveal what survives in healthy individuals, biobank
datasets that link genetic variation to phenotypes at scale, functional
genomics atlases that map biochemical activity across cell types and
conditions, and clinical databases that aggregate expert variant
interpretations. Each resource contributes a different type of evidence.
Reference genomes provide the scaffold against which all variants are
defined. Population databases like gnomAD establish baseline
expectations for variant frequency. Functional assays from ENCODE and
Roadmap Epigenomics indicate where the genome shows evidence of
regulatory activity. Clinical databases like ClinVar provide
ground-truth labels for pathogenic and benign variants, at least for the
subset of variants that have been expertly reviewed.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-data-ecosystem}{[}Essential{]} Conceptual map
showing how major data resources interconnect and flow into downstream
applications. Organize into layers: (1) Foundation layer: Reference
genomes (GRCh38, T2T-CHM13) and gene annotations (GENCODE, RefSeq,
MANE); (2) Population layer: Variant catalogs (gnomAD, 1000 Genomes) and
biobanks (UK Biobank, All of Us); (3) Functional layer: ENCODE, Roadmap
Epigenomics, Cistrome, GTEx; (4) Clinical layer: ClinVar, ClinGen, OMIM,
PharmGKB. Draw arrows showing dependencies. Indicate which resources
provide ``coordinates,'' ``frequencies,'' ``functions,'' or ``labels.''}

\end{figure}%

This chapter surveys the major data resources that underpin genomic
machine learning, emphasizing what each resource measures, how it was
constructed, and where its coverage breaks down. The goal is not
encyclopedic completeness but critical literacy: the ability to
recognize when a model's training data may not represent the population,
condition, or variant class at hand. This literacy becomes essential
when deploying models in clinical contexts where failures have
consequences.

\section{Reference Genomes and Gene
Annotations}\label{reference-genomes-and-gene-annotations}

A family arrives at a genetics clinic after their newborn's screening
reveals a potential metabolic disorder. The clinical team orders
whole-genome sequencing and receives a report identifying a novel
variant in a gene associated with the condition. The variant's
coordinates place it at the boundary between an exon and an intron,
potentially disrupting splicing. Yet whether this interpretation is
correct depends on decisions made years before the child was born: which
positions constitute exon boundaries, which transcript model defines the
canonical gene structure, and which sequence serves as the reference
against which ``variant'' is defined. Reference genomes and gene
annotations are so foundational that their assumptions often become
invisible, yet every downstream analysis inherits the choices embedded
in these resources. A model cannot learn about a regulatory element for
a transcript that does not exist in the annotation.

\subsection{Reference Assemblies}\label{reference-assemblies}

A patient's clinical sequencing reveals a potentially pathogenic variant
in a duplicated region of chromosome 17. The variant calling pipeline
reports a confident genotype, the annotation tool predicts a frameshift,
and the clinical team prepares to discuss the finding with the family.
Yet the ``variant'' may be an artifact of misalignment: reads from a
paralogous sequence elsewhere in the genome mapped incorrectly because
the reference assembly collapsed two distinct loci into one. Whether
this error occurs, whether it can be detected, and whether the clinical
interpretation has any foundation in biological reality all depend on
the choice of reference genome.

Most modern pipelines align reads to a small number of reference
assemblies, predominantly GRCh38 or the newer T2T-CHM13 (Nurk et al.
2022). A reference genome is not simply a consensus sequence; it encodes
a series of consequential decisions about how to represent duplications,
alternate haplotypes, and unresolved gaps. These decisions determine
which regions are mappable by short reads, how structural variants are
represented, and how comparable results will be across cohorts built on
different assemblies.

Graph-based and \textbf{pangenome} references relax the assumption of a
single linear reference, representing multiple haplotypes and ancestries
within a unified coordinate system (Liao et al. 2023). Comparative
multi-species references, such as those used in mammalian constraint
maps from the Zoonomia consortium (Sullivan et al. 2023), extend this
idea across species, providing evolutionary conservation scores that
feed directly into deleteriousness predictors and gene-level constraint
metrics.

For most datasets used in this book, the practical reality is still
GRCh37 or GRCh38 coordinates, often with incremental patches. Models
trained on these resources therefore inherit their blind spots:
incomplete or collapsed segmental duplications, underrepresented
ancestries in pangenome construction, and uneven quality across
chromosomes and regions. These limitations concentrate in precisely the
regions where variant interpretation matters most (such as the HLA
locus, pharmacogenes with structural variation, and segmental
duplications harboring disease genes), creating a systematic mismatch
between clinical importance and reference quality.

\subsection{Gene Models}\label{gene-models}

A child presents with developmental delay and muscle weakness.
Whole-genome sequencing identifies a novel variant near the \emph{DMD}
gene, which encodes dystrophin and causes Duchenne muscular dystrophy
when disrupted. The annotation pipeline reports the variant as intronic
and unlikely to affect protein function. Yet \emph{DMD} spans 2.2
megabases and includes 79 exons with complex alternative splicing;
whether this variant disrupts a tissue-specific isoform depends entirely
on which transcript model the annotation tool uses. The clinical
implications are entirely different, yet the underlying sequence is
identical: only the annotation changes.

Gene annotation databases such as GENCODE and RefSeq define the
biological vocabulary overlaid on reference coordinates: exon-intron
structures, canonical and alternative transcripts, start and stop
codons, and untranslated regions (Frankish et al. 2019; O'Leary et al.
2016). These annotations distinguish coding from non-coding variants,
identify splice-disrupting mutations, and map functional genomics
signals to genes. They also establish the units (genes, transcripts,
exons) that downstream models implicitly operate on.

The MANE Select project provides a single matched transcript per
protein-coding gene that is identical between GENCODE and RefSeq,
simplifying clinical interpretation and variant reporting (Morales et
al. 2022). This standardization makes variant descriptions consistent
across laboratories, yet it privileges a single isoform over biological
complexity. In contexts where tissue-specific or developmentally
regulated isoforms drive disease (alternative splicing in muscular
dystrophies, isoform-specific expression in neuropsychiatric
conditions), the canonical transcript may miss the relevant biology.

New isoforms continue to be discovered, alternative splicing remains
incompletely cataloged, and cell-type-specific transcripts may be absent
from bulk-derived annotations. Non-coding RNA genes and pseudogenes are
even more unevenly annotated. These gaps propagate through every tool
built on them: variant effect predictors cannot score consequences for
transcripts that do not exist in their reference annotation, and
expression models cannot predict isoforms they were never trained on.

\section{Population Variant Catalogs and Allele
Frequencies}\label{population-variant-catalogs-and-allele-frequencies}

A clinical geneticist evaluates a child with an undiagnosed syndrome and
identifies a novel missense variant in a candidate gene. The question
that determines what happens next is deceptively simple: has anyone else
carried this variant? If the variant appears in thousands of healthy
adults, it is almost certainly benign. If it has never been observed
across hundreds of thousands of sequenced genomes, that absence becomes
evidence of selective pressure against the variant, strongly suggesting
functional consequence. Without population-scale variant catalogs, this
inference is impossible, and every rare variant would demand the same
level of scrutiny regardless of its actual likelihood of causing
disease.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-ancestry-representation}{[}High{]} Stacked bar chart
or waffle plot showing ancestry composition across key resources: gnomAD
v4, UK Biobank, ClinVar submissions, GTEx donors, GWAS Catalog
participants. Highlight the persistent European overrepresentation
(approximately 78\% of GWAS participants as of 2019) against global
population proportions. Include a small world map inset showing which
continental ancestries are represented vs underrepresented.}

\end{figure}%

\textbf{Allele frequency}, the proportion of chromosomes in a reference
population carrying a given variant, serves as one of the most powerful
priors in variant interpretation. Beyond simple filtering, allele
frequencies inform statistical frameworks for case-control association,
provide training signal for deleteriousness predictors, and enable
\textbf{imputation} of ungenotyped variants through \textbf{linkage
disequilibrium}. The catalogs described below have progressively
expanded in sample size, ancestral diversity, and annotation depth,
transforming variant interpretation from an ad hoc exercise into a
quantitative discipline.

A crucial nuance shapes everything that follows: these catalogs record
variants that are compatible with being sampled in the first place.
Gene-lethal variants that cause embryonic death or severe childhood
disease rarely appear, even when they are biologically informative.
Variants causing late-onset conditions (Alzheimer's risk alleles,
adult-onset cancer predisposition) can persist at appreciable
frequencies because selection has not had time to remove them.
Throughout this book, models trained on population data can only learn
from variants present in these catalogs, which means they systematically
underrepresent the most severe loss-of-function mutations.

\subsection{dbSNP and Variant
Identifiers}\label{dbsnp-and-variant-identifiers}

Two laboratories sequence the same patient and report their findings to
a tumor board. Laboratory A describes a variant using genomic
coordinates on GRCh38; Laboratory B uses HGVS nomenclature relative to a
specific transcript. Are they discussing the same variant? Without
standardized identifiers, this simple question can consume hours of
manual reconciliation. The database of Single Nucleotide Polymorphisms
(dbSNP) provides the common currency that cuts through this ambiguity:
stable identifiers (rsIDs) that enable integration across tools and
publications (Sherry et al. 2001).

When a laboratory reports a variant, when a researcher publishes a GWAS
finding, and when a clinician queries a pathogenicity database, they
need a common language to ensure they are discussing the same genomic
position. Modern whole-exome and whole-genome sequencing routinely
discovers millions of previously unseen variants per large cohort, but
dbSNP identifiers remain the standard way to reference known
\textbf{single nucleotide polymorphisms (SNPs)} and link disparate
resources. When a GWAS publication reports an association at rs12345,
that identifier traces back to dbSNP and enables integration with
functional annotations, clinical databases, and other catalogs
throughout this chapter.

\subsection{1000 Genomes and Early Reference
Panels}\label{genomes-and-early-reference-panels}

Genotyping arrays measure only a sparse subset of genomic positions, yet
disease-associated variants may lie anywhere in the genome. How can
researchers infer variants at unmeasured positions? The answer lies in
patterns of co-inheritance: variants that travel together on ancestral
chromosome segments can be inferred from neighboring measured positions.
This process of imputation depends entirely on having reference panels
that capture the haplotype structure of the population being studied.

The 1000 Genomes Project provided one of the first widely used
multi-population panels for imputation, sampling individuals from
African, European, East Asian, South Asian, and admixed American
populations (Auton et al. 2015). The resulting haplotype structure
underlies many imputation servers and downstream analyses, enabling
genotyping arrays with millions of markers to impute tens of millions of
untyped variants through linkage disequilibrium (Yun et al. 2021).
Although its sample size (approximately 2,500 individuals) is modest by
current standards, 1000 Genomes established the template for how to
build and distribute multi-population reference panels, and its samples
continue to serve as benchmarks for variant calling performance.

\subsection{Genome Aggregation Database
(gnomAD)}\label{genome-aggregation-database-gnomad}

A clinical geneticist evaluates a child with unexplained developmental
delay. Exome sequencing reveals a missense variant in a
neurodevelopmental gene. Is this the cause? The answer depends
critically on whether the variant has been observed in healthy
individuals. If it appears in thousands of people without developmental
delay, it is unlikely to be pathogenic. If it has never been observed
across hundreds of thousands of sequenced genomes, its absence becomes
evidence of selective pressure against the variant. This filtering logic
requires population-scale variant catalogs with sufficient sample size
and ancestral diversity to distinguish genuinely rare variants from
those that are simply common in underrepresented populations.

The Genome Aggregation Database (gnomAD) aggregates exome and genome
sequencing data from research and clinical cohorts worldwide into
harmonized allele frequency resources spanning hundreds of thousands of
individuals (Karczewski et al. 2020). gnomAD provides high-resolution
allele frequencies stratified by genetic ancestry, enabling
population-matched filtering that accounts for variants common in one
ancestry but rare in others. This stratification matters because a
variant observed at 1\% frequency in African populations but absent from
European cohorts would be incorrectly flagged as ultra-rare by a model
trained predominantly on European data.

gnomAD also introduced \textbf{constraint metrics} that have become
standard features in variant prioritization. The probability of
loss-of-function intolerance (pLI) and loss-of-function
observed/expected upper bound fraction (LOEUF) summarize how depleted a
gene is for protein-truncating variants relative to expectation. Genes
essential for viability show far fewer loss-of-function variants than
neutral mutation rates would predict; this depletion provides evidence
of selective constraint that transfers to variant interpretation. A
novel truncating variant in a highly constrained gene warrants more
concern than the same variant class in an unconstrained gene.

These resources are indispensable for filtering common variants in
Mendelian disease diagnostics, distinguishing ultra-rare variants from
recurrent ones, and providing population genetics priors for
deleteriousness scores like CADD (Rentzsch et al. 2019; Schubach et al.
2024). At the same time, they reflect the composition of the cohorts
they aggregate: ancestry representation remains uneven despite ongoing
efforts, structural variants and repeat expansions are less completely
cataloged than SNVs and short indels, and individuals with severe
early-onset disease are underrepresented by design. These biases
propagate into every model that uses gnomAD frequencies or constraint
scores as features.

\section{Cohorts, Biobanks, and GWAS Summary
Data}\label{cohorts-biobanks-and-gwas-summary-data}

A pharmaceutical company developing a new cardiac drug needs to
understand which genetic variants influence drug response. A health
system implementing pharmacogenomic testing needs to know which patients
are at risk for adverse reactions. A researcher studying the genetics of
depression needs cases and controls with standardized phenotyping. None
of these questions can be answered by sequencing alone; they require
linking genetic variation to phenotypes at scale, across thousands or
hundreds of thousands of individuals. Yet assembling such cohorts
introduces its own biases: participants must consent, provide samples,
and have phenotypes recorded in standardized ways. The populations
enrolled in major biobanks reflect patterns of healthcare access,
research infrastructure, and historical priorities that do not represent
global genetic diversity.

The overrepresentation of European-ancestry individuals in most major
biobanks creates systematic gaps in variant discovery, effect-size
estimation, and \textbf{polygenic score} portability that propagate
through downstream analyses (Sirugo, Williams, and Tishkoff 2019). A
variant common in West African populations may be absent or rare in
European-dominated catalogs, rendering it invisible to association
studies and underrepresented in predictive models. This tension between
scientific utility and representational equity shapes every
biobank-derived resource in this chapter and is discussed in detail in
Chapter~\ref{sec-confounding}.

\subsection{Large Population Cohorts}\label{large-population-cohorts}

A variant that increases heart disease risk by 5\% requires
approximately 50,000 cases and controls to detect reliably at
genome-wide significance. A variant with a 1\% effect on a continuous
trait like blood pressure demands even larger samples. The fundamental
constraint is statistical: small effect sizes, which characterize most
common variant associations, disappear into noise without massive sample
sizes. This explains why genetic discovery accelerated dramatically when
biobanks reached the scale of hundreds of thousands of participants.

UK Biobank, with approximately 500,000 participants and deep phenotyping
across thousands of traits, has become a dominant resource for methods
development and benchmarking (Bycroft et al. 2018). FinnGen leverages
Finland's population history and unified healthcare records for
large-scale disease association discovery (Kurki et al. 2023). The All
of Us Research Program prioritizes diversity, aiming to enroll one
million participants with deliberate oversampling of historically
underrepresented groups (All of Us Research Program Investigators 2019).
deCODE genetics has genotyped a substantial fraction of Iceland's
population, enabling unique studies of rare variants and founder effects
in a population with detailed genealogical records
(\textbf{gudbjartsson\_decode\_2015?}). Additional resources include the
Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie
Biobank, and emerging African genomics initiatives such as H3Africa
(Sirugo, Williams, and Tishkoff 2019).

Together, these efforts enable \textbf{genome-wide association studies
(GWAS)} for thousands of traits, development and evaluation of polygenic
scores, and fine-mapping of causal variants and genes (Marees et al.
2018; Mountjoy et al. 2021). From a modeling perspective, they provide
the large-scale genotype-phenotype matrices that power architectures
ranging from classical linear mixed models to foundation models trained
on biobank-scale data. The practical reality for most GWAS and polygenic
score methods in Chapter~\ref{sec-gwas} is data from either array
genotyping with imputation or whole-exome/whole-genome sequencing with
joint calling, as in DeepVariant/GLnexus-style pipelines (Yun et al.
2021).

\subsection{GWAS Summary Statistics}\label{gwas-summary-statistics}

Individual-level genotype and phenotype data are powerful but sensitive.
Sharing such data across institutions requires complex data use
agreements, institutional review board approvals, and secure computing
infrastructure. These barriers would slow scientific progress if every
analysis required access to raw data. Summary statistics offer an
alternative: per-variant effect sizes, standard errors, and p-values
that capture the essential association signal without revealing
individual genotypes.

The GWAS Catalog compiles published results across thousands of traits
(Sollis et al. 2023), while the PGS Catalog provides curated polygenic
score weights and metadata for reproducibility (Lambert et al. 2021).
Frameworks like Open Targets Genetics integrate fine-mapped signals with
functional annotations to prioritize candidate causal genes at
associated loci (Mountjoy et al. 2021).

Summary statistics enable meta-analysis across cohorts without sharing
individual-level data, transfer of genetic findings to new populations
through methods like PRS-CSx, and integration with functional
annotations to distinguish causal variants from linked bystanders. For
deep learning, summary statistics provide a sparse, trait-level view of
the genome that can be combined with richer sequence-based or functional
labels, though the sparsity and noise in GWAS signals pose challenges
that differ from the dense labels available in functional genomics.

\section{Functional Genomics and Regulatory
Landscapes}\label{functional-genomics-and-regulatory-landscapes}

Protein-coding exons constitute roughly 1.5\% of the human genome, yet
most disease-associated variants from GWAS fall outside coding regions.
A massive study identifies 100 loci associated with schizophrenia, but
90 of them lie in non-coding regions with no obvious connection to any
gene. This mismatch creates a fundamental interpretability problem: we
can identify non-coding loci that harbor disease risk, but we cannot
easily determine which base pairs matter, which genes they regulate, or
in which cell types they act. Understanding these non-coding variants
requires mapping the regulatory logic that governs when, where, and how
much each gene is expressed. Functional genomics assays provide this
map, identifying \textbf{transcription factor} binding sites, nucleosome
positioning, \textbf{chromatin accessibility}, \textbf{histone
modifications}, and three-dimensional genome organization across cell
types and conditions.

For this book, functional genomics datasets serve a dual role. First,
they supply the biological vocabulary for interpreting non-coding
variants, linking sequence changes to potential regulatory consequences.
Second, and more directly, they provide the training labels for
sequence-to-function deep learning models. When a model learns to
predict chromatin accessibility or histone marks from DNA sequence
alone, it compresses into its parameters the regulatory code implicit in
thousands of functional genomics experiments.

\subsection{ENCODE, Roadmap, and Related
Consortia}\label{encode-roadmap-and-related-consortia}

A single ChIP-seq experiment for one transcription factor in one cell
line provides useful signal, but models that learn general regulatory
grammar require thousands of such experiments spanning many factors,
marks, and cell types. A researcher training a regulatory model on her
own laboratory's data will produce a model that works well in her
specific experimental context but fails to generalize. The key insight
behind ENCODE and Roadmap was that coordinated experimental campaigns,
with standardized methods and quality control, could create reference
datasets serving the entire field.

The Encyclopedia of DNA Elements (ENCODE) and Roadmap Epigenomics
consortia designed coordinated experimental campaigns that profiled
transcription factor binding (ChIP-seq), histone modifications,
chromatin accessibility (DNase-seq, ATAC-seq), and chromatin
conformation (Hi-C) across cell lines and primary tissues (Kagda et al.
2025; Kundaje et al. 2015). Gene Expression Omnibus (GEO) archives these
and many other functional genomics datasets with standardized metadata
(Edgar, Domrachev, and Lash 2002).

The significance of these consortia lies less in any individual
experiment than in the scale and standardization they provide. By
generating hundreds of assays across dozens of cell types with
consistent protocols, ENCODE and Roadmap created canonical reference
datasets that define the regulatory landscape for the cell types they
profiled. Many regulatory deep learning models in
Chapter~\ref{sec-regulatory} and Chapter~\ref{sec-cnn} are effectively
trained on these resources, learning to predict multi-task label vectors
where each task corresponds to a ChIP-seq or accessibility experiment.
The consequence is that models trained on ENCODE data inherit ENCODE's
choices about which cell types, factors, and experimental conditions
merit inclusion.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-functional-genomics-matrix}{[}High{]} Heatmap-style
visualization showing the ENCODE/Roadmap data compendium structure. Rows
represent cell types or tissues (group by category: cell lines, primary
cells, tissues). Columns represent assay types (ChIP-seq for various TFs
and histone marks, DNase-seq, ATAC-seq, RNA-seq). Color intensity
indicates data availability (present/absent or coverage depth).
Highlight which cell types have comprehensive coverage vs sparse
coverage. Annotate example cell types that are well-profiled (K562,
GM12878, HepG2) vs disease-relevant tissues that remain undersampled.}

\end{figure}%

\subsection{Cistrome Data Browser}\label{cistrome-data-browser}

ENCODE and Roadmap provide authoritative datasets for their chosen cell
types and factors, but they represent only a fraction of publicly
available functional genomics experiments. A researcher interested in a
specific transcription factor or a disease-relevant cell type may find
that ENCODE lacks the relevant data, even though dozens of laboratories
have generated relevant ChIP-seq experiments. These experiments exist
scattered across GEO with heterogeneous processing and quality.

The Cistrome Data Browser addresses this gap by aggregating thousands of
human and mouse ChIP-seq and chromatin accessibility datasets from
ENCODE, Roadmap, GEO, and individual publications into a uniformly
reprocessed repository (R. Zheng et al. 2019). All datasets pass through
standardized quality control and peak calling, enabling comparisons
across experiments originally generated with different protocols.

Cistrome provides uniform peak calls, signal tracks, and metadata for
cell type, factor, and experimental conditions. The tradeoff is
heterogeneity: while reprocessing harmonizes computational steps, the
underlying experiments vary in sample preparation, antibody quality,
sequencing depth, and experimental design. Cistrome expands coverage at
the cost of the tight experimental control found in the primary
consortia, a tradeoff that matters when models learn from noisy or
inconsistent labels.

\subsection{From Assays to Training
Labels}\label{from-assays-to-training-labels}

Sequence-to-function models transform functional genomics resources into
supervised learning problems. Models like DeepSEA (see
Chapter~\ref{sec-regulatory}) draw training labels from ENCODE, Roadmap,
and Cistrome-style datasets: each genomic window is associated with
binary or quantitative signals indicating transcription factor binding,
histone modifications, or chromatin accessibility across hundreds of
assays and cell types (J. Zhou and Troyanskaya 2015; J. Zhou et al.
2018).

The quality, coverage, and biases of these labels directly constrain
what models can learn. Cell types absent from the training compendium
cannot be predicted reliably. Factors with few high-quality ChIP-seq
experiments will have noisier labels. Systematic differences between
assay types (binary peak calls versus quantitative signal tracks) shape
whether models learn to predict occupancy, accessibility, or something
in between. These considerations become central when examining model
architectures and training strategies in Chapter~\ref{sec-regulatory}.

\subsection{Deep Mutational Scanning and Multiplexed Variant
Assays}\label{deep-mutational-scanning-and-multiplexed-variant-assays}

Population variant catalogs tell us which variants survive in healthy
individuals, but they cannot tell us what happens when a specific amino
acid is changed to every possible alternative. Functional genomics
experiments reveal where the genome is active, but they do not directly
measure the consequence of each possible mutation. \textbf{Deep
mutational scanning (DMS)} fills this gap by measuring the fitness or
functional impact of thousands of protein or regulatory variants in a
single experiment.

These assays systematically introduce mutations (often approaching
saturation mutagenesis for a protein domain or regulatory element),
subject the resulting library to selection or screening, and use
sequencing to quantify the representation of each variant before and
after selection. The result is dense, quantitative measurements of
variant effects under controlled conditions. Benchmarks such as
ProteinGym compile large DMS datasets across proteins to evaluate
variant effect predictors (Notin et al. 2023), while TraitGym curates
multiplexed reporter assays and other high-throughput readouts of
regulatory variant effects (Benegas, Eraslan, and Song 2025).

These resources sit at the interface between genomic and protein-level
modeling. They provide dense, quantitative labels for synthetic or
near-saturated variant libraries, complementing the sparse, naturally
occurring variation in gnomAD and biobanks. DMS data differ
fundamentally from population catalogs: they measure functional impact
directly under controlled conditions rather than inferring it from
population survival. Later chapters on protein sequence models and
regulatory variant prediction return to these DMS-style datasets as key
benchmarks and training sources.

\section{Expression and eQTL
Resources}\label{expression-and-eqtl-resources}

Functional genomics assays reveal where transcription factors bind and
which chromatin regions are accessible, but they do not directly answer
the downstream question: does regulatory activity actually change how
much RNA a gene produces? A transcription factor may bind a genomic
region without altering expression of nearby genes; an accessible
chromatin region may not contain active regulatory elements. Regulatory
binding and gene expression exist in a many-to-many relationship that
cannot be resolved by either measurement alone. Expression datasets
complete this link, measuring transcript abundance across tissues, cell
types, and genetic backgrounds.

\textbf{Expression quantitative trait loci (eQTLs)} formalize the
genotype-expression relationship statistically, identifying genetic
variants associated with changes in transcript levels. For variant
interpretation, eQTLs offer mechanistic hypotheses connecting non-coding
variants to specific genes and tissues: if a GWAS signal colocalizes
with an eQTL for a nearby gene in a disease-relevant tissue, that gene
becomes a candidate effector. For model training, expression data
provide quantitative labels that integrate across many regulatory inputs
converging on a single promoter.

\subsection{Bulk Expression Atlases}\label{bulk-expression-atlases}

A GWAS identifies a locus associated with coronary artery disease in a
non-coding region. Dozens of genes lie within the associated interval.
Which one mediates the disease risk? If the lead variant also associates
with expression of a nearby gene specifically in arterial endothelial
cells, that gene becomes the prime candidate. Without tissue-specific
expression data linked to genotypes, this inference is impossible.

The Genotype-Tissue Expression (GTEx) consortium provides the most
comprehensive resource linking genetic variation to gene expression
across human tissues, with RNA-seq profiles from 948 post-mortem donors
across 54 tissues (The GTEx Consortium 2020). GTEx established
foundational insights that inform models throughout this book: most
genes harbor tissue-specific eQTLs, regulatory variants typically act in
cis over distances of hundreds of kilobases, and expression variation
explains a meaningful fraction of complex trait heritability.

GTEx underlies expression prediction models such as PrediXcan, which
trains tissue-specific models to impute gene expression from genotypes
alone (Gamazon et al. 2015). Transcriptome-wide association studies
(TWAS) extend this idea to associate imputed expression with phenotypes
(Gusev et al. 2016). Colocalization methods ask whether a GWAS signal
and an eQTL share the same causal variant, providing evidence that the
associated gene mediates the trait effect.

The GTEx design has limitations worth acknowledging. Post-mortem
collection introduces agonal stress artifacts that may not reflect
living tissue biology. Sample sizes vary considerably across tissues
(hundreds for some, dozens for others), affecting statistical power.
Some disease-relevant tissues, such as pancreatic islets or specific
brain subregions, remain undersampled. Complementary resources like the
eQTLGen Consortium aggregate eQTL results from blood across much larger
sample sizes, trading tissue diversity for statistical power (VÃµsa et
al. 2021).

\subsection{Single-Cell and Context-Specific
Expression}\label{single-cell-and-context-specific-expression}

Bulk RNA-seq averages expression across all cells in a tissue sample,
obscuring the cell-type-specific programs that often mediate disease
biology. A bulk eQTL in brain tissue might reflect astrocytes, neurons,
microglia, or oligodendrocytes; the causal cell type matters for
understanding mechanism. This averaging creates a fundamental resolution
problem: variants may have strong effects in rare cell populations that
are diluted to undetectability when mixed with other cell types.

Single-cell RNA-seq resolves this heterogeneity, identifying expression
signatures for individual cell types, rare populations, and transitional
states. Large-scale efforts including the Human Cell Atlas and Tabula
Sapiens are building reference atlases that catalog cell types across
organs and developmental stages (Regev et al. 2017; The Tabula Sapiens
Consortium 2022). For variant interpretation, single-cell data enable
cell-type-specific eQTL mapping, revealing that a variant may influence
expression in one cell type but not others within the same tissue.
Spatial transcriptomics adds anatomical context, preserving tissue
architecture while measuring gene expression.

These technologies introduce computational challenges: sparsity from
dropout effects, batch variation across samples and technologies, and
massive scale with millions of cells per study. They also offer an
increasingly fine-grained view of the link between genotype, regulatory
state, and cellular phenotype. Single-cell and spatial resources appear
primarily in later chapters on multi-omics integration and systems-level
models.

\section{Variant Interpretation Databases and Clinical
Labels}\label{variant-interpretation-databases-and-clinical-labels}

A family receives whole-exome sequencing results for their child with
developmental delay. The laboratory report lists 50 rare variants in
genes associated with neurodevelopmental disorders. For each variant,
the clinical team must answer: is this the cause? Allele frequencies
tell us what variants survive in healthy populations, and functional
genomics data reveal where the genome is biochemically active, but
neither directly answers this question. That determination requires
integrating multiple lines of evidence (family segregation, functional
assays, computational predictions, phenotypic observations) into a
structured framework that can be applied consistently.

Clinical variant interpretation databases aggregate these assessments
from laboratories, expert panels, and research groups. These databases
have become critical infrastructure for both clinical genomics and
computational method development, providing labels that inform
diagnostic decisions and serve as training data for machine learning
models. Their labels carry biases and circularity that propagate through
any analysis built on them, yet no viable alternative exists for
large-scale model training and evaluation.

\subsection{ClinVar and Clinical
Assertions}\label{clinvar-and-clinical-assertions}

A clinical laboratory sequences a patient with suspected hereditary
cancer syndrome and identifies a missense variant in \emph{BRCA2}.
Before returning results, the laboratory searches ClinVar and finds that
three other laboratories have evaluated this variant: two classified it
as likely pathogenic, one as a variant of uncertain significance. How
should this conflicting evidence inform the final report? ClinVar
aggregates assertions of variant pathogenicity from clinical
laboratories and researchers worldwide, making it the central
clearinghouse for clinical variant interpretations (Landrum et al.
2018).

ClinVar provides standardized classifications following ACMG/AMP
guidelines (pathogenic, likely pathogenic, benign, likely benign,
\textbf{variant of uncertain significance}) that are central to
diagnostic pipelines and to benchmarking variant effect predictors. It
has become the de facto reference for variant pathogenicity labels, but
its contents reflect systematic biases that affect any downstream use.
These biases operate at multiple levels and warrant careful
consideration.

Submission heterogeneity poses a fundamental challenge. Annotations come
from diverse submitters, including diagnostic laboratories, research
groups, expert panels, and database exports. Submitters apply varying
evidentiary standards; some provide detailed supporting evidence while
others offer only assertions. Conflicting interpretations are common,
particularly for variants of uncertain significance.

Version sensitivity means that classifications evolve as evidence
accumulates. A variant classified as VUS in 2018 may be reclassified as
likely pathogenic by 2023 based on new functional studies or additional
patient observations. Models trained on historical ClinVar snapshots may
learn outdated classifications. When reporting performance, specifying
the ClinVar version used is essential for reproducibility.

Ancestry and gene coverage biases create uneven representation. Variants
in well-studied populations (particularly European ancestry) and
well-characterized disease genes are heavily overrepresented. Variants
from underrepresented populations are more likely to remain classified
as VUS due to insufficient evidence. This creates feedback loops:
predictive models perform better on European-ancestry variants because
training data is richer, reinforcing the disparity (Landrum et al.
2018).

Circularity with computational predictors represents a subtle but
important concern. Clinical submissions increasingly incorporate
computational scores like CADD, REVEL, and AlphaMissense as supporting
evidence for pathogenicity classification. When these same ClinVar
labels are then used to train or evaluate computational predictors,
circularity emerges (Schubach et al. 2024). If a laboratory used a high
CADD score as supporting evidence for classifying a variant as likely
pathogenic, and that variant later appears as a positive label in
ClinVar, models trained on ClinVar may partly learn to reproduce CADD
itself rather than discovering independent signal. This circularity
operates at two levels: evaluation circularity (when models are assessed
on benchmarks influenced by the model's own predictions) and training
circularity (when features used in training derive from the same
underlying information as the labels). Both forms inflate apparent
performance without demonstrating genuine predictive power. We return to
these issues in Chapter~\ref{sec-vep-classical} and
Chapter~\ref{sec-confounding}.

Variants of uncertain significance constitute the majority of rare
variant classifications, reflecting genuinely limited evidence. These
variants are both targets for predictive modeling (can computational
methods resolve uncertainty?) and potential pitfalls (models trained
only on confidently classified variants may not generalize to VUS with
different characteristics).

Despite these limitations, ClinVar remains invaluable. The key is using
it appropriately: recognizing biases when training models, accounting
for version differences when comparing studies, stratifying performance
by ancestry and gene coverage, and treating computational predictions as
one line of evidence rather than definitive classifications.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-clinvar-landscape}{[}High{]} Three-panel figure.
Panel A: Pie or bar chart showing distribution of ClinVar
classifications (Pathogenic, Likely Pathogenic, VUS, Likely Benign,
Benign, Conflicting). Highlight that VUS dominates. Panel B: Heatmap
showing classification density by gene, with well-studied genes (BRCA1,
BRCA2, CFTR) having many submissions vs sparse coverage elsewhere. Panel
C: Timeline showing how classifications evolve (example of a variant
reclassified from VUS to Pathogenic over time, illustrating version
sensitivity).}

\end{figure}%

\subsection{Complementary Clinical
Databases}\label{complementary-clinical-databases}

ClinVar's open-access model and broad submission base make it the most
widely used resource, but it is not the only source of clinical variant
interpretations. The Human Gene Mutation Database (HGMD) maintains a
curated collection of disease-causing mutations compiled from the
published literature, with particular depth in rare Mendelian disorders
(\textbf{stenson\_hgmd\_2017?}). HGMD's professional version includes
variants not yet publicly released, and its curation emphasizes
literature-reported pathogenic variants rather than the full spectrum of
classifications in ClinVar. The Leiden Open Variation Database (LOVD)
takes a gene-centric approach, with individual databases maintained by
gene experts who curate variants according to locus-specific knowledge
(\textbf{fokkema\_lovd\_2011?}). LOVD instances often capture variants
and functional evidence specific to particular disease communities that
may not appear in broader databases.

These resources complement ClinVar in important ways: HGMD provides
literature-derived pathogenic variants that may precede ClinVar
submissions, while LOVD captures expert knowledge from disease-specific
research communities. For model development and benchmarking, awareness
of these alternative sources matters because training exclusively on
ClinVar may miss variants documented elsewhere, and apparent novel
predictions may simply reflect incomplete training data rather than
genuine generalization.

\subsection{ClinGen and Expert
Curation}\label{clingen-and-expert-curation}

Clinical laboratories submitting to ClinVar vary enormously in expertise
and evidentiary standards. A submission from a general diagnostic
laboratory applying ACMG guidelines to an unfamiliar gene may differ
substantially from an assessment by researchers who have studied that
gene for decades. The Clinical Genome Resource (ClinGen) addresses this
heterogeneity by providing expert-curated assessments at multiple levels
(Rehm et al. 2015).

ClinGen expert panels evaluate \textbf{gene-disease validity} (whether
variation in a gene can cause a specific disease) and \textbf{dosage
sensitivity} (whether haploinsufficiency or triplosensitivity leads to
clinical phenotypes). These evaluations build on the catalog of
Mendelian phenotypes maintained by OMIM, which provides curated
gene-disease associations and clinical synopses (Amberger et al. 2015).

For individual variants, ClinGen Variant Curation Expert Panels apply
ACMG/AMP criteria systematically, assigning levels of evidence for
pathogenicity or benignity. The FDA has recognized these curations as
valid scientific evidence for clinical validity (Pejaver et al. 2022).
ClinGen also develops calibrated thresholds for computational
predictors, specifying score intervals that justify different strengths
of evidence (supporting, moderate, strong) for pathogenicity or
benignity. These calibrations directly inform how computational scores
should be incorporated into variant classification workflows.

\subsection{Pharmacogenomics
Resources}\label{pharmacogenomics-resources}

Most variant interpretation focuses on rare mutations that cause or
predispose to disease. \textbf{Pharmacogenomics} presents a different
paradigm: common polymorphisms that individually may have no disease
consequences but profoundly influence how individuals respond to
medications. These variants matter not because they cause disease but
because they determine whether a drug will work, fail, or cause harm.

ClinPGx integrates the PharmGKB knowledge base, CPIC clinical
guidelines, and PharmCAT annotation tool into a unified pharmacogenomics
resource (Whirl-Carrillo et al. 2012). The \emph{CYP2D6} gene, for
example, encodes a cytochrome P450 enzyme responsible for metabolizing
approximately 25\% of clinically used drugs, including codeine,
tamoxifen, and many antidepressants (Whirl-Carrillo et al. 2012).
Patients with loss-of-function \emph{CYP2D6} variants cannot activate
codeine to morphine, rendering the drug ineffective; patients with gene
duplications may experience dangerous opioid toxicity from standard
doses.

Star-allele haplotypes (combinations of variants that travel together on
a chromosome) determine metabolizer status, requiring phasing and
structural variant detection that extend beyond simple SNV calling. The
CPIC guidelines provide evidence-based recommendations for adjusting
drug selection or dosing based on pharmacogene diplotypes, and FDA drug
labels document regulatory recognition of these associations. From a
modeling perspective, pharmacogenomic resources offer a complementary
type of label linking variants to molecular and clinical outcomes
through different mechanisms than Mendelian disease pathogenicity.

\section{Inherited Constraints}\label{inherited-constraints}

Every model in this book inherits both the power and the biases of its
training data. A variant effect predictor trained on ClinVar labels
absorbs the ascertainment patterns of clinical sequencing: European
ancestry overrepresented, rare diseases enriched, incidental findings
undersampled. A chromatin model trained on ENCODE immortalized cell
lines learns regulatory patterns that may not generalize to primary
tissues with different epigenetic landscapes. A constraint model trained
on human population databases systematically misses gene-lethal variants
that never appear because carriers do not survive to be sequenced.

These biases compound as data flows through analysis pipelines. GWAS
summary statistics carry ancestry composition forward into polygenic
scores. Conservation scores calculated from biased multiple sequence
alignments propagate into variant effect predictions. Foundation model
pretraining on reference genomes from limited populations shapes the
representations available for all downstream applications. Each
transformation amplifies some biases while masking others, making the
provenance of model behavior increasingly difficult to trace.

The critical question is not whether models trained on these data
contain biases; they do. The question is whether those biases can be
characterized, bounded, and ultimately corrected. The chapters that
follow return repeatedly to these foundational datasets, sometimes as
training labels, sometimes as evaluation benchmarks, sometimes as both.
Recognizing when the same data sources appear in multiple roles is
essential for interpreting model performance honestly and anticipating
where generalization will fail.

\chapter{GWAS and Polygenic Scores}\label{sec-gwas}

Genome-wide association studies do not identify causal variants; they
identify signposts. When a GWAS reports that a particular SNP associates
with coronary artery disease, that SNP is almost certainly not the
variant that alters cardiac biology. It is correlated with the causal
variant through linkage disequilibrium, the non-random association of
nearby alleles that persists across generations. The statistical
machinery of GWAS is exquisitely sensitive to these correlations but
fundamentally agnostic about mechanism. It can identify a region of the
genome that harbors trait-relevant variation without distinguishing the
causal variant from its correlated neighbors, without explaining which
genes or pathways are affected, and without revealing whether the same
associations hold in populations with different linkage patterns.

This distinction between association and causation defines the central
intellectual challenge of statistical genetics. GWAS have identified
thousands of genomic regions associated with hundreds of complex traits,
from height and blood pressure to schizophrenia and type 2 diabetes.
These associations replicate across studies with remarkable consistency,
confirming that the signals are real. Yet the path from associated
region to biological mechanism remains obscure for most loci. The
majority of GWAS signals fall in non-coding regions where there is no
obvious gene to implicate. Even when a signal overlaps a gene, whether
it affects expression, splicing, or protein function is rarely apparent
from the association alone.

Polygenic scores aggregate these associations into predictions, summing
risk alleles across thousands of loci to estimate an individual's
genetic predisposition. For some traits, these scores achieve clinically
meaningful discrimination: individuals in the top percentile of coronary
artery disease risk have odds ratios comparable to monogenic familial
hypercholesterolemia. Yet polygenic scores inherit all the limitations
of the associations they aggregate. They predict without explaining,
correlate without identifying mechanism, and transfer poorly across
populations with different allele frequencies and linkage patterns.
Understanding both their power and their limitations is essential for
the mechanistic approaches developed throughout this book, where
regulatory sequence models and variant effect predictors attempt to move
from statistical association to biological explanation.

\section{The GWAS Framework}\label{the-gwas-framework}

Consider a clinician counseling a patient about cardiovascular disease
risk. Traditional risk factors (age, smoking, cholesterol, blood
pressure) explain roughly 50\% of the variation in who develops disease
(\textbf{khera\_genetics\_2017?}). Family history suggests that genetics
contributes substantially to the remainder, but which genetic variants
matter, and how much does each contribute? \textbf{Genome-wide
association studies (GWAS)} provide a systematic approach to answering
these questions by testing each of millions of variants for association
with the trait of interest.

The scale required for well-powered GWAS explains why large-scale
biobanks (Chapter~\ref{sec-data}) have become essential infrastructure
for statistical genetics. UK Biobank, with its 500,000 participants
genotyped across hundreds of thousands of variants and linked to
extensive phenotypic data, has enabled GWAS for thousands of traits at
sample sizes that were unimaginable a decade ago. Similar resources,
including the Million Veteran Program, FinnGen, and All of Us, continue
to expand the scope of discoverable associations. The biobank paradigm
of combining dense genotyping with rich phenotyping at population scale
has transformed GWAS from underpowered fishing expeditions into reliable
discovery engines.

The core logic is straightforward. For each variant in turn, researchers
ask whether individuals carrying more copies of a particular allele tend
to have higher or lower values of the phenotype (for quantitative
traits) or higher or lower probability of disease (for binary outcomes).
They estimate an effect size, compute a test statistic under the null
hypothesis of no association, and record a p-value. After testing
millions of variants, those exceeding a stringent significance threshold
are identified, the associated loci reported, and interpretation begins
regarding which genes and pathways might be involved.

This apparently simple procedure requires careful attention to study
design, quality control, and statistical modeling. The phenotype must be
measured consistently across individuals. The genotypes must be accurate
and the variants well-defined. Confounders that correlate with both
genotype and phenotype (most notably population structure) must be
controlled. Multiple testing across millions of variants demands
stringent significance thresholds. Only after addressing these
challenges can GWAS results be trusted and translated into downstream
applications.

\subsection{Association Models for Quantitative
Traits}\label{association-models-for-quantitative-traits}

Choosing the wrong statistical model for a GWAS does not merely
introduce imprecision; it distorts effect size estimates in ways that
propagate through every downstream analysis, from fine-mapping to
polygenic scores to drug target prioritization. A height GWAS and a
schizophrenia GWAS require fundamentally different approaches because
one outcome is continuous and the other binary. Applying linear
regression to a binary outcome produces fitted values outside the 0-1
probability range and effect estimates that misrepresent biological
reality.

For continuous phenotypes such as height, LDL cholesterol, or blood
pressure, the standard approach is linear regression. Let \(y_i\) denote
the phenotype for individual \(i\), and let \(g_{ij}\) denote the
genotype dosage at variant \(j\), encoded as 0, 1, or 2 copies of the
alternative allele (or as a fractional value for imputed genotypes). The
model is:

\[
y_i = \alpha + \beta_j g_{ij} + \gamma^\top c_i + \varepsilon_i
\]

The coefficient \(\beta_j\) represents the expected change in phenotype
per additional copy of the alternative allele, holding covariates
\(c_i\) fixed. When phenotypes are standardized to zero mean and unit
variance, \(\beta_j\) is expressed in standard deviation units per
allele. The vector \(c_i\) typically includes age, sex, genotyping
batch, and principal components capturing ancestry (discussed below).
The residual \(\varepsilon_i\) captures unexplained variation, assumed
to be independent and identically distributed across individuals.

For each variant, a test statistic is computed for the null hypothesis
\(H_0: \beta_j = 0\). In large samples, the t-statistic follows
approximately a standard normal distribution under the null, yielding a
two-sided p-value. With \(M\) variants tested (typically \(10^6\) to
\(10^7\) after imputation), multiple comparison correction is essential.
The conventional \textbf{genome-wide significance threshold} of
\(5 \times 10^{-8}\) approximates a Bonferroni correction for roughly
one million effectively independent tests, accounting for correlation
among variants due to linkage disequilibrium
(\textbf{risch\_future\_1996?}; Pe'er et al. 2008).

\subsection{Association Models for Disease
Outcomes}\label{association-models-for-disease-outcomes}

Binary outcomes create a specific statistical problem that, if ignored,
systematically distorts effect size estimates in ways that compound
through downstream applications. When the phenotype is disease status
(affected or unaffected), linear regression produces nonsensical
predictions: fitted values outside the 0-1 probability range and
residuals that violate normality assumptions. The consequence extends
beyond statistical inelegance. Effect sizes estimated under the wrong
model propagate into polygenic scores and risk prediction, potentially
misclassifying patients who sit near clinical decision thresholds where
intervention recommendations change.

For binary phenotypes, \textbf{logistic regression} replaces linear
regression. The model relates genotype to the log-odds of disease:

\[
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \beta_j g_{ij} + \gamma^\top c_i
\]

Here \(\beta_j\) is the log-odds ratio per allele, and \(\exp(\beta_j)\)
gives the \textbf{odds ratio (OR)}. An odds ratio of 1.2 means that each
additional copy of the alternative allele increases the odds of disease
by 20\%. For rare diseases (prevalence below approximately 10\%), odds
ratios approximate relative risks, but the distinction matters for
common conditions and when communicating absolute risk to patients.

Case-control sampling, in which cases are enriched relative to their
population frequency, distorts absolute risk estimates but preserves the
validity of odds ratio estimation. This mathematical property explains
why GWAS conducted in case-control designs can still produce effect
sizes useful for polygenic scores, provided downstream applications
account for baseline disease incidence. The likelihood function
conditions on disease status, making the odds ratio identifiable
regardless of sampling scheme.

\subsection{Visualizing Genome-Wide
Results}\label{visualizing-genome-wide-results}

The \textbf{Manhattan plot} has become the iconic visualization of GWAS
results, named for its resemblance to the New York City skyline. Each
point represents a tested variant, with genomic position along the
x-axis (ordered by chromosome) and negative log-transformed p-value on
the y-axis. Variants with stronger associations rise higher; those
exceeding the genome-wide significance threshold of \(5 \times 10^{-8}\)
(typically drawn as a horizontal line at
\(-\log_{10}(5 \times 10^{-8}) \approx 7.3\)) are considered significant
hits.

The Manhattan plot reveals both the successes and limitations of GWAS at
a glance. Prominent peaks indicate genomic regions harboring
trait-associated variants, but each peak typically contains dozens or
hundreds of correlated variants rather than a single causal nucleotide.
The width of peaks reflects local linkage disequilibrium structure:
broader peaks indicate regions where many variants are correlated with
the lead signal. The height reflects statistical strength, which depends
on effect size, allele frequency, and sample size. Tall, narrow peaks
suggest strong, well-localized signals; broad peaks spanning megabases
indicate that fine-mapping will be challenging.

Quantile-quantile (Q-Q) plots complement Manhattan plots by assessing
whether the observed p-value distribution matches theoretical
expectations under the null hypothesis. Systematic deviation from the
diagonal (genomic inflation) suggests either true polygenic signal or
residual confounding from population structure. The genomic inflation
factor \(\lambda\) quantifies this deviation, with values substantially
above 1.0 warranting investigation of potential confounders.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-manhattan-anatomy}{[}Essential{]} Annotated
Manhattan plot from a real or realistic GWAS (e.g., height or CAD). Key
annotations: (1) Genome-wide significance threshold at âˆ’logâ‚â‚€(5Ã—10â»â¸) â‰ˆ
7.3; (2) Example peak with lead SNP labeled; (3) Width of peak showing
LD extent (many correlated variants, not just one); (4) Chromosomes
color-alternated along x-axis; (5) Inset zoom on one peak showing how
multiple variants exceed threshold---illustrating that GWAS identifies
loci, not causal variants. Include Q-Q plot inset showing expected vs
observed p-value distribution with genomic inflation factor Î»
annotated.}

\end{figure}%

\subsection{Controlling for Population
Structure}\label{controlling-for-population-structure}

Population structure poses a fundamental challenge to GWAS
interpretation because it can generate association signals
indistinguishable from true biological effects. If allele frequencies
differ systematically across subpopulations and the phenotype also
varies across these groups for non-genetic reasons (differences in
environment, diet, healthcare access, socioeconomic status), naive
association testing will detect variants that mark ancestry rather than
causal biology. A variant that is simply more common in one population
will appear associated with any trait that differs between populations,
regardless of biological mechanism. The resulting false positives waste
resources on follow-up studies and, more insidiously, can embed
ancestry-related confounding into polygenic scores that are then
deployed as if they measured pure genetic risk.

\textbf{Principal component analysis (PCA)} on the genotype matrix
captures the major axes of genetic variation across individuals (Price
et al. 2006; Patterson, Price, and Reich 2006). The leading principal
components often correspond to continental ancestry gradients or
finer-scale population structure within a study. Including these PCs as
covariates in the regression model attenuates spurious associations
driven by ancestry stratification.

This correction is imperfect. Subtle structure not captured by the
included PCs, cryptic relatedness among individuals, and the
interweaving of genetic ancestry with environmental exposures all
complicate interpretation. The challenges extend far beyond technical
statistical adjustment: ancestry is entangled with healthcare access,
environmental exposures, and socioeconomic factors in ways that simple
covariate correction cannot fully resolve. These issues become critical
when translating GWAS results to clinical applications and when
evaluating whether polygenic scores perform equitably across
populations. We return to the full complexity of ancestry as a
confounder in Chapter~\ref{sec-confounding}.

\section{Heritability: What Genetics Can
Explain}\label{heritability-what-genetics-can-explain}

Before GWAS can identify specific variants, a more fundamental question
must be answered: how much of the variation in a trait is attributable
to genetics at all? A trait entirely determined by environment would
yield no GWAS hits regardless of sample size. A trait entirely
determined by genetics would, in principle, be fully predictable from
genotype. \textbf{Heritability} quantifies where traits fall along this
spectrum, but the concept is more subtle than it first appears, and
different estimation methods yield systematically different answers.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-heritability-decomposition}{[}High{]} Conceptual
diagram showing nested components of phenotypic variance. Outer ring:
Total phenotypic variance (100\%). First partition: Genetic vs
Environmental components (e.g., 80/20 for height). Within genetic:
Additive (narrow-sense hÂ²) vs non-additive (dominance, epistasis).
Within additive: SNP-heritability (what GWAS can capture) vs ``missing
heritability'' (rare variants, structural variants, imperfect tagging).
Annotate with approximate values for a well-studied trait like height.}

\end{figure}%

\subsection{Pedigree Heritability}\label{pedigree-heritability}

Classical genetics estimated heritability by comparing phenotypic
similarity among relatives. Identical twins share all their genetic
variation; fraternal twins share on average half; full siblings also
share half; parents and offspring share half; cousins share one-eighth.
If genetic variation influences a trait, closer relatives should be more
similar. The correlation structure across relationship types allows
partitioning of phenotypic variance into genetic and environmental
components.

\textbf{Narrow-sense heritability} (\(h^2\)) represents the proportion
of phenotypic variance attributable to additive genetic effects. For
height, pedigree studies consistently estimate \(h^2\) around 0.80,
meaning that 80\% of the variation in height across individuals in the
studied population can be attributed to genetic differences
(\textbf{visscher\_heritability\_2008?}). For schizophrenia, twin
studies estimate \(h^2\) around 0.80 as well. For body mass index,
estimates cluster around 0.40 to 0.70 depending on the population and
study design.

These high heritability estimates established that genetics
substantially influences most traits of biomedical interest, motivating
the search for specific causal variants. If 80\% of height variation is
genetic, then genetic variants collectively must explain most of that
variation. Finding those variants became the goal of GWAS.

\subsection{SNP-Heritability and the Missing Heritability
Problem}\label{snp-heritability-and-the-missing-heritability-problem}

GWAS delivered a puzzle. For height, even the largest studies with
hundreds of significant hits explained only a fraction of the
heritability estimated from family studies. Early GWAS collectively
explained perhaps 5\% of height variance when pedigree studies suggested
80\% should be genetic. This gap, termed \textbf{missing heritability},
sparked intense debate about where the remaining genetic variance might
hide (Manolio et al. 2009).

The concept of \textbf{SNP-heritability} (\(h^2_{SNP}\)) emerged to
address this puzzle. Rather than asking how much variance is explained
by genome-wide significant variants, researchers asked how much variance
is explained by all common SNPs on genotyping arrays, including those
that fail to reach significance. Methods such as GCTA-GREML estimate
this quantity by modeling phenotypic similarity as a function of genetic
similarity computed across all SNPs (J. Yang et al. 2010). For height,
SNP-heritability estimates reach approximately 0.50 to 0.60,
substantially higher than variance explained by significant hits alone
but still below pedigree estimates.

The gap between pedigree heritability (0.80 for height) and
SNP-heritability (0.50 to 0.60) reflects genetic variation not captured
by common SNPs on genotyping arrays. Rare variants, structural variants,
and variants not in linkage disequilibrium with array content all
contribute to the difference. The gap between SNP-heritability and
variance explained by significant hits reflects the polygenic
architecture of complex traits: thousands of variants each contribute
effects too small to reach genome-wide significance individually, yet
they collectively explain substantial variance.

\subsection{Implications for GWAS and Polygenic
Scores}\label{implications-for-gwas-and-polygenic-scores}

The heritability landscape carries practical implications for what GWAS
and polygenic scores can achieve. SNP-heritability sets an upper bound
on the predictive accuracy of polygenic scores built from common
variants: a PGS cannot explain more variance than is captured by the
SNPs it uses. For height, with SNP-heritability around 0.50, the best
possible common-variant PGS could explain at most half of phenotypic
variance. Current PGS for height in European-ancestry populations
approach this bound, explaining roughly 25\% of variance with continued
gains as sample sizes grow (\textbf{yengo\_meta-analysis\_2022?}).

For diseases, the relationship between heritability and predictive
accuracy is more complex. A highly heritable disease might have low
predictive accuracy if the causal variants are rare, if gene-environment
interactions dominate, or if the heritability is distributed across
thousands of variants each with tiny effects. Conversely, a moderately
heritable disease with a few common variants of large effect might be
more predictable. The architecture of genetic effects matters as much as
total heritability.

Missing heritability also motivates the integration of rare variant
analysis with GWAS of common variants. Whole-genome sequencing studies
can capture rare variants invisible to genotyping arrays, potentially
recovering some of the genetic variance missing from common-variant
analyses. Foundation models trained on sequence data, rather than
genotype arrays, may ultimately capture genetic effects across the full
allele frequency spectrum, a possibility we explore in
Chapter~\ref{sec-vep-fm}.

\section{Linkage Disequilibrium and the Association-Causation
Gap}\label{linkage-disequilibrium-and-the-association-causation-gap}

GWAS test variants one at a time, but the genome is not inherited one
variant at a time. Nearby variants travel together on haplotypes and are
co-inherited across generations except when recombination separates
them. This correlation structure, known as \textbf{linkage
disequilibrium (LD)}, is both essential to GWAS power and the source of
their fundamental interpretive limitation. Without LD, GWAS would need
to genotype every variant in the genome directly; with LD, statistical
association cannot distinguish cause from correlation.

When a GWAS identifies a significant association at variant \(j\), three
possibilities exist. The variant itself may be causal, directly
influencing the phenotype through some molecular mechanism.
Alternatively, variant \(j\) may simply be correlated with a nearby
causal variant \(k\) due to LD, with the association signal reflecting
this correlation rather than direct causation. In complex regions,
multiple causal variants may exist, and the observed association pattern
reflects their joint effects filtered through the local LD structure.
Distinguishing these scenarios from GWAS summary statistics alone is
often impossible. The causal variant and its tag look identical in the
association data, yet only the causal variant represents a valid drug
target or mechanistic insight.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-ld-tag-causal}{[}Essential{]} Three-panel figure.
Panel A: Haplotype diagram showing how LD creates correlation between
variants on the same chromosome segment; show example haplotypes with
causal variant (star) and tag variants traveling together. Panel B: rÂ²
matrix (triangular heatmap) for a genomic region showing block structure
of LD. Panel C: Same causal variant shown in two populations with
different LD structure---in one population, tags are highly correlated
with causal variant; in another, the correlation is weaker, illustrating
why portability fails.}

\end{figure}%

\subsection{The Structure of Linkage
Disequilibrium}\label{the-structure-of-linkage-disequilibrium}

Understanding why LD creates interpretive ambiguity requires
understanding how LD arises and decays. Recombination during meiosis
shuffles genetic material between parental chromosomes. Over many
generations, recombination breaks down long-range correlations between
variants while preserving short-range structure. The result is a mosaic
pattern: regions of high LD (haplotype blocks) where many variants are
strongly correlated, interspersed with recombination hotspots where LD
decays rapidly.

The squared correlation coefficient \(r^2\) quantifies LD between pairs
of variants. When \(r^2\) approaches 1, the two variants are nearly
always observed together on the same haplotypes; when \(r^2\) approaches
0, they segregate independently. From a GWAS perspective, if a causal
variant \(k\) has strong association with the phenotype and variant
\(j\) is in high LD with \(k\) (high \(r^2\)), then variant \(j\) will
also show strong association even if it has no direct causal role. The
statistical signal propagates through LD, creating ambiguity about which
variant is actually functional.

LD patterns vary across populations because demographic history shapes
which haplotypes persist and at what frequencies. Founder effects
concentrate certain haplotypes; bottlenecks reduce diversity; admixture
creates novel combinations; population expansion allows rare haplotypes
to drift to higher frequency. A variant that tags a causal allele
effectively in one population may be a poor proxy in another where
different recombination history has decoupled the correlation. This
population-specificity of LD structure is one reason polygenic scores
fail to transfer across ancestries, a problem we examine in detail
below.

\subsection{Causal Variants, Tag Variants, and GWAS
Catalogs}\label{causal-variants-tag-variants-and-gwas-catalogs}

The distinction between causal and tag variants determines whether GWAS
results can translate into biological insight or clinical action. A
\textbf{causal variant} directly influences the phenotype, whether by
altering protein sequence, disrupting transcription factor binding,
affecting splicing, or modifying chromatin state. A \textbf{tag variant}
is merely correlated with a causal variant through LD, serving as a
statistical proxy without direct functional consequence. The distinction
is invisible to GWAS: both produce association signals, and in the
presence of strong LD, those signals are statistically
indistinguishable.

GWAS catalogs therefore report associated loci, not causal variants. The
``lead SNP'' at each locus (the variant with the smallest p-value) is
often a tag rather than the causal variant, particularly when the causal
variant is rare, poorly genotyped, or not present on the array. Even
when a locus is robustly associated, dozens or hundreds of correlated
variants may be statistically indistinguishable from the lead SNP.

This limitation has concrete practical consequences. Drug development
requires identifying causal genes and mechanisms, not just associated
regions; targeting a tag variant or the wrong gene wastes years of
development effort. Clinical variant interpretation needs to distinguish
functional mutations from neutral passengers; reporting a tag as
pathogenic misleads patients and clinicians. Polygenic scores built on
tag SNPs may lose power when applied to populations with different LD
patterns, since the tag-causal correlation that made the tag useful may
not hold. The gap between association and causation motivates the
fine-mapping approaches we consider next.

\section{Fine-Mapping: From Loci to Causal
Variants}\label{fine-mapping-from-loci-to-causal-variants}

A pharmaceutical company evaluating a GWAS hit for drug development
faces a concrete problem: the associated locus spans 500 kilobases,
contains 200 correlated variants, and overlaps three genes. Which gene
should they target? Which variant drives the association? Investing
hundreds of millions of dollars in a program targeting the wrong gene
would be catastrophic, yet GWAS summary statistics alone cannot resolve
the ambiguity. \textbf{Fine-mapping} attempts to address this gap,
moving from ``this region is associated'' to ``these specific variants
are most likely causal'' by exploiting the joint behavior of correlated
variants under explicit statistical models.

\subsection{The Statistical Framework}\label{the-statistical-framework}

The core insight of fine-mapping is that while multiple variants may
show similar marginal association statistics, their joint behavior under
a model that accounts for LD can discriminate among them. A causal
variant should show association beyond what can be explained by LD with
its neighbors; a tag variant should not. This distinction, invisible
when variants are tested one at a time, becomes apparent when their
correlations are modeled jointly.

Bayesian fine-mapping methods approach the problem by specifying a prior
distribution over which variants in a region might be causal, then
computing posterior probabilities given the observed association
statistics and local LD structure. The key outputs are \textbf{posterior
inclusion probabilities (PIPs)}, which estimate the probability that
each variant is among the causal set, and \textbf{credible sets}, which
are minimal sets of variants that contain the true causal variant(s)
with specified probability (commonly 95\%).

The procedure typically proceeds as follows. First, researchers define a
region around an index SNP, often all variants within 1 megabase.
Second, they specify a prior: perhaps at most \(K\) variants in the
region are causal, and causal effect sizes follow some distribution
(often Gaussian). Third, the observed marginal association statistics
(effect sizes and standard errors) together with an LD matrix
(correlations among variants) inform the likelihood of the data under
each possible configuration of causal variants. Fourth, summation over
configurations yields the marginal PIP for each variant.

Variants with high PIPs (above 0.5 or 0.9) are strong candidates for
functional follow-up. Credible sets that contain few variants are more
actionable than those containing dozens. The width of credible sets
reflects both the strength of the association signal and the local LD
structure: tight LD means many variants remain plausible even with
strong statistical evidence. In some regions, fine-mapping narrows
thousands of candidates to a handful; in others, the ambiguity remains
irreducible given available data.

\subsection{Leveraging Functional
Annotations}\label{leveraging-functional-annotations}

Statistical fine-mapping alone cannot resolve regions where multiple
variants are in near-perfect LD; the data simply cannot distinguish
variants that are always co-inherited. Functional annotations offer a
path forward by incorporating biological plausibility: not all genomic
positions are equally likely to harbor causal variants. Variants
disrupting coding sequences, altering transcription factor binding
sites, or falling within active enhancers carry higher prior probability
of functional relevance than variants in unannotated intergenic regions.

Annotation-informed approaches update fine-mapping priors based on these
external data sources. Variants in coding regions, promoters, enhancers,
or regions of evolutionary constraint may be assigned higher prior
probability of causality. Integration with chromatin accessibility data
(from ATAC-seq or DNase-seq), transcription factor binding maps (from
ChIP-seq), or expression quantitative trait loci (eQTL) can further
prioritize variants with plausible regulatory mechanisms.

The functional scores introduced in Chapter~\ref{sec-data} provide
systematic frameworks for quantifying variant-level annotations. Scores
such as CADD, DANN, and Eigen integrate diverse genomic features into
single numbers that can inform fine-mapping priors. More recently,
foundation models trained on genomic sequence have produced variant
effect predictions that capture functional information beyond what
traditional annotations provide (Chapter~\ref{sec-vep-fm}). These scores
transform fine-mapping from a purely statistical exercise into an
integrative analysis that combines association evidence with mechanistic
plausibility.

Large-scale resources now link GWAS summary statistics, fine-mapping
results, and functional genomic annotations across hundreds of traits
and thousands of loci (Mountjoy et al. 2021). These datasets enable
systematic identification of variants that are both statistically
prioritized and functionally plausible, though the biological validation
required to confirm causal mechanisms remains laborious and is completed
for only a small fraction of associated loci.

\subsection{Multi-Ancestry
Fine-Mapping}\label{multi-ancestry-fine-mapping}

Single-ancestry fine-mapping encounters a fundamental resolution limit:
when variants are in tight LD within the study population, no amount of
statistical sophistication can distinguish them. Multi-ancestry
approaches break through this limit by exploiting the
population-specificity of LD structure. A variant in tight LD with
twenty neighbors in Europeans may have only three correlated variants in
African-ancestry populations, where shorter LD blocks (reflecting larger
historical effective population size) provide greater resolution.

Joint fine-mapping across ancestries leverages these differences
systematically (\textbf{kichaev\_improved\_2017?}). When a variant
remains strongly associated across populations despite different local
LD structure, confidence in its causal role increases. The logic is
straightforward: a true causal variant should show consistent
association regardless of which other variants happen to be correlated
with it in any particular population. A tag variant, by contrast, may
appear associated in one population (where it correlates with the causal
variant) but not in another (where that correlation is absent).

Multi-ancestry approaches are increasingly important as large biobanks
expand to include diverse populations, though they require careful
attention to potential effect size heterogeneity across populations.
Gene-environment interactions or genetic background effects could cause
genuine differences in variant effects, complicating the assumption that
causal variants have consistent effects worldwide.

\section{Polygenic Score
Construction}\label{polygenic-score-construction}

A 35-year-old woman with a family history of breast cancer asks her
physician whether she should begin mammography screening earlier than
guidelines recommend. Traditional risk models incorporate family
history, age, and reproductive factors, but cannot capture the
cumulative effect of thousands of common variants, each conferring small
increases in risk, that together may substantially elevate her
probability of disease. \textbf{Polygenic scores} address this gap by
aggregating variant effects across the genome into a single number:

\[
\text{PGS}_i = \sum_{j} w_j g_{ij}
\]

The weight \(w_j\) reflects the estimated effect of variant \(j\), and
\(g_{ij}\) is the genotype dosage for individual \(i\). The simplest
approach uses GWAS effect size estimates directly as weights; more
sophisticated methods adjust for LD, apply shrinkage, or incorporate
fine-mapping information. The clinical promise is substantial: for
diseases with significant genetic components, polygenic scores can
identify individuals at elevated risk years or decades before disease
onset, potentially enabling targeted screening or prevention.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-pgs-construction}{[}High{]} Side-by-side comparison
of C+T vs LD-aware Bayesian methods. Left panel (C+T): Start with GWAS
summary statistics â†’ apply p-value threshold â†’ clump by LD â†’ retain
independent lead SNPs â†’ weight by effect size. Show visually how most
variants are discarded. Right panel (LDpred/PRS-CS): Same summary
statistics â†’ model LD structure jointly â†’ shrink effects toward zero
based on prior â†’ retain all variants with modulated weights. Highlight
that C+T discards information while Bayesian methods model it.}

\end{figure}%

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Terminology: PGS versus PRS}, opacitybacktitle=0.6, colframe=quarto-callout-note-color-frame, leftrule=.75mm, left=2mm, toprule=.15mm, toptitle=1mm, breakable, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-note-color!10!white, arc=.35mm, titlerule=0mm, opacityback=0, bottomrule=.15mm, rightrule=.15mm, colback=white]

The literature uses overlapping terminology. \textbf{Polygenic risk
score (PRS)} is common in clinical contexts, emphasizing disease risk
prediction. \textbf{Polygenic score (PGS)} is more general, encompassing
both disease and quantitative trait prediction. \textbf{Genomic risk
score} and related terms also appear, often interchangeably. This book
uses PGS as the default, adding ``risk'' when specifically discussing
disease outcomes. Methodological overviews provide detailed guidance on
construction and evaluation (Choi, Mak, and O'Reilly 2020).

\end{tcolorbox}

\subsection{Clumping and Thresholding}\label{clumping-and-thresholding}

The challenge of constructing a useful polygenic score is not
mathematical but statistical: GWAS provide noisy estimates of millions
of effects, many of which are correlated through LD, and naive summation
produces scores dominated by noise rather than signal. \textbf{Clumping
and thresholding (C+T)} represents the simplest solution: reduce both
the noise and the correlation by aggressive filtering, accepting
substantial information loss in exchange for robustness.

The procedure involves three steps. First, clumping: rank variants by
p-value, then iteratively select the most significant variant and remove
all variants within a specified window (typically 250 kb) that are in LD
above a threshold (typically \(r^2 > 0.1\)). This yields a set of
approximately independent index variants. Second, thresholding: apply a
p-value cutoff and retain only variants below this threshold. Third,
weighting: set \(w_j\) equal to the GWAS effect size estimate for
retained variants, and zero otherwise.

The hyperparameters (LD window, \(r^2\) threshold, p-value threshold)
are typically chosen by grid search to maximize predictive performance
in a held-out validation set. This tuning introduces overfitting risk,
particularly in small samples or when the validation population differs
from the eventual deployment population.

C+T is transparent and computationally simple, but it discards
substantial information. Most variants are excluded, LD is handled only
through coarse pruning, and variants with modest p-values that
collectively explain meaningful variance may be entirely omitted. For
highly polygenic traits where thousands of variants each contribute
small effects, this information loss substantially degrades prediction
accuracy. The method treats LD as a problem to be eliminated rather than
a correlation structure to be modeled, an approach that sacrifices power
for simplicity.

\subsection{LD-Aware Bayesian Methods}\label{ld-aware-bayesian-methods}

The information discarded by C+T is not random noise; it contains
genuine signal about genetic effects distributed across correlated
variants. Rather than pruning away this structure, a more principled
approach models the joint distribution of effect sizes explicitly,
treating the true effects \(\beta = (\beta_1, \ldots, \beta_M)\) as
random variables drawn from a prior distribution. Given GWAS summary
statistics and an LD reference panel, these methods infer posterior mean
effect sizes that serve as PGS weights. The key insight is that LD
becomes information rather than nuisance: correlated variants constrain
each other's likely effects, improving estimation for all.

LDpred assumes that a fraction \(p\) of variants have nonzero effects
drawn from a Gaussian distribution, while the remainder have zero effect
(VilhjÃ¡lmsson et al. 2015). The method uses GWAS summary statistics and
LD from a reference panel (computed from a subset of individuals or
external dataset matching the target ancestry) to compute approximate
posterior effect sizes. These posteriors shrink noisy estimates toward
zero, borrow strength across correlated variants, and generally
outperform C+T when properly tuned.

PRS-CS extends this framework by placing a continuous shrinkage prior on
effect sizes, which better accommodates the highly polygenic
architecture of complex traits and reduces sensitivity to the sparsity
hyperparameter (\textbf{ge\_polygenic\_2019?}). The continuous prior
assigns most variants small but nonzero effects rather than forcing a
binary causal/non-causal distinction. The method has shown strong
performance across a range of traits and ancestries, though like all
methods it requires an LD reference that reasonably matches the target
population.

Related approaches (lassosum, SBayesR, and others) use different priors
or optimization strategies but share the core insight: jointly modeling
effect sizes under LD yields better predictions than pruning LD away.
Performance differences among methods are often modest when each is
well-tuned, and the choice may depend on computational resources,
availability of validation data, and specific trait architecture.

\subsection{Fine-Mapping-Informed
Scores}\label{fine-mapping-informed-scores}

Polygenic scores built on tag SNPs face a fundamental portability
problem: the tag-causal correlation that justified including a variant
may not hold in populations with different LD structure. Fine-mapping
outputs, particularly posterior inclusion probabilities, offer a
potential solution by identifying variants more likely to be causal.
Causal variants should remain predictive regardless of
population-specific LD patterns, since their effects are direct rather
than mediated through correlation.

Two strategies incorporate fine-mapping information into PGS
construction. Selection approaches retain only variants above a PIP
threshold (typically 0.1 or 0.5), focusing the score on high-confidence
causal candidates. Weighting approaches modulate each variant's
contribution by its PIP, downweighting likely tags while preserving
information from variants with intermediate evidence.

Fine-mapping-informed approaches aim to concentrate weight on variants
that are biologically meaningful rather than merely statistically
associated. In principle, this should improve cross-ancestry
transferability since causal variants remain causal regardless of
population-specific LD patterns. In practice, gains depend on
fine-mapping resolution, which is limited in regions of tight LD. The
approaches remain an active area of methodological development, with
potential for substantial improvement as multi-ancestry fine-mapping
resources expand.

\section{Interpreting Polygenic
Scores}\label{interpreting-polygenic-scores}

A polygenic score is a number, but numbers do not make clinical
decisions. A patient told they are in the 95th percentile of genetic
risk may interpret this as near-certain disease development, while a
physician may recognize it as modest risk elevation insufficient to
change management. Converting a score into actionable information
requires understanding what it represents, how it relates to disease
risk or trait values, and where its interpretation breaks down.
Miscommunication at this stage can transform a useful risk
stratification tool into a source of inappropriate anxiety or false
reassurance.

\subsection{Relative Risk and
Percentiles}\label{relative-risk-and-percentiles}

The most immediate clinical question about a high polygenic score is:
how much does it increase risk? Polygenic scores are most naturally
interpreted in relative terms by fitting a logistic regression in a
validation cohort:

\[
\log \frac{P(y_i = 1)}{P(y_i = 0)} = \alpha + \theta \cdot \text{PGS}_i + \eta^\top z_i
\]

where \(z_i\) contains covariates and \(\theta\) captures the effect of
the PGS. After standardizing the score to unit variance,
\(\exp(\theta)\) gives the odds ratio per standard deviation of the PGS.
This metric allows statements such as ``individuals one standard
deviation above the mean have 1.5-fold higher odds of disease.''

Percentile-based communication is common in clinical contexts. The risk
for individuals in the top 1\% or 5\% of the PGS distribution can be
compared to those near the median or in the bottom percentiles. For some
conditions, individuals in the top percentiles have risk comparable to
or exceeding that conferred by single high-penetrance mutations: the top
8\% of the coronary artery disease PGS distribution has risk equivalent
to familial hypercholesterolemia carriers, and the top 1\% of the breast
cancer PGS distribution has lifetime risk approaching that of
\emph{BRCA2} mutation carriers (\textbf{khera\_genome-wide\_2018?};
\textbf{mavaddat\_polygenic\_2019?}). This finding makes polygenic
scores potentially relevant for clinical risk stratification, though the
appropriate thresholds and clinical actions remain subjects of ongoing
research and debate.

\subsection{Absolute Risk}\label{absolute-risk}

A physician cannot act on relative risk alone; clinical decisions
require knowing the probability that this specific patient will develop
disease over a specified time horizon. Relative risk statements can
mislead when baseline risk varies substantially. A 1.5-fold increase in
odds for a disease with 1\% baseline risk means absolute risk rises from
1\% to roughly 1.5\%; the same relative increase for a disease with 20\%
baseline risk means absolute risk rises from 20\% to roughly 26\%. A
patient told they have ``50\% higher risk'' may react very differently
depending on whether baseline risk is low or high.

Converting PGS to absolute risk requires combining the score with
baseline incidence rates, which vary by age, sex, and other factors. The
hazard ratio per standard deviation of PGS, combined with age-specific
incidence curves from population registries, can yield personalized risk
trajectories. Such calculations demand careful attention to calibration:
the model must produce well-calibrated probabilities in the population
where it will be deployed, not just the population where it was trained.
A model calibrated in UK Biobank may systematically over- or
under-estimate risk when applied to a U.S. clinical population with
different baseline incidence rates or healthcare practices. Clinical
deployment of PGS is addressed in detail in
Chapter~\ref{sec-clinical-risk}.

\subsection{Explained Variance and
Discrimination}\label{explained-variance-and-discrimination}

Population-level performance metrics determine whether a polygenic score
has any utility, but they can mask the substantial uncertainty that
remains for any individual patient. For quantitative traits, the squared
correlation between PGS and phenotype (\(R^2\)) provides a direct
measure of explanatory power. Height PGS now explain roughly 25\% of
phenotypic variance in European-ancestry populations, approaching the
theoretical maximum given current sample sizes and the heritability of
the trait (\textbf{yengo\_meta-analysis\_2022?}). For binary traits, the
\(R^2\) on the liability scale (the underlying continuous risk) is more
interpretable than the observed-scale \(R^2\), which depends on disease
prevalence.

Area under the receiver operating characteristic curve (AUC) measures
discrimination: the probability that a randomly selected case has a
higher PGS than a randomly selected control. AUC values of 0.5 indicate
no discrimination (random guessing); values approaching 1.0 indicate
near-perfect separation. For most complex diseases, PGS achieve AUC
values in the 0.55 to 0.70 range when used alone, with incremental gains
when combined with traditional risk factors (Torkamani, Wineinger, and
Topol 2018; \textbf{lambert\_polygenic\_2019?}). These values reflect
meaningful stratification at the population level but limited utility
for individual prediction.

Even a PGS that explains 10\% of trait variance leaves 90\% unexplained
by factors genetic and environmental. High-risk individuals by PGS may
never develop disease; low-risk individuals may be affected. Polygenic
scores provide probabilistic risk stratification, not deterministic
prediction. This distinction is critical for clinical communication and
for setting appropriate expectations about what genomic risk information
can and cannot offer.

\section{Ancestry, Portability, and
Fairness}\label{ancestry-portability-and-fairness}

The vast majority of GWAS participants have been of European ancestry:
as of 2019, approximately 78\% of participants were European despite
Europeans comprising roughly 16\% of the global population
(\textbf{martin\_clinical\_2019?}). This historical imbalance has
profound consequences for who benefits from polygenic scores and who may
be harmed by their limitations. A technology that works well for some
populations and poorly for others is not merely incomplete; deployed
without appropriate caution, it risks widening existing health
disparities rather than narrowing them.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-pgs-portability}{[}High{]} Bar chart showing
relative prediction accuracy (RÂ² or AUC ratio) of European-derived PGS
when applied to different ancestry groups. European as reference
(100\%), then decreasing accuracy for East Asian, South Asian,
Hispanic/Latino, and African-ancestry populations (often 25-60\% of
European performance). Include error bars. Overlay or annotate factors
contributing to the gap: LD differences, allele frequency differences,
potential effect heterogeneity, training sample size disparities.}

\end{figure}%

\subsection{The Portability Problem}\label{the-portability-problem}

Polygenic scores derived from European-ancestry GWAS show markedly
reduced performance in other populations. African-ancestry individuals
typically experience 40\% to 75\% reductions in prediction accuracy
compared to European-ancestry individuals, even for the same trait
measured in the same study (Duncan et al. 2019;
\textbf{martin\_clinical\_2019?}). The pattern holds across traits and
across methods, though the magnitude varies with genetic architecture
and the degree of shared causal variants.

Several factors contribute to this portability failure. LD structure
differs across populations: tag SNPs that effectively proxy causal
variants in Europeans may be poor proxies in populations with different
recombination history. Allele frequencies differ: variants common in one
population may be rare or absent in another. Effect sizes may genuinely
differ across populations due to gene-environment interactions or
genetic background effects. And GWAS in smaller non-European samples
have less power to detect associations, yielding noisier effect
estimates that further degrade prediction.

Multi-ancestry GWAS and methods designed to leverage diverse training
data offer partial solutions. Including multiple ancestries in discovery
improves transferability, and methods that explicitly model
ancestry-specific LD or effect sizes can enhance performance
(\textbf{marquez-luna\_multiethnic\_2017?}). Yet even state-of-the-art
approaches do not fully close the gap, and substantial research is
needed before PGS perform equitably across populations.

\subsection{Fairness and Health
Equity}\label{fairness-and-health-equity}

The performance gap across ancestries is not merely a technical
nuisance; it raises fundamental questions about fairness in precision
medicine. If genomic models work primarily for individuals of European
ancestry, deploying these models in diverse clinical populations risks
exacerbating existing health disparities rather than ameliorating them.
The communities historically excluded from genetic research would
continue to receive inferior genomic medicine, now encoded in
algorithmic form.

Consider a scenario where PGS are used for risk-stratified screening. If
the score identifies high-risk individuals more accurately in Europeans
than in other groups, Europeans receive more targeted and efficient
screening while others receive either under-screening (if falsely
classified as low risk) or over-screening (if falsely classified as high
risk). The benefits of precision medicine accrue disproportionately to
those already overrepresented in research, while the costs of
miscalibration fall on those historically excluded.

These challenges extend beyond PGS to every genomic model in this book.
Foundation models can learn to exploit ancestry signals as shortcuts,
achieving high benchmark performance while performing poorly on
underrepresented groups. Aggregate performance metrics mask inequities
across populations. Deployment in diverse clinical settings requires
explicit evaluation of performance stratified by ancestry, along with
transparent reporting of limitations and appropriate caution in
populations where validation is limited. We examine these issues
comprehensively in Chapter~\ref{sec-confounding}.

\section{From Association to
Mechanism}\label{from-association-to-mechanism}

GWAS and polygenic scores have delivered thousands of robust trait
associations, clinically useful risk stratification for some conditions,
and fundamental insights into the polygenic architecture of complex
phenotypes. They have also exposed a persistent gap between statistical
association and biological understanding. Most GWAS hits lie in
noncoding regions, often within enhancers, promoters, or other
regulatory elements. The variant is associated; the mechanism is
obscure. Fine-mapping narrows the list of candidates but rarely
identifies a single causal nucleotide with confidence. Even when a
variant is prioritized, the path from sequence change to molecular
consequence to cellular phenotype to disease remains opaque.

This mechanistic gap limits translation in concrete ways. Drug
development requires actionable targets, not associated regions.
Clinical variant interpretation needs to explain why a variant matters,
not just that it correlates with disease. Polygenic scores stratify
population risk but offer little guidance on individual intervention.
The models developed in subsequent chapters address this gap through
complementary strategies: regulatory sequence models that predict how
variants alter transcription factor binding and chromatin accessibility
(Chapter~\ref{sec-regulatory}), variant effect predictors that assess
functional impact at nucleotide resolution (Chapter~\ref{sec-vep-fm}),
and multi-omics integration approaches that connect genetic variation to
intermediate molecular phenotypes (Chapter~\ref{sec-multi-omics}).

The goal is not to replace statistical genetics but to build on it.
Association provides the map of where trait-relevant variation resides;
mechanistic modeling attempts to explain how that variation produces its
effects. The combination of statistical association and mechanistic
interpretation offers the most promising path toward genomic medicine
that is both predictive and understood.

\chapter{Classical Variant Prediction}\label{sec-vep-classical}

Conservation scores measure evolutionary constraint, not disease
relevance. Protein-level predictors estimate structural disruption, not
clinical pathogenicity. Splice site algorithms identify sequence motifs,
not functional consequences. Every classical variant effect predictor
measures a proxy for what clinicians actually need to know: will this
variant cause disease in this patient? The gap between measurable signal
and clinical question is irreducible. Evolutionary conservation reflects
reproductive fitness, not human health; protein structure does not
determine disease penetrance; and splice motifs do not guarantee
splicing outcomes. Classical methods achieve what they achieve by
combining multiple imperfect proxies, hoping that their convergence
approximates clinical truth.

This chapter examines the conceptual foundations and practical methods
that dominated variant interpretation before the foundation model era.
The trajectory traces from single-signal predictors through increasingly
sophisticated ensemble methods. Conservation scores like PhyloP and GERP
quantify purifying selection across evolutionary time, identifying
positions where variation is depleted relative to neutral expectations.
Protein-level tools like SIFT and PolyPhen assess amino acid
substitutions through sequence conservation and structural features.
Splice predictors identify the sequence motifs that mark intron-exon
boundaries. Each approach captures genuine biological signal, but each
also fails in characteristic ways: conservation misses recently evolved
human-specific functions, protein predictors cannot assess non-coding
variants, splice algorithms miss cryptic sites and tissue-specific
regulation.

The field's response was to develop integrative methods that combine
multiple signals into unified scores. Combined Annotation-Dependent
Depletion (CADD) receives particular attention here because it
introduced design patterns that recur throughout genomic machine
learning: using evolutionary signals as proxy labels, training on
large-scale genomic data, integrating dozens of diverse annotations, and
precomputing scores genome-wide for downstream reuse. CADD and its
successors, including REVEL, PrimateAI, and M-CAP, remain in active
clinical use today. Understanding their construction and limitations
illuminates both what classical methods achieved and why the field
ultimately moved toward the learned representations examined in the
following chapter.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-variant-funnel}{[}Essential{]} Funnel diagram
showing progressive reduction of variant burden through computational
filters. Stages with approximate numbers: (1) Raw variants from WGS:
\textasciitilde4-5 million; (2) After common variant filter (gnomAD AF
\textgreater{} 1\%): \textasciitilde100,000; (3) After consequence
filter (coding, splice, UTR): \textasciitilde25,000; (4) After
conservation filter (phyloP \textgreater{} 2): \textasciitilde5,000; (5)
After ensemble predictor (CADD â‰¥ 20): \textasciitilde500-1,000; (6)
Expert review candidates: \textasciitilde50-100. Annotate which
tools/resources apply at each stage.}

\end{figure}%

\section{Conservation-Based
Approaches}\label{conservation-based-approaches}

A clinical geneticist evaluating a novel intronic variant faces an
immediate problem: no functional annotation exists for most of the
genome, and no clinical database has seen this specific change before.
The variant lies outside any protein-coding region, no regulatory
element overlaps it, and the patient's phenotype offers no clear
mechanistic hypothesis. Yet one source of information spans the entire
genome and predates any experimental annotation by billions of years. If
a genomic position has remained unchanged across species separated by
hundreds of millions of years of evolution, mutations at that position
are likely to be deleterious. \textbf{Natural selection has already
performed the largest functional screen imaginable, running experiments
across countless organisms over evolutionary time, and conservation
scores quantify the results.}

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-conservation-scores}{[}High{]} Multi-panel figure.
Panel A: Multiple sequence alignment at a highly conserved position
(same nucleotide across 30+ species) vs a neutrally evolving position
(variable nucleotides). Show phylogenetic tree alongside alignment.
Panel B: Distribution of phyloP or GERP scores genome-wide, with long
right tail representing constrained elements. Mark threshold zones
(e.g., phyloP \textgreater{} 2 = strong evidence). Panel C: Example
intronic variant at deeply conserved position with no other
annotation---illustrating how conservation provides evidence in
annotation-sparse regions.}

\end{figure}%

\subsection{Measuring Evolutionary
Constraint}\label{measuring-evolutionary-constraint}

The logic of conservation is straightforward: if a position matters for
survival or reproduction, mutations there will be removed by selection
before they can spread through the population. Quantifying this signal
requires comparing sequences across species to identify positions where
substitutions occur less frequently than expected under neutral
evolution. \textbf{Conservation scores} translate this evolutionary
signal into numerical values that can inform variant interpretation.

\textbf{PhyloP} scores quantify the deviation of observed substitution
rates from neutral expectation at individual positions (Siepel et al.
2005). The score is computed by comparing the observed pattern of bases
at each alignment column against a neutral evolutionary model (typically
fit to ancestral repeat sequences that are assumed to evolve without
selective constraint). Positive phyloP scores indicate conservation,
meaning evolution is slower than expected under neutrality. Negative
scores indicate acceleration, suggesting faster evolution that may
reflect positive selection. A phyloP score of 2 indicates that the
observed base is approximately 100-fold more conserved than expected
under neutrality, providing strong evidence that mutations at this
position have been systematically removed by selection.

\textbf{Genomic Evolutionary Rate Profiling (GERP)} takes a
complementary approach by estimating ``rejected substitutions'' at each
position: the number of substitutions that would have been expected
under neutrality but are absent from the observed alignment (Davydov et
al. 2010). Large positive GERP scores indicate strong constraint. For a
position conserved across 30 mammalian species, a GERP score of 5
implies that approximately five substitutions were rejected by selection
over mammalian evolution. This interpretation connects directly to the
biological process of purifying selection but depends on accurate
neutral rate estimation and alignment quality.

\textbf{PhastCons} provides element-level rather than position-level
conservation by identifying contiguous stretches of constrained sequence
(Siepel et al. 2005). Using a hidden Markov model, phastCons classifies
each position as belonging to a conserved or non-conserved state, then
outputs the posterior probability of conservation. The resulting scores
are smoother than position-level metrics, capturing functional elements
that span multiple nucleotides even when individual positions show
moderate conservation. This element-level view proves particularly
valuable for identifying regulatory sequences where the overall
constraint matters more than any single nucleotide.

\subsection{What Conservation Measures Versus What Clinicians
Need}\label{what-conservation-measures-versus-what-clinicians-need}

Conservation scores measure evolutionary constraint: the degree to which
a position has resisted substitution over millions of years. This is not
the same as clinical relevance. A position can be evolutionarily
constrained for functions unrelated to human disease, or clinically
important despite modest conservation. The assumption underlying
conservation-based interpretation is that positions under strong
constraint are more likely to be functionally important and therefore
more likely to cause disease when mutated. This assumption is often
correct but not universally so.

The clinician wants to know: will this variant cause disease in my
patient? Conservation provides indirect evidence: this position has been
important for organismal fitness across evolutionary time. The gap
between these questions creates interpretive challenges. A variant at a
highly conserved position in a gene with no known disease association
provides evolutionary evidence of functional importance but no direct
path to clinical interpretation. Conversely, a variant at a modestly
conserved position in a well-established disease gene may be clinically
significant despite weak conservation signal.

\subsection{Clinical Application and
Boundaries}\label{clinical-application-and-boundaries}

Conservation scores prove particularly valuable for non-coding variant
interpretation, where direct functional annotations are often incomplete
or absent. A deeply conserved intronic position likely participates in
splicing regulation, gene expression control, or other functional
processes even if no explicit annotation overlaps it. Under ACMG-AMP
guidelines for variant classification, strong conservation provides
computational evidence (the PP3 criterion) supporting pathogenicity
(\textbf{richards\_standards\_2015?}). A variant falling at a position
with phyloP greater than 2 and GERP greater than 4 carries significantly
more weight than one at an unconserved position, even when no other
annotation is available. These scores remain central to clinical variant
interpretation workflows, as examined in
Chapter~\ref{sec-clinical-risk}, where they contribute evidence
alongside population frequency, functional studies, and segregation
data.

The boundaries of conservation-based approaches are equally important to
recognize, and these boundaries are not merely technical inconveniences
but reflect fundamental gaps in what evolutionary signal can reveal.
Conservation requires evolutionary time to accumulate signal. Recently
evolved functional elements, including human-specific regulatory
sequences and primate-specific genes, may show little conservation
despite genuine function. A position can be functionally critical in
humans yet unconserved because the function arose too recently for
selection to leave a detectable signature. The 3\% of the human genome
that shows evidence of human-specific function since the
human-chimpanzee split presents exactly this challenge: important to
human biology, yet invisible to conservation metrics.

Conservation reflects the aggregate of selective pressures across the
species in the alignment, which may differ from the specific functional
role in humans. A position conserved for its role in neural development
across vertebrates may be less constrained for immune function, which
evolves more rapidly. Lack of conservation does not prove neutrality; it
may simply indicate rapid evolution under positive selection or
lineage-specific function.

Conservation scores also face technical challenges from alignment
quality. In repetitive regions, segmental duplications, and rapidly
evolving gene families, reliable alignments may be impossible to
construct, leaving conservation scores undefined or unreliable precisely
where variant interpretation is most difficult. The HLA region,
immunoglobulin loci, and centromeric sequences are clinically important
yet systematically difficult to assess by conservation. \textbf{The
regions most difficult to interpret computationally are frequently those
of greatest clinical interest.}

These boundaries do not diminish the value of conservation; they define
where that value applies. Conservation provides information largely
orthogonal to population frequency (which reflects recent human history
rather than deep evolutionary constraint) and to functional genomics
annotations (which capture biochemical activity rather than selective
importance). The integrative methods discussed later in this chapter
combine conservation with these other signals to achieve better
performance than any single source. Protein language models, examined in
Chapter~\ref{sec-protein-lm}, learn conservation-like signals directly
from sequence data without requiring explicit alignments, potentially
addressing some technical limitations while introducing their own
assumptions about what constitutes functional constraint.

\section{Protein-Level Predictors}\label{protein-level-predictors}

A diagnostic laboratory receives exome sequencing results for a
45-year-old woman with early-onset breast cancer and a family history
suggesting hereditary cancer syndrome. Among hundreds of rare variants,
one stands out: a missense change in \emph{BRCA2} substituting glycine
for arginine at a conserved position. Is this the explanation for her
cancer, or an incidental finding? No previous case report exists for
this exact variant. No functional assay has tested its effect. The
question of whether this amino acid substitution disrupts \emph{BRCA2}
function determines whether her siblings should be tested and whether
she qualifies for PARP inhibitor therapy. \textbf{The clinical stakes
could not be higher, yet the evidence available is entirely
computational.} Protein-level predictors attempt to answer such
questions by encoding biological intuition about which amino acid
changes matter.

\subsection{SIFT: Sequence Homology as Functional
Constraint}\label{sift-sequence-homology-as-functional-constraint}

Conservation scores can identify constrained positions, but they cannot
distinguish which substitutions at those positions are tolerated. A
position might be highly conserved overall yet accept certain amino acid
changes that preserve function. For missense variants specifically, the
relevant question is not whether the position is constrained but whether
the specific amino acid substitution disrupts function. \textbf{Sorting
Intolerant From Tolerant (SIFT)} addresses this distinction by examining
which amino acids have been accepted at each position across
evolutionary history (Ng and Henikoff 2003).

SIFT collects homologous protein sequences from diverse species,
constructs a multiple sequence alignment, and examines which amino acids
appear at each position across the alignment. Positions that are highly
conserved (showing the same or similar amino acids across species) are
predicted to be functionally important; substitutions introducing amino
acids not observed at that position are predicted to be deleterious.

The method computes a normalized probability for each possible amino
acid at each position based on the diversity observed in the alignment.
The SIFT score for a substitution is the probability of observing the
mutant amino acid, scaled by the position's overall diversity. Scores
range from 0 to 1, with low scores (typically below 0.05) indicating
predicted damage. A SIFT score of 0.01 for a particular missense variant
indicates that the mutant amino acid is rarely or never observed at that
position across the sequence family, suggesting functional constraint
has prevented its fixation throughout evolution.

SIFT's simplicity is both its strength and its limitation. The method
requires only protein sequence information and a database of homologs;
it makes no assumptions about protein structure, physicochemistry, or
mechanism of damage. This generality allows application to any protein
with sufficient homologs in sequence databases. For proteins with few
homologs, young gene families, or positions with limited alignment
depth, predictions may be unreliable. The method captures only the
evolutionary signal present in the alignment, missing functional
constraints that arose recently or affect only a subset of species.

\subsection{PolyPhen-2: Integrating Structure and
Sequence}\label{polyphen-2-integrating-structure-and-sequence}

SIFT's reliance on sequence alone ignores substantial information about
how amino acid substitutions affect protein function. A glycine buried
in a protein's hydrophobic core will disrupt structure differently than
one on a surface loop. A substitution at a catalytic site matters more
than one far from any functional region. \textbf{Polymorphism
Phenotyping (PolyPhen-2)} extends sequence-based prediction by
incorporating protein structure features and amino acid
physicochemistry, recognizing that the same substitution can have
different consequences depending on its structural context (Adzhubei et
al. 2010).

The method uses a naive Bayes classifier trained to distinguish
disease-causing mutations from neutral polymorphisms based on a
collection of sequence-derived and structure-derived features. The
feature set includes sequence conservation (similar to SIFT) but adds
several structural descriptors when three-dimensional structure data is
available: solvent accessibility (whether the position is buried or
exposed), secondary structure context (helix, sheet, or coil), and
proximity to known functional sites. Amino acid physicochemical
properties inform predictions about whether substitutions are
conservative or radical. The \textbf{Grantham distance}, a measure of
biochemical dissimilarity between amino acid pairs based on composition,
polarity, and molecular volume, contributes to assessing substitution
severity. A glycine-to-arginine substitution (Grantham distance of 125)
represents a far more radical change than a leucine-to-isoleucine
substitution (Grantham distance of 5).

PolyPhen-2 provides two models trained on different datasets: HumDiv,
trained on disease-causing and neutral variants from protein sequence
databases, and HumVar, trained on Mendelian disease mutations versus
common human polymorphisms. The choice of training set affects score
interpretation; HumVar produces more conservative predictions
appropriate for clinical Mendelian disease variant classification, while
HumDiv is more sensitive and may be preferable for research applications
where missing a true positive is more costly than false positives.

PolyPhen-2 scores range from 0 to 1, with higher scores indicating
greater predicted deleteriousness. The output includes qualitative
classifications (benign, possibly damaging, probably damaging) based on
score thresholds. A PolyPhen-2 score of 0.95 with a ``probably
damaging'' classification indicates high confidence that the
substitution disrupts protein function, though the clinical significance
depends on additional evidence about the specific disease context.

\subsection{What Protein-Level Predictors Measure Versus What Clinicians
Need}\label{what-protein-level-predictors-measure-versus-what-clinicians-need}

Protein-level predictors estimate the impact of amino acid substitutions
on protein function. They answer the question: does this substitution
disrupt how the protein works? This is related to but distinct from the
clinical question: does this variant cause disease in this patient?

A substitution can disrupt protein function without causing disease (if
the protein has redundant function, if heterozygous loss is tolerated,
or if the disrupted function is not relevant to the phenotype).
Conversely, a substitution can cause disease through mechanisms that
protein-level predictors cannot assess: gain-of-function effects,
dominant-negative interactions, or tissue-specific expression changes.
The SIFT and PolyPhen-2 scores for our \emph{BRCA2} variant estimate
functional disruption, but the clinical interpretation requires
additional reasoning about \emph{BRCA2}'s role in DNA repair, the
consequences of haploinsufficiency, and the patient's specific cancer
phenotype.

This distinction becomes critical in clinical practice. A ``probably
damaging'' PolyPhen-2 score for a variant in a gene with no established
disease association provides evidence of functional impact but no direct
clinical utility. A ``benign'' prediction for a variant in a gene where
even mild functional reduction causes disease may be misleading if the
prediction method is insufficiently sensitive. Understanding what these
tools measure, and how that relates to clinical questions, is essential
for appropriate use.

\subsection{Boundaries of Protein-Level
Prediction}\label{boundaries-of-protein-level-prediction}

Several fundamental boundaries constrain all protein-level predictors,
and these boundaries are not merely technical inconveniences but reflect
deep gaps in what sequence and structure analysis alone can reveal.
Protein-level tools are restricted to missense variants; nonsense,
frameshift, splice-altering, and non-coding variants lie entirely
outside their scope. A patient's most important variant may be intronic
or synonymous, yet protein-level predictors have nothing to say about
it. This constraint is absolute: these methods analyze amino acid
substitutions and cannot be extended to other variant types without
fundamental redesign.

Protein-level predictors estimate impact on protein function without
specifying the mechanism or clinical consequence. A variant predicted to
damage function might impair enzymatic activity, disrupt protein
folding, eliminate a binding interface, or alter stability. The clinical
relevance depends on which function is affected and whether the
phenotype results from loss or gain of function. A predicted-damaging
variant in a tumor suppressor behaves very differently from one in an
oncogene, yet protein-level predictors provide no information about this
distinction. The same high score can indicate completely different
clinical implications depending on biological context.

These tools also provide no information about inheritance mode,
penetrance, or expressivity. A strongly predicted-damaging variant in a
gene with high tolerance to heterozygous loss may be clinically benign
in carriers. Protein-level predictors cannot distinguish between a
variant causing severe disease in homozygotes and one causing no disease
at all when heterozygous. This distinction becomes critical when
counseling families, where the mode of inheritance fundamentally changes
recurrence risk and management recommendations.

Protein-level predictors inherit the training data biases present in
their underlying databases. Disease mutations in training sets are
enriched for severe, early-onset Mendelian conditions with clear
inheritance patterns. Variants causing subtle effects, incomplete
penetrance, or complex phenotypes may be systematically mispredicted.
The well-studied genes that dominate training data may not generalize to
poorly characterized genes where variants are most difficult to
interpret.

SIFT and PolyPhen-2 remain widely used in clinical practice and serve as
features within more sophisticated ensemble methods. Their scores appear
in diagnostic reports, contribute to ACMG-AMP classification criteria,
and inform variant prioritization in research and clinical pipelines.
Understanding their construction and limitations is essential for
appropriate interpretation.

\section{The CADD Framework}\label{the-cadd-framework}

The protein-level predictors and conservation scores examined above each
capture one aspect of variant function, yet clinical interpretation
requires weighing multiple lines of evidence simultaneously. A variant
might fall in a conserved region, alter a moderately constrained amino
acid, and overlap a predicted enhancer. How should these signals be
combined? More fundamentally, how can we train a predictor when curated
pathogenic variants number in the thousands while the genome contains
billions of possible mutations? These questions expose a fundamental
tension: the variants we most need to interpret (rare, novel, never
before seen) are precisely those for which training labels do not exist.

\textbf{Combined Annotation-Dependent Depletion (CADD)} addressed these
challenges by reframing variant effect prediction as a large-scale
machine learning problem (Kircher et al. 2014; Rentzsch et al. 2019).
The key insight was not better feature engineering or more sophisticated
classification, but rather a reconceptualization of the labeling problem
itself. Instead of training directly on small sets of known pathogenic
versus benign variants, which are scarce and biased toward certain genes
and variant types, CADD contrasts variants that have survived purifying
selection in the human lineage with matched simulated variants that
could have occurred but did not. \textbf{This evolutionary proxy
strategy transforms the labeling problem, yielding millions of training
examples where curated datasets provide thousands.}

\subsection{Evolutionary Proxy Training and Label
Sources}\label{evolutionary-proxy-training-and-label-sources}

The conceptual foundation of CADD rests on constructing training labels
from evolutionary signal rather than clinical curation. The method
builds two proxy classes of variants that serve as training labels, each
designed to approximate a category that cannot be observed directly.
Understanding these label sources is essential because the same proxy
labeling strategies reappear throughout genomic machine learning,
including in the foundation models discussed in
Chapter~\ref{sec-fm-principles}.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-cadd-training}{[}Essential{]} Three-panel conceptual
figure. Panel A (Proxy-Neutral): Human-derived alleles fixed since
human-chimpanzee split; these variants survived selection, representing
tolerated changes. Show evolutionary tree with human branch highlighted.
Panel B (Proxy-Deleterious): Simulated variants matching human
mutational processes (trinucleotide context); these represent
possible-but-not-observed mutations enriched for deleterious effects.
Show simulation schematic with mutation spectrum. Panel C
(Classification): SVM learning to distinguish classes based on
annotation features; output is ``evolutionary tolerance'' score, not
pathogenicity directly.}

\end{figure}%

The \textbf{proxy-neutral class} consists of variants that have been
tolerated by purifying selection. CADD draws these from sequence
differences that arose on the human lineage since the split from
chimpanzees and became fixed or nearly fixed in modern humans. These are
identified by their \textbf{derived allele frequency}: alleles that
differ from the inferred ancestral state (typically determined by
comparison to chimpanzee and other great ape sequences) and are present
at very high frequency in human populations. Because these derived
alleles have persisted over millions of years of evolution, most are
presumed to be neutral or only weakly deleterious. This is not a perfect
proxy: some observed alleles are genuinely pathogenic, particularly
those with incomplete penetrance, late onset, or context-dependent
effects. The proxy-neutral class is, on average, substantially enriched
for tolerated alleles relative to a random sample of possible mutations.

The \textbf{proxy-deleterious class} is constructed by simulating
mutations across the genome according to realistic mutational processes.
The simulation matches local sequence context (typically using
trinucleotide frequencies to capture the strong dependence of mutation
rates on flanking bases). CpG dinucleotides, for example, have elevated
mutation rates due to spontaneous deamination of methylated cytosines,
and the simulation accounts for this by generating more CpG transitions.
Regional variation in mutation rates, driven by factors including
replication timing and chromatin state, is similarly incorporated.

The logic underlying this construction is subtle but powerful. Simulated
variants represent changes that could plausibly occur under human
mutational processes but are generally not observed at high frequency in
population databases. The proxy-deleterious class as a whole is enriched
for alleles disfavored by selection, because the set of possible
mutations includes many that disrupt conserved elements, alter protein
function, or perturb regulatory sequences. By contrasting this set with
the proxy-neutral class (high derived allele frequency variants that
survived selection), CADD learns to recognize the annotation signatures
that distinguish variants under purifying selection from those that have
been tolerated.

This proxy labeling strategy has important implications. CADD does not
learn to distinguish pathogenic from benign variants directly; it learns
to distinguish tolerated-by-evolution from possible-but-not-observed.
The assumption is that variants depleted by selection are enriched for
functional effects and therefore enriched for disease relevance. This
assumption is often correct but introduces a systematic gap between what
CADD measures (evolutionary tolerance) and what clinicians need (disease
causation).

\subsection{Feature Integration}\label{feature-integration}

Conservation scores measure evolutionary constraint. Protein-level
predictors assess amino acid substitution severity. Regulatory
annotations mark biochemically active regions. Each signal captures
genuine biology, but no single annotation captures the full complexity
of variant function. A missense variant in a constrained gene might be
tolerated if it falls in an unconserved loop region; a synonymous
variant might be pathogenic if it disrupts splicing. \textbf{The power
of CADD lies in learning how these heterogeneous signals interact,
upweighting annotations that distinguish proxy-deleterious from
proxy-neutral variants and downweighting those that do not.}

CADD integrates more than 60 features, far exceeding what explicit
combination rules could accommodate. \textbf{Gene model annotations}
describe the local transcript and coding context of each variant. The
most fundamental is the predicted sequence consequence: whether a
variant is synonymous, missense, nonsense, frameshift, splice-site
disrupting, or located in untranslated or intronic regions. Distance to
exon-intron boundaries and proximity to canonical splice sites provide
additional context. Gene-level attributes including constraint metrics
(pLI, LOEUF from gnomAD) quantify how tolerant each gene is to damaging
variation.

\textbf{Conservation features} from phyloP, GERP, and phastCons provide
the evolutionary signals described earlier. By incorporating multiple
conservation metrics computed from different alignments and using
different methodologies, CADD captures complementary aspects of
evolutionary constraint.

\textbf{Protein-level predictions} from SIFT and PolyPhen-2 contribute
assessments of amino acid substitution impact for coding variants. Amino
acid physicochemical properties, Grantham distances, and domain
annotations from databases like Pfam provide additional structural
context.

\textbf{Regulatory annotations} derived from ENCODE and Roadmap
Epigenomics data capture chromatin accessibility, histone modifications,
and transcription factor binding. These features help prioritize
non-coding variants that disrupt active regulatory regions.

Additional features capture local sequence context (GC content, CpG
density), genomic architecture (segmental duplications, repetitive
elements), and chromosomal position. The model learns how to weight and
combine these heterogeneous signals from the data rather than from
expert specification.

\subsection{Model Architecture and
Scoring}\label{model-architecture-and-scoring}

Raw classifier outputs are not directly interpretable as probabilities
or biological effect sizes. A clinician presented with a support vector
machine decision value has no intuitive understanding of what that
number means. To address this, CADD defines \textbf{PHRED-scaled scores}
based on the rank of each variant among all possible single-nucleotide
substitutions in the reference genome. A scaled score of 10 indicates
that a variant falls in the top 10\% of predicted deleteriousness. A
score of 20 indicates the top 1\%, and a score of 30 indicates the top
0.1\%. This rank-based transformation ensures comparability across CADD
versions and provides immediate interpretability: a clinician can
understand that a score of 25 places this variant among the most extreme
0.3\% of possible mutations without needing to understand the underlying
classifier.

CADD's classifier operates on the high-dimensional feature vector
assembled for each variant. The original CADD model uses a linear
support vector machine trained to discriminate proxy-neutral and
proxy-deleterious variants based on approximately 30 million training
examples. The choice of a linear model was deliberate and pragmatic:
with tens of millions of training examples and dozens of features, a
linear SVM is computationally tractable while capturing the main
structure of the classification problem.

In clinical laboratories, CADD scaled scores commonly serve as filters
to enrich for potentially pathogenic variants. Typical thresholds range
from 15 (top 3\%) to 20 (top 1\%) or higher. Variants with scores at or
above 20 are considered moderately high deleteriousness candidates,
while scores at or above 30 are frequently interpreted as strongly
enriched for functional impact. A diagnostic pipeline might use CADD
greater than or equal to 20 as an initial filter, reducing 25,000 exome
variants to several hundred candidates for expert review. These filters
serve as prioritization tools that reduce the variant burden to a
manageable number rather than as definitive pathogenicity calls.

\subsection{What CADD Measures Versus What Clinicians
Need}\label{what-cadd-measures-versus-what-clinicians-need}

CADD measures the probability that a variant resembles those depleted by
purifying selection rather than those tolerated over evolutionary time.
This is a proxy for functional importance but not a direct measure of
disease causation. The distinction matters in several clinical
scenarios.

A variant can receive a high CADD score because it disrupts an
evolutionarily constrained element that has no relevance to the
patient's phenotype. A deeply conserved neural enhancer variant will
score highly even in a patient with a cardiac phenotype if the
constraint derives from neural function. CADD cannot distinguish which
functions are relevant to which diseases.

Conversely, a variant can cause disease through mechanisms that leave no
evolutionary signature. Gain-of-function mutations, dominant-negative
effects, and tissue-specific pathogenic mechanisms may not be depleted
by selection in the same way as loss-of-function alleles. A variant
causing disease through a novel mechanism absent from evolutionary
history will not be recognized by CADD's training framework.

CADD also cannot account for genetic background, environmental
interactions, or incomplete penetrance. A variant might be highly
deleterious in one genetic context and tolerated in another, but CADD
assigns a single score regardless of context. These limitations are not
unique to CADD; they reflect the fundamental gap between evolutionary
proxy labels and clinical disease causation that affects all methods in
this chapter.

\section{Other Ensemble Methods}\label{other-ensemble-methods}

The clinical geneticist focused exclusively on rare missense variants in
Mendelian disease faces a different optimization problem than the
researcher screening the entire genome for regulatory variants. CADD's
genome-wide generality may sacrifice accuracy within specific variant
classes, accepting modest performance everywhere to achieve coverage
anywhere. For diagnostic laboratories where missense variants in known
disease genes dominate the caseload, specialized ensemble methods offer
an alternative: models trained directly on curated disease variants,
optimized for the specific task rather than general prioritization.
\textbf{This tension between generality and specialization recurs
throughout computational biology, and different clinical contexts demand
different tradeoffs.}

\subsection{REVEL}\label{revel}

A missense variant in a known disease gene presents a narrower
interpretive challenge than an arbitrary variant anywhere in the genome.
The variant is protein-coding, the gene has established disease
associations, and the question is specifically whether this amino acid
substitution is pathogenic. This focused scope permits a different
training strategy than CADD's evolutionary proxy approach.

\textbf{Rare Exome Variant Ensemble Learner (REVEL)} represents a
missense-specific ensemble predictor widely used in clinical
laboratories (Ioannidis et al. 2016). Rather than training on
evolutionary proxy labels, REVEL directly discriminates pathogenic
missense variants (curated from HGMD and other disease databases) from
rare putatively neutral missense variants observed in population
datasets.

REVEL integrates predictions from a panel of individual tools: SIFT,
PolyPhen-2, PROVEAN, MutationAssessor, FATHMM, GERP++, phyloP, and
phastCons, among others. A random forest model learns to combine these
scores, weighting each according to its discriminative power for the
pathogenic versus neutral classification. The training set is carefully
constructed to avoid label contamination, excluding variants present in
both disease and population databases.

REVEL scores range from 0 to 1, with higher values implying greater
pathogenicity likelihood. Common interpretation thresholds treat scores
above 0.5 as supporting evidence for pathogenicity, with scores above
0.75 providing stronger evidence. REVEL is restricted to missense
single-nucleotide variants, making it more specialized than CADD but
often more accurate within its scope. This specialization reflects a
deliberate choice: by giving up coverage of non-coding and structural
variants, REVEL gains the ability to train on directly relevant labels
rather than evolutionary proxies.

\subsection{M-CAP}\label{m-cap}

Diagnostic laboratories evaluating potential Mendelian disease variants
face asymmetric consequences for errors. Calling a benign variant
pathogenic can lead to unnecessary surgeries, psychological burden, and
inappropriate cascade testing of family members. Missing a pathogenic
variant delays diagnosis but typically permits later reclassification as
evidence accumulates. This asymmetry argues for prioritizing specificity
over sensitivity in clinical settings.

\textbf{Mendelian Clinically Applicable Pathogenicity (M-CAP)} addresses
specifically the challenge of distinguishing pathogenic from benign rare
missense variants in Mendelian disease contexts, with explicit attention
to this asymmetry (\textbf{jagadeesh\_m-cap\_2016?}). The method uses
gradient boosting on a feature set including conservation scores,
protein structure features, and amino acid properties.

M-CAP was explicitly designed to minimize false positives while
maintaining reasonable sensitivity. The developers tuned their
classifier to achieve less than 5\% false positive rate on known
pathogenic variants, accepting some reduction in sensitivity as a
tradeoff. This design philosophy differs from methods that balance
sensitivity and specificity equally, reflecting M-CAP's intended use in
diagnostic settings where false positive pathogenicity calls have
serious consequences.

\subsection{Comparison and Selection}\label{comparison-and-selection}

No single ensemble method dominates across all variant types and
clinical contexts. CADD provides the broadest coverage (genome-wide, all
variant types) but may sacrifice accuracy within specific variant
classes to achieve this generality. REVEL often outperforms CADD on
missense-only benchmarks, reflecting its focused training objective.
M-CAP prioritizes specificity over sensitivity, appropriate for clinical
settings where avoiding false positives is paramount.

Clinical variant interpretation typically incorporates multiple
computational scores rather than relying on any single predictor.
Different scores may agree, providing stronger evidence, or disagree,
flagging variants requiring careful manual review. A variant with CADD
greater than or equal to 25, REVEL greater than or equal to 0.8, and
M-CAP ``possibly pathogenic'' presents a consistent computational
picture; one where CADD and REVEL disagree prompts closer examination of
the underlying features. Understanding the construction, training data,
and intended use case of each method is essential for appropriate
interpretation. The integration of these scores into clinical workflows
is examined in detail in Chapter~\ref{sec-clinical-risk}, where
computational evidence must be weighed alongside functional studies,
segregation data, and clinical presentation.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-ensemble-performance}{[}High{]} ROC curves (or
precision-recall curves) comparing CADD, REVEL, and M-CAP on a held-out
benchmark of missense variants. Include curves for individual component
scores (SIFT, PolyPhen-2, phyloP) to show improvement from integration.
Mark operating points corresponding to common clinical thresholds (CADD
â‰¥ 20, REVEL â‰¥ 0.75, M-CAP ``possibly pathogenic''). Annotate sensitivity
and specificity at each threshold. Include note about benchmark
circularity caveats.}

\end{figure}%

\section{Circularity and Ascertainment
Bias}\label{circularity-and-ascertainment-bias}

A diagnostic laboratory classifies a novel missense variant as
pathogenic based partly on its high CADD score. That classification
enters ClinVar. Two years later, a benchmarking study evaluates CADD
performance on ClinVar pathogenic variants and reports excellent
accuracy. Is the high performance genuine, or has the benchmark been
contaminated by the predictor's own influence on the labels it is
evaluated against? This scenario illustrates one of two pervasive
problems that affect all variant effect predictors: circularity between
scores and clinical databases. The second problem, \textbf{ascertainment
bias} in available training and testing variants, compounds the first.
\textbf{These issues do not invalidate classical scores, but they
counsel appropriate humility about performance claims and careful
attention in both development and application.}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-circularity-problem}{[}High{]} Circular diagram
illustrating the feedback loop between computational predictors and
clinical databases. Show cycle: (1) Computational score (e.g., high
CADD) influences clinical classification; (2) Classified variant enters
ClinVar as ``Pathogenic''; (3) Benchmarking study evaluates CADD on
ClinVar variants; (4) High benchmark performance encourages clinical
adoption; (5) Return to step 1. Indicate intervention points: temporal
holdouts, functional assay ground truth, prospective evaluation. Use
visual metaphor of self-reinforcing spiral.}

\end{figure}%

\subsection{The Circularity Problem}\label{the-circularity-problem}

ClinVar and similar clinical databases increasingly incorporate
computational predictions as evidence supporting variant classification.
When a clinical laboratory classifies a variant as pathogenic, the CADD
score, PolyPhen-2 prediction, or other computational evidence may have
contributed to that determination. When CADD is subsequently evaluated
on ClinVar pathogenic variants, its performance is artificially
inflated: the benchmark contains variants that were labeled partly
because CADD assigned them high scores. \textbf{The predictor appears to
perform well because it was already part of the labeling process.}

This circularity operates through multiple pathways. Direct use occurs
when clinical laboratories explicitly cite computational scores in their
classifications. Indirect influence arises when computational
predictions shape clinical suspicion, affecting which variants receive
functional testing or expert review. Selection bias in benchmark
construction can compound the problem: benchmark creators may
preferentially include variants with strong computational evidence,
excluding ambiguous cases that would provide a more stringent test.

The consequence is that benchmark performance may overestimate
real-world utility. A method that has been widely adopted will appear to
perform well on benchmarks populated by variants classified using that
method, even if its true discriminative power is more limited. This
concern applies to all established computational tools, including
conservation scores, protein-level predictors, and ensemble methods. The
more influential a method becomes, the more its benchmark performance
becomes self-reinforcing.

Addressing circularity requires careful benchmark construction. Temporal
holdouts (using only classifications made before a method's widespread
adoption) can reduce but not eliminate the problem. Functional assays
that directly measure variant effects provide ground truth independent
of computational predictions but are available for only a small fraction
of variants. Prospective evaluation on newly classified variants offers
the cleanest test but requires patience and ongoing data collection. The
foundation model evaluations discussed in Chapter~\ref{sec-vep-fm} face
these same challenges, and the confounding issues examined in
Chapter~\ref{sec-confounding} show how these problems persist and evolve
as methods become more sophisticated.

\subsection{Ascertainment Bias}\label{ascertainment-bias}

Beyond circularity lies a more fundamental problem: the variants
available for training and evaluation represent a systematically skewed
sample of disease-causing mutations. Clinical databases are dominated by
variants in well-studied genes, particularly those causing severe
Mendelian phenotypes with clear inheritance patterns. Protein-coding
variants are overrepresented because they are easier to interpret and
more often tested. Variants in genes associated with common diagnostic
panels appear frequently; those in rarely tested genes are sparse.

This ascertainment bias shapes what models learn and how they perform. A
predictor trained predominantly on variants in constrained genes may
learn that gene-level constraint is the primary signal for
pathogenicity. When applied to variants in less constrained genes (where
pathogenic variants also occur, but less frequently), the model may
systematically underestimate risk. Similarly, models trained on
European-ancestry samples may encode population-specific patterns that
transfer poorly to other populations, compounding health disparities by
providing less accurate predictions for underrepresented groups.

The consequences extend to evaluation. Benchmark variants inherit the
ascertainment biases of their source databases. Strong performance on
benchmark sets may not translate to the rare genes, unusual variant
types, or underrepresented populations encountered in real clinical
practice. Variants that are ``easy'' to classify (stop-gains in highly
constrained genes) are overrepresented in benchmarks, while
diagnostically challenging variants (missense variants in
moderate-constraint genes, non-coding variants) are underrepresented.
\textbf{The benchmark tells us how well the method performs on the easy
cases; it may reveal little about the hard cases where computational
assistance is most needed.}

\subsection{Implications for Clinical
Use}\label{implications-for-clinical-use}

These limitations do not render classical scores useless, but they
counsel appropriate humility in interpretation. Computational
predictions provide one line of evidence among several in variant
interpretation. Strong scores in expected directions support clinical
suspicion; unexpected scores prompt careful review. No computational
score should override clear clinical or functional evidence, and
borderline scores in complex cases may warrant agnosticism rather than
confident prediction.

The ACMG-AMP framework for variant classification appropriately treats
computational predictions as supporting evidence (PP3 for predictions
supporting pathogenicity, BP4 for predictions supporting benign status)
rather than standalone criteria (\textbf{richards\_standards\_2015?}).
Multiple lines of computational evidence may be combined, but the weight
assigned should reflect the limitations outlined here. Variants
classified primarily on computational grounds should be flagged for
potential reclassification as additional evidence emerges.

\section{Limitations of the Feature Engineering
Paradigm}\label{limitations-of-the-feature-engineering-paradigm}

Classical variant effect prediction achieved substantial success in
prioritizing potentially pathogenic variants and established conceptual
foundations that persist in modern methods. The integration of diverse
annotations, use of evolutionary signals as proxy labels, and
genome-wide precomputation all anticipate contemporary practices. Yet a
fundamental tension remains: manually designed features encode only what
biologists already know, and the complexity of genotype-phenotype
relationships exceeds what explicit feature engineering can capture.
\textbf{The features are not wrong; they are incomplete in ways that
cannot be remedied by adding more of the same.}

\subsection{The Feature Ceiling}\label{the-feature-ceiling}

Feature-engineered methods encode human knowledge about which genomic
properties matter for variant function. Conservation scores capture
evolutionary constraint; protein-level predictors encode structural
intuitions; regulatory annotations mark biochemically active regions.
This encoded knowledge is valuable but necessarily incomplete.
Biologists cannot specify all relevant patterns in advance, and the
interactions between features may be too complex for simple combination
rules to capture.

The performance of feature-engineered methods is therefore bounded by
the quality and completeness of the features themselves. Adding more
features provides diminishing returns as the most informative signals
are exhausted. Interactions between features (a variant in a conserved
enhancer within a constrained gene) may require explicit specification
or rely on simple combination rules that miss nonlinear relationships.
The linear SVM at the heart of CADD cannot represent the complex feature
interactions that characterize biological regulation.

\subsection{Limited Context}\label{limited-context}

Classical features typically describe variants in isolation or with
minimal context. Conservation scores examine each position
independently. Protein-level predictors consider amino acid
substitutions without full protein context. Gene-level features apply
uniformly across entire genes regardless of position-specific effects.
This limited context prevents classical methods from learning the
complex sequence patterns that determine variant effects.

A variant disrupting a critical transcription factor binding motif may
escape detection if the motif is not annotated, even though the
underlying sequence pattern is learnable from data. A missense variant
at a protein-protein interface may be more damaging than one in a loop
region, but capturing this distinction requires understanding protein
structure and interactions that feature engineering incompletely
represents. The local sequence context surrounding a variant often
matters, but which contexts matter and how requires learning from data
rather than specification by experts.

\subsection{The Persistent Gap Between Measurement and
Need}\label{the-persistent-gap-between-measurement-and-need}

Throughout this chapter, we have seen how each classical method measures
something related to but distinct from clinical pathogenicity.
Conservation scores measure evolutionary constraint. Protein-level
predictors measure functional disruption. CADD measures evolutionary
tolerance. Each provides genuine biological signal, but none directly
answers the clinical question: will this variant cause disease in this
patient?

This gap is not merely a limitation of specific methods but reflects
something deeper about the variant interpretation problem. The
clinically relevant question depends on context (which tissue, which
genetic background, which environmental exposures) that no current
method captures. Even perfect prediction of functional disruption would
not resolve questions of penetrance, expressivity, and disease
mechanism. The methods in this chapter provide important evidence for
variant interpretation, but they cannot substitute for the integrative
clinical reasoning examined in Chapter~\ref{sec-clinical-risk}.

\subsection{From Features to
Representations}\label{from-features-to-representations}

The transition from CADD to deep learning methods represents a
fundamental shift in how variant effect prediction is approached: from
encoding biological knowledge as hand-crafted features to learning
representations directly from sequence data. Where SIFT computes
conservation from explicit multiple sequence alignments, protein
language models learn similar signals implicitly from sequence alone.
Where PolyPhen-2 engineers features from solved protein structures,
structure prediction models learn geometric constraints from
evolutionary covariation. The shift is real and consequential.

Yet learned representations do not automatically overcome the
limitations that constrain classical methods. The circularity between
training labels and evaluation benchmarks affects transformer-based
predictors exactly as it affects logistic regression. A model trained on
ClinVar pathogenic variants inherits ClinVar's ascertainment biases
whether it uses 100 features or 100 million parameters. Rare variants
remain difficult because they are rare in training data. Novel
mechanisms remain invisible because no labeled examples exist. The
variant effect prediction problem is fundamentally difficult, and
methodology alone cannot resolve difficulties rooted in data
availability and biological complexity.

Classical methods remain valuable: as baselines that establish the
performance floor modern methods must exceed, as interpretable
components when understanding matters as much as prediction, and as
reminders of what the field has learned about which signals carry
predictive information. The chapters that follow examine how foundation
models have advanced variant effect prediction
(Chapter~\ref{sec-vep-fm}), but honest evaluation requires understanding
what classical methods achieved and where all approaches, classical and
modern alike, continue to struggle.

\part{Part II: Sequence Architectures}

Every neural network architecture encodes assumptions about biology.
Convolutional networks assume that local patterns matter and that the
same motifs are meaningful regardless of genomic position. Attention
mechanisms assume that distant positions can interact directly without
passing information through intermediate representations. Pretraining
objectives assume that certain patterns in unlabeled sequence provide
useful supervision in the absence of functional labels. These
assumptions, embedded in architectural choices made before any training
begins, determine which biological phenomena the model can capture and
which remain invisible to it.

This part examines each architectural component in turn, building from
fundamental representation choices through pattern-recognition
mechanisms to the training and adaptation strategies that make
foundation models practical. Chapter~\ref{sec-representations}
establishes how tokenization choices propagate through model design,
from one-hot encoding through byte-pair encoding to biologically
informed vocabularies. Chapter~\ref{sec-cnn} introduces convolutional
neural networks, the architecture that first demonstrated deep learning
could outperform handcrafted features for regulatory genomics by
learning sequence-to-function mappings directly from data.
Chapter~\ref{sec-attention} unpacks the self-attention mechanism and
transformer architecture, showing how these components enable both local
pattern recognition and long-range dependency modeling across genomic
sequences.

Chapter~\ref{sec-pretraining} surveys the landscape of self-supervised
objectives, examining how masked language modeling, next-token
prediction, and denoising approaches each shape learned representations
and encourage models to discover different biological patterns.
Chapter~\ref{sec-transfer} closes the loop by addressing how pretrained
models are adapted to downstream tasks through fine-tuning, few-shot
learning, and deployment strategies. Together, these chapters provide
the conceptual foundation needed to understand the specific model
families examined in Part III and the applications developed throughout
the remainder of the book.

\chapter{Tokens and Embeddings}\label{sec-representations}

Before a genomic model learns any parameters, before it sees any
training data, before architecture choices are made, a prior decision
has already constrained what it can discover: how will sequence be
represented? This decision is not merely technical. A tokenization
scheme that merges nucleotides into coarse multi-base units may obscure
the single-nucleotide resolution needed to detect pathogenic splice
variants. An embedding strategy that encodes local context may lose the
long-range dependencies that connect distal enhancers to their target
genes. A position encoding that assumes fixed sequence length may fail
on the variable-length inputs that clinical applications require. These
choices propagate through every subsequent design decision, shaping what
patterns the model can detect, what resolution it can achieve, and
ultimately which biological questions it can answer.

The challenge is that biological structure operates at multiple scales
simultaneously, and no single representation captures all scales equally
well. Transcription factor binding sites span 6 to 12 nucleotides; the
regulatory grammar linking multiple sites extends over hundreds of base
pairs. Coding sequences follow a strict three-nucleotide codon
structure; noncoding regions have no such constraint. A splice acceptor
consists of just two nucleotides (the AG dinucleotide marking exon
boundaries), yet splice regulation depends on sequences spanning the
entire intron. Any representation scheme must navigate these biological
realities while remaining computationally tractable for sequences that
dwarf typical language model inputs by orders of magnitude.

An analogy to natural language processing illuminates the fundamental
tradeoffs. Training a language model on English text requires deciding
how to segment the continuous character stream into discrete tokens.
Character-level tokenization preserves maximum resolution but creates
sequences too long for efficient processing. Word-level tokenization
compresses the sequence but loses information about morphology and
subword structure. Learned subword vocabularies (byte-pair encoding,
SentencePiece) balance these concerns by letting corpus statistics guide
segmentation. DNA presents similar choices but with critical
differences: only four letters rather than dozens, no natural word
boundaries, and the biological structure operating at multiple scales
that language lacks. This chapter examines the representation strategies
that genomic foundation models employ, from fixed k-mer vocabularies
through learned tokenization schemes, and traces how each choice shapes
what downstream models can learn.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-tokenization-comparison}{[}Essential{]} Four-panel
comparison showing the same 30-nucleotide regulatory sequence tokenized
four different ways. Panel A (One-hot): 30 tokens, one per nucleotide,
shown as 4Ã—30 binary matrix with color-coded nucleotides. Panel B
(Overlapping 6-mers): Still \textasciitilde30 tokens due to overlap,
with brackets showing how tokens share nucleotides. Panel C (BPE):
\textasciitilde10-15 variable-length tokens, with token boundaries
marked showing compression of repetitive regions. Panel D
(Single-nucleotide for Hyena/Mamba): 30 tokens like one-hot but with
learned embeddings indicated. Annotate sequence length, token count, and
effective compression ratio for each.}

\end{figure}%

\section{One-Hot Encoding: The CNN
Foundation}\label{one-hot-encoding-the-cnn-foundation}

A child inherits a \emph{DMD} variant from her mother. Whether this
variant causes Duchenne muscular dystrophy or remains clinically silent
depends on its exact position relative to the exon-intron boundary: one
nucleotide can determine whether the splicing machinery recognizes the
junction. This is why single-nucleotide resolution is not a technical
nicety but a clinical necessity. The earliest deep learning approaches
to genomic sequence modeling recognized this requirement and adopted the
simplest representation capable of preserving it: one-hot encoding,
where each nucleotide becomes a sparse binary vector with a single
active element indicating its identity. Adenine is encoded as {[}1, 0,
0, 0{]}, cytosine as {[}0, 1, 0, 0{]}, guanine as {[}0, 0, 1, 0{]}, and
thymine as {[}0, 0, 0, 1{]}. A sequence of length \(L\) thus becomes a
matrix of dimensions \(4 \times L\), interpretable as four channels
analogous to the RGB channels of an image plus one.

The properties that made one-hot encoding dominant in the CNN era stem
from this simple design. The representation is lossless, preserving
every nucleotide explicitly without information compression. It
maintains single-nucleotide resolution, enabling detection of effects
from individual SNPs. The encoding exhibits translation equivariance,
meaning convolutional filters learn position-invariant motifs
recognizable anywhere in the sequence. And it requires no preprocessing,
vocabulary construction, or tokenizer training, making implementation
straightforward. \emph{DeepSEA}, \emph{ExPecto}, and \emph{SpliceAI} all
employed one-hot encoding without modification, with convolutional
layers learning to detect sequence patterns directly from the binary
representation.

The key insight underlying CNN success with one-hot encoding is that
convolutions process sequences through local operations. Each filter
examines only a small window of positions at a time, and the sparse,
orthogonal nature of one-hot vectors poses no obstacle to this local
processing. First-layer filters effectively learn position weight
matrices that score short k-mer patterns, while deeper layers capture
combinations and spatial arrangements of these primitive motifs. The
representation worked because it aligned with the architectural
inductive bias of convolutions: local pattern detection does not require
global sequence compression.

For transformer architectures, one-hot encoding creates a fundamental
mismatch. Transformers compute attention between all pairs of positions
in a sequence, with computational cost scaling as \(O(L^2)\) where \(L\)
is sequence length. A 10 kb sequence requires 10,000 tokens, demanding
100 million pairwise attention computations per layer. This quickly
becomes prohibitive for the long sequences genomic applications require.
The problem compounds because transformers typically learn dense
embeddings for each token, but with only four possible nucleotides, the
embedding layer has minimal opportunity for rich representation
learning. The sparse one-hot vectors provide too little structure for
the embedding to transform meaningfully.

This mismatch produces severe context limitation. Transformer context
windows of 512 to 4,096 tokens translate to only 512 to 4,096 base pairs
when using one-hot encoding, representing a tiny fraction of genes or
regulatory regions. Compare this to \emph{Enformer}'s 200 kb receptive
field or \emph{SpliceAI}'s 10 kb context, both achieved through
architectural innovations operating on one-hot encoded sequence.
\textbf{For transformer-based foundation models, one-hot encoding forces
an impossible choice between the long contexts needed for regulatory
modeling and computational tractability.} This tension motivated the
search for alternative representations that could compress genomic
sequences into fewer tokens while preserving information needed for
biological prediction.

\section{K-mer Tokenization: The DNABERT
Approach}\label{k-mer-tokenization-the-dnabert-approach}

The computational constraints of one-hot encoding for transformers led
researchers to explore sequence compression through k-mer tokenization.
This approach treats overlapping subsequences of length \(k\) as tokens,
drawing an analogy between k-mers and words in natural language. Just as
sentences compose words carrying meaning through sequence and
combination, genomic sequences might be understood as k-mer ``words''
encoding biological function through their arrangement. \emph{DNABERT}
pioneered this approach for genomic transformers in 2021, using 6-mers
as tokens and training a BERT-style masked language model on human
reference sequences (Ji et al. 2021).

The k-mer vocabulary has a fixed size of \(4^k\) possible tokens. For
6-mers, this yields 4,096 distinct tokens, comparable to vocabulary
sizes in some natural language models. Each token represents six
consecutive nucleotides, creating direct correspondence between
subsequence and token identity. \emph{DNABERT} used overlapping k-mers:
for a sequence like ACGTACGT, successive 6-mer tokens share five
nucleotides with their neighbors. The sequence position advances by one
nucleotide at a time, generating one token per position (minus the
\(k-1\) positions at the sequence end where a complete k-mer cannot
form).

\emph{DNABERT} provided valuable proof of concept for genomic
transformers. It demonstrated that self-supervised pretraining on raw
DNA sequences could improve performance over training from scratch, that
learned embeddings could capture biologically meaningful regularities
even when trained only on the reference genome, and that BERT-style
architectures could transfer across multiple downstream tasks.
\emph{DNABERT} achieved strong performance on promoter prediction,
splice site identification, and transcription factor binding site
recognition after fine-tuning with relatively small amounts of
task-specific labeled data.

Subsequent analysis revealed fundamental limitations rooted in the
overlapping design. \emph{DNABERT-2} articulated these problems clearly
in 2024 (Z. Zhou et al. 2024). Overlapping k-mers provide no sequence
compression: the number of tokens equals the number of nucleotides
(minus a small constant), so context window limitations persist
unchanged. A 10 kb sequence still requires approximately 10,000 tokens,
and the quadratic attention complexity remains prohibitive for long
sequences. \textbf{The very design that seemed to add biological meaning
through k-mer structure failed to address the computational bottleneck
motivating the approach.}

The overlapping design creates additional complications beyond
computational cost. A single nucleotide contributes to \(k\) different
tokens (each k-mer containing that position), complicating
interpretation of which token drives any given prediction. This
ambiguity becomes particularly problematic for variant effect
interpretation, where understanding how a specific nucleotide change
alters model predictions is essential. The effect of a single
substitution propagates through \(k\) different tokens in ways that can
be difficult to disentangle. The model must also learn that overlapping
tokens share nucleotides, a relationship obvious from the tokenization
scheme but requiring discovery through training. This redundancy
consumes model capacity that could otherwise capture more complex
biological patterns. The fixed \(4^k\) vocabulary does not adapt to
corpus statistics; frequent and rare k-mers receive equal representation
capacity in the embedding table despite potentially differing importance
for prediction.

\section{Byte Pair Encoding: Learning the
Vocabulary}\label{byte-pair-encoding-learning-the-vocabulary}

The limitations of k-mer tokenization raise a question: what if the
vocabulary itself could be learned from data? Byte Pair Encoding
addresses this by constructing vocabulary through iterative discovery of
frequent subsequences rather than defining tokens through a fixed rule.
The algorithm, originally developed for data compression, builds
vocabulary through a simple procedure. BPE initializes the vocabulary
with single nucleotides: \{A, C, G, T\}. It then scans the training
corpus to count all adjacent token pairs, identifies the most frequent
pair, merges this pair into a new token added to the vocabulary, and
replaces all instances in the corpus with the merged token. The process
repeats through many iterations (typically thousands), building a
vocabulary of variable-length tokens capturing frequently occurring
sequence patterns.

The critical difference from k-mer tokenization is that BPE produces
genuine sequence compression through non-overlapping tokens. Unlike
overlapping k-mers where each nucleotide generates its own token, BPE
creates tokens spanning multiple nucleotides without overlap. A 10 kb
sequence might compress to 2,000 or 3,000 tokens depending on its
repetitive structure, enabling transformers to process substantially
longer sequences within the same context window.

\emph{DNABERT-2} replaced 6-mer tokenization with BPE and demonstrated
dramatic improvements (Z. Zhou et al. 2024). The new model achieved
comparable performance to state-of-the-art approaches while using 21
times fewer parameters and requiring approximately 92 times less GPU
time in pretraining. These efficiency gains stem directly from
non-overlapping tokenization: actual sequence compression enables
processing longer sequences with the same computational budget, and
eliminating overlapping token redundancy allows the model to focus
capacity on learning biological patterns rather than token
relationships.

The BPE vocabulary learns corpus statistics through its construction
process. Repetitive elements appearing frequently throughout the genome
(such as Alu sequences or common regulatory motifs) receive dedicated
tokens spanning many nucleotides. These long tokens enable efficient
representation of repetitive regions while preserving single-nucleotide
resolution for unique sequences. Rare sequences that BPE never
encountered during vocabulary construction are represented as
concatenations of shorter subunits, maintaining the ability to encode
any sequence while allocating more representation capacity to common
patterns.

\emph{GROVER} (Genome Rules Obtained Via Extracted Representations)
extended this approach by training BPE specifically on the human genome
and selecting vocabulary using a custom next-k-mer prediction task
(Sanabria et al. 2024). Analysis of the resulting token embeddings
revealed that the learned vocabulary encodes biologically meaningful
structure without explicit supervision. Common tokens cluster separately
from rare ones in embedding space. GC-rich tokens segregate from AT-rich
tokens, reflecting the different properties of these sequence
compositions. Token length correlates with specific embedding
dimensions, allowing the model to represent both the content and extent
of each token. Some tokens appear primarily in repetitive regions while
others distribute broadly across the genome, and this localization
pattern emerges in the learned representations.

BPE introduces complications of its own that matter for clinical
applications. Variable-length tokens mean that variant positions fall at
different locations relative to token boundaries depending on local
sequence context. A SNP might fall in the middle of a long token in one
sequence context but at a token boundary in another, potentially
affecting how the model represents and processes the variant.
\textbf{The same nucleotide change may alter different numbers of tokens
depending on surrounding sequence, creating inconsistent input
representations for what should be comparable biological events.} The
trade-off between compression and interpretability becomes a design
choice depending on intended application.

\section{Single-Nucleotide Tokenization: Maximum
Resolution}\label{single-nucleotide-tokenization-maximum-resolution}

While k-mer and BPE tokenization compress sequences to enable longer
context windows, they sacrifice the single-nucleotide resolution
essential for variant effect prediction. A single nucleotide
polymorphism can completely alter protein function through mechanisms
ranging from amino acid substitution to splice site disruption to
regulatory element ablation. When a pathogenic variant and a benign
variant differ by one nucleotide position, multi-nucleotide tokens
obscure exactly where variants fall and how they relate to the
boundaries of biological features.

\emph{HyenaDNA} took the opposite approach in 2023, using
single-nucleotide tokens with no compression whatsoever (Nguyen et al.
2023). Each nucleotide (A, C, G, T) becomes a separate token,
maintaining maximum possible resolution. Every nucleotide is
independently represented, SNP effects can be isolated to specific token
positions without ambiguity, and no tokenization artifacts depend on
surrounding sequence context.

The challenge is sequence length. A 1 Mb region requires 1 million
tokens, far beyond the capacity of any standard transformer. The
quadratic attention complexity would demand a trillion pairwise
computations per layer, rendering the approach computationally
infeasible with conventional architectures.

\emph{HyenaDNA} addressed this challenge through architectural
innovation rather than tokenization compromise. The Hyena architecture
replaces the attention mechanism with implicit convolutions (long
convolutions parameterized by a small neural network) that scale
sub-quadratically with sequence length. Where attention computes
explicit pairwise interactions between all positions, Hyena achieves
similar representational power with \(O(L \log L)\) complexity rather
than \(O(L^2)\). This enables processing sequences hundreds of times
longer than attention-based transformers within the same computational
budget.

The practical impact was substantial: a 500-fold increase in context
length over dense attention models while maintaining single-nucleotide
resolution. \emph{HyenaDNA} could process 1 Mb sequences where
\emph{DNABERT} was limited to approximately 500 bp and the
\emph{Nucleotide Transformer} to approximately 6 kb. On the Nucleotide
Transformer benchmarks, \emph{HyenaDNA} reached state-of-the-art
performance on 12 of 18 datasets with orders of magnitude fewer
parameters and less pretraining data. On GenomicBenchmarks, it surpassed
prior state-of-the-art on 7 of 8 datasets by an average of 10 accuracy
points.

\emph{HyenaDNA} also demonstrated the first use of in-context learning
in genomics. The model could perform tasks based on examples provided in
the context window without any fine-tuning (conditioning on
demonstration sequences rather than updating parameters). This
capability, familiar from large language models, had not previously been
shown for genomic sequences and suggests that very long context combined
with high resolution enables qualitatively new forms of biological
reasoning.

The development of sub-quadratic architectures including Hyena, Mamba,
and state space models has fundamentally changed the tokenization
calculus. \textbf{When computational constraints no longer force a
choice between resolution and context length, single-nucleotide
tokenization becomes the natural choice for applications requiring
precise variant interpretation.} The architectural innovations examined
in Chapter~\ref{sec-attention} effectively decouple the resolution
decision from the context length decision, eliminating what had seemed
like an inherent trade-off.

\section{Biologically-Informed
Tokenization}\label{biologically-informed-tokenization}

Standard tokenization schemes treat DNA as a homogeneous string of
characters, ignoring the biological reality that different genomic
regions serve fundamentally different functions and follow different
structural rules. Coding sequences obey a strict codon structure where
every three nucleotides encode an amino acid; noncoding regions have no
such constraint. Treating these regions identically wastes an
opportunity to build biological knowledge directly into the
representation.

For protein-coding regions, the natural unit of sequence is the codon
rather than the individual nucleotide. \emph{GenSLMs} pioneered
codon-level tokenization for genomic foundation models in 2022, treating
each three-nucleotide codon as a single token and exploiting the fact
that codons are the biologically meaningful units of protein-coding
sequence (Zvyagin et al. 2022). The 64-codon vocabulary captures the
complete space of possible genetic code words, with each token
corresponding to either an amino acid or a stop signal. This alignment
with translation semantics means that mutations affecting amino acid
identity (nonsynonymous changes) alter the token sequence, while
synonymous mutations within a codon alter the specific token used but
maintain broader codon-family structure.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-biological-tokenization}{[}Enhancing{]} Comparison
of standard vs.~biologically-informed tokenization on a gene structure
diagram. Show: (1) Standard BPE tokenizing across codon boundaries in
coding regions; (2) Codon-aware tokenization (GenSLMs/Life-Code)
respecting reading frame with 64-codon vocabulary; (3) BioToken-style
representation with explicit variant tokens, regulatory element markers,
and structural annotations. Highlight how biological structure can be
encoded directly into tokenization.}

\end{figure}%

\emph{Life-Code} extended codon-aware tokenization to broader genomic
contexts in 2025, encoding coding and noncoding regions in a way that
respects reading frame and local biological function (Liu et al. 2025).
Coding regions are tokenized by codons, aligning token boundaries with
the fundamental unit of protein translation. Noncoding regions, lacking
codon structure, are tokenized by learned patterns capturing regulatory
motifs and other functional elements. This biologically-informed design
enables \emph{Life-Code} to learn protein structure through knowledge
distillation from protein language models, capture interactions between
coding and noncoding regions within a unified framework, and achieve
state-of-the-art results across tasks involving DNA, RNA, and protein.

\emph{BioToken} extends tokenization further to include explicit genomic
structural annotations (Medvedev et al. 2025). Rather than treating
variants as implicit changes in the sequence string, \emph{BioToken}
creates tokens explicitly representing SNPs, insertions, and deletions.
Known regulatory elements receive dedicated tokens encoding their
presence and type. Gene structure, chromatin state, and other functional
annotations integrate directly into the token representation. This
approach treats tokens as rich entities bundling nucleotides with
positional, functional, or experimental context.

Variant-aware representations hold particular promise for clinical
applications, where the input is often ``reference plus variant'' rather
than a generic sequence. By incorporating biological inductive biases
directly into tokenization, \emph{BioToken}'s associated model achieves
competitive or superior performance to specialized models like
\emph{Enformer} and \emph{SpliceAI} with significantly fewer parameters.
\textbf{This efficiency suggests that appropriate representation can
partially substitute for model scale by making the learning problem
easier through informed structure.}

The broader principle is that tokenization can and should incorporate
biological structure when that structure is known and relevant. BPE
learns statistical patterns from the corpus, but those patterns need not
correspond to biological units. Codon tokenization imposes biological
semantics directly, at the cost of applicability to noncoding regions.
Future approaches might combine these strategies: codon-aware
tokenization for coding regions, BPE or single-nucleotide tokens for
noncoding sequence, and explicit variant tokens for clinical
interpretation tasks.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-embedding-space}{[}High{]} UMAP or t-SNE
visualization of k-mer or BPE token embeddings from a trained DNA
language model (e.g., DNABERT-2 or GROVER). Color points by: Panel A: GC
content (gradient from AT-rich to GC-rich). Panel B: Token frequency
(common vs.~rare). Panel C: Genomic context (coding, regulatory,
repetitive). Show how biologically meaningful structure emerges without
explicit supervision.}

\end{figure}%

\section{From Tokens to Embeddings: Learning
Representations}\label{from-tokens-to-embeddings-learning-representations}

A patient's genome contains a variant of uncertain significance in
\emph{SCN5A}, a cardiac ion channel gene. Whether this variant affects
protein function depends on subtle sequence features that determine how
the protein folds, where it localizes, and how it interacts with other
cellular components. The clinical question is binary (pathogenic or
benign), but the biological answer emerges from continuous biophysical
properties. This gap between discrete genetic variation and continuous
biological effect is precisely what embedding layers must bridge:
transforming discrete tokens into dense numerical representations that
neural networks can process and from which they can learn.

The operation itself is simple: a lookup table assigns each token to a
learned vector. The embedding layer maintains a matrix \(E\) of
dimensions \(V \times d\), where \(V\) is vocabulary size and \(d\) is
embedding dimension. Each token maps to a row of this matrix, and during
training, backpropagation adjusts the embedding vectors to support
downstream prediction. This simplicity belies its importance; the
distinction between discrete tokens and their dense representations
shapes what models can learn.

Consider the difference between one-hot encoding and learned embeddings.
A one-hot representation treats each nucleotide as maximally distinct
from every other: the dot product between any two different nucleotides
is zero, providing no information about their relationships. Adenine and
thymine are equally different from each other as adenine and guanine,
despite the biological reality that purines (A, G) share structural
properties distinct from pyrimidines (C, T), and that complementary base
pairs (A-T, G-C) have special significance for DNA structure and
function.

Learned embeddings allow the model to discover such relationships from
data. If distinguishing purines from pyrimidines helps the model predict
regulatory function, the embedding space will organize to reflect this
distinction. If complementary relationships matter, they will emerge in
the geometry of the learned space.

The embedding dimension \(d\) controls representational capacity. Small
embeddings of 32 to 64 dimensions suffice for simple tokenization
schemes like single nucleotides, where only four vectors must be
distinguished. Larger vocabularies require larger embeddings:
\emph{DNABERT-2}'s BPE tokens use 768-dimensional embeddings, comparable
to natural language models. The choice involves a trade-off between
expressiveness and efficiency, as larger embeddings increase both model
capacity and computational cost.

Analysis of trained DNA language models reveals that embedding spaces
organize around biologically meaningful properties even without explicit
supervision. GC content, often considered a nuisance variable in
genomics, emerges as a major axis of variation in embedding space
because it correlates with many functional properties including gene
density, chromatin accessibility, and mutation rate. Repetitive elements
cluster together in embedding space. Coding sequence embeddings differ
systematically from noncoding embeddings, even when the tokenization
scheme makes no explicit distinction between these region types.

This emergent organization has practical implications. The structure
learned in the embedding layer propagates through all subsequent
computations. If embeddings fail to capture relevant distinctions, later
layers must learn them from scratch. If embeddings encode spurious
correlations, the model may exploit them inappropriately. Understanding
what embeddings learn, and whether that learning aligns with biological
reality, becomes an important diagnostic for model behavior
(Chapter~\ref{sec-interpretability}).

The relationship between tokenization and embedding deserves emphasis.
Coarse tokenization through large k-mers or aggressive BPE creates more
token types, each with room for rich embedding representations but
requiring the model to learn more parameters. Fine tokenization through
single nucleotides creates fewer token types with simpler embeddings but
forces the model to build complex representations through composition
across layers. Neither approach is uniformly superior; the optimal
choice depends on available training data, model scale, and task
requirements.

\section{Position Encodings: Where Tokens
Live}\label{position-encodings-where-tokens-live}

A regulatory variant 50 kilobases upstream of \emph{MYC} can drive
oncogenesis by disrupting an enhancer; the identical sequence change at
a different genomic location might have no phenotypic consequence.
Position is not merely metadata but fundamental biological information.
Yet transformers process tokens as sets rather than sequences. The
attention mechanism computes interactions between all pairs of tokens
regardless of their positions, treating a sequence as a bag of elements
with no inherent order. For language, this creates a problem: ``dog
bites man'' and ``man bites dog'' contain identical tokens but mean very
different things. In language, this positional dependency operates at
the scale of sentences or paragraphs; word order determines syntax and
meaning, but the relevant context rarely extends beyond a few hundred
tokens. Genomic position dependencies operate across fundamentally
different scales. A transcription factor binding site functions
differently depending on whether it sits in a promoter, an intron, or an
intergenic desert, and these distinctions can span tens of kilobases.
Enhancer-promoter interactions routinely bridge 50 to 500 kilobases of
intervening sequence. Moreover, absolute genomic coordinates carry
accumulated knowledge with no linguistic analog: the position
chr17:41,276,045 indexes decades of clinical observations, population
frequencies, and functional studies that inform variant interpretation
before a model processes a single nucleotide. For genomics, the problem
is more severe: a transcription factor binding site has entirely
different effects depending on whether it appears in a promoter, an
enhancer, or a gene body. \textbf{Position must somehow be encoded, and
the encoding strategy determines what spatial relationships the model
can learn.}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-position-encodings}{[}High{]} Four-panel comparison
of position encoding approaches. Panel A (Absolute/Learned): Heatmap
showing learned position embeddings across dimensions, with note about
fixed maximum length. Panel B (Sinusoidal): Wave patterns at different
frequencies across positions, showing how different dimensions capture
different scales. Panel C (ALiBi): Attention bias matrix showing linear
decay with distance, highlighting the implicit local preference. Panel D
(RoPE): 2D rotation visualization in embedding subspace showing how
relative position is encoded through rotation angle.}

\end{figure}%

The standard solution adds positional information to token embeddings
before attention computation. The combined representation carries both
content (what nucleotide or k-mer) and position (where in the sequence).
Several strategies have emerged, each with distinct properties that
matter for genomic applications.

\subsection{Absolute Positional
Embeddings}\label{absolute-positional-embeddings}

Absolute positional embeddings assign a learnable vector to each
position in the sequence. Position 1 receives embedding \(p_1\),
position 2 receives \(p_2\), and so forth. These embeddings add to the
token embeddings, creating combined representations carrying both
identity and location. BERT and early genomic transformers like
\emph{DNABERT} used this approach. The limitation is that the model can
only handle sequences up to the maximum position seen during training. A
model trained with 512-position embeddings cannot process position 513;
no embedding exists for it. This fixed maximum context proves
particularly restrictive for genomics, where biological phenomena span
scales from individual binding sites to megabase regulatory domains.

\subsection{Sinusoidal Positional
Encodings}\label{sinusoidal-positional-encodings}

Sinusoidal positional encodings address the fixed-length limitation by
computing position embeddings from mathematical functions rather than
learning them. The original Transformer paper used sinusoids of
different frequencies: \(PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})\) and
\(PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})\). Each position receives
a unique pattern of sines and cosines, with lower-frequency components
capturing coarse position and higher-frequency components capturing fine
position. The mathematical structure means any position, even one never
seen during training, can receive a well-defined encoding. Sinusoidal
encodings also have the property that relative positions are represented
consistently: the relationship between positions 10 and 20 mirrors that
between positions 110 and 120.

\subsection{Relative Positional
Encodings}\label{relative-positional-encodings}

Relative positional encodings directly represent the distance between
tokens rather than their absolute locations. When computing attention
between positions \(i\) and \(j\), the model incorporates information
about \((j - i)\), the relative offset. This approach recognizes that
for many biological phenomena, relative positioning matters more than
absolute coordinates. A transcription factor binding site 50 bases
upstream of a transcription start site has similar effects whether the
TSS is at genomic position 1,000 or 1,000,000. Relative encodings also
generalize naturally to sequences longer than those seen during
training, since relative distances remain bounded even as absolute
positions grow.

\subsection{Rotary Position
Embeddings}\label{rotary-position-embeddings}

Rotary position embeddings (RoPE) encode position by rotating token
embeddings in the complex plane (multiplying embeddings by a rotation
matrix whose angle depends on position) rather than adding a position
vector. This approach preserves relative distance information in the dot
product used for attention: the attention score between two positions
depends on their relative separation regardless of absolute location.
RoPE has become popular in recent large language models and has been
adopted by several genomic foundation models including variants of the
\emph{Nucleotide Transformer}.

\subsection{Attention with Linear
Biases}\label{attention-with-linear-biases}

ALiBi (Attention with Linear Biases) takes a different approach
entirely, adding position-dependent biases directly to attention scores
rather than modifying embeddings. The bias penalizes attention between
distant positions, with the penalty increasing linearly with distance.
ALiBi requires no learned position parameters and generalizes
straightforwardly to longer sequences. The linear distance penalty may
not perfectly capture biological relationships (where some regulatory
interactions span consistent long distances while others operate
locally), but the simplicity and extrapolation properties have proven
valuable.

\subsection{Genomic-Specific
Considerations}\label{genomic-specific-considerations}

For genomic applications, the choice of position encoding has
implications beyond sequence length. Biological coordinates matter: a
variant at chr17:41,276,045 has specific meaning that should be
preserved, and knowing the genomic coordinate enables lookup of prior
knowledge from population databases and clinical repositories.
Cross-strand relationships exist: the reverse complement of a sequence
carries related but distinct information. Circular genomes like
mitochondrial DNA and bacterial chromosomes have no beginning or end,
creating wraparound relationships that linear position encodings cannot
naturally represent.

Several recent models have explored genomic-specific position encoding
strategies. Some incorporate absolute genomic coordinates, allowing
models to learn position-specific patterns like centromeric sequences or
telomeric regions. Others encode strand explicitly, representing Watson
and Crick strands as distinct position modalities. Models for bacterial
or viral genomes sometimes use circular position encodings that respect
the topology of circular chromosomes. These adaptations illustrate that
position encoding is not merely a technical detail but a design choice
shaping what biological patterns a model can capture.

\section{Special Considerations for Biological
Sequences}\label{special-considerations-for-biological-sequences}

The double-stranded nature of DNA creates an ambiguity that has no
parallel in natural language: should a model treat the forward and
reverse complement strands as the same sequence, different sequences, or
related-but-distinct entities? A transcription factor binding site for
p53 functions when bound to either strand, yet the gene it regulates is
transcribed from only one. This strand ambiguity ripples through every
aspect of model design, from data augmentation to architectural
constraints to output interpretation.

A sequence ACGT on the forward strand corresponds to ACGT read 5' to 3',
but also implies the reverse complement TGCA on the opposite strand read
in the opposite direction. Some biological features are strand-specific:
a gene on the forward strand is transcribed from that strand only. Other
features are strand-agnostic: many transcription factor binding sites
function identically on either strand. Representation schemes must
decide whether to treat strands as equivalent through data augmentation
with reverse complements, as distinct through explicit strand encoding,
or as related-but-different through equivariant architectures processing
both strands jointly.

The \emph{Nucleotide Transformer} addressed strand by including both
orientations during training, using data augmentation to ensure the
model sees sequences from both directions. \emph{Caduceus} introduced a
more elegant solution in 2024: a bidirectional architecture processing
forward and reverse complement strands simultaneously through shared
computation (Schiff et al. 2024). The model outputs are equivariant to
reverse complementation (reversing and complementing the input produces
correspondingly transformed outputs). This inductive bias ensures
consistent treatment of strand without requiring augmentation or
doubling computational cost.

Circular genomes present another topological consideration. Bacterial
chromosomes and plasmids, mitochondrial DNA, and many viral genomes are
circular, with no natural start or end position. Linear position
encodings impose arbitrary boundaries on these sequences. Some models
address this through circular position encodings that wrap around at
sequence boundaries, while others process circular genomes as linear
sequences with the understanding that boundary effects may introduce
artifacts.

Genomic coordinates carry information absent from raw sequence. The
position chr17:41,276,045 refers to a specific location in the
\emph{BRCA1} gene, and variants at this position have been extensively
studied. Knowing the genomic coordinate enables lookup of prior
knowledge: population frequencies from gnomAD, clinical interpretations
from ClinVar, functional annotations from ENCODE. Some representation
schemes incorporate coordinate information explicitly, enabling models
to learn position-specific patterns and integrate with external
databases. Others deliberately exclude coordinates to force models to
learn purely from sequence, trading prior knowledge for generalization
to novel sequences or other species.

Multiple sequence inputs arise frequently in genomic applications.
Variant effect prediction requires comparing reference and alternate
alleles. Comparative genomics involves aligned sequences from multiple
species. Some regulatory predictions require input from multiple genomic
regions, such as promoter plus enhancer. Representation schemes must
accommodate these multi-sequence inputs through concatenation, paired
encoding, or specialized architectures processing multiple sequences
jointly.

\section{Trade-offs and Practical
Guidance}\label{trade-offs-and-practical-guidance}

The choice between tokenization strategies involves multiple competing
considerations depending on the intended application. Understanding
these trade-offs enables informed design decisions rather than arbitrary
choices.

\subsection{Resolution Versus
Compression}\label{resolution-versus-compression}

The tension between compression and resolution represents the
fundamental trade-off. Higher compression enables longer context windows
within fixed computational budgets but loses precision for identifying
exactly where variants fall and how they relate to biological features.
One-hot encoding and single-nucleotide tokenization provide no
compression but maintain full resolution. Non-overlapping k-mers achieve
approximately k-fold compression at the cost of k-nucleotide resolution.
BPE provides variable compression depending on sequence repetitiveness,
with correspondingly variable resolution. For variant effect prediction,
where single nucleotide changes can have dramatic phenotypic
consequences, resolution is paramount and the computational costs of
long single-nucleotide sequences are often justified.

\subsection{Vocabulary Size and Model
Capacity}\label{vocabulary-size-and-model-capacity}

Vocabulary size affects both model capacity and efficiency in ways that
interact with embedding design. Larger vocabularies require bigger
embedding tables but may capture more complex patterns directly in the
token representation. Smaller vocabularies are parameter-efficient but
require the model to learn compositional structure through multiple
layers. One-hot encoding's vocabulary of four tokens (plus special
tokens) minimizes embedding parameters but maximizes the compositional
learning burden. K-mer vocabularies scale exponentially with \(k\),
reaching 4,096 for 6-mers. BPE vocabularies are tunable, typically
ranging from 4,096 to 32,000 tokens for genomic applications.

\subsection{Computational Efficiency}\label{computational-efficiency}

Computational efficiency depends on both tokenization and architecture
in ways that have shifted as new architectures have emerged. For
standard attention with \(O(L^2)\) complexity, any compression directly
reduces cost: non-overlapping k-mers reduce attention cost by a factor
of \(k^2\), and BPE with average compression \(c\) reduces cost by
\(c^2\). Sub-quadratic architectures like Hyena and Mamba change this
calculus entirely, making single-nucleotide tokenization computationally
feasible at long contexts and eliminating the need to trade resolution
for efficiency.

\subsection{Variant Interpretation
Requirements}\label{variant-interpretation-requirements}

Variant interpretation has specific requirements favoring certain
representation choices. Single-nucleotide tokens enable clean comparison
of reference and alternate alleles at the same token position with no
ambiguity about effect localization. K-mer tokens complicate matters
because a single SNP changes \(k\) overlapping tokens, requiring
aggregation across affected tokens and introducing potential boundary
effects. BPE tokens create context-dependent effects where the same
variant may fall at different positions relative to token boundaries
depending on surrounding sequence.

\subsection{Practical Heuristics}\label{practical-heuristics}

Several heuristics have emerged from practical experience.
Single-nucleotide tokens work best when variant-level reasoning or
high-resolution interpretability is central to the application. K-mers
or BPE provide advantages when context length is the primary bottleneck
and tasks do not require base-level precision. Biologically-informed
tokens merit consideration when integrating multi-modal or
annotation-rich data. Position encoding should match task requirements:
relative encodings for tasks where absolute position is arbitrary,
coordinate-aware encodings for clinical applications requiring
integration with external databases.

\section{The Compression-Resolution
Trade-off}\label{the-compression-resolution-trade-off}

Every tokenization scheme embodies a trade-off between sequence
compression and nucleotide resolution. K-mer tokenization reduces
sequence length by a factor of k, enabling longer genomic contexts
within fixed attention windows, but a single-nucleotide variant may
alter multiple overlapping tokens in ways that obscure the mutation's
identity. Byte-pair encoding learns compression patterns from sequence
statistics, achieving efficient representation of common motifs while
preserving rare sequences at higher resolution. One-hot encoding
maintains single-nucleotide precision but offers no compression,
limiting context length in attention-based architectures. No choice is
universally optimal; the right representation depends on whether the
downstream task prioritizes context length, variant sensitivity, or
computational efficiency.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-compression-resolution}{[}High{]} Two-axis plot with
``Sequence Compression'' on x-axis (tokens per kilobase) and
``Nucleotide Resolution'' on y-axis. Position different approaches:
One-hot/single-nucleotide (no compression, full resolution), overlapping
k-mers (no compression, k-nucleotide resolution), non-overlapping k-mers
(k-fold compression, k-nucleotide resolution), BPE (variable
compression, variable resolution). Annotate practical context lengths
achievable with standard transformers (\textasciitilde4K tokens) for
each approach. Include callout showing clinical implication: ``Single
SNP affects\ldots{}'' with number of tokens per approach.}

\end{figure}%

These choices propagate through every subsequent modeling decision.
Position encodings in transformers must align with token boundaries.
Convolutional receptive fields span tokens, not nucleotides, making
effective genomic range dependent on tokenization. Transfer learning
inherits the tokenization of the pretrained model, constraining how
representations can be adapted to new tasks. A model pretrained with
6-mer tokenization cannot be fine-tuned for single-nucleotide variant
interpretation without architectural modification.

The field has moved from treating tokenization as fixed preprocessing to
recognizing it as a fundamental design decision shaping what models can
learn. Some architectures now learn tokenization jointly with
prediction, discovering representations optimized for specific tasks
rather than fixed in advance. As contexts extend to chromosome scale and
models grow to billions of parameters, the representation problem will
remain central to genomic foundation model design. Understanding these
trade-offs is prerequisite to understanding the architectural choices
examined throughout the remainder of this book.

\chapter{Convolutional Networks}\label{sec-cnn}

In 2015, a convolutional neural network trained on ENCODE chromatin data
learned to recognize transcription factor binding motifs that matched
entries in the JASPAR database, despite never seeing those motifs during
training (J. Zhou and Troyanskaya 2015). The network had discovered,
through gradient descent on raw sequence, patterns that experimental
biologists had spent decades cataloging. This was not merely a
demonstration that deep learning could match human-curated databases.
The deeper insight was that learned representations could transcend
existing annotations: predicting regulatory effects for any sequence, in
any genomic context, including regions never assayed in any experiment.
For the first time, computational methods could move beyond annotating
known regulatory elements to predicting the functional consequences of
sequence variation genome-wide.

The models examined in this chapter established paradigms that persist
in modern genomic AI. DeepSEA demonstrated that CNNs could predict
chromatin marks and transcription factor binding directly from DNA
sequence, enabling variant effect prediction without requiring
experimental measurements for every variant of interest. Basset extended
this approach to chromatin accessibility across cell types, learning
representations that transferred to new cellular contexts. SpliceAI
achieved clinical-grade accuracy for splice site prediction,
demonstrating that deep learning could match or exceed hand-crafted
algorithms developed over decades. Each model followed a common pattern:
train on functional genomics data, learn sequence features through
convolutional filters, and apply to variant interpretation. The success
was remarkable; the paradigm seemed complete.

Yet these models revealed a fundamental architectural limitation.
Convolutional networks integrate information only within their receptive
fields, the local region of input that contributes to each output
position. Genomic regulation routinely operates across distances that
exceed practical receptive field sizes: enhancers control genes across
tens of kilobases, topologically associating domains span megabases,
GWAS variants often lie far from the genes they affect. A model
analyzing a variant 50 kilobases from a gene promoter cannot connect the
variant to its target using local convolutions alone. Understanding both
what CNNs achieved and where they reached this architectural ceiling
establishes the foundation for the attention mechanisms examined in the
following chapter.

\section{Convolutions as Sequence Pattern
Detectors}\label{convolutions-as-sequence-pattern-detectors}

A variant in an enhancer 50 kilobases from its target gene cannot be
connected to that gene by a model that sees only 1,000 base pairs of
context. Consider a patient with familial hypercholesterolemia whose
whole-genome sequencing reveals a novel variant upstream of \emph{LDLR}.
The variant sits within a known enhancer region, but the enhancer and
the \emph{LDLR} promoter lie beyond the window any convolutional layer
can span. The model might correctly identify regulatory features at the
variant position, but it cannot learn that those features regulate
\emph{LDLR} rather than some other gene. This receptive field
constraint, inherent to convolutional architectures, determines what
relationships these networks can and cannot discover. The constraint is
not a limitation of training data or compute; it is architectural.

A convolutional filter slides across an input sequence, computing
similarity scores at each position. For genomic applications, the input
is typically \textbf{one-hot encoded} DNA: a binary matrix with four
rows (A, C, G, T) and columns for each position. Filters learn weight
patterns that respond to specific nucleotide arrangements. A filter of
width 8 nucleotides, for instance, computes a weighted sum of the
underlying nucleotides at each position, producing high activation when
the sequence matches its learned pattern and low activation otherwise.
This operation is mathematically equivalent to scanning a position
weight matrix across the sequence, but with a crucial difference: the
filter weights are learned during training rather than derived from
aligned binding site sequences.

The first layer of a genomic CNN typically contains hundreds of such
filters, each learning to detect different local patterns. Analysis of
trained filters consistently reveals correspondence to known
transcription factor binding motifs. The CTCF insulator motif, the ETS
family consensus sequence, the AP-1 binding site: these patterns emerge
from training on chromatin data without any explicit motif supervision.
The network identifies them because they predict the training labels.
Supervision on chromatin state induces discovery of the sequence
patterns that create chromatin state, providing unsupervised motif
learning as a byproduct of supervised prediction.

Deeper layers operate on the output of earlier layers rather than raw
sequence. A second-layer filter might learn to detect specific
arrangements of first-layer motifs: two ETS sites within 20 base pairs,
or a CTCF motif flanked by particular spacing patterns. This
hierarchical feature learning enables CNNs to capture regulatory grammar
beyond individual motifs, including spacing constraints, orientation
preferences, and combinatorial requirements that govern transcription
factor cooperativity.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-conv-pattern-detector}{[}Essential{]} Three-panel
figure showing convolution mechanics. Panel A: Single convolutional
filter (width 8) sliding across one-hot encoded DNA, producing
activation scores at each position; show high activation where filter
matches a motif. Panel B: Learned filter weights visualized as sequence
logo (PWM-style), aligned to corresponding JASPAR motif showing the
biological pattern discovered. Panel C: Multiple filters from first
convolutional layer, each detecting different motifs (CTCF, ETS, AP-1),
showing diverse pattern detection.}

\end{figure}%

\textbf{Pooling operations} between convolutional layers reduce spatial
resolution while increasing the receptive field. Max pooling selects the
strongest activation within a window, achieving position-invariant
detection where the network responds to a motif's presence somewhere in
a region rather than its exact position. This property suits regulatory
genomics, where binding site positions within an enhancer often matter
less than their presence and combination.

The receptive field of a convolutional network defines how much input
sequence can influence a single output prediction. For a network with
kernel width \(k\), pooling factor \(p\), and \(L\) layers, the
receptive field grows with depth but remains fundamentally limited by
architecture. A three-layer network with typical parameters might
integrate information from 200 to 1,000 base pairs. Reaching further
requires either more layers (increasing computational cost and training
difficulty) or \textbf{dilated convolutions} that space filter weights
to sample larger regions. When biological dependencies span tens of
kilobases, this receptive field ceiling becomes the fundamental
constraint that no amount of training data can overcome.

\section{DeepSEA: Regulatory Prediction from
Sequence}\label{sec-deepsea}

\subsection{The Noncoding Variant
Problem}\label{the-noncoding-variant-problem}

A patient presents with a rare disease phenotype, and whole-genome
sequencing reveals a novel variant in an intron 15 kilobases from the
nearest exon. The variant does not disrupt any annotated regulatory
element. No prior patient in any database carries this exact change. The
clinician must decide: is this variant pathogenic, or is it an
irrelevant passenger? Annotation-based methods offer no guidance. The
variant overlaps nothing cataloged, so overlap-based interpretation
returns nothing useful. Yet introns harbor splice regulatory elements,
and 15 kilobases places the variant well within range of enhancers that
might control the adjacent gene.

Existing approaches to noncoding variant interpretation relied on this
overlap paradigm. If a variant fell within a ChIP-seq peak or DNase
hypersensitive site, it might be flagged as potentially regulatory. The
strategy grounded predictions in experimental observations, but it could
not predict whether a variant would strengthen or weaken regulatory
activity, could not score variants in regions lacking experimental
coverage, and provided no mechanism for quantifying effect magnitude. A
variant might fall within an enhancer, but would it matter? The data
indicated where regulatory elements existed; they did not indicate how
sequence changes would affect them.

DeepSEA, introduced by Zhou and Troyanskaya, reframed the problem:
rather than asking whether a variant overlaps known annotations, ask
what regulatory activities a sequence encodes and how mutations would
alter them (J. Zhou and Troyanskaya 2015). The shift from annotation
lookup to sequence-based prediction enabled scoring any variant in any
genomic context, including regions never assayed in any experiment. This
reframing would prove more consequential than any specific architectural
choice.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-deepsea-architecture}{[}High{]} Three-panel figure.
Panel A: Architecture schematic showing input (1000bp one-hot), three
convolutional layers with pooling (320â†’480â†’960 filters), fully connected
layer, and 919 sigmoid outputs for chromatin features. Panel B:
First-layer filter aligned to JASPAR motif (e.g., CTCF), demonstrating
learned = known biology. Panel C: Scatter plot of predicted vs.~observed
allelic imbalance for DNase-seq, showing correlation that validates
variant effect prediction.}

\end{figure}%

\subsection{Architecture and
Training}\label{architecture-and-training-1}

The clinical scenario described above demands a model that can predict
function from sequence alone. DeepSEA's architecture was deliberately
simple by contemporary standards (placing the emphasis on the learning
framework rather than architectural complexity).

Input sequences of 1,000 base pairs, one-hot encoded, passed through
three convolutional layers with 320, 480, and 960 filters respectively.
Max pooling after each convolution compressed spatial dimensions. A
fully connected layer with 925 units integrated information across the
compressed representation, and a final output layer with 919 sigmoid
units produced independent probability predictions for each chromatin
profile.

Training data came from ENCODE and Roadmap Epigenomics: 690
transcription factor binding profiles, 104 histone modification
profiles, and 125 DNase I hypersensitivity profiles spanning diverse
cell types (Kagda et al. 2025; Kundaje et al. 2015). For each 1,000 bp
input, the model predicted whether the central 200 bp region exhibited
each chromatin feature. Chromosome 8 was held out for evaluation.

The \textbf{multi-task learning} formulation proved essential for
generalization. Predicting 919 features simultaneously forced the
network to learn shared representations useful across many prediction
problems. The first convolutional layer learns general sequence patterns
(GC content, common dinucleotides, ubiquitous motifs); these
representations then feed task-specific combinations in later layers.
Joint training provides implicit regularization, preventing overfitting
to any single task while amortizing the cost of learning basic sequence
features across all outputs.

\subsection{Learned Representations and Biological
Validation}\label{learned-representations-and-biological-validation}

Any sequence model faces a fundamental question: do learned features
correspond to biological reality, or do they exploit statistical
shortcuts that happen to correlate with labels? DeepSEA provided the
first large-scale evidence that deep learning could recover genuine
regulatory logic.

Analysis of first-layer filters revealed learned patterns matching known
transcription factor motifs. The network had independently recovered
sequence preferences cataloged in JASPAR and TRANSFAC, confirming that
the training objective (predicting chromatin state) induced biologically
meaningful feature extraction. This interpretability distinguished deep
learning from prior black-box approaches and suggested that the models
captured genuine regulatory logic rather than spurious correlations.
Systematic methods for extracting and visualizing these learned
representations, from filter analysis to attribution mapping, are
examined in Chapter~\ref{sec-interpretability}.

Deeper layers combined first-layer patterns into more complex
representations, capturing motif spacing requirements, orientation
preferences, and cooperative binding arrangements. The network encoded
relationships between sequence features that position weight matrices,
operating independently at each motif, could not represent.

DeepSEA outperformed gkm-SVM (gapped k-mer support vector machines) on
nearly all transcription factor binding prediction tasks. The pattern of
improvement revealed something fundamental: gkm-SVM showed no benefit
from longer input sequences, while DeepSEA performance improved
substantially with additional context. K-mer methods tally motif
occurrences but cannot learn relationships between patterns at different
positions. Hierarchical feature learning enables exactly what k-mer
methods cannot provide: representations of combinatorial regulatory
logic.

\subsection{Variant Effect Prediction}\label{variant-effect-prediction}

With a trained sequence-to-chromatin model, variant scoring becomes
straightforward: predict chromatin profiles for reference and
alternative sequences, compute the difference. This \textbf{in silico
mutagenesis} produces a 919-dimensional vector describing predicted
changes across all features. The model never encounters variant data
during training; effect prediction emerges from learned
sequence-function relationships applied to mutations the model has never
seen.

Validation used allelic imbalance data from digital genomic
footprinting. For variants showing allele-specific DNase I sensitivity,
DeepSEA predictions correlated with experimentally observed biases:
variants predicted to increase accessibility tended to show higher
accessibility on the corresponding allele. This correlation would not
exist if the model merely learned coarse sequence features insensitive
to point mutations.

\textbf{In silico saturation mutagenesis} extends single-variant scoring
to systematic characterization. By predicting effects of all possible
substitutions across a regulatory element, one identifies positions
where mutations most strongly perturb function. These critical positions
typically correspond to transcription factor binding motifs, providing
motif discovery that emerges from learned representations rather than
explicit sequence alignment. The approach enables characterization of
any regulatory element, including those in cell types or conditions
never experimentally profiled.

\section{Cell-Type Specificity and Regulatory
Grammar}\label{sec-basset-danq}

A variant that disrupts cardiac-specific gene regulation may be lethal
in the heart but entirely silent in neurons. A regulatory element active
during embryonic development may be permanently silenced in adult
tissues. Clinical variant interpretation therefore requires models that
capture not just what sequence patterns predict regulatory activity, but
how those predictions vary across the dozens of cell types and
developmental stages where a variant might act. DeepSEA's 919 chromatin
features spanned multiple cell types, but the question remained: could
architectural modifications better capture cell-type-specific programs
or learn richer representations of the combinatorial grammar governing
transcription factor cooperativity?

Basset, introduced by Kelley et al.~in 2016, focused specifically on
predicting chromatin accessibility from sequence
(\textbf{kelley\_basset\_2016?}). Rather than DeepSEA's diverse
chromatin features, Basset predicted DNase-seq peaks across 164 cell
types, enabling detailed analysis of cell-type-specific regulatory
activity. The architectural refinements Basset introduced would
influence subsequent models: \textbf{batch normalization} after
convolutional layers stabilized training and enabled deeper networks,
while larger filters in early layers (19 nucleotides in the first layer)
captured longer motifs directly rather than requiring the network to
compose them from smaller patterns.

The key contribution was demonstrating that in silico saturation
mutagenesis profiles from trained models could identify causal variants
underlying disease-associated haplotypes. GWAS identifies associated
regions but cannot distinguish the causal variant from nearby variants
in linkage disequilibrium. Basset's saturation mutagenesis provided a
principled approach: the variant with the strongest predicted regulatory
effect within an associated haplotype is the most likely causal
candidate. This moved beyond simple peak overlap toward mechanistic
variant prioritization, and subsequent validation studies confirmed that
model-prioritized variants showed higher rates of experimental
confirmation than lead SNPs selected purely by association strength.

DanQ explored whether regulatory grammar involves sequential
dependencies that convolutions alone might miss, combining convolutional
layers with bidirectional LSTMs to integrate motif detections across the
input window (\textbf{quang\_danq\_2016?}). The hybrid architecture
achieved modest improvements on chromatin prediction benchmarks, though
the recurrent components introduced costs examined in
Section~\ref{sec-rnn}.

These variations illustrated a broader principle: multiple architectures
could learn useful regulatory representations from sequence. The
specific choices (filter sizes, layer depths, recurrent components)
mattered less than the fundamental framework of learning from one-hot
encoded sequence to predict chromatin labels. This robustness suggested
that the underlying signal, sequence determinants of regulatory
activity, was strong enough to be captured by diverse architectural
approaches. For clinical applications, prediction quality depends more
on training data quality and task definition than on architectural
details within the CNN family.

\section{ExPecto: From Chromatin to Expression}\label{sec-expecto}

\subsection{Beyond Intermediate
Phenotypes}\label{beyond-intermediate-phenotypes}

A patient's tumor harbors a somatic variant in a putative enhancer
region. Chromatin profiling in matching tissue shows the region is
accessible. The variant is predicted to disrupt a transcription factor
binding site. Yet the clinician's question remains unanswered: does this
variant actually change expression of a target gene? Which gene? By how
much? In which tissues?

Chromatin accessibility and transcription factor binding are
intermediate phenotypes, means rather than ends. The ultimate functional
readout for most regulatory variants is their effect on gene expression.
A variant might disrupt a binding site, but sites can be redundant,
effects can be buffered, and the relationship between binding and
expression is not one-to-one. Predicting expression change from sequence
requires integrating regulatory signals across distances that determine
which enhancers control which promoters. A variant that disrupts binding
but does not alter expression is unlikely to be pathogenic, while a
variant with modest chromatin effects but strong expression consequences
may drive disease.

ExPecto, introduced by Zhou et al.~in 2018, addressed these questions by
extending sequence-to-chromatin prediction toward tissue-specific gene
expression (J. Zhou et al. 2018). The framework predicts expression
levels across 218 tissues and cell types by integrating predicted
chromatin signals across a 40 kb promoter-proximal window. This context
expansion, from DeepSEA's 1 kb to ExPecto's 40 kb, represented a
significant architectural commitment: expression prediction requires
integrating regulatory signals from distances far exceeding typical
motif sizes.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-expecto-pipeline}{[}Enhancing{]} Three-component
pipeline diagram. Component 1: Beluga CNN scanning 40kb window around
TSS with sliding 2kb windows, producing chromatin predictions at 200
spatial positions. Component 2: Spatial transformation with exponential
decay functions (upstream and downstream), reducing to
\textasciitilde20,000 features. Component 3: 218 tissue-specific linear
regression models, producing per-tissue expression predictions. Show
example delta scores for variant effect.}

\end{figure}%

\subsection{The Modular Architecture}\label{the-modular-architecture}

ExPecto comprises three sequential components, each addressing a
distinct computational challenge. The separation proved essential:
jointly optimizing all components end-to-end would be computationally
prohibitive, and the modular design enables interpretability at each
stage.

The first component, an enhanced CNN called Beluga, predicts 2,002
chromatin profiles from 2,000 bp input sequences. Beluga incorporated
architectural improvements over DeepSEA: six convolutional layers with
\textbf{residual connections}, expanded chromatin targets, and broader
cell-type coverage. This CNN scans the 40 kb region surrounding each
transcription start site with a moving window, generating chromatin
predictions at 200 spatial positions and producing over 400,000 features
per gene.

The second component transforms these high-dimensional features through
spatial aggregation. Ten exponential decay functions, applied separately
to upstream and downstream regions, encode the prior belief that nearby
elements contribute more than distant ones. This transformation reduces
dimensionality while preserving spatial relationships, producing
approximately 20,000 features per gene that capture both which chromatin
features are predicted and where they occur relative to the TSS.

The final component comprises 218 L2-regularized linear regression
models, one per tissue, predicting log expression from
spatially-transformed features. Linear models were chosen deliberately:
they provide interpretability, prevent overfitting given the
high-dimensional feature space, and enable coefficient analysis to
identify which chromatin features drive expression in each tissue. The
combination of a shared sequence-to-chromatin CNN with separate
tissue-specific linear heads cleanly separates sequence-level regulatory
grammar from tissue-specific regulatory programs.

\subsection{Expression Prediction and Variant
Effects}\label{expression-prediction-and-variant-effects}

ExPecto achieved 0.819 median Spearman correlation between predicted and
observed expression across tissues. Analysis of model coefficients
revealed automatic learning of cell-type-relevant features: the liver
expression model weighted HepG2-derived transcription factor features
most heavily; breast tissue models emphasized estrogen receptor features
from breast cancer cell lines. These tissue-specific patterns emerged
purely from learning to predict expression, without tissue identity
information provided to the chromatin model.

Variant effect prediction follows the same logic as DeepSEA: compare
expression predictions for reference and alternative sequences. Because
the model never trains on variant data, predictions are unconfounded by
linkage disequilibrium, a critical distinction from association-based
methods. ExPecto correctly predicted expression change direction for
92\% of the strongest GTEx eQTL variants, and experimental validation
confirmed that model-prioritized variants (not the GWAS lead SNPs)
showed allele-specific regulatory activity in reporter assays.

The 40 kb window represents an empirically optimized trade-off. Smaller
windows decreased performance; larger windows showed negligible
improvement. Most promoter-proximal regulatory information lies within
40 kb of the TSS, at least within ExPecto's linear modeling framework.
Distal enhancers beyond this window, while biologically important,
require architectural approaches that can model longer-range
dependencies. This limitation points toward the transformer
architectures examined in Chapter~\ref{sec-regulatory}.

\section{SpliceAI: Clinical-Grade Splicing
Prediction}\label{sec-spliceai}

\subsection{The Cryptic Splice
Problem}\label{the-cryptic-splice-problem}

A child presents with developmental delay and dysmorphic features
consistent with a known genetic syndrome. Clinical exome sequencing
reveals no pathogenic coding variants in the implicated gene. The case
is signed out as ``unsolved,'' the family left without answers. Three
years later, research RNA sequencing identifies aberrant splicing in the
syndromic gene: an intronic variant 150 base pairs from the nearest exon
creates a cryptic splice site, inserting a premature stop codon. The
diagnosis was hiding in plain sight, invisible to methods that only
examine canonical splice dinucleotides.

This scenario, replicated across thousands of unsolved rare disease
cases, illustrates a systematic blind spot in clinical genomics.
Splice-disrupting mutations represent a major mechanism of Mendelian
disease, yet variants affecting splicing outside canonical GT/AG
dinucleotides are systematically underascertained. Prior splice
prediction methods captured essential splice site motifs but could not
model the long-range determinants contributing to splicing specificity.
MaxEntScan operates on approximately 9 bp of context around donor and
acceptor sites (Yeo and Burge 2004). These methods produced many false
positives and missed variants acting through distal mechanisms: branch
points, exonic splicing enhancers, and intron length constraints that
previous models could not see.

SpliceAI, introduced by Jaganathan et al.~in 2019, demonstrated that
deep neural networks could learn splicing rules with near-spliceosomal
precision (Jaganathan et al. 2019). The model predicts splice site
locations directly from pre-mRNA sequence using 10,000 nucleotides of
context, an order of magnitude beyond prior methods. This context
expansion enabled recognition of distant splicing determinants invisible
to annotation-based approaches.

\subsection{Architecture: Depth and
Dilation}\label{architecture-depth-and-dilation}

Learning splicing rules from 10 kb of sequence context requires an
architecture that can integrate information across this entire span
while maintaining nucleotide-level resolution. SpliceAI achieves this
through two innovations: extreme depth enabled by residual connections,
and dilated convolutions that expand receptive fields without
proportional parameter growth.

SpliceAI employs an ultra-deep residual network with 32 convolutional
layers. Residual connections address the vanishing gradient problem that
otherwise prevents training at this depth:

\[
\text{output} = \text{input} + F(\text{input})
\]

By learning residual functions rather than direct mappings, the network
can propagate gradients through dozens of layers. Skip connections from
every fourth residual block feed directly to the penultimate layer,
further stabilizing training dynamics.

Dilated convolutions expand the receptive field efficiently. A dilated
convolution with rate \(d\) samples input positions at intervals of
\(d\) rather than consecutively. Stacking convolutions with increasing
dilation rates (1, 2, 4, 8, 16, and so on) allows the network to
integrate information across the full 10 kb window while maintaining
sensitivity to local patterns. Standard convolutions with small kernels
would require impractical depth to achieve equivalent receptive fields.

For each position in the pre-mRNA sequence, SpliceAI outputs three
probabilities: splice acceptor, splice donor, or neither. This
per-position classification enables fine-grained predictions across
entire transcripts. Training used GENCODE annotations, with odd and even
chromosomes split for training and testing.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-spliceai-architecture}{[}High{]} Two-panel figure.
Panel A: Diagram showing how dilated convolutions expand receptive field
without proportional parameter growth. Show dilation rates (1, 2, 4, 8,
16\ldots) with gaps between filter taps, illustrating how 32 layers with
dilation reach 10kb context. Panel B: SpliceAI's residual block
structure with skip connections from every 4th block to output, enabling
gradient flow through 32 layers.}

\end{figure}%

\subsection{Performance and
Validation}\label{performance-and-validation}

SpliceAI achieved 95\% top-k accuracy for splice site identification
(compared to 57\% for MaxEntScan) and 0.98 precision-recall AUC. Complex
genes exceeding 100 kb are often reconstructed to nucleotide precision.
Performance improved dramatically with context length:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Model Variant & Context (each side) & PR-AUC \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SpliceAI-80nt & 40 bp & 0.87 \\
SpliceAI-400nt & 200 bp & 0.93 \\
SpliceAI-2k & 1,000 bp & 0.96 \\
SpliceAI-10k & 5,000 bp & 0.98 \\
\end{longtable}

This progression confirms that distal sequence features contribute
meaningfully to splicing decisions. The diminishing returns above 2 kb
suggest that most splicing determinants lie within this range, though
the additional context still provides measurable benefit.

The \textbf{delta score} quantifies variant effects by comparing
predictions for reference and alternative sequences:

\[
\Delta\text{score} = \max_{|p - v| \leq 50} \left| P_{\text{alt}}(p) - P_{\text{ref}}(p) \right|
\]

Validation against GTEx RNA-seq showed that mutations with higher delta
scores showed higher validation rates at novel splice junctions:
approximately 50\% at Î” â‰¥ 0.2, 75\% at Î” â‰¥ 0.5, and 85\% at Î” â‰¥ 0.8.
Population genetics provided orthogonal support: predicted cryptic
splice variants showed 78\% depletion at common allele frequencies,
nearly matching the depletion of frameshift and stop-gain variants.
Natural selection treats these variants as deleterious, confirming their
functional impact.

\subsection{Clinical Impact}\label{clinical-impact}

SpliceAI's most significant contribution may be quantifying cryptic
splice mutations as a major, previously underappreciated cause of rare
genetic disorders. Analysis of de novo mutations in over 4,000
individuals with intellectual disability found significant enrichment of
predicted splice-disrupting variants compared to unaffected controls
(1.51-fold, p = 4.2Ã—10â»â´). Approximately 9\% of pathogenic de novo
mutations in intellectual disability act through cryptic splicing.
Including these variants in gene discovery analyses identified
additional candidate genes that would have fallen below discovery
thresholds when considering only protein-coding mutations.

This clinical utility explains SpliceAI's rapid adoption. Illumina
integrated SpliceAI into their annotation pipelines. Clinical genetics
laboratories worldwide use delta scores to flag potential
splice-affecting variants for RNA-seq follow-up. The model exemplifies
how task-specific deep learning can achieve clinical-grade accuracy on
well-defined problems. We return to SpliceAI's role in modern variant
interpretation workflows in Chapter~\ref{sec-vep-fm}.

\section{The Receptive Field Ceiling}\label{sec-receptive-field}

Consider a 45-year-old woman with early-onset breast cancer and a family
history suggesting hereditary risk. Whole-genome sequencing identifies a
novel variant 80 kilobases upstream of \emph{BRCA1}, within an
established enhancer region. The enhancer is known to regulate
\emph{BRCA1} expression in mammary epithelium. Does this variant reduce
\emph{BRCA1} expression enough to increase cancer risk? DeepSEA can
predict whether the variant disrupts transcription factor binding at
that position. SpliceAI confirms no splice effects. ExPecto's 40 kb
window cannot reach from the variant to the \emph{BRCA1} promoter. No
convolutional model can connect the enhancer variant to its target gene
because the distance exceeds their receptive fields. The clinical
question remains unanswered.

This case illustrates a fundamental limitation rooted in architecture:
convolutional networks can only integrate information within their
receptive fields. DeepSEA's three-layer architecture effectively
considers roughly 1 kb of context. ExPecto's Beluga component operates
on 2 kb windows, aggregated across a 40 kb region by the spatial
transformation layer. SpliceAI pushes to 10 kb through dilated
convolutions and 32 layers. Each expansion required significant
architectural engineering, and each reached a practical ceiling beyond
which further expansion yielded diminishing returns or became
computationally prohibitive.

The limitation matters because genomic regulation routinely operates
across distances these models cannot reach. Enhancers regulate promoters
50 to 500 kilobases away. The \emph{beta-globin} locus control region
sits 40 to 60 kb from the genes it activates. Polycomb-mediated
repression involves chromatin contacts spanning megabases. Topologically
associating domains organize regulatory interactions across hundreds of
kilobases. When regulatory elements and their targets lie beyond a
model's receptive field, the model cannot learn their relationship
regardless of how much training data is available. The constraint is
architectural, not statistical.

This creates a systematic mismatch between biological importance and
computational accessibility. A variant within a distal enhancer may have
profound effects on gene expression, but a model with a 10 kb receptive
field cannot connect the enhancer sequence to its target promoter. The
model might correctly predict that the enhancer sequence contains
regulatory features, but it cannot predict which gene those features
regulate or how strongly. We can predict local regulatory potential, yet
we cannot predict long-range regulatory effects.

The architectural response to this challenge evolved through two stages.
Recurrent networks initially seemed promising, carrying context through
hidden states rather than expanding receptive fields. When recurrence
proved insufficient, attention mechanisms offered the breakthrough that
modern genomic models required.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-receptive-field-ceiling}{[}Essential{]} Horizontal
genome diagram comparing effective context windows across CNN
architectures. Show: DeepSEA (\textasciitilde1 kb), ExPecto/Beluga (2 kb
windows, 40 kb aggregated), SpliceAI (10 kb), and for contrast, Enformer
(200 kb). Overlay biologically relevant distances: typical TF binding
site (\textasciitilde10bp), promoter region (\textasciitilde1kb),
enhancer-gene distance (10-100kb), TAD size (\textasciitilde1Mb).
Highlight the gap: ``Most enhancer-promoter interactions exceed CNN
receptive fields.''}

\end{figure}%

\section{Sequential Processing and Its Costs}\label{sec-rnn}

If convolutional networks cannot reach far enough, why not simply carry
information forward through the sequence? Recurrent neural networks
offered an intuitive solution to the receptive field problem: maintain a
hidden state that accumulates context as the network processes each
position in turn. Where a convolutional filter sees only its local
window, an RNN's hidden state can, in principle, carry information from
the beginning of a sequence to its end. For biological sequences, this
seemed natural. DNA is read by polymerases in one direction; transcripts
are processed sequentially by ribosomes; regulatory elements exert
effects that propagate through chromatin. A computational architecture
that mirrors this sequential logic appeared well-suited to genomic
modeling.

The hidden state mechanism works as follows. At each position \(t\), the
network combines the current input \(x_t\) with the previous hidden
state \(h_{t-1}\) to produce a new hidden state \(h_t\). This recurrence
allows information from early positions to influence computations at
later positions through the chain of hidden states. A regulatory element
at position 1,000 can, in theory, affect predictions at position 50,000
because its influence persists in the hidden state across all
intervening positions. No receptive field limits this reach; the
constraint becomes whether information survives the journey.

\subsection{The Vanishing Gradient
Problem}\label{the-vanishing-gradient-problem}

Information rarely survives. Training RNNs requires backpropagating
gradients through time, computing how errors at late positions depend on
parameters applied at early positions. These gradients pass through the
same recurrent weight matrix at each step. When gradients are multiplied
through hundreds or thousands of steps, they either explode (growing
exponentially) or vanish (shrinking toward zero). The vanishing gradient
problem makes it nearly impossible for RNNs to learn dependencies
spanning more than a few dozen positions. A regulatory element 10,000
base pairs upstream might as well not exist: by the time gradients
propagate backward through 10,000 recurrent steps, they have decayed to
numerical insignificance.

\textbf{Long Short-Term Memory (LSTM)} networks
(\textbf{hochreiter\_long\_1997?}) addressed this limitation through
gating mechanisms that control information flow. An LSTM cell maintains
a separate cell state alongside the hidden state, with learned gates
that determine what information to store, what to forget, and what to
output. The forget gate can preserve information indefinitely by setting
its value near one, allowing gradients to flow through the cell state
without repeated multiplication by small values. \textbf{Gated Recurrent
Units (GRUs)} (\textbf{cho\_learning\_2014?}) simplified this design by
combining gates while retaining the core insight: learned gating
prevents gradient decay.

These gated architectures extended effective memory from tens to
hundreds of positions, sometimes thousands. For natural language, where
most dependencies span fewer than 50 words, LSTMs proved transformative.
For genomic sequences, where relevant context can span tens of kilobases
(tens of thousands of nucleotides), even gated recurrence falls short.
The mathematics of gradient propagation through recurrent connections
imposes limits that no gating mechanism fully overcomes.

\subsection{DanQ: Combining Convolutions and
Recurrence}\label{danq-combining-convolutions-and-recurrence}

The DanQ model (\textbf{quang\_danq\_2016?}) represented the most
influential attempt to apply recurrent architectures to regulatory
genomics. Rather than replacing convolutions entirely, DanQ combined
them: convolutional layers first extracted local sequence motifs, then a
bidirectional LSTM integrated these motif detections across the 1,000
base pair input window. The architecture recognized that convolutions
excel at detecting local patterns while recurrence might capture their
long-range relationships.

DanQ processed sequences in both directions simultaneously
(bidirectional recurrence), allowing each position to incorporate
context from both upstream and downstream. Training on the same DeepSEA
chromatin prediction task, DanQ achieved modest improvements over the
purely convolutional baseline, with the LSTM component learning to
weight motif combinations based on their relative positions and
co-occurrence patterns.

The improvement was real but limited. Within a 1,000 base pair window,
convolutional receptive fields already capture most relevant
dependencies, leaving less room for recurrence to contribute. The
fundamental problem remained: neither convolutions nor recurrence could
reach the 50 to 100 kilobase distances where enhancers regulate their
target genes. DanQ demonstrated that hybrid architectures could
outperform pure convolutions, but the gains did not justify the added
complexity for most applications. The model saw limited adoption
compared to simpler convolutional alternatives.

\subsection{The Sequential Bottleneck}\label{the-sequential-bottleneck}

Even if recurrence could maintain gradients across genomic distances, a
more fundamental constraint would remain. RNNs process sequences one
position at a time. Each hidden state \(h_t\) depends on the previous
hidden state \(h_{t-1}\), creating an inherently sequential computation
that cannot be parallelized. Training on a 100,000 base pair sequence
requires 100,000 sequential steps, each waiting for the previous step to
complete. Modern GPUs achieve their speed through massive parallelism;
sequential dependencies eliminate this advantage.

This computational bottleneck made RNNs impractical for the long
contexts that genomic applications require. A transformer processes all
positions simultaneously, computing attention scores in parallel across
the entire sequence. For a 100 kilobase context, a transformer performs
one parallel operation where an RNN would require 100,000 sequential
steps. The difference in training time is not incremental; it is the
difference between feasible and infeasible. When Enformer extended
genomic modeling to 200 kilobase contexts, recurrent architectures were
not considered. The sequential bottleneck had already disqualified them.

The attention mechanism resolved both limitations simultaneously.
Self-attention computes direct interactions between all positions
without sequential dependencies, enabling parallel processing across
arbitrary context lengths. Attention weights are computed through matrix
operations that GPUs execute efficiently, and gradients flow directly
between any two positions without passing through intermediate states.
The path from position 1 to position 100,000 involves a single attention
computation rather than 100,000 recurrent steps. This architectural
shift, examined in Chapter~\ref{sec-attention}, enabled the long-range
modeling that genomic applications demand.

\section{Specialization and Its
Limits}\label{specialization-and-its-limits}

The convolutional models examined here established paradigms that
persist in modern genomic AI. End-to-end learning from one-hot encoded
sequence demonstrated that gradient descent on functional labels could
discover regulatory patterns without encoding human assumptions about
what matters. Multi-task training across hundreds of chromatin features
showed that shared representations improve both accuracy and
generalization. In silico mutagenesis, comparing predictions for
reference and alternative sequences, established the dominant approach
for deep learning-based variant effect prediction: scoring variants
without training on variant labels, thereby avoiding the ascertainment
biases that confound association-based methods.

These principles carry forward into foundation model architectures. What
CNNs could not resolve was the receptive field limitation. Genomic
regulation operates across scales that exceed practical convolutional
depth: enhancers modulating genes across hundreds of kilobases,
topologically associating domains spanning megabases. Dilated
convolutions and deeper networks extend reach but cannot fundamentally
escape the constraint that convolutions aggregate local information
through hierarchical composition.

Yet specialization retains value even as general-purpose models advance.
SpliceAI achieves clinical-grade splice site prediction that broader
foundation models have not matched. When the prediction target is
well-defined, training data abundant, and the relevant context fits
within architectural constraints, task-specific models remain
competitive with or superior to general-purpose approaches. This tension
between specialized accuracy and general capability recurs throughout
subsequent chapters. For clinical deployment requiring high reliability
on specific tasks, specialized architectures may remain preferred. For
discovery applications requiring broad coverage across diverse molecular
mechanisms, the foundation model paradigm offers different trade-offs.
The attention mechanisms examined next provide the architectural
substrate for long-range modeling while inheriting the end-to-end
learning principles that convolutional networks established.

\chapter{Transformers and Attention}\label{sec-attention}

Where convolutional networks ask ``what local pattern exists here,''
attention asks a different question: ``what distant information matters
here?'' This reformulation changed what genomic models could learn. A
convolutional filter scanning across a sequence detects motifs,
chromatin marks, and regulatory grammar with high fidelity, but it
remains blind to dependencies beyond its receptive field. Attention
computes direct interactions between all positions simultaneously,
allowing a nucleotide near a gene promoter to attend to an enhancer 100
kilobases away without information passing through intermediate layers.
The shift is not merely architectural; it reflects a different
assumption about how sequence encodes function. Local patterns matter,
but so do long-range relationships that convolutions cannot capture.

The attention mechanism, introduced for machine translation by Vaswani
et al. (Vaswani et al. 2017), resolved a tension that had constrained
sequence modeling for years. Recurrent networks could maintain context
across arbitrary distances but processed sequences one position at a
time, creating training bottlenecks that limited practical sequence
length. Convolutional networks processed sequences in parallel but could
only integrate information within fixed receptive fields. Attention
achieves both: parallel computation across all positions with direct
modeling of arbitrary-range dependencies. For genomic sequences, where
enhancer-promoter interactions span tens of kilobases and topologically
associating domains organize contacts across megabases, this capacity
for long-range modeling proved transformative.

This chapter examines the attention mechanism and transformer
architecture from the perspective of genomic applications. The treatment
emphasizes how attention enables capabilities that convolutions cannot
achieve: modeling regulatory interactions across distances that exceed
CNN receptive fields, learning position-dependent patterns through
attention heads that specialize for different relationship types, and
scaling to the context lengths that genomic applications require. The
goal is not comprehensive coverage of transformer variants (which would
fill its own textbook) but targeted understanding of the components most
relevant to genomic foundation models. Understanding how attention
works, what biological patterns attention heads learn to detect, and
where attention mechanisms still struggle establishes the foundation for
the models examined throughout the remainder of this book.

\section{The Self-Attention
Mechanism}\label{the-self-attention-mechanism}

A 28-year-old woman presents with dilated cardiomyopathy and a variant
of uncertain significance in the \emph{LMNA} gene's promoter region. Her
clinician needs to determine whether this variant disrupts regulatory
elements that control \emph{LMNA} expression in cardiac tissue. The
relevant information spans thousands of base pairs: transcription factor
binding sites flanking the variant, enhancers that drive
cardiac-specific expression, and insulators that constrain regulatory
interactions. A model that can only aggregate local context (through
convolutional or recurrent operations) must pass information through
many intermediate layers, each adding noise and limiting what survives
the journey. When this information pathway fails, the variant appears as
noise rather than the pathogenic regulatory disruption it may represent.
The fundamental question is how to let any position in a sequence
directly access information from any other position, regardless of
distance.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-self-attention}{[}Essential{]} Step-by-step
visualization of self-attention on a short regulatory sequence. Panel A:
Input embeddings for \textasciitilde10 positions. Panel B: Query, Key,
Value projections (show W\^{}Q, W\^{}K, W\^{}V matrices). Panel C:
Attention score matrix (query-key dot products, pre-softmax). Panel D:
Attention weight matrix (post-softmax, showing which positions attend to
which). Panel E: Weighted sum of values producing output. Annotate the
key equation at each step.}

\end{figure}%

Self-attention answers this question by computing all pairwise
interactions simultaneously, allowing the model to directly relate any
position to any other regardless of distance. Where convolutions apply
fixed filters uniformly across the sequence, attention performs dynamic
routing: each position queries the entire sequence and aggregates
information based on content-dependent relevance scores. The routing
changes for every input because attention weights depend on what the
sequence contains, not just where positions sit relative to each other.
For the \emph{LMNA} variant, this means the model can directly assess
whether the variant position interacts with known cardiac enhancers
without that signal degrading through layer after layer of local
aggregation.

\subsection{Query, Key, and Value
Vectors}\label{query-key-and-value-vectors}

At each position in the input sequence, self-attention computes three
vectors: a \textbf{query}, a \textbf{key}, and a \textbf{value}. These
vectors emerge from multiplying the input embedding at that position by
three learned weight matrices \(W^Q\), \(W^K\), and \(W^V\). The query
represents what information this position seeks from other positions.
The key represents what information this position offers to queries from
elsewhere. The value represents the actual information this position
contributes when attended to. This query-key-value structure separates
the question of ``which positions should interact'' (determined by
query-key similarity) from ``what information flows between them''
(determined by values).

The attention mechanism computes similarity scores between each query
and all keys. For position \(i\), we compute the dot product between its
query \(q_i\) and every key \(k_j\) across all positions
\(j = 1, \ldots, L\), where \(L\) is sequence length. These scores are
scaled by \(\sqrt{d_k}\) (the square root of the key dimension) to
prevent the dot products from growing large in high dimensions, which
would push softmax outputs toward extreme values and create vanishing
gradients:

\[
\text{score}(q_i, k_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}}
\]

A softmax function converts these scores into attention weights
\(\alpha_{ij}\) that form a probability distribution over positions:

\[
\alpha_{ij} = \frac{\exp(\text{score}(q_i, k_j))}{\sum_{j'=1}^L \exp(\text{score}(q_i, k_{j'}))}
\]

These weights determine how strongly position \(i\) attends to each
other position. High weight means position \(i\) aggregates substantial
information from position \(j\); low weight means position \(j\)
contributes little to the output at position \(i\). The final output at
position \(i\) is a weighted sum of all value vectors:

\[
\text{output}_i = \sum_{j=1}^L \alpha_{ij} v_j
\]

This weighted aggregation forms the core of self-attention. Each output
position receives a mixture of information from across the entire
sequence, with mixture proportions learned through backpropagation. For
genomic sequences, this means a position near a splice site can attend
to both the upstream exon and downstream intron, integrating context
that determines whether splicing occurs. A position in a promoter can
attend to distant enhancers, learning which distal elements influence
expression at this gene. When predicting the pathogenicity of a variant
in the \emph{SCN5A} promoter (mutations in which cause Brugada syndrome
and long QT syndrome, affecting approximately 1 in 2,000 individuals),
the model can simultaneously consider the core promoter elements,
upstream enhancers that drive cardiac-specific expression, and
downstream regulatory regions that modulate expression levels.

{[}FIGURE RECOMMENDATION: Attention weight visualization showing a
genomic sequence with attention patterns. Panel A: heatmap of attention
weights between all position pairs, highlighting strong attention
between a promoter region and a distal enhancer. Panel B: the same
attention pattern overlaid on a linear genome diagram showing the
biological interpretation of learned attention. Emphasize how attention
learns to connect biologically related positions without explicit
supervision.{]}

\subsection{Multi-Head Attention}\label{multi-head-attention}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-multihead-attention}{[}High{]} Panel showing 4-6
attention heads from a trained genomic transformer, each displaying
different learned patterns. Head 1: Local attention (attending to nearby
positions). Head 2: Periodic attention (nucleosome spacing
\textasciitilde200bp). Head 3: Motif-specific attention (attending to
CTCF sites). Head 4: Long-range attention (enhancer-promoter). Annotate
the biological interpretation of each pattern.}

\end{figure}%

A patient presenting with a complex arrhythmia may carry variants
affecting both a cardiac ion channel's coding sequence and its distal
enhancer. Understanding this case requires the model to simultaneously
track local splice site context around the coding variant and
enhancer-promoter relationships spanning 50 kilobases. A single
attention operation cannot capture both patterns effectively: when
forced to learn one pattern of position interactions, the model faces an
impossible choice between attending strongly to nearby positions for
local regulatory context or attending to distant positions for
enhancer-gene relationships. Genomic sequences exhibit multiple types of
dependencies simultaneously, and forcing all these interaction types
through a single attention pattern creates destructive competition.

\textbf{Multi-head attention} extends the basic mechanism by running
multiple attention operations in parallel, each with independent learned
projections (Vaswani et al. 2017). If we use \(H\) heads, we split the
model dimension \(d\) into \(H\) subspaces of dimension \(d/H\), compute
separate queries, keys, and values for each head, run attention
independently, concatenate outputs, and project back to dimension \(d\).
Different heads can specialize in different interaction types without
competing for attention capacity.

In genomic models, one head might attend to nearby positions (capturing
local motif context) while another attends to positions at
characteristic distances (capturing nucleosome spacing or
enhancer-promoter loops). Empirical analysis of trained genomic
transformers reveals diverse attention patterns: some heads attend
locally regardless of content, others attend to specific sequence motifs
like TATA boxes or CTCF binding sites, and still others show
distance-dependent patterns suggestive of chromatin organization (Å½.
Avsec et al. 2021). This specialization emerges from training without
explicit supervision, reflecting the model's discovery that different
types of interactions require different aggregation patterns.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-attention-patterns}{[}Essential{]} Heatmap
visualization of attention weights from a trained genomic transformer
(e.g., Enformer). Panel A: Attention pattern showing strong weights
between a promoter region and a distal enhancer \textasciitilde50kb
away, demonstrating learned enhancer-promoter relationships. Panel B:
Same attention overlaid on linear genome diagram showing the biological
interpretation. Panel C: Different attention head showing local patterns
(attending to nearby positions), demonstrating head specialization.}

\end{figure}%

The multi-head structure also provides redundancy that aids training. If
one head fails to learn useful patterns, others can compensate. Gradient
flow through multiple parallel paths stabilizes optimization. For
genomic applications where training data may be limited compared to
natural language corpora, this redundancy helps prevent individual heads
from overfitting to spurious correlations. The number of heads
represents a design choice: too few heads limit the diversity of
learnable patterns, while too many heads reduce the dimensionality
available to each head, potentially limiting their individual
expressiveness. Most genomic transformers use 8 to 16 heads, balancing
diversity against per-head capacity.

\section{Positional Encoding}\label{sec-positional-encoding}

A patient with hypertrophic cardiomyopathy carries a variant in the
\emph{MYH7} gene's promoter region. Determining pathogenicity requires
knowing precisely where the variant sits relative to the transcription
start site: a variant at position -30 (where the TATA box resides)
carries entirely different implications than the same sequence at
position +500 (within the 5' UTR). Position is not merely bookkeeping
for genomic sequences; it encodes biological function. The canonical
TATA box must appear 25 to 30 base pairs upstream of transcription
initiation to function; the same sequence elsewhere carries no
regulatory significance. Splice site recognition depends on the
invariant GT and AG dinucleotides appearing at precise distances from
exon boundaries. Enhancer-promoter communication requires specific
distance relationships that vary by locus and cell type. A model that
cannot distinguish position 100 from position 10,000 cannot learn the
positional grammar that governs gene regulation.

Self-attention, by design, computes interactions based purely on
content: the attention weight between positions depends only on their
query and key vectors, not on where they sit in the sequence. Shuffling
input token order changes nothing about how attention weights are
computed. The model has no inherent notion of sequence order, a property
called \textbf{permutation invariance}. For genomic data where position
matters fundamentally, this blindness to order would be catastrophic.
DNA has 5' to 3' directionality that determines transcription direction.
Distance from transcription start sites determines promoter versus
enhancer classification. Strand orientation distinguishes sense from
antisense transcription. \textbf{Positional encodings} inject
information about token positions into the model, breaking permutation
invariance by making the model aware of where each token sits in the
sequence.

\subsection{Absolute Position
Encodings}\label{absolute-position-encodings}

The original transformer used sinusoidal functions with different
frequencies for each embedding dimension (Vaswani et al. 2017). For
position \(pos\) and dimension \(i\):

\[
\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)
\]

\[
\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\]

These fixed patterns have useful properties. They are deterministic (the
same for all sequences), allow the model to learn to attend by relative
positions (since \(\text{PE}(pos+k)\) can be expressed as a linear
function of \(\text{PE}(pos)\)), and generalize to sequence lengths not
seen during training. The different frequencies across dimensions create
a unique ``fingerprint'' for each position that the model can learn to
decode.

Many genomic models use \textbf{learned positional embeddings} instead:
lookup tables where each position has a learned vector added to the
input embedding. DNABERT (Ji et al. 2021) and Nucleotide Transformer
(Dalla-Torre et al. 2023) both employ learned positional embeddings,
allowing the model to discover position-dependent patterns specific to
genomic data. The trade-off is that learned embeddings require training
and do not automatically extrapolate to longer sequences. A model
trained with maximum sequence length of 512 tokens has no learned
embedding for position 513, creating a hard boundary on sequence length
at inference time.

\subsection{Relative Position
Encodings}\label{relative-position-encodings}

Absolute encodings treat position 1,000 and position 1,001 as having
different representations even though their relative relationship
(adjacent positions) may matter more than their absolute locations. For
genomic applications, relative distance often carries more biological
meaning than absolute coordinates: nucleosomes are spaced approximately
200 base pairs apart regardless of genomic location, and
enhancer-promoter interactions depend on distance rather than absolute
position. \textbf{Relative positional encodings} address this by
encoding distances between positions rather than absolute coordinates.

T5-style relative position bias adds a learnable scalar to attention
scores based on the distance between query and key positions (Raffel et
al. 2019). This bias helps the model learn that nearby positions often
interact more strongly than distant ones while remaining agnostic about
absolute position in the sequence. The learned biases can capture
genomic-specific distance preferences, such as the characteristic
spacing of regulatory elements or the periodicity of nucleosome
positioning.

\textbf{Attention with Linear Biases (ALiBi)} adds a fixed linear
penalty to attention scores based on distance, without learned
parameters (\textbf{press\_train\_2022?}). For a head with slope \(m\),
attention between positions separated by distance \(|i - j|\) is
penalized by \(m|i - j|\). Different heads use different slopes,
encouraging some to focus locally and others globally. ALiBi generalizes
well to longer contexts than seen during training because the linear
penalty extrapolates naturally, making it attractive for genomic
applications where sequence length varies dramatically. A model trained
on 1-kilobase sequences can process 10-kilobase sequences at inference
time without architectural modification.

\textbf{Rotary Position Embeddings (RoPE)} encode positions by rotating
query and key vectors in a high-dimensional space, with rotation angle
depending on position (\textbf{su\_roformer\_2024?}). The dot product
between rotated query and key depends on their relative distance,
combining benefits of relative encoding with efficient implementation.
RoPE has become standard in recent language models and appears
increasingly in genomic transformers, offering a balance between the
flexibility of learned embeddings and the extrapolation capability of
fixed schemes.

{[}FIGURE RECOMMENDATION: Comparison of positional encoding schemes.
Panel A: sinusoidal absolute encodings showing the wave patterns across
positions. Panel B: learned embeddings as a heatmap of position
vs.~dimension. Panel C: ALiBi attention bias matrix showing linear decay
with distance. Panel D: RoPE rotation visualization in 2D subspace.
Include genomic context showing how each scheme handles a 10kb
regulatory region.{]}

\subsection{Genomic Position
Considerations}\label{genomic-position-considerations}

Genomic sequences impose additional requirements on positional encoding
beyond what natural language demands. DNA has strand directionality:
ACGT on the forward strand has different regulatory meaning than the
same sequence on the reverse strand (which reads as ACGT from the
complementary strand's perspective but represents TGCA in the reference
orientation). Positional encodings should enable the model to learn
strand-specific patterns. Some genomic transformers encode both strands
separately and combine predictions; others rely on the model learning
strand orientation from sequence content alone.

Genomic coordinates pose another design choice. Should position 1
correspond to a fixed genomic landmark (transcription start site, gene
start) or simply indicate sequence order without biological reference?
Models predicting regulatory activity often center sequences on
promoters, using positions relative to the TSS. Foundation models
trained on random genomic segments typically use positional encodings
reflecting sequence order without genomic coordinate reference. The
choice affects what the model can learn: TSS-relative positions enable
learning of distance-dependent regulatory patterns (such as the
preference for certain motifs at specific distances from transcription
start), while sequence-order positions require the model to learn these
patterns implicitly from content.

The choice of positional encoding interacts with tokenization strategies
discussed in Chapter~\ref{sec-representations}. K-mer tokenization
reduces sequence length (and thus attention cost) but changes what
``position'' means: position 1 might represent nucleotides 1 through 6
rather than a single base. Positional encodings must be interpreted
relative to the tokenization scheme. A model using 6-mer tokens with
learned positional embeddings learns different position-dependent
patterns than one using single-nucleotide tokens, even if both cover the
same genomic region.

\section{The Transformer Block}\label{the-transformer-block}

A clinician interpreting a \emph{BRCA1} variant needs a model that does
more than identify isolated motifs or single long-range interactions.
The variant's pathogenicity depends on how multiple regulatory signals
integrate: local splice site grammar, enhancer contacts from 20
kilobases upstream, and transcription factor binding sites whose effects
depend on chromatin context. Single attention layers identify pairwise
relationships, but understanding complex regulatory logic requires
building hierarchical representations where simple patterns combine into
compound signals. This hierarchical integration emerges from stacking
\textbf{transformer blocks}, the modular units that combine attention
and nonlinear processing to build increasingly abstract representations
through repeated application.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-transformer-block}{[}High{]} Detailed diagram of a
single transformer block with pre-norm configuration. Show: Input â†’
Layer Norm â†’ Multi-Head Attention â†’ Residual Add â†’ Layer Norm â†’
Feed-Forward Network (expand 4x, GELU, project back) â†’ Residual Add â†’
Output. Annotate dimension changes. Include small inset showing 2-3
stacked blocks to illustrate depth.}

\end{figure}%

\subsection{Block Components}\label{block-components}

Each transformer block accomplishes two distinct functions: enabling
positions to share information across the sequence, and transforming
that aggregated information through nonlinear processing. The
\textbf{multi-head self-attention layer} handles global communication,
allowing each position to gather information from the entire sequence.
The \textbf{position-wise feed-forward network} processes each position
independently, applying nonlinear transformations to the aggregated
information. Separating these functions into distinct components allows
each to be optimized independently and provides clear computational
semantics: attention determines which positions are relevant to each
other (the ``what to consider'' question), while the feed-forward
network determines how to combine that information (the ``what to
conclude'' question).

The feed-forward network consists of two linear transformations with a
nonlinearity between them. Typically, this expands the dimension by a
factor of four (from model dimension \(d\) to \(4d\)), applies GELU or
similar activation, then projects back to dimension \(d\). This
expansion allows processing through a high-dimensional nonlinear
transformation before producing output for the next layer. The
position-wise nature means each position is transformed identically but
independently; cross-position information flows only through attention.

\textbf{Layer normalization} stabilizes training by normalizing
activations across the feature dimension at each position. Two
conventions exist for placement. Post-norm places normalization after
each sublayer, applying it to output before the residual connection.
Pre-norm places normalization before each sublayer, normalizing input to
attention or feed-forward operations. Pre-norm has become standard
because it improves training stability for deep networks, though
post-norm can achieve slightly better final performance with careful
tuning (\textbf{xiong\_layer\_2020?}).

\textbf{Residual connections} (which add input directly to sublayer
output, providing gradient highways during backpropagation) wrap around
both attention and feed-forward sublayers. These connections serve two
critical functions. First, they allow gradients to flow directly through
many layers without repeated transformation, enabling training of very
deep networks that would otherwise suffer from vanishing gradients.
Second, they create an inductive bias toward incremental refinement:
each layer makes small adjustments to the representation rather than
constructing entirely new representations from scratch. For genomic
models, this incremental refinement maps naturally onto biological
interpretation: early layers might identify motifs, middle layers might
recognize motif combinations, and later layers might integrate these
patterns into regulatory predictions.

\subsection{Information Flow and
Depth}\label{information-flow-and-depth}

The flow through a pre-norm transformer block proceeds as follows. Input
\(X\) is normalized, processed by multi-head attention to produce
\(X'\), and added back via residual connection, yielding \(X + X'\).
This sum is normalized, passed through the feed-forward network to
produce \(X''\), and added via another residual connection, yielding
final output \(X + X' + X''\). Each layer thus adds refinements to the
representation while preserving information from earlier processing.

Stacking depth determines how many times this refinement occurs. Shallow
transformers (6 layers or fewer) are parameter-efficient but may lack
capacity for complex hierarchical patterns. Deep transformers (12 to 24
layers) can learn sophisticated representations that capture how
promoter elements, enhancer contacts, and chromatin state combine to
determine expression. Most genomic transformers use 6 to 24 layers,
varying by application. Models for short sequences (small RNAs,
individual binding sites) might use fewer layers, while foundation
models for long genomic contexts often use deeper stacks to build
representations that integrate information across multiple biological
scales.

The choice of depth balances capacity against trainability. Deeper
networks learn more complex functions but are harder to optimize, prone
to overfitting without sufficient data, and more expensive at training
and inference. For genomic models, depth often correlates with the
complexity of patterns being modeled. Simple motif recognition tasks
might benefit more from wider layers (larger \(d\)) than deeper stacks,
while tasks requiring hierarchical integration (understanding how
promoter-enhancer-insulator relationships determine expression) may
benefit from additional depth that builds increasingly abstract
representations layer by layer.

{[}FIGURE RECOMMENDATION: Transformer block architecture diagram. Show
the complete block structure with multi-head attention, feed-forward
network, layer normalization, and residual connections. Include
annotations showing information flow and dimension changes. Highlight
how residual connections create gradient highways. Consider showing 2-3
stacked blocks to illustrate how representations build through depth.{]}

\section{Scaling to Genomic
Sequences}\label{scaling-to-genomic-sequences}

A 52-year-old patient presents with unexplained cardiomyopathy, and
whole-genome sequencing reveals a structural variant spanning 500
kilobases on chromosome 14, disrupting the \emph{MYH7} locus and several
upstream regulatory elements. The clinical team needs to assess whether
this variant explains the patient's phenotype. Standard transformers
cannot help: the quadratic complexity of self-attention makes
500-kilobase contexts computationally intractable. This gap between
clinical need and computational capability defines a central challenge
for genomic AI. The attention mechanism enables long-range modeling in
principle, but practical constraints on memory and computation limit
what contexts can actually be processed. Effective application of
transformers to genomics requires strategies for managing these
constraints.

\subsection{The Quadratic Barrier}\label{the-quadratic-barrier}

Computing all pairwise attention scores requires \(O(L^2)\) operations,
where \(L\) is sequence length. For a 10-kilobase sequence tokenized at
single-nucleotide resolution, this means 100 million attention
computations per layer. A 200-kilobase sequence requires 40 billion
computations per layer. Memory requirements scale similarly because the
attention matrix must be stored for backpropagation.

This scaling constraint directly limits what clinical questions
transformers can address. The \emph{HLA} region (critical for transplant
matching and autoimmune disease risk in the approximately 40,000 organ
transplants performed annually in the United States) spans approximately
4 megabases and contains the most polymorphic genes in the human genome.
Modeling this region with standard self-attention would require
\(16 \times 10^{12}\) attention computations per layer, far exceeding
practical limits. Structural variant detection often requires analyzing
megabase-scale contexts to identify breakpoints and assess functional
impact, yet these contexts remain computationally intractable for
standard transformers. A patient with a suspected chromosomal
translocation cannot benefit from transformer-based analysis when the
relevant context exceeds computational capacity.

\subsection{Parameter Considerations}\label{parameter-considerations}

The number of parameters a transformer can effectively utilize depends
on both training data quantity and the complexity of patterns to be
learned. Transformer parameters come primarily from two sources. Width
(model dimension \(d\)) determines embedding and hidden state sizes;
increasing width allows more complex pattern representation at each
position but increases parameters quadratically because weight matrices
scale as \(d \times d\). Depth (number of layers) determines how many
refinement steps occur; increasing depth allows hierarchical
abstractions through repeated processing but increases parameters
linearly.

Scaling laws from natural language processing suggest performance
improves smoothly with increased parameters, data, and compute
(\textbf{kaplan\_scaling\_2020?}). Similar principles apply to genomics,
though optimal ratios may differ. Genomic sequences are less
compressible than natural language: each nucleotide carries less
predictable information than words in structured sentences. The entropy
of DNA sequence is higher than English text, meaning more parameters may
be needed to model the same sequence length. This asymmetry suggests
genomic models might benefit relatively more from depth (more processing
of high-entropy information) than from width (more dimensions per
position when each position carries limited structure).

Genomic foundation models span a wide parameter range. DNABERT uses
approximately 110 million parameters, comparable to BERT-base (Ji et al.
2021). Nucleotide Transformer scales from 50 million to 2.5 billion
parameters across model variants (Dalla-Torre et al. 2023). The largest
genomic transformers approach 10 billion parameters, though whether this
scale provides commensurate benefit for genomic tasks remains under
investigation. The relationship between parameter count and downstream
task performance is not always monotonic: a well-trained smaller model
can outperform a poorly trained larger one, and task-specific
fine-tuning often matters more than pretraining scale for focused
clinical applications.

\subsection{Context Length Strategies}\label{context-length-strategies}

Standard self-attention's \(O(L^2)\) complexity becomes prohibitive for
long genomic contexts, forcing architectural choices that trade
expressiveness for tractability. The strategies employed reflect
different assumptions about which interactions matter most for genomic
modeling.

\textbf{Sparse attention patterns} restrict which positions attend to
which others. Local windowing allows each position to attend only within
a fixed window, reducing complexity to \(O(Lw)\) where \(w\) is window
size. This approach works when most relevant interactions are local, as
often holds for regulatory sequences where nearby elements interact more
strongly than distant ones. For clinical variant interpretation in
coding sequences, where splice sites and reading frame context typically
lie within a few hundred bases, local attention may capture the relevant
biology. The trade-off is missing long-range interactions that fall
outside windows, potentially critical for understanding distal enhancer
effects or structural variant consequences.

\textbf{Strided attention} creates hierarchy: lower layers use local
windows while upper layers attend to every \(k\)-th position. This
configuration captures both local fine-grained patterns and global
coarse-grained structure while maintaining sub-quadratic complexity.
Hybrid models like Enformer apply CNNs to downsample sequences before
transformer layers, reducing the effective sequence length that
attention must handle (Å½. Avsec et al. 2021). A 200-kilobase genomic
region might be compressed to roughly 1,500 positions after CNN
processing, making full attention tractable at the cost of
single-nucleotide resolution in transformer layers.

\textbf{Approximations to full attention} offer another approach.
Linformer approximates the attention matrix through low-rank
decomposition, reducing complexity to linear in sequence length
(\textbf{wang\_linformer\_2020?}). Performer uses random feature methods
to approximate attention scores without explicitly computing the full
\(L \times L\) matrix (\textbf{choromanski\_rethinking\_2021?}). These
approximations trade some expressiveness for efficiency and may miss
certain long-range dependencies that low-rank structure cannot capture.

\subsection{Memory and Precision}\label{memory-and-precision}

Memory requirements compound computational challenges for genomic
transformers. Training requires storing activations for backpropagation,
and attention matrices are particularly memory-intensive. A 100-kilobase
sequence with 16 attention heads and 12 layers requires storing
\(16 \times 12 \times 100,000 \times 100,000\) attention weights,
approximately 2 terabytes at 32-bit precision before considering other
activations.

\textbf{Gradient checkpointing} trades compute for memory by recomputing
activations during the backward pass rather than storing them. This
technique enables training larger models or longer sequences on fixed
hardware at the cost of additional computation time, typically
increasing training time by 20 to 30 percent. \textbf{Mixed precision
training} uses 16-bit floating point for most computations while
maintaining 32-bit precision for critical operations like loss
computation and optimizer updates. Modern GPUs accelerate 16-bit
arithmetic substantially, providing near 2Ã— speedup with minimal
precision loss. Flash Attention implements memory-efficient attention
computation that avoids materializing the full attention matrix,
enabling longer contexts within fixed memory budgets
(\textbf{dao\_flashattention\_2022?}).

\section{Architectural Variants for
Genomics}\label{architectural-variants-for-genomics}

A clinical laboratory evaluating a novel missense variant needs a model
that integrates bidirectional protein context to assess pathogenicity. A
biotechnology company designing synthetic promoters needs a model that
generates novel sequences with specified expression properties. A
research team studying protein-RNA interactions needs a model that
predicts binding from sequence features. These distinct requirements
have driven development of architectural variants that make different
trade-offs between representation learning, generation capability, and
computational efficiency. The choice of architecture shapes what
questions can be asked and how answers emerge.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-encoder-decoder}{[}Enhancing{]} Three-panel
comparison. Panel A (Encoder-only, e.g., BERT/DNABERT): Bidirectional
attention pattern (full matrix), typical use for
classification/embedding. Panel B (Decoder-only, e.g., GPT/Evo): Causal
attention pattern (lower triangular), typical use for generation. Panel
C (Hybrid CNN-Transformer, e.g., Enformer): CNN downsampling followed by
transformer, showing how hybrid reduces sequence length before
attention.}

\end{figure}%

\subsection{Encoder-Only Transformers}\label{encoder-only-transformers}

When a clinical laboratory queries a pathogenicity database for a novel
missense variant, they need a model that integrates information from the
entire protein sequence: upstream domains that establish structural
context, downstream regions that complete functional units, and
evolutionary patterns that distinguish tolerated from deleterious
changes. \textbf{Encoder-only transformers} process sequences
bidirectionally, allowing each position to attend to all other positions
including those that follow in the sequence. This bidirectional context
produces richer representations than unidirectional processing because
each position's representation incorporates information from the entire
sequence.

DNABERT exemplifies this architecture, trained with \textbf{masked
language modeling} objectives where random tokens are masked and
predicted from bidirectional context (Ji et al. 2021). The model learns
to predict held-out k-mers based on surrounding sequence, implicitly
learning sequence patterns and constraints that transfer to downstream
tasks. Nucleotide Transformer follows similar principles at larger scale
(Dalla-Torre et al. 2023). These models excel at \textbf{representation
learning}: producing embeddings that capture biological properties
useful for variant effect prediction, function classification, or other
tasks that require fixed-length representations of variable-length
sequences.

Bidirectional attention suits tasks where both upstream and downstream
context matters for understanding a position. Transcription factor
binding depends on flanking sequence in both directions. Splice site
recognition requires seeing both exonic and intronic context. Variant
pathogenicity may depend on protein domain context from both N-terminal
and C-terminal directions. The limitation is that encoder-only
architectures cannot generate sequences autoregressively because they
require seeing the full sequence to produce any output; they answer
``what does this sequence mean'' rather than ``what sequence should come
next.''

\subsection{Decoder-Only Transformers}\label{decoder-only-transformers}

Generating synthetic genomic sequences for therapeutic design, creating
diverse antibody libraries for drug discovery, or sampling from learned
regulatory grammars all require models that produce sequences one token
at a time. \textbf{Decoder-only transformers} use \textbf{causal
attention} where each position attends only to itself and preceding
positions. This structure enables \textbf{autoregressive generation}:
the model produces sequences one token at a time, conditioning each new
token on all previous tokens.

GenSLM applies this architecture to genomic data, training on next-token
prediction to learn sequence distributions (Zvyagin et al. 2022). The
model learns to predict the next nucleotide or k-mer given all preceding
context, implicitly learning the statistical regularities of genomic
sequence. This objective aligns naturally with generation tasks:
sampling proceeds by repeatedly predicting the next token and appending
it to the sequence. Causal attention is essential for generation because
the model must produce each position before it can condition subsequent
positions on that output.

The trade-off is that causal attention produces less rich
representations for fixed sequences because each position sees only
partial context. Position 100 in a 1000-position sequence has access to
only the first 100 positions, not the remaining 900 that might provide
relevant information. For variant effect prediction where downstream
context matters, this limitation can be substantial. The choice between
encoder and decoder architectures reflects a fundamental tension:
representation learning benefits from bidirectional context, while
generation requires causal structure.

\subsection{Encoder-Decoder
Transformers}\label{encoder-decoder-transformers}

Some genomic tasks require transforming one sequence into another of
different length or structure. Predicting protein sequence from coding
DNA, generating variant descriptions from sequence context, or
translating between sequence representations all involve input-output
relationships that neither pure encoder nor pure decoder architectures
handle naturally. \textbf{Encoder-decoder architectures} combine
bidirectional encoding with autoregressive decoding (Vaswani et al.
2017).

The encoder processes an input sequence with full bidirectional
attention, producing contextualized representations. The decoder then
generates output tokens autoregressively, attending both to its own
previous outputs (through causal self-attention) and to encoder
representations (through \textbf{cross-attention}). This cross-attention
allows each decoder position to query the full encoded input when
generating output, combining the benefits of bidirectional understanding
with autoregressive generation.

Encoder-decoder models are less common in genomic applications than
encoder-only or decoder-only variants because most genomic tasks either
need representations (favoring encoders) or generation (favoring
decoders), not both simultaneously. Machine translation exemplifies the
encoder-decoder use case: encode a sentence in one language, decode into
another. Genomic analogs might include predicting protein sequences from
codon-optimized DNA or generating clinical variant reports from sequence
features, but these applications remain less developed than pure
representation or generation tasks.

\subsection{Hybrid CNN-Transformer
Models}\label{hybrid-cnn-transformer-models}

The most successful genomic transformers combine convolutional and
attention mechanisms rather than using transformers alone. This hybrid
approach exploits CNNs' efficiency for local pattern extraction while
using transformers for long-range integration, matching the multi-scale
structure of genomic regulation where both local motifs and distal
interactions determine function.

Enformer and Borzoi apply convolutional stems to long sequences,
downsampling through pooling, then pass compressed representations
through transformer layers (Å½. Avsec et al. 2021). The CNN layers handle
motif recognition, nucleosome positioning signals, and local chromatin
features with parameter efficiency that pure transformers cannot match.
Transformer layers then integrate across the broader regulatory
landscape, learning enhancer-promoter relationships and TAD boundary
effects. This division of labor achieves state-of-the-art performance on
regulatory prediction tasks while remaining computationally tractable
for 200-kilobase contexts.

The hybrid approach also addresses the quadratic attention bottleneck
indirectly. By downsampling sequences before transformer layers (often
by factors of 128 or more), hybrids reduce effective sequence length and
thus attention cost. A 200-kilobase genomic region compressed to 1,500
positions requires only 2.25 million attention computations per layer
rather than 40 billion for the uncompressed sequence. The cost is loss
of single-nucleotide resolution in transformer layers, though the CNN
stem preserves local detail that attention layers integrate but do not
need to resolve. Chapter~\ref{sec-regulatory} examines Enformer and
related hybrid architectures in detail.

{[}FIGURE RECOMMENDATION: Architecture comparison diagram showing
encoder-only, decoder-only, encoder-decoder, and hybrid CNN-transformer
architectures. For each, show the attention pattern (bidirectional
vs.~causal), typical training objective, and example genomic
applications. Highlight how hybrid architectures reduce sequence length
before attention through CNN downsampling.{]}

\section{Training Dynamics}\label{training-dynamics}

When a model trained to predict pathogenic variants misclassifies a
disease-causing \emph{BRCA1} mutation as benign, the consequences extend
beyond benchmark metrics. Clinical laboratories may return incorrect
results; patients may forego preventive surgeries that could save their
lives. Training failures matter clinically because they determine what
models learn and what they miss. A model that overfits to common
polymorphisms in training data will fail on the rare variants that
matter most for diagnosis. A model whose gradients vanish during
training will never learn the subtle regulatory patterns that
distinguish pathogenic from benign promoter variants. Understanding
training dynamics helps predict and prevent these failures.

\subsection{Optimization}\label{optimization}

Genomic transformers inherit their training foundations from natural
language processing but require adjustments for biological data. The
Adam optimizer and its variant AdamW remain standard, using adaptive
learning rates that maintain per-parameter estimates adjusted based on
gradient statistics (\textbf{loshchilov\_decoupled\_2019?}). AdamW
applies weight decay directly to parameter updates rather than to the
loss function, improving generalization and training stability.

Learning rate schedules typically use warmup (linearly increasing
learning rate from near-zero to peak over the first several thousand
steps) followed by decay (linear or cosine decrease over the remaining
training). Warmup addresses a specific instability: transformers with
random initialization can produce extreme gradients early in training,
and adaptive optimizers need time to build accurate gradient statistics.
Warmup allows the optimizer to stabilize before applying full learning
rates. Skipping warmup often causes training collapse in the first few
hundred steps, manifesting as loss spikes or NaN values.

For genomic data, learning rate tuning may require adjustment from NLP
defaults. Regulatory sequences with highly conserved motifs (TATA boxes,
splice site dinucleotides) create strong signals that models can overfit
quickly; lower learning rates may prevent latching onto these patterns
before learning subtler regulatory grammar. Protein sequences exhibit
weaker positional conservation than regulatory DNA, potentially
benefiting from higher rates that encourage broader exploration of the
loss landscape. Empirically, genomic transformers often use peak
learning rates of 1e-4 to 5e-4, somewhat lower than the 1e-3 to 3e-3
common in language modeling.

\subsection{Regularization}\label{regularization}

Regularization prevents overfitting, particularly important when
training data is limited relative to model size. Genomic datasets, while
growing rapidly, remain smaller than the trillion-token corpora used for
large language models. A model with 100 million parameters trained on 10
billion nucleotides faces different overfitting risks than one trained
on 1 trillion tokens.

\textbf{Dropout} randomly zeros activations during training, forcing the
network to learn robust features independent of specific neurons.
\textbf{Attention dropout} applies this to attention weights, randomly
dropping connections between positions. This mechanism prevents
over-reliance on specific position pairs and encourages distributed
representations that remain informative even when some information
pathways are blocked. Dropout rates of 0.1 to 0.2 are typical for
genomic transformers.

\textbf{Weight decay} penalizes large parameter values, encouraging
smaller, smoother weights. For transformers, weight decay is typically
applied to all parameters except biases and layer normalization
parameters. The coefficient requires careful tuning: too little provides
insufficient regularization; too much constrains capacity and reduces
model expressiveness. Values of 0.01 to 0.1 are common, with higher
values for smaller datasets where overfitting risk is greater.

\subsection{Gradient Stability}\label{gradient-stability}

Gradient issues plague deep network training and require specific
attention for genomic transformers. \textbf{Vanishing gradients} occur
when gradients become extremely small through many layers, preventing
learning in early layers. \textbf{Exploding gradients} are the opposite:
gradients grow exponentially and destabilize training. Transformers
mitigate vanishing gradients through residual connections that provide
direct gradient paths, allowing gradients to flow from output to early
layers without passing through potentially attenuating transformations.
Exploding gradients are addressed through \textbf{gradient clipping},
which rescales gradients when their norm exceeds a threshold.

For genomic transformers, gradient issues manifest differently than in
language models. Genomic sequences have less hierarchical structure than
natural language (no grammatical sentence organization), which affects
gradient flow through attention layers. Imbalanced token frequencies
create gradient imbalances: common k-mers receive large gradients from
frequent occurrence while rare but biologically important tokens (such
as k-mers containing the stop codon TAG) receive small gradients from
infrequent occurrence. Addressing this imbalance may require loss
reweighting or adaptive sampling that ensures rare tokens appear
frequently enough for effective learning.

\subsection{Distributed Training}\label{distributed-training}

The computational scale of genomic foundation models typically exceeds
single-GPU capacity, requiring distributed training strategies.
\textbf{Data parallelism} replicates the model across GPUs, splitting
batches across devices and aggregating gradients. This approach scales
well up to batch sizes limited by convergence requirements but does not
help when the model itself exceeds GPU memory. \textbf{Model
parallelism} splits the model across devices, necessary when parameters
exceed single-GPU memory. \textbf{Pipeline parallelism} divides layers
across devices and pipelines forward and backward passes, interleaving
computation to improve device utilization.

Batch size selection involves competing considerations. Larger batches
provide more stable gradient estimates and better GPU utilization but
require more memory and may reduce generalization. Genomic transformers
often use \textbf{gradient accumulation} to simulate large batches:
small batches process sequentially, gradients accumulate, then a single
parameter update occurs. This strategy provides large-batch gradient
stability without the memory cost, though it increases training time
proportionally. Effective batch sizes of 256 to 4096 sequences are
common for genomic transformers, achieved through accumulation over many
smaller physical batches.

\section{Limitations and Emerging
Alternatives}\label{limitations-and-emerging-alternatives}

A 48-year-old patient presents with a suspected Lynch syndrome
diagnosis, and genetic testing reveals a structural variant spanning 3
megabases on chromosome 2 that may disrupt the \emph{MSH2} gene and its
upstream regulatory region. The clinical team needs to determine whether
this variant explains the patient's early-onset colorectal cancer and
guides surveillance recommendations for family members. Standard
transformers cannot address this question: the quadratic complexity of
self-attention makes 3-megabase contexts computationally intractable.
Current models can span 200 kilobases with hybrid architectures, yet the
structural variants and chromosomal rearrangements that cause many
inherited cancer syndromes remain beyond reach. This gap between
clinical need and computational capability defines the frontier of
genomic AI.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-quadratic-ceiling}{[}High{]} Log-log plot showing
computational cost (FLOPs or memory) vs.~sequence length for different
architectures. Show: Standard attention (quadratic curve, O(LÂ²)),
sparse/local attention (sub-quadratic), state space models like
Hyena/Mamba (linear). Annotate biologically relevant context lengths:
promoter (\textasciitilde1kb), gene (\textasciitilde10kb),
enhancer-promoter (\textasciitilde100kb), TAD (\textasciitilde1Mb),
chromosome arm (\textasciitilde100Mb). Draw vertical lines at each scale
showing which architectures are tractable.}

\end{figure}%

\subsection{The Quadratic Ceiling}\label{the-quadratic-ceiling}

The quadratic complexity of self-attention remains transformers' most
severe limitation for genomics. Computing all pairwise attention scores
requires \(O(L^2)\) operations and memory. For genomic contexts
exceeding 100 kilobases (roughly 100,000 single-nucleotide tokens), this
becomes prohibitive. Even with sparse approximations and efficient
implementations, transformers struggle at megabase scales where many
regulatory interactions occur and structural variants manifest their
effects.

Recent models have pushed context lengths substantially. Enformer
handles 200 kilobases; emerging models approach 1 megabase. But these
achievements rely on hybrid architectures with significant downsampling
or hierarchical windowing that may miss certain long-range patterns or
single-nucleotide resolution details. Pure transformers without such
modifications remain limited to shorter contexts. The fundamental
constraint shapes what questions transformers can address and motivates
alternatives that escape quadratic scaling.

\subsection{State Space Models}\label{state-space-models}

\textbf{State space models (SSMs)} address the quadratic barrier
directly by achieving linear complexity while maintaining long-range
modeling capability. Rather than computing all pairwise interactions,
SSMs represent sequences as continuous-time dynamical systems,
maintaining memory through recurrent state updates that propagate
information across positions without explicit pairwise computation.

Architectures like S4 (\textbf{gu\_efficiently\_2022?}), Hyena
(\textbf{poli\_hyena\_2023?}), and Mamba (\textbf{gu\_mamba\_2024?})
have demonstrated competitive or superior performance to transformers on
various sequence modeling tasks while scaling to much longer contexts.
For genomics, this capability enables whole-chromosome or potentially
whole-genome modeling that remains intractable for standard
transformers. HyenaDNA processes sequences up to 1 million nucleotides
at single-nucleotide resolution, enabling analysis of structural
variants and long-range regulatory interactions that transformers cannot
approach (Nguyen et al. 2023). The Evo model extends this further,
achieving context lengths sufficient for bacterial genome-scale
modeling. Chapter~\ref{sec-dna-lm} examines these architectures in
detail, exploring how linear complexity enables new categories of
genomic analysis.

\subsection{Choosing Architectures}\label{choosing-architectures}

The choice between transformers and alternatives depends on the
biological question and computational constraints. Transformers excel
when global context matters but sequences are not extremely long (under
10 to 50 kilobases depending on computational resources). Attention maps
provide interpretability, showing which positions the model considers
relevant for predictions. Transformers benefit from extensive tooling
and pretrained models from NLP that transfer readily to genomics.

CNNs remain preferable when computational efficiency is paramount and
local patterns dominate. For splice site prediction or promoter
classification where relevant context spans at most a few hundred base
pairs, a well-designed CNN may outperform transformers while using far
fewer parameters. The inductive bias toward local patterns also
regularizes against overfitting when training data is limited.

Hybrid approaches often achieve the best practical results for
intermediate-scale problems. Models combining CNNs for local feature
extraction with transformers for long-range integration outperform pure
architectures on regulatory prediction tasks, as
Chapter~\ref{sec-regulatory} demonstrates with Enformer and related
models. The optimal combination depends on the specific biological
question and the scale of relevant interactions.

The transition toward sub-quadratic architectures continues. Early
results suggest SSMs match or exceed transformers on some genomic
benchmarks while scaling to longer contexts. The question is no longer
whether alternatives to quadratic attention exist, but which tasks
benefit most from linear-complexity architectures and which retain
advantages from explicit pairwise attention computation.

{[}FIGURE RECOMMENDATION: Scaling comparison showing computational cost
vs.~context length for standard attention (quadratic curve), sparse
attention variants (sub-quadratic), and state space models (linear).
Annotate with biologically relevant context lengths: promoter regions
(\textasciitilde1kb), enhancer-gene distances (\textasciitilde100kb),
TAD sizes (\textasciitilde1Mb), chromosome arms (\textasciitilde100Mb).
Show which architectures are tractable at each scale.{]}

\section{Architecture and Learning}\label{architecture-and-learning}

The transformer architecture provides the computational substrate for
modern genomic foundation models, but architecture alone does not
determine what models learn. Attention mechanisms enable pairwise
interaction modeling across arbitrary sequence distances. Position
encodings break permutation invariance to preserve the sequential
structure essential to regulatory grammar. Stacked blocks build
hierarchical representations through iterative refinement. These
components create capacity; training objectives and data determine how
that capacity is used.

Self-supervised pretraining transforms architectural capacity into
biological knowledge. Masked language modeling teaches models to predict
held-out tokens from context, implicitly learning the sequence patterns
and evolutionary constraints that determine biological function.
Next-token prediction in autoregressive models captures sequential
dependencies required for sequence generation. Applied to massive
genomic datasets, these objectives enable transformers to learn
representations that transfer across diverse downstream tasks without
task-specific supervision. The foundation models examined in subsequent
chapters (DNABERT for regulatory sequence, ESM-2 for proteins, Enformer
for expression prediction) each demonstrate that transformers trained on
biological sequence capture patterns that generalize beyond their
training objectives.

Attention introduced a paradigm shift in how genomic models access
context. Where convolutional networks aggregate local information
through hierarchical composition, attention enables direct communication
between any two positions regardless of distance. The computational
challenge shifts from extending receptive fields to managing the
quadratic complexity of pairwise attention. State space models and
linear attention variants address this bottleneck while maintaining
long-range capability, and whether these alternatives ultimately
complement or displace standard transformers remains an open question.
What is clear is that attention-based architectures have become the
default substrate for genomic foundation models, with the pretraining
objectives examined next determining what biological knowledge they
acquire.

\chapter{Pretraining Strategies}\label{sec-pretraining}

The choice of pretraining objective is not merely technical; it encodes
assumptions about what matters in biological sequence. Masked language
modeling encourages bidirectional context integration: the model learns
to predict missing tokens using information from both upstream and
downstream sequence. Next-token prediction builds autoregressive
capabilities: the model learns to generate sequence one position at a
time, enabling design of novel proteins or regulatory elements.
Contrastive learning teaches invariance: the model learns that
functionally equivalent sequences should map to similar representations
regardless of species or polymorphism. Each objective produces a
different model, and those differences propagate to downstream
performance. A model pretrained with masked language modeling may excel
at variant effect prediction (where context on both sides matters) but
struggle at sequence generation. A model pretrained for generation may
produce plausible sequences but provide representations less suited for
classification tasks. Understanding what each objective teaches, and
what assumptions each encodes, is prerequisite to selecting the right
foundation model for a given application.

Self-supervised pretraining addresses a fundamental asymmetry in genomic
data. Reference genomes span billions of nucleotides across thousands of
species. Population sequencing projects catalog genetic variation in
millions of individuals. Functional genomics consortia measure chromatin
accessibility and gene expression across hundreds of cell types. Yet
experimental labels remain sparse: for any given sequence, we typically
lack direct measurements of its regulatory function, its effect on
splicing, or its contribution to disease risk. Self-supervised
objectives extract training signal from the sequences themselves,
without requiring experimental labels. The resulting models learn
representations that capture evolutionary constraints, sequence grammar,
and functional relationships, all from the patterns present in unlabeled
data. When these representations are applied to downstream tasks with
limited labels, the pretrained knowledge makes scarce labeled data go
further.

This chapter examines the major pretraining strategies employed by
genomic foundation models, from masked language modeling in DNABERT and
Nucleotide Transformer through autoregressive objectives in generative
models. The focus is on understanding what each objective teaches the
model to learn, what biological patterns emerge from each training
approach, and how objective choice shapes downstream capabilities. The
goal is not encyclopedic coverage of every pretraining variant but
principled understanding of the design space: which objectives suit
which applications, what tradeoffs each involves, and how to reason
about novel objectives as the field continues to develop.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-pretraining-objectives}{[}Essential{]} Three-panel
schematic comparing major pretraining objectives on the same input
sequence. Panel A (MLM): Sequence with masked positions (shown as
{[}MASK{]}), bidirectional context arrows from both sides, prediction
targets at masked positions. Panel B (Next-token prediction):
Causal/unidirectional arrows, each position predicting the next, with
sampling process for generation. Panel C (Contrastive): Anchor sequence,
positive pair (augmented version), negative samples, mapped to embedding
space with distance relationships.}

\end{figure}%

\section{Masked Language Modeling}\label{masked-language-modeling}

Consider predicting whether a splice site variant in \emph{DMD} will
cause exon skipping in Duchenne muscular dystrophy. The model must
recognize the canonical GT-AG splice signals, understand how flanking
sequences modulate splicing efficiency, and integrate information from
both the upstream exon and downstream intron. A model trained only on
labeled splice variants would see perhaps a few hundred \emph{DMD}
examples across the entire clinical literature. A model pretrained on
billions of nucleotides learns splice grammar across the entire genome,
then applies that knowledge to the specific clinical question.
\textbf{Masked language modeling} provides this pretraining by teaching
models to predict missing sequence content from surrounding context, and
the bidirectional attention it requires captures exactly the
upstream-downstream integration that splice prediction demands.

MLM treats sequences as partially observed and trains models to
reconstruct missing content. The procedure is straightforward: randomly
mask portions of an input sequence, feed the corrupted sequence to the
model, and train the model to predict the original tokens at masked
positions. A \textbf{masking strategy} replaces selected tokens with a
special \texttt{{[}MASK{]}} token, leaving the surrounding context
intact. The model processes the masked sequence through its layers and
produces predictions for the masked positions, typically optimizing
cross-entropy loss over the vocabulary at each masked location.

The key insight is that accurate prediction requires learning genuine
sequence structure. To predict a masked position in a transcription
factor binding site, the model must recognize the surrounding motif
context. To predict masked splice donor sequences, the model must encode
the consensus GT dinucleotide and the flanking patterns that modulate
splicing strength. Over millions of training examples, models build
distributed representations of motifs, compositional rules, and sequence
constraints that transfer to tasks never seen during pretraining. The
\emph{DMD} splice variant can be evaluated using patterns learned from
every splice site in the genome.

MLM encourages bidirectional context integration, and this
bidirectionality has direct clinical relevance. Unlike autoregressive
models that condition only on preceding tokens, MLM models see both left
and right context when predicting masked positions. For genomics, this
matches biological reality: regulatory function depends on patterns both
upstream and downstream of any given position. A transcription factor
binding site is recognized through flanking sequences on both sides.
Splicing signals require coordination between donor and acceptor sites
separated by hundreds of bases. Missense variants disrupt protein
function through effects that depend on the entire domain context, not
just the preceding amino acids. The bidirectional attention in MLM
naturally captures these dependencies.

\subsection{Masking Strategies and Their
Implications}\label{masking-strategies-and-their-implications}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-masking-strategies}{[}High{]} Two-panel
visualization comparing masking strategies on a regulatory sequence
containing a TF binding motif. Panel A (Random token masking):
Individual tokens masked throughout, including partial motif masking;
show how local context allows easy prediction. Panel B (Span masking):
Entire motif region masked as contiguous span; show how prediction
requires reasoning from distal regulatory context. Include attention
patterns for each, showing how span masking forces longer-range
dependencies.}

\end{figure}%

Predicting whether a regulatory variant disrupts an entire transcription
factor binding site or merely alters its affinity requires models that
learn compositional patterns, not just local nucleotide statistics. The
tension between local and compositional learning plays out in masking
strategy design.

Random masking of individual tokens creates predictions that are
relatively local: each masked position can often be inferred from
immediately adjacent nucleotides. This approach is efficient but may not
force models to learn higher-order structure. \textbf{Span masking},
which masks contiguous blocks of tokens, forces models to infer
longer-range dependencies and compositional patterns. If an entire
transcription factor binding motif is masked, the model cannot rely on
partial motif information and must instead recognize the motif's role
from surrounding regulatory context. For clinical variant
interpretation, span masking may better capture the compositional
grammar that determines whether a regulatory variant disrupts an entire
binding site or merely modulates its affinity.

Masking rates present a fundamental tradeoff between supervision density
and prediction difficulty. Higher masking rates (30-40\% of tokens)
provide more supervision per sequence but make prediction harder and may
destabilize training. Lower masking rates (10-15\%) produce more stable
training but require more data to achieve equivalent coverage. The
standard 15\% rate from BERT (Devlin et al. 2019) represents a
reasonable compromise, though genomic models have explored values
ranging from 10\% to 40\% depending on context length and tokenization
granularity. \emph{DNABERT} used 15\% masking on 6-mer tokens (Ji et al.
2021), while later models have experimented with adaptive masking rates
that increase as training progresses, starting conservatively and
becoming more aggressive as the model's predictions improve.

Tokenization interacts with masking in ways that affect what biological
patterns models learn. \emph{DNABERT} pioneered MLM for genomic
sequences by applying it to overlapping k-mer tokens: rather than
treating DNA as individual nucleotides, \emph{DNABERT} tokenizes
sequences into all possible 6-mers with overlapping windows (Ji et al.
2021). Masking then operates at the k-mer level, with entire 6-mers
masked as units. This design encourages learning of k-mer level patterns
corresponding to transcription factor binding motifs (typically 6-12
base pairs) and other short functional elements. \emph{DNABERT-2}
adopted \textbf{byte-pair encoding} tokenization, which learns a
vocabulary of variable-length subword units from the training corpus
(\textbf{zhou\_dnabert2\_2023?}). BPE tokens represent single
nucleotides, common motifs, or repeated elements depending on their
frequency. MLM with BPE balances flexibility with compositional
structure, though the learned vocabulary may not align with biological
functional units in interpretable ways.

The design decisions explored by \emph{DNABERT} and \emph{DNABERT-2}
established patterns that subsequent DNA language models have built upon
and refined. Chapter~\ref{sec-dna-lm} examines how these architectural
and tokenization choices have evolved as the field has scaled to longer
contexts and larger training corpora.

\subsection{What Masked Language Models
Learn}\label{what-masked-language-models-learn}

MLM objectives drive models to capture multiple levels of sequence
organization, from local nucleotide statistics to long-range regulatory
grammar. At the lowest level, models learn base composition and local
constraints: CpG dinucleotide frequencies, GC content biases, and simple
repeat patterns. These basic properties are necessary but not sufficient
for biological function prediction.

At higher levels, MLM captures motif patterns and sequence grammar.
Predicting masked positions in regulatory regions requires recognizing
transcription factor binding sites, understanding how motifs combine in
enhancers and promoters, and learning context-dependent usage patterns.
If certain transcription factor motifs co-occur at specific distances
(as they do in developmental enhancers where factors like \emph{HOX}
proteins bind cooperatively), masking one motif and predicting it from
the other reinforces this grammatical relationship. This compositional
learning is difficult to achieve with supervised learning alone, which
typically provides coarse binary labels (``enhancer'' versus
``non-enhancer'') rather than fine-grained structural information about
sequence organization.

MLM also captures evolutionary conservation patterns implicitly, and
this has direct relevance for clinical variant interpretation. Conserved
sequences are constrained because mutations would disrupt function. By
learning to predict conserved patterns from surrounding context, models
encode which sequence features are under selection. This knowledge
transfers to variant effect prediction, where the model recognizes when
a mutation disrupts a learned conserved pattern. A variant that replaces
a highly predictable position (one the model confidently fills in during
MLM) is more likely to be damaging than one at a position where the
model is uncertain. The connection between pretraining on raw sequence
and downstream variant interpretation illustrates how self-supervised
objectives capture biologically meaningful structure without explicit
functional labels.

\section{Next-Token Prediction}\label{next-token-prediction}

Designing a novel promoter sequence for gene therapy requires generating
DNA that respects learned regulatory grammar while achieving specific
expression characteristics. Masked language modeling can evaluate
whether a candidate sequence looks ``natural,'' but it cannot generate
sequences from scratch. A gene therapy team optimizing a CAR-T construct
needs promoter variants to test; they cannot simply evaluate candidates
one by one when the search space spans \(4^{500}\) possible
500-base-pair sequences. Next-token prediction provides the generative
capability missing from MLM, learning to predict each token given only
preceding tokens and thereby acquiring the ability to sample coherent
novel sequences that respect learned biological constraints.

\textbf{Next-token prediction} represents an alternative paradigm where
models learn to predict each token in a sequence given only the
preceding tokens. This \textbf{autoregressive} approach, popularized by
GPT-style language models, treats sequence generation as a core
capability rather than a secondary feature. For a sequence of length
\(T\), the model predicts token \(t\) from tokens \(1\) through \(t-1\),
maximizing the likelihood of the observed sequence under the model's
learned distribution. The probability of a sequence factors as the
product of conditional probabilities for each token given its
predecessors:

\[P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} P(x_t | x_1, \ldots, x_{t-1})\]

Algorithmically, next-token prediction requires \textbf{causal masking}
in the attention mechanism. Each position attends only to earlier
positions, ensuring predictions at position \(t\) depend exclusively on
positions \(1\) through \(t-1\). The loss function is cross-entropy over
the vocabulary, computed at every position rather than only at masked
locations. During training, \textbf{teacher forcing} allows efficient
parallel computation: the model predicts all positions simultaneously by
feeding in the ground truth sequence shifted by one position. Generation
at inference time is inherently sequential, predicting one token at a
time and conditioning each prediction on all previous outputs.

The fundamental difference from MLM lies in what the model can see
during prediction. Autoregressive models build representations from
unidirectional context, learning to generate sequences that respect
learned constraints. This makes autoregressive pretraining attractive
for sequence design applications. Sampling new sequences proceeds
naturally: predict the first token, condition on it to predict the
second, and continue token by token. The generation process directly
uses the learned conditional distributions without requiring additional
architectural modifications or iterative refinement procedures.

\subsection{Genomic Applications}\label{genomic-applications}

DNA sequences present a complication that natural language does not:
they have no inherent directionality. Both strands encode information,
and regulatory function is often strand-agnostic. A transcription factor
binding site functions identically whether read 5'-to-3' or on the
reverse complement strand. This contrasts with natural language, where
left-to-right reading order carries meaning. Early autoregressive
genomic models addressed this by training separate models for forward
and reverse strands or by augmenting training data with
reverse-complement sequences. More recent approaches treat strand
symmetry as an architectural constraint, ensuring that forward and
reverse complement sequences produce equivalent representations through
weight sharing or explicit symmetrization.

\emph{Evo} represents a large-scale autoregressive genomic model trained
on whole genomes with long-context architectures
(\textbf{nguyen\_sequence\_2024?}). Using StripedHyena layers to achieve
contexts exceeding 100 kilobases, \emph{Evo} learns long-range
dependencies including gene structure, repeat organization, and
regulatory architecture spanning tens of kilobases. This enables
generating coherent synthetic genomes that respect higher-order
structure, not just local motif patterns. For therapeutic applications,
\emph{Evo}'s generative capability could design synthetic regulatory
circuits, generate diverse candidate sequences for directed evolution,
or produce training data through synthetic augmentation when real
labeled data is scarce.

Protein sequence models benefit from autoregressive pretraining with
clearer biological justification. The N-terminus to C-terminus
directionality of protein synthesis provides a natural left-to-right
ordering: ribosomes translate mRNA sequentially, and co-translational
folding means that protein structure emerges progressively from the
N-terminus. \emph{ESM} models and protein design systems like
\emph{ProtGPT2} predict amino acid sequences autoregressively, learning
protein grammar and evolutionary constraints that transfer to structure
prediction and function annotation (Ferruz, Schmidt, and HÃ¶cker 2022).
For designing therapeutic proteins (antibodies, enzymes, peptide drugs),
autoregressive generation produces candidates that respect learned
constraints on foldability and function.

\subsection{Comparing MLM and Autoregressive
Objectives}\label{comparing-mlm-and-autoregressive-objectives}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-bidirectional-vs-autoregressive}{[}Essential{]}
Two-panel figure showing information flow during prediction. Panel A
(MLM): Position in center of sequence with arrows coming from BOTH left
and right context, showing bidirectional conditioning. Annotate: ``Sees
full context â†’ better for understanding.'' Panel B (Autoregressive):
Same position with arrows only from left (preceding tokens), showing
causal restriction. Annotate: ``Sees only past â†’ enables generation.''}

\end{figure}%

The tension between bidirectional understanding and generative
capability represents the fundamental tradeoff between these objectives.
For tasks requiring understanding of full sequence context, MLM's
bidirectional attention provides richer representations. Predicting
transcription factor binding at a specific location benefits from seeing
both upstream and downstream sequence, information that autoregressive
models cannot access during inference. Variant effect prediction
similarly benefits from full context: a missense variant's impact
depends on the entire domain, not just the preceding residues.

Autoregressive models offer more principled generation. Their sequential
prediction structure matches the generation process exactly, whereas
generating from MLM models requires iterative masking and filling
procedures that were not part of pretraining. A promoter design task
using MLM would require starting with random sequence, masking
positions, predicting fills, remasking, and iterating until convergence.
This procedure is ad hoc and may not produce sequences that lie on the
learned distribution. Autoregressive generation is direct: sample token
by token from learned conditionals.

Training efficiency differs between objectives in ways that affect
practical decisions. MLM predicts only 15\% of tokens per sequence but
uses bidirectional context for each prediction. Autoregressive models
predict all tokens but with unidirectional context. The effective
supervision per sequence is higher for autoregressive training, but each
prediction is less informed. For fixed compute budgets, the tradeoffs
roughly balance, with optimal choice depending on downstream
applications rather than training efficiency alone.

Task-specific performance depends on alignment between pretraining and
downstream objectives. If the downstream task involves predicting
missing information from context (variant effect prediction, binding
site identification, conservation scoring), MLM pretraining provides
better transfer. If the downstream task involves generation or
sequential decision-making (sequence design, sampling from conditional
distributions, therapeutic protein generation), autoregressive
pretraining aligns more naturally. For applications requiring both
understanding and generation, hybrid architectures that combine
bidirectional encoding with autoregressive decoding offer a middle
ground, though these add complexity.

\section{Span Corruption and
Denoising}\label{span-corruption-and-denoising}

Clinical variant interpretation must be robust to sequencing errors,
population polymorphisms, and batch effects between discovery and
validation cohorts. A pathogenic variant identified in a research study
must remain classifiable as pathogenic when sequenced on a different
platform in a clinical laboratory, surrounded by different technical
artifacts and population-specific polymorphisms. A model trained only on
pristine reference sequence may fail when encountering the noise and
variation present in real patient data. \textbf{Denoising objectives}
address this by training models on corrupted inputs, building tolerance
to the kinds of perturbations that occur in clinical genomics pipelines.

\textbf{Span corruption} generalizes masked language modeling by
introducing more complex forms of input degradation. The T5 model
popularized this approach for natural language (Raffel et al. 2019), and
the principles transfer to genomic sequences with biological
adaptations. Rather than masking individual tokens, span corruption
masks contiguous spans of variable length and replaces each span with a
single sentinel token. The model then generates the original content of
all masked spans in sequence, learning to reconstruct substantial
missing regions rather than isolated positions.

This objective teaches different aspects of sequence structure than
standard MLM. Reconstructing entire spans requires understanding
longer-range dependencies and compositional patterns. If a span
encompasses an entire transcription factor binding motif (typically 6-12
base pairs), the model cannot infer the motif from partial information
and must instead reason about the motif's role from surrounding
regulatory context. Span lengths are typically sampled from a
distribution (geometric or uniform) with a mean around 3-5 tokens,
creating a mix of short and long reconstruction challenges within each
training example.

Denoising objectives extend beyond masking to include other forms of
corruption that mirror real-world data degradation. \textbf{Token
substitution} replaces input tokens with random tokens from the
vocabulary, creating corrupted sequences that resemble sequencing errors
or natural variation. The model learns to distinguish correct from
incorrect tokens based on surrounding context, encouraging
representations that capture local consistency and motif structure.
\textbf{Deletion and insertion corruptions} remove or add tokens at
random positions, teaching models about position-invariant features that
remain identifiable despite surrounding changes. For genomics,
insertions and deletions are biologically realistic mutation types
(indels account for approximately 15\% of pathogenic variants in ClinVar
(Landrum et al. 2018)), and models that handle them during pretraining
may better predict their effects downstream.

\subsection{Biologically Motivated
Corruption}\label{biologically-motivated-corruption}

The most effective corruption strategies mirror actual sources of noise
in clinical genomics data. Simulating sequencing errors provides
corruption strategies that match experimental reality. Base miscalls
follow platform-specific patterns: Illumina sequencing shows
characteristic substitution biases (favoring certain nucleotide
transitions over transversions, with error rates of 0.1-1\% depending on
read position and quality score), while nanopore sequencing exhibits
distinct error profiles concentrated in homopolymer regions where the
signal for consecutive identical bases becomes ambiguous. Training with
corruptions that mimic these error patterns may improve generalization
to real sequencing data with platform-specific artifacts.

\textbf{Variant augmentation} introduces biologically realistic sequence
changes based on population variation. Randomly substituting alleles at
known polymorphic sites or injecting variants from databases like gnomAD
(Karczewski et al. 2020) creates corrupted sequences reflecting natural
genetic diversity. This teaches models that common polymorphisms are
normal variation rather than errors to be corrected, potentially
improving robustness for variant effect prediction where distinguishing
pathogenic variants from benign polymorphisms is the central challenge.
A model trained only on reference sequence might flag any deviation as
potentially damaging; a model trained with variant augmentation learns
which deviations are within normal population variation.

Structural variation simulation models larger-scale genomic changes:
tandem duplications, copy number variation, and segmental
rearrangements. These corruptions are harder to implement but capture
realistic sources of genomic diversity beyond single-nucleotide changes.
Models trained with structural variation corruptions may better
understand how gene dosage changes, enhancer duplications, or domain
boundary disruptions affect function. For clinical applications
involving copy number variants (which underlie conditions ranging from
developmental disorders like DiGeorge syndrome to cancer predisposition
in hereditary breast cancer), this training signal could improve
predictive accuracy.

The benefit of denoising pretraining extends to robustness under
distribution shift. If downstream applications involve sequences from
different populations, experimental platforms, or tissue contexts than
the pretraining corpus, models pretrained with appropriate corruptions
can maintain performance despite distribution mismatch. This matters in
clinical genomics, where validation cohorts often differ from discovery
cohorts in ancestry composition, sequencing technology, or phenotyping
protocols. A model trained with corruptions spanning these sources of
variation generalizes more reliably than one trained only on pristine
reference sequence.

\section{Contrastive Learning}\label{contrastive-learning}

Cross-population generalization presents a persistent challenge in
clinical genomics. A variant classifier trained on European ancestry
cohorts may perform poorly on African ancestry patients due to different
patterns of linkage disequilibrium and background polymorphism. The
classifier learned to recognize pathogenic variants against a European
genetic background; African genomes present the same functional variants
but surrounded by different neutral polymorphisms. \textbf{Contrastive
learning} addresses this by teaching models to recognize functional
equivalence despite sequence-level differences, producing
representations where a regulatory element is recognizable regardless of
the population-specific variants surrounding it.

Contrastive learning takes a fundamentally different approach to
self-supervised pretraining than reconstruction-based objectives. Rather
than recovering corrupted inputs, contrastive objectives train models to
produce similar representations for different views of the same sequence
while distinguishing them from representations of unrelated sequences.
The intuition is that augmented versions of a sequence (with minor
corruptions, reverse complementation, or variants) should map to nearby
points in representation space, while unrelated sequences should map to
distant points. This teaches invariance to transformations that do not
change function.

The algorithmic framework constructs \textbf{positive pairs} and
\textbf{negative samples}. For a given anchor sequence, positive pairs
are created through augmentation: reverse complementation, random
cropping, variant injection, or other transformations that preserve
functional identity. Negative samples are drawn from other sequences in
the training batch. The model produces \textbf{embeddings} for all
sequences, and the contrastive loss encourages anchor and positive
embeddings to be similar (high cosine similarity) while pushing apart
anchor and negative embeddings.

\textbf{InfoNCE loss} is the most common contrastive objective
(\textbf{oord\_representation\_2018?}). For an anchor embedding \(z_i\)
and positive embedding \(z_i^+\), InfoNCE maximizes:

\[\mathcal{L} = -\log \frac{\exp(z_i \cdot z_i^+ / \tau)}{\sum_j \exp(z_i \cdot z_j / \tau)}\]

where the sum runs over the positive and all negative samples, and
\(\tau\) is a temperature parameter controlling the sharpness of the
distribution. Lower temperatures make the model more discriminative,
requiring cleaner separation between positives and negatives. The
objective is equivalent to classifying the positive pair among all
possible pairs, and the model learns representations that make this
classification easy.

\subsection{Augmentation Design for Genomic
Sequences}\label{augmentation-design-for-genomic-sequences}

A CTCF binding site must be recognizable whether it appears on a
European or African genetic background, whether read on the forward or
reverse strand, and whether the surrounding sequence contains common
polymorphisms or reference alleles. Augmentation design is critical for
contrastive learning because augmentations must preserve functional
identity while introducing variability. If augmentations change
function, the contrastive objective will learn meaningless invariances.
Several augmentation strategies are biologically grounded and preserve
the functional relationships that matter for downstream clinical
applications.

\textbf{Reverse complementation} is the simplest and most reliable
augmentation. DNA is double-stranded, and many regulatory elements
function identically on either strand. Training the model to treat
forward and reverse complement sequences as equivalent captures strand
symmetry inherent in molecular biology. This augmentation is universally
applicable and introduces no risk of changing functional identity.

\textbf{Random cropping} extracts overlapping windows from longer
sequences. If a transcription factor binding site appears in multiple
cropped windows, the model learns that the binding site is the
functionally relevant feature regardless of absolute position or
surrounding context. This teaches position-invariant representations
useful for tasks where genomic coordinates matter less than local
sequence content. A binding site predictor benefits from learning that
the core motif is what matters, not its position within the input
window.

\textbf{Variant injection} introduces common polymorphisms or simulated
mutations. If the variants are neutral (common variants from gnomAD with
high allele frequency, which are unlikely to be damaging), treating
variant and reference sequences as positive pairs teaches robustness to
genetic variation. This is particularly valuable for cross-population
generalization, where models must recognize functional elements despite
surrounding sequence polymorphism that differs between ancestry groups.
A model trained with variant augmentation learns that a CTCF binding
site is functionally equivalent whether it appears surrounded by
European or African background variants.

Negative sampling strategies also affect what models learn. Random
genomic sequences provide straightforward negatives but may be too easy
to distinguish: any functional regulatory sequence is readily separable
from random intergenic sequence. Harder negatives, such as sequences
from paralogous genes, pseudogenes, or orthologous regions in distant
species, provide more informative supervision that forces the model to
learn subtle discriminative features.

\subsection{Cross-Species Contrastive
Learning}\label{cross-species-contrastive-learning}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-cross-species-contrastive}{[}Enhancing{]}
Illustration of contrastive pretraining using orthologous sequences.
Show: Human enhancer sequence and mouse ortholog (aligned, showing
nucleotide divergence but conserved functional elements). Both mapped to
embedding space as nearby points (positive pair). Non-orthologous
sequence mapped to distant point (negative). Include phylogenetic
context showing \textasciitilde75 million years of divergence. Annotate:
``Same function despite sequence divergence â†’ learns species-invariant
features.''}

\end{figure}%

Leveraging evolutionary relationships for self-supervision enables a
particularly powerful form of contrastive learning. Orthologous
sequences from different species share functional identity despite
nucleotide divergence accumulated over millions of years of evolution.
Treating orthologous pairs as positives and non-orthologous pairs as
negatives teaches the model to extract species-invariant functional
features. A human enhancer and its mouse ortholog should map to similar
embeddings despite 75 million years of sequence divergence, while
unrelated sequences should map to distant embeddings.

This approach has direct implications for drug development and
therapeutic translation. Many drug targets are validated in mouse models
before human clinical trials; roughly 95\% of cancer drugs that succeed
in mouse models fail in human trials, often because the models do not
adequately capture human biology. A model pretrained with human-mouse
contrastive pairs may generalize better to predicting drug response in
humans based on mouse efficacy data, or to transferring regulatory
circuit designs from model organisms to human cell types. The
evolutionary record provides implicit labels about functional
equivalence that would be expensive to obtain through direct
experimental annotation.

Sequence \textbf{embedding} quality improves with contrastive
pretraining in ways that benefit clinical applications. Models trained
contrastively produce embedding spaces where functionally similar
sequences cluster together, enabling nearest-neighbor search for
annotating novel variants (finding similar characterized variants),
sequence retrieval for identifying regulatory homologs, and unsupervised
clustering of regulatory elements. For variant effect prediction,
contrastive pretraining improves robustness: if the model learns that
sequences differing only by neutral variants are functionally
equivalent, it will better distinguish truly disruptive variants from
benign polymorphisms.

\section{Multi-Task Pretraining}\label{multi-task-pretraining}

Predicting variant pathogenicity requires integrating multiple lines of
evidence: evolutionary conservation, protein structure effects, splicing
changes, and regulatory disruption. A variant in \emph{TTN} (the gene
encoding titin, mutated in 25\% of dilated cardiomyopathy cases) might
be pathogenic because it disrupts protein folding, because it alters
splicing, or because it affects regulatory binding sites. No single
assay captures all these dimensions. \textbf{Multi-task pretraining}
addresses this by jointly optimizing for diverse prediction tasks,
learning representations that capture the multiple facets of genomic
function relevant to clinical interpretation.

Multi-task pretraining combines multiple related objectives during the
same training run, jointly optimizing for several prediction tasks.
Different tasks provide complementary supervision signals: masking
captures local sequence patterns, chromatin prediction captures
regulatory function, conservation scoring captures evolutionary
constraint, and expression prediction captures transcriptional
consequences. Representations that satisfy all tasks simultaneously
develop richer and more general features than any single objective
alone.

\textbf{Task selection} is the first design decision. Ideally, tasks
should be diverse enough to provide distinct supervision signals but
related enough to benefit from shared representations. For genomic
models, effective combinations include masked language modeling for
general sequence structure, chromatin accessibility prediction for
regulatory function, gene expression prediction for transcriptional
output, evolutionary conservation scoring for functional constraint, and
variant frequency prediction from population databases. Each task
operates on the same input sequence but predicts different outputs using
task-specific head layers. The shared backbone encoder processes the
sequence into intermediate representations, and separate prediction
heads map these representations to task-specific outputs.

\textbf{Task weighting} determines how much each task contributes to the
total loss. With \(\mathcal{L}_1, \ldots, \mathcal{L}_K\) representing
individual task losses, the multi-task loss combines them:

\[\mathcal{L}_{\text{total}} = \sum_{k=1}^K w_k \mathcal{L}_k\]

where \(w_k\) are task weights. Equal weighting is simple but may lead
to imbalanced learning if tasks have different scales or difficulties. A
task with high variance loss may dominate gradient updates, starving
other tasks of learning signal. \textbf{Dynamic weighting} approaches
adjust weights during training based on learning progress, using
uncertainty estimation, gradient norms, or task-specific validation
performance as signals for rebalancing.

\subsection{Large-Scale Multi-Task
Examples}\label{large-scale-multi-task-examples}

\emph{Enformer} exemplifies large-scale multi-task pretraining for
genomics (Å½. Avsec et al. 2021). The model predicts over 5,000 genomic
assays simultaneously: ChIP-seq signals for hundreds of transcription
factors and histone marks, DNase-seq and ATAC-seq accessibility across
cell types, CAGE transcription initiation profiles, and more. This
massive multi-task objective (covering 674 DNase-seq, 4,675 ChIP-seq,
and 638 CAGE experiments from ENCODE and Roadmap Epigenomics
(\textbf{encode\_project\_integrated\_2012?})) forces the model to learn
representations capturing diverse regulatory signals.

The task diversity in \emph{Enformer} provides supervision far richer
than any single assay. A model trained only on DNase-seq learns general
accessibility patterns but misses transcription factor specificity: it
cannot distinguish which factors bind to accessible regions. A model
trained only on H3K27ac ChIP-seq captures active enhancers but misses
repressive marks that indicate silenced regulatory elements. Training on
all assays jointly allows the model to disentangle overlapping and
complementary signals, learning representations that generalize across
regulatory contexts. For clinical variant interpretation, this means
\emph{Enformer} can predict how a regulatory variant affects enhancer
activity, chromatin state, transcription factor binding, and gene
expression simultaneously.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-multitask-pretraining}{[}High{]} Diagram showing
shared encoder with multiple prediction heads. Structure: Sequence input
â†’ Convolutional layers â†’ Transformer layers (labeled ``Shared
Backbone'') â†’ Branching to separate heads for: Chromatin accessibility
(DNase-seq, ATAC-seq), Histone modifications (H3K27ac, H3K4me3, etc.),
Transcription factor binding (hundreds of factors), Gene expression
(CAGE). Annotate approximate task counts from ENCODE: ``674 DNase +
4,675 ChIP-seq + 638 CAGE.''}

\end{figure}%

\emph{Borzoi} extends this paradigm to full RNA-seq coverage prediction,
jointly modeling transcription initiation, splicing, and transcript
abundance (\textbf{linder\_borzoi\_2023?}). By predicting continuous
coverage across gene bodies rather than just expression levels,
\emph{Borzoi} captures splicing patterns that are invisible to models
predicting only total expression. This has direct clinical relevance:
many pathogenic variants act through splicing disruption rather than
protein-coding changes, and models that capture splicing patterns can
identify variants that traditional expression-based approaches miss.

Combining MLM with functional prediction represents another multi-task
configuration. The model predicts masked tokens through a language
modeling head while simultaneously predicting chromatin accessibility or
other functional readouts through regression heads. This hybrid
objective balances sequence-level pretraining with functional
supervision. The MLM component ensures the model learns general sequence
patterns even in regions without functional annotations (the majority of
the genome lacks chromatin or expression measurements in any given cell
type), while the functional prediction component focuses learning on
biologically relevant features.

\subsection{When Multi-Task Helps and When It
Hurts}\label{when-multi-task-helps-and-when-it-hurts}

\textbf{Task interference} is the primary concern with multi-task
learning. If tasks require conflicting representations, jointly
optimizing for both may compromise performance on each compared to
single-task baselines. In genomics, this might occur if one task
benefits from very local features (splice site prediction, which depends
on short consensus sequences spanning roughly 10 base pairs) while
another requires long-range context (enhancer activity prediction, which
depends on distant promoter interactions spanning 100 kilobases). The
shared backbone must compromise, potentially learning suboptimal
representations for both.

\textbf{Negative transfer} occurs when adding a task actually hurts
downstream performance compared to training without it. This can happen
if the additional task introduces noise (poorly measured assays with
high experimental variance), if task weights are poorly balanced
(causing one task to dominate gradients), or if the auxiliary task
shifts learned representations away from features useful for target
applications. The risk of negative transfer increases with task
diversity: distantly related tasks are more likely to require
conflicting representations.

The benefits of multi-task pretraining are largest when tasks are
complementary and data for individual tasks is limited. If chromatin
data is sparse for a particular cell type but gene expression data is
abundant, jointly training on both may improve performance on both
compared to single-task models. The shared representations allow
information to flow between tasks, compensating for data scarcity in any
single modality. When functional labels exist at scale and tasks are
genuinely related, multi-task pretraining consistently outperforms
single-task alternatives.

\section{Data Strategies for
Pretraining}\label{data-strategies-for-pretraining}

Corpus construction establishes the foundation for pretraining and
determines what patterns the model can learn. A clinical variant
classifier is only as good as the evolutionary and population diversity
captured in its pretraining corpus. If the training data underrepresents
African genetic variation (African populations harbor more genetic
diversity than all other continental populations combined, yet
constitute a small fraction of most reference panels), the resulting
model will underperform on African ancestry patients. These data
decisions have direct consequences for health equity and clinical
utility.

\textbf{Reference genomes} are the standard starting point. Human genome
assemblies like GRCh38 provide high-quality, contiguous sequence
spanning all chromosomes (roughly 3.1 billion base pairs of assembled
sequence, representing about 92\% of the full genome before
telomere-to-telomere completion). Training on the reference genome
allows models to learn patterns characteristic of human DNA: base
composition, repeat structure, gene organization, and regulatory
architecture. The reference genome represents a single haploid
consensus, missing variation present in human populations, but provides
the foundation for most pretraining approaches.

Population-scale variation can be incorporated through variant
databases. Rather than training only on reference sequence, injecting
variants at observed population frequencies creates synthetic diploid
genomes reflecting real genetic diversity. This teaches models that
common polymorphisms are normal variation, potentially improving
robustness and variant effect prediction. gnomAD provides allele
frequencies across over 800,000 individuals spanning diverse ancestries
(Karczewski et al. 2020), enabling population-aware training.
\textbf{Pan-genome} approaches extend this by representing multiple
high-quality assemblies from diverse individuals, capturing structural
variation and population-specific haplotypes that a single reference
cannot represent.

Repeat handling impacts pretraining in ways that depend on downstream
applications. Simple repeats, tandem repeats, and transposable elements
occupy roughly half of the human genome but contribute less directly to
protein-coding function than unique sequences. \textbf{Hard-masking}
repeats (replacing them with Ns) reduces training data but may discard
information relevant to some tasks; many regulatory elements derive from
transposable elements, and some disease-associated repeats (like the CGG
expansion in \emph{FMR1} causing Fragile X syndrome, or the CAG
expansion in \emph{HTT} causing Huntington disease) are clinically
important. \textbf{Soft-masking} retains sequence information while
marking repetitive regions, allowing models to learn differential
representations for repeats and unique sequences.

\textbf{Multi-species pretraining} incorporates genomes from model
organisms and related species. Including mouse, zebrafish, and other
commonly used experimental organisms enables models to learn
evolutionary conservation patterns and may improve transfer between
species. For therapeutic development that relies on animal model data,
multi-species pretraining provides the foundation for cross-species
generalization.

\textbf{Data augmentation} artificially increases training diversity.
Reverse complementation exploits DNA strand symmetry, effectively
doubling training data without collecting new sequences. Random cropping
extracts variable-length windows, teaching position-invariant features.
Variant injection simulates genetic variation, building robustness to
population diversity. These augmentations are typically applied
on-the-fly during training rather than pre-computed, maintaining
flexibility in the training pipeline.

\section{Optimization and Scaling}\label{optimization-and-scaling}

Training a model to predict variant effects in genes like \emph{BRCA1}
requires not just the right objective but also stable optimization that
converges to useful representations. A model that diverges during
training or gets stuck in poor local minima will fail clinically
regardless of how well-designed its architecture may be. The
optimization details that seem merely technical have direct consequences
for whether the final model can reliably distinguish pathogenic from
benign variants.

\textbf{Learning rate warmup} gradually increases the learning rate from
near-zero over the first several thousand steps. This prevents early
training instability when the model has random initializations and large
gradient variance. After warmup, \textbf{cosine decay} schedules reduce
the learning rate following a cosine curve from peak to near-zero over
training, providing aggressive learning early when gradients are most
informative and gentle refinement late as the model approaches
convergence.

\textbf{Gradient clipping} prevents training instability from occasional
large gradients. Clipping by global norm scales all gradients
proportionally when the total norm exceeds a threshold (typically 1.0),
maintaining gradient direction while controlling magnitude. This is
standard practice for transformer models where exploding gradients can
occur, particularly with long sequences where attention matrices span
many positions.

\textbf{Mixed precision training} uses lower-precision arithmetic
(\texttt{float16} or \texttt{bfloat16} instead of \texttt{float32}) to
reduce memory consumption and accelerate computation on modern GPUs.
Loss scaling prevents numerical underflow in \texttt{float16}, and
careful handling of gradient updates ensures stability. Mixed precision
is now standard for large-scale pretraining, roughly doubling throughput
with minimal impact on model quality.

Pretraining scales with model size, sequence length, and dataset size in
predictable ways that have profound implications for what models can
learn. Larger models with more parameters capture more complex patterns
but require more data and compute to train. \emph{ESM-2}'s largest
variant has 15 billion parameters (\textbf{lin\_evolutionary\_2023?})
(roughly one parameter for every two amino acids in its training
corpus), enabling it to capture subtle evolutionary constraints
invisible to smaller models. Longer sequence contexts enable learning of
long-range dependencies but increase memory requirements quadratically
for standard attention. More diverse training data improves
generalization but requires proportionally more training time.

The relationships between scale and capability follow power laws that
predict optimal resource allocation
(\textbf{hoffmann\_training\_2022?}). For a fixed computational budget,
there exists an optimal balance between model size and training data:
models that are too large undertrain on available data, while models
that are too small cannot capture the complexity present in abundant
data. These \textbf{scaling laws}, first characterized systematically
for language models (\textbf{kaplan\_scaling\_2020?}), appear to hold
for genomic foundation models as well, though the precise exponents and
constants differ. Understanding these relationships guides decisions
about when to scale up versus when to improve data quality or model
architecture. Chapter~\ref{sec-fm-principles} examines these scaling
relationships in detail, formalizing the observations introduced here
into quantitative laws that define the foundation model paradigm.

Beyond smooth improvements in loss, scale produces qualitative changes
in model capabilities that were absent at smaller scales. Language
models exhibit \textbf{emergent behaviors} (in-context learning,
chain-of-thought reasoning, few-shot generalization) that appear only
above certain parameter thresholds. Whether genomic models exhibit
analogous emergent capabilities remains an active research question with
early evidence suggesting they do. \emph{ESM-2}, trained on evolutionary
sequence databases containing hundreds of millions of protein sequences
from UniRef (\textbf{suzek\_uniref\_2015?}), develops structural
understanding of proteins despite receiving no explicit structural
supervision: the three-dimensional contacts emerge from predicting amino
acid sequences alone. \emph{Evo}, trained autoregressively on genomes,
learns to generate sequences with realistic gene structure and
regulatory organization. These emergent properties cannot be predicted
by extrapolating from smaller models, making them both scientifically
interesting and practically difficult to anticipate.

\section{Monitoring and Debugging}\label{monitoring-and-debugging}

A two-week pretraining run that fails on day thirteen represents not
just wasted compute but delayed clinical deployment and missed
opportunities for patient benefit. Early detection of training issues is
essential for avoiding wasted computation and ensuring models achieve
the representations necessary for clinical utility.

\textbf{Training loss curves} should decrease smoothly in early stages,
eventually plateauing as the model approaches convergence. Sudden spikes
suggest numerical instability (often from learning rate issues or
gradient explosion), inappropriate optimization hyperparameters, or
corrupted data batches. Persistent plateaus may indicate insufficient
model capacity, inappropriate objectives, or learning rates that prevent
further improvement. Tracking loss on held-out validation data monitors
generalization: if training loss decreases while validation loss
increases, the model is \textbf{overfitting} to the training corpus.

\textbf{Gradient norms} indicate whether optimization is proceeding
normally. Very small gradients suggest the \textbf{vanishing gradient
problem}, preventing effective learning in early layers. Very large
gradients suggest instability that gradient clipping should catch.
Tracking per-layer gradient norms helps diagnose where problems
originate in deep networks.

\textbf{Probing tasks} provide functional sanity checks during
pretraining. Simple downstream evaluations (predicting known splice
sites, identifying transcription factor binding motifs, distinguishing
exons from introns) can be run periodically on intermediate checkpoints
to verify that learned representations capture biologically meaningful
patterns. If probing performance plateaus or degrades while pretraining
loss continues improving, the model may be learning patterns that do not
transfer to downstream tasks.

\section{Choosing the Right Strategy}\label{choosing-the-right-strategy}

A clinician asking ``will this \emph{BRCA1} variant cause disease?''
needs a model pretrained with objectives that capture protein function
and evolutionary constraint. A synthetic biologist asking ``can you
design me a promoter with 10-fold higher expression?'' needs generative
capabilities that MLM does not provide. Selecting a pretraining approach
involves matching computational investment to the clinical or research
questions the model must ultimately answer.

For most general-purpose DNA or protein models, MLM pretraining provides
a strong default. It learns bidirectional context, scales efficiently,
and transfers well to diverse downstream tasks. \emph{DNABERT} and
\emph{DNABERT-2} exemplify this approach for genomics, while \emph{ESM}
models demonstrate its effectiveness for proteins. Start with MLM unless
there is a specific reason to prefer alternatives.

Next-token prediction is preferred when generation is the primary goal.
If designing sequences from scratch (therapeutic proteins, synthetic
promoters, regulatory circuits), sampling from autoregressive models
produces coherent outputs respecting learned grammar. \emph{Evo} and
similar models demonstrate this for genomic sequence generation. The
autoregressive structure makes conditional generation straightforward,
enabling design applications that MLM does not naturally support.

Multi-task pretraining makes sense when functional labels are available
at scale and tasks are complementary. \emph{Enformer}'s success with
thousands of chromatin assays demonstrates the power of multi-task
learning when data supports it. The infrastructure requirements are
higher (handling heterogeneous data, balancing losses across tasks,
maintaining separate prediction heads), but the resulting
representations capture functional information that pure sequence-based
objectives miss.

Contrastive learning is valuable for cross-species applications or when
robustness to variation is critical. If transferring models trained on
model organisms to related species, or improving robustness to genetic
polymorphism across human populations, contrastive pretraining on
orthologous pairs or variant-augmented sequences provides targeted
benefits.

When deciding whether to pretrain from scratch or start from existing
models, starting from pretrained checkpoints is almost always preferable
if an appropriate model exists. \textbf{Fine-tuning} a \emph{DNABERT-2}
checkpoint on a new task is faster and more data-efficient than training
from scratch. Pretraining from scratch is necessary when using new
tokenization schemes (incompatible vocabularies prevent weight
transfer), targeting species without suitable existing models, or
experimenting with fundamentally different architectures where
pretrained weights cannot transfer.

\section{Pretraining in Practice: Case
Studies}\label{pretraining-in-practice-case-studies}

Examining how successful models were pretrained provides concrete
lessons and design patterns that inform new projects. Each case study
illustrates how architectural choices, data decisions, and optimization
strategies combine to produce models with distinct capabilities.

\emph{DNABERT} introduced MLM pretraining to genomics by adapting BERT's
architecture to DNA sequences with overlapping k-mer tokenization (Ji et
al. 2021). The model was pretrained on the human genome with 6-mer
tokens, masking 15\% of tokens at random. Standard BERT hyperparameters
proved effective: AdamW optimizer with warmup, dropout regularization,
and layer normalization. The key lessons include the importance of
tokenization choice (k-mers capture motif-level patterns better than
single nucleotides for regulatory prediction), the value of reverse
complement augmentation for strand symmetry, and the transferability of
representations across tasks never seen during pretraining.

\emph{HyenaDNA} demonstrated that efficient long-range architectures
enable pretraining on extremely long contexts (Nguyen et al. 2023). By
using Hyena layers with subquadratic complexity, \emph{HyenaDNA} scaled
to contexts spanning one million bases (compared to typical transformer
limits of a few thousand bases), far beyond standard transformers.
Pretraining used single-nucleotide next-token prediction with a
\textbf{curriculum} that progressively increased context length from
shorter windows to full million-base sequences. This curriculum learning
proved essential: training directly on long contexts without warmup led
to instability. The lessons include the feasibility of million-base
contexts with appropriate architectures, the benefits of curriculum
learning for context scaling, and the emergence of long-range regulatory
patterns when models have sufficient receptive field.

\emph{Enformer} pioneered multi-task chromatin prediction at scale (Å½.
Avsec et al. 2021). The model was pretrained jointly on over 5,000
assays from ENCODE, Roadmap Epigenomics, and related consortia, using a
hybrid convolutional-transformer architecture with 200 kilobase context
(spanning typical enhancer-promoter distances in mammalian genomes).
Task weighting was balanced to prevent any single assay from dominating.
Key insights include the power of large-scale multi-task learning for
capturing diverse regulatory signals, the effectiveness of combining
convolutions for local patterns with transformers for long-range
interactions, and the interpretability benefits of attention patterns
that reveal learned enhancer-promoter relationships.

\emph{ESM-2} represents the state of the art for protein language
models, scaling to 15 billion parameters trained on UniRef databases
containing sequences from hundreds of millions of protein families
(\textbf{lin\_evolutionary\_2023?}). Pretraining used standard MLM on
amino acid sequences at unprecedented scale. The lessons include the
continued benefit of scaling (larger models and more data improve even
at billions of parameters, with no plateau in sight), the value of
evolutionary diversity (pretraining on distinct protein families
captures constraints invisible in any single genome), and the emergence
of structural understanding from sequence alone (\emph{ESM-2}
representations encode three-dimensional contacts despite no explicit
structural supervision during pretraining).

\section{Open Questions}\label{open-questions}

Despite rapid progress, fundamental questions about genomic pretraining
remain open, and resolving them will determine whether the next
generation of models can achieve clinical-grade reliability.

Optimal objective combinations remain unclear: should we jointly train
with MLM and chromatin prediction, or train sequentially? How many
auxiliary tasks help before diminishing returns? Do contrastive and
generative objectives complement each other or interfere? These
questions have different answers for different downstream applications,
and systematic characterization is incomplete.

Incorporating biological priors versus learning from scratch presents a
design tension. Known motifs, pathway structure, and evolutionary
constraints could be encoded in model architecture or initialization.
Hand-engineered features risk encoding false assumptions, but pure
data-driven learning may rediscover basic biology inefficiently. Hybrid
approaches combining priors with learned representations remain
underexplored.

\textbf{Continual pretraining} as new data arrives is increasingly
relevant. As sequencing technologies improve and new assays emerge,
updating pretrained models without catastrophic forgetting of prior
knowledge presents challenges. Online learning and elastic weight
consolidation are potential solutions that remain largely untested in
genomics at scale.

The relationship between pretraining scale and downstream performance
follows predictable patterns that are still being characterized for
genomic models. Understanding these relationships more precisely would
guide resource allocation and set realistic expectations for what
different scales of pretraining can achieve. These scaling
considerations connect to the broader foundation model paradigm examined
in Chapter~\ref{sec-fm-principles}.

\section{From Sequence Statistics to Biological
Knowledge}\label{from-sequence-statistics-to-biological-knowledge}

The fundamental insight underlying self-supervised pretraining is that
patterns relevant to biological function are embedded in sequence
statistics themselves. A model that learns to predict masked nucleotides
must implicitly capture the evolutionary constraints, regulatory
grammar, and structural requirements that determine what sequences are
viable. A model that learns to generate plausible protein sequences must
internalize the constraints that distinguish functional proteins from
random polymers. These objectives extract biological knowledge from
sequence without requiring explicit functional labels, transforming
abundant unlabeled data into learned representations that improve data
efficiency for downstream applications.

The choice of pretraining objective shapes what models learn in ways
that propagate to clinical utility. Masked language modeling teaches
bidirectional sequence understanding, making it the natural choice for
variant interpretation and regulatory prediction where full flanking
context informs the prediction. Next-token prediction teaches generative
capabilities essential for therapeutic protein design and synthetic
sequence generation. Contrastive learning teaches invariance to
perturbations, building robustness that transfers across species and
populations. Aligning pretraining objectives with intended applications
improves transfer; misalignment creates representational gaps that
fine-tuning may struggle to bridge.

Self-supervised pretraining has become the default approach for building
genomic foundation models. The DNA language models in
Chapter~\ref{sec-dna-lm}, protein language models in
Chapter~\ref{sec-protein-lm}, and regulatory sequence models in
Chapter~\ref{sec-regulatory} each employ variants of these objectives
tailored to their sequence modalities and downstream applications. The
transfer learning methods examined next determine how effectively
pretrained representations can be adapted to specific clinical and
research tasks, completing the pipeline from raw sequence through
learned representation to deployed application.

\chapter{Transfer and Adaptation}\label{sec-transfer}

Transfer learning fails as often as it succeeds, and the failures are
silent. A protein language model trained on human sequences may
confidently score variants in mouse orthologs, producing predictions
that look reasonable but reflect human-specific evolutionary pressures
irrelevant to mouse biology. A foundation model pretrained on coding
sequences may extract features actively misleading for noncoding
regulatory elements. A classifier achieving 90\% accuracy on common
variants may collapse to chance performance on the rare variants that
matter most clinically. Nothing in the model's outputs signals these
failures. The predictions look the same whether transfer has succeeded
or catastrophically failed. This asymmetry between confident outputs and
actual reliability creates the central methodological challenge of
applying pretrained models: detecting when transfer works and when it
does not, before the predictions reach clinical applications where
failures have consequences.

The promise of transfer learning is substantial. Foundation models
trained on billions of evolutionary sequences learn representations that
capture protein structure, functional constraints, and sequence grammar
without task-specific supervision. When these representations are
applied to downstream tasks with limited labeled data, they can achieve
performance that would be impossible for models trained from scratch. A
variant effect predictor fine-tuned from ESM-2 can classify novel
missense mutations using patterns learned from the entire protein
universe, not just the handful of variants with clinical annotations.
This capacity to generalize from abundant unlabeled data to rare
clinical scenarios has driven much of the enthusiasm for genomic
foundation models.

The reality requires careful navigation. Every adaptation decision
involves tradeoffs: preserving pretrained knowledge versus enabling
task-specific learning, computational efficiency versus model
flexibility, rapid deployment versus careful validation. Full
fine-tuning updates all parameters, risking catastrophic forgetting of
pretrained knowledge. Feature extraction freezes all pretrained
parameters, limiting adaptation to task-specific patterns.
Parameter-efficient methods (adapters, LoRA, prompt tuning) navigate
between these extremes, but each makes different assumptions about where
adaptation should occur. This chapter examines when each strategy is
appropriate, provides diagnostic tools for detecting transfer failures,
and develops the methodological framework that separates practitioners
who apply foundation models effectively from those who treat them as
black boxes.

\section{Source and Target Domains}\label{source-and-target-domains}

When a cardiologist requests variant interpretation for a patient with
hypertrophic cardiomyopathy, the clinical need (classifying a specific
\emph{MYH7} variant) differs fundamentally from the data available
during model development (millions of protein sequences sampled across
all of evolution). Bridging this gap requires understanding what
properties of pretraining determine whether transfer will succeed. When
this bridge fails, patients receive confident predictions based on
patterns irrelevant to their clinical context.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-domain-alignment}{[}Essential{]} Schematic
illustrating domain shift in genomic transfer learning. Left panel
(Source Domain): Diverse genomic sequences during pretraining, with
learned representations capturing statistical regularities (local
motifs, composition, conservation). Right panel (Target Domain): Sparse
labeled examples for clinical task (e.g., pathogenic variants,
tissue-specific enhancers), highlighting distributional differences.
Center: Representation space showing well-transferred features (local
motifs, conservation patterns) connected by solid arrows
vs.~poorly-transferred features (long-range regulatory logic,
tissue-specific patterns) with dashed arrows indicating transfer
failure.}

\end{figure}%

\subsection{The Gap Between Pretraining and
Deployment}\label{the-gap-between-pretraining-and-deployment}

The \textbf{source domain} encompasses the data and objectives used
during pretraining. For DNA foundation models, source domains typically
include reference genomes, pan-genomic collections spanning population
diversity, or metagenomic assemblies sampling environmental sequence
space (Ji et al. 2021; Dalla-Torre et al. 2023). For protein models,
databases like UniRef provide billions of sequences representing the
diversity of evolutionary history (\textbf{suzek\_uniref\_2015?}).
Pretraining objectives (masked language modeling, next-token prediction,
contrastive learning) encourage models to capture statistical
regularities that help predict held-out tokens: local motifs,
compositional patterns, and the signatures distinguishing functional
from random sequence. These learned regularities become the
representations that might transfer to downstream tasks.

The \textbf{target domain} presents a fundamentally different challenge.
Rather than abundant unlabeled sequence, the target domain offers sparse
labeled examples of a specific clinical or biological question: a few
thousand enhancer sequences with luciferase measurements, several
hundred variants with expert pathogenicity classifications, chromatin
profiles across a handful of disease-relevant cell types. The target
distribution often looks nothing like pretraining data. Pathogenic
variants are rare outliers, not typical protein sequences.
Tissue-specific enhancers exhibit patterns that genome-wide pretraining
may never emphasize. Disease-associated regulatory elements may have
been systematically underrepresented in reference data (Kircher et al.
2014).

Four factors determine whether this distributional gap can be bridged.
Task relatedness measures whether target predictions depend on patterns
the model learned during pretraining; predicting transcription factor
binding after sequence pretraining succeeds because both involve local
motif recognition, while predicting three-dimensional chromatin contacts
may require spatial relationships the pretraining objective never
captured. Target data quantity constrains which adaptation strategies
avoid overfitting; with thousands of labeled examples, aggressive
fine-tuning can reshape representations, but with dozens, only the
lightest approaches remain viable. Model expressiveness influences
adaptation flexibility, as larger models encode richer internal
representations that can potentially serve more diverse downstream tasks
but also risk memorizing small target datasets. Distribution overlap
between source and target determines how much learned knowledge applies;
human regulatory elements share patterns with mouse elements (enabling
cross-species transfer) but diverge in species-specific enhancers
(limiting it).

\subsection{Recognizing Transfer
Outcomes}\label{recognizing-transfer-outcomes}

Not all transfer helps, and distinguishing outcomes requires explicit
validation. \textbf{Positive transfer} accelerates learning or improves
final performance beyond training from scratch. \textbf{Negative
transfer} occurs when pretraining actively hurts, either because learned
features conflict with task requirements or because pretrained
initialization creates optimization difficulties
(\textbf{wang\_characterizing\_2019?}). Neutral transfer describes
situations where pretraining neither helps nor hurts, wasting
computational resources on pretrained models without benefit. When a
pharmacogenomics team adapts a DNA language model for \emph{CYP2D6}
metabolizer status prediction, they must empirically verify which
outcome applies to their specific task rather than assuming transfer
will help because it helped elsewhere.

\section{Feature Extraction with Frozen
Backbones}\label{feature-extraction-with-frozen-backbones}

Clinical laboratories processing hundreds of variants daily cannot
afford to fine-tune models for each new gene or variant class. When a
novel gene enters diagnostic panels, classifiers must be deployed
rapidly using whatever labeled examples exist. A molecular diagnostics
team with 200 annotated \emph{RYR1} variants for malignant hyperthermia
risk prediction cannot fine-tune a 500-million parameter model; they
need an approach that works with minimal data while avoiding adaptation
risk entirely.

\textbf{Frozen feature extraction} addresses this constraint by treating
pretrained models as fixed representation engines. All backbone
parameters remain frozen; only a lightweight classifier trained on the
extracted representations learns from labeled data. The backbone never
changes, eliminating catastrophic forgetting entirely and enabling
deployment within hours rather than weeks. The fundamental tradeoff is
clear: frozen features sacrifice adaptation flexibility for speed,
safety, and efficiency.

\subsection{Linear Probing}\label{linear-probing}

Why does the simplest possible classifier often suffice? If pretrained
representations already encode task-relevant features in linearly
separable form, adding complexity provides no benefit and risks
overfitting. \textbf{Linear probing} tests this hypothesis by
introducing only \(d \times c\) parameters (where \(d\) is the embedding
dimension and \(c\) is the number of output classes). Pass input
sequences through the frozen model to obtain embeddings, typically from
the final layer or from a designated {[}CLS{]} token aggregating
sequence information, then train a linear classifier mapping embeddings
to task labels.

Ji et al.~demonstrated that \emph{DNABERT} embeddings paired with linear
probes achieve competitive chromatin accessibility prediction from a few
hundred positive and negative examples, matching CNN baselines requiring
far more labeled data (Ji et al. 2021). Dalla-Torre et al.~showed
similar results with \emph{Nucleotide Transformer}, where linear probes
on frozen embeddings approached fine-tuned performance for promoter
detection and splice site recognition (Dalla-Torre et al. 2023). These
successes reflect alignment between pretraining objectives (predicting
masked tokens from local context) and target tasks (distinguishing
sequences based on motif patterns the model already learned to
recognize).

\subsection{When Linear Probing Fails}\label{when-linear-probing-fails}

Linear probes fail when relevant information exists in embeddings but
requires nonlinear transformation to extract. Shallow multilayer
perceptrons (one or two hidden layers) extend linear probing by enabling
more complex decision boundaries while maintaining computational
efficiency. With several thousand labeled examples, shallow MLPs on
\emph{HyenaDNA} embeddings improve splice site prediction over linear
probes by capturing interactions between features that linear models
cannot represent (Nguyen et al. 2023). The additional expressiveness
helps when task-relevant patterns are distributed across embedding
dimensions in ways that linear combination cannot capture.

The more fundamental limitation cannot be addressed by classifier
complexity: performance caps at how well pretrained representations
already encode task-relevant features. If the pretraining objective
emphasized patterns irrelevant to the downstream task, or if required
features were actively suppressed during pretraining, frozen features
will underperform models trained from scratch regardless of classifier
sophistication. A model pretrained exclusively on coding sequence may
encode features misleading for noncoding regulatory prediction; no
linear probe can overcome representations that point in the wrong
direction.

\section{Parameter-Efficient
Fine-Tuning}\label{parameter-efficient-fine-tuning}

A research hospital developing tissue-specific expression predictors
faces an impossible choice. Frozen features from \emph{Enformer} provide
reasonable baselines, but full fine-tuning for each of fifty tissue
types would require months of GPU time and risk overfitting the
thousands of tissue-specific training examples. The team needs an
intermediate approach: enough flexibility to improve over frozen
features, enough constraint to prevent overfitting, enough efficiency to
iterate across dozens of tissues.

\textbf{Parameter-efficient fine-tuning (PEFT)} methods resolve this
tension by updating a small subset of parameters while keeping the
majority frozen, enabling task-specific adaptation without the
computational expense or overfitting risk of modifying all weights
(\textbf{houlsby\_parameter-efficient\_2019?}). The key insight is that
useful adaptation often requires changing only a small subspace of model
behavior, not rewriting everything the model learned during pretraining.

\subsection{Low-Rank Adaptation}\label{low-rank-adaptation}

\textbf{Low-Rank Adaptation (LoRA)} has emerged as the dominant PEFT
technique in genomic applications because it directly operationalizes
this insight. Rather than updating a large weight matrix \(W\) directly,
LoRA introduces two smaller matrices \(A\) and \(B\) whose product
approximates the desired weight change: \(W' = W + BA\)
(\textbf{hu\_lora\_2021?}). During fine-tuning, \(W\) remains frozen
while only \(A\) and \(B\) receive gradient updates. The rank of these
matrices (typically 8 to 64 for genomic models) controls adaptation
expressiveness: lower ranks introduce fewer parameters and stronger
implicit regularization; higher ranks enable more flexible task-specific
modification at greater overfitting risk.

The efficiency gains prove substantial. A transformer with 500 million
parameters might require updating only 2 to 5 million LoRA parameters
(representing the low-rank decompositions applied to attention weight
matrices), reducing memory requirements by an order of magnitude
compared with full fine-tuning. This efficiency enables training on
consumer GPUs for models that would otherwise require specialized
infrastructure, and enables systematic hyperparameter search that would
be prohibitive with full parameter updates. Zhou et al.~demonstrated
that LoRA adapters on \emph{Nucleotide Transformer} enable
tissue-specific chromatin accessibility prediction, where separate
low-rank matrices capture tissue-specific regulatory patterns while the
pretrained backbone encodes general sequence understanding
(\textbf{zhou\_dnabert-2\_2023?}).

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-lora-architecture}{[}High{]} Diagram showing LoRA
modification to a transformer weight matrix. Show: Original frozen
weight matrix W (large, e.g., 768Ã—768). LoRA decomposition: small
matrices A (768Ã—r) and B (rÃ—768) where r \textless\textless{} 768 (e.g.,
r=16). Combined transformation: W + BA. Annotate parameter counts:
``500M frozen + 2M trainable.'' Show how this applies to attention
weight matrices (W\^{}Q, W\^{}K, W\^{}V, W\^{}O).}

\end{figure}%

\subsection{Alternative PEFT
Approaches}\label{alternative-peft-approaches}

\textbf{Adapter layers} take a different architectural approach by
inserting small bottleneck modules between transformer layers rather
than modifying existing weights. Each adapter consists of a
down-projection reducing dimensionality (typically by a factor of 4 to
16), a nonlinear activation, and an up-projection restoring the original
dimension. Original transformer parameters remain frozen; only adapter
parameters update. Different adapters can be trained for different
tasks, enabling multi-task deployment from a single shared backbone
without interference between tasks
(\textbf{houlsby\_parameter-efficient\_2019?}).

\textbf{Prefix tuning} prepends learnable embeddings to the input
sequence, providing task-specific context that conditions model behavior
without modifying internal parameters
(\textbf{li\_prefix-tuning\_2021?}). While less common in genomics
(where natural prompt structure is absent), prefix tuning has found
limited application in models accepting both sequence and task
description inputs. Other variants include \textbf{BitFit} (tuning only
bias terms, introducing minimal parameters) and soft prompt tuning
(\textbf{zaken\_bitfit\_2022?}).

PEFT methods suit intermediate data regimes: 1,000 to 10,000 labeled
examples, where frozen features underperform but full fine-tuning risks
overfitting. The implicit regularization from constrained parameter
spaces often yields better final models than unconstrained optimization
on limited data.

\section{Full Fine-Tuning}\label{full-fine-tuning}

When Avsec et al.~sought to predict gene expression from sequence across
hundreds of cell types, they required a model capturing tissue-specific
regulatory logic unavailable from any generic pretrained representation
(Å½. Avsec et al. 2021). With millions of labeled examples spanning
thousands of genomic tracks, they could afford to update all model
parameters, reshaping internal representations entirely for their
specific predictive task. Constrained adaptation would have left
tissue-specific regulatory patterns unlearned.

\textbf{Full fine-tuning} offers maximum flexibility: every parameter
becomes tunable, enabling the model to learn whatever features the
target task requires regardless of pretraining emphasis. This
flexibility comes with risks proportional to its power.

\subsection{Making Full Fine-Tuning
Work}\label{making-full-fine-tuning-work}

Full fine-tuning updates all model parameters during adaptation but
requires careful attention to optimization dynamics. Learning rates must
be substantially lower than during pretraining (often 10 to 100 times
smaller) to avoid catastrophically disrupting learned representations in
early training steps (\textbf{howard\_universal\_2018?}).
\textbf{Gradual unfreezing}, where top layers update first and deeper
layers progressively join training, helps preserve low-level features
(local motifs, basic sequence statistics) while allowing high-level
task-specific adjustment. Regularization through weight decay, dropout,
and early stopping on validation data prevents overfitting to target
datasets.

The approach suits scenarios when labeled datasets are large (tens of
thousands of examples or more), when the target task diverges
substantially from pretraining such that constrained adaptation proves
insufficient, or when performance requirements justify computational
investment. \emph{Enformer} fine-tuning for new chromatin assays
requires updating most parameters to capture assay-specific signal
characteristics distinct from original training conditions. Expression
prediction across novel cell types benefits from full adaptation when
sufficient tissue-specific data exists.

\subsection{The Risks of Unconstrained
Adaptation}\label{the-risks-of-unconstrained-adaptation}

\textbf{Catastrophic forgetting} occurs when fine-tuning overwrites
general knowledge useful for related tasks or out-of-distribution
inputs; a model fine-tuned aggressively on lymphocyte data may lose
performance on epithelial cells it previously handled well
(\textbf{mccloskey\_catastrophic\_1989?}). Overfitting afflicts small
target datasets, where the model memorizes training examples rather than
learning generalizable patterns. Computational expense can be
prohibitive for models with billions of parameters. When negative
transfer occurs (pretraining initialization actually hurts
optimization), full fine-tuning may underperform training from scratch
despite the additional expense.

The conservative strategy is to start with simpler methods and escalate
only when they demonstrably fail. Establish frozen feature baselines
first. If frozen features outperform random initialization, try PEFT
methods before committing to full fine-tuning. Compare fine-tuned models
against properly-tuned from-scratch baselines on the same target data.
Monitor for overfitting through validation curves and early stopping.
The goal is achieving required performance with minimal adaptation
complexity.

\section{Probing Representations}\label{probing-representations}

A variant effect predictor built on \emph{ESM} embeddings achieves 85\%
accuracy in initial testing, but the team deploying it needs to
understand why. Does the model genuinely capture evolutionary constraint
relevant to pathogenicity, or has it learned spurious correlations that
will fail on out-of-distribution variants? Before committing
computational resources to adaptation, practitioners benefit from
understanding what the pretrained model actually learned.

\textbf{Probing classifiers} answer these diagnostic questions by
systematically interrogating representations before deployment. The
methodology converts the abstract question ``will transfer help?'' into
concrete evidence about representation content: train lightweight
classifiers to predict properties of interest from frozen embeddings,
then examine how accurately different properties can be decoded. If
chromatin accessibility can be predicted with 85\% accuracy from a
linear probe, the representations already encode accessibility-relevant
features and frozen feature extraction will likely succeed. If
transcription factor binding requires a deep nonlinear classifier to
reach the same accuracy, relevant information exists but is not linearly
separable, suggesting PEFT might help by reorganizing representations
for easier extraction. If a property cannot be predicted above chance
even with flexible classifiers, the representations may lack necessary
information entirely, and transfer to this task may fail regardless of
adaptation strategy.

\subsection{What Probing Reveals About Pretrained
Models}\label{what-probing-reveals-about-pretrained-models}

Systematic probing reveals what models learn during pretraining. Rives
et al.~demonstrated that \emph{ESM} protein embeddings encode secondary
structure so thoroughly that linear probes achieve near state-of-the-art
helix/sheet/coil prediction accuracy
(\textbf{rives\_biological\_2021?}). Contact prediction (which residues
are spatially close in folded structure) requires nonlinear probes but
still achieves strong performance, indicating that tertiary structure
information is present but requires transformation to extract. DNA
language models show similar patterns: local motif information is
recoverable by linear probes while long-range dependencies require
multi-layer networks (Ji et al. 2021).

\textbf{Layer-wise probing} reveals how information transforms through
the model. Early layers typically encode local compositional features
(k-mer frequencies, simple motifs, sequence statistics) while later
layers capture more abstract patterns (regulatory signatures,
evolutionary constraints, functional classifications)
(\textbf{jawahar\_what\_2019?}). For tasks depending on local features,
representations from early or middle layers may outperform final-layer
embeddings that have abstracted away relevant details. Layer selection
becomes another hyperparameter to optimize during adaptation.

\subsection{Probing Guides Adaptation
Strategy}\label{probing-guides-adaptation-strategy}

The diagnostic value extends beyond predicting which adaptation strategy
to use. When probing reveals that required features are absent from
pretrained representations, practitioners face a choice: commit to full
fine-tuning with sufficient target data (hoping the model can learn
missing features), switch to a different foundation model whose
pretraining objective better aligns with task requirements, or proceed
with from-scratch training that does not inherit inappropriate inductive
biases. The investment in probing before adaptation often saves months
of wasted effort on transfer that was doomed from the start.

\section{Choosing an Adaptation
Strategy}\label{choosing-an-adaptation-strategy}

A clinical genomics laboratory evaluating foundation models for
deployment must translate abstract transfer learning principles into
concrete decisions. The molecular diagnostics team has 2,000 validated
enhancer sequences: should they use frozen features, LoRA, or full
fine-tuning? Which approach maximizes performance given available data?
When should transfer be abandoned entirely in favor of task-specific
training? Making the wrong choice means either leaving performance on
the table or wasting months on approaches that will fail.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-adaptation-decision-tree}{[}Essential{]} Flowchart
guiding adaptation strategy selection. Decision nodes: (1) ``Labeled
data quantity?'' with branches \textless500, 500-5000,
\textgreater10000. (2) ``Task similarity to pretraining?'' with branches
high/moderate/low. (3) ``Computational constraints?'' with branches
limited/moderate/substantial. Terminal nodes recommend: Linear probing,
LoRA/Adapters, Full fine-tuning, or From-scratch training. Include
expected tradeoffs at each terminal (accuracy, compute, overfitting
risk).}

\end{figure}%

\subsection{Data Quantity Determines What Is
Possible}\label{data-quantity-determines-what-is-possible}

Data quantity provides the primary decision axis, though the thresholds
depend on model size and task complexity.

With fewer than 500 labeled examples, linear probing typically
represents the only viable approach. More complex adaptation overfits
catastrophically, and the limited signal cannot support meaningful
parameter updates. The molecular diagnostics team with 200 annotated
\emph{RYR1} variants for malignant hyperthermia risk prediction cannot
fine-tune a 500-million parameter model; they must rely on whether
pretrained representations already capture relevant evolutionary
constraint.

With 500 to 5,000 labeled examples, PEFT methods offer favorable
tradeoffs. LoRA adapters introduce enough flexibility to improve over
frozen features while maintaining implicit regularization that prevents
overfitting. Systematic hyperparameter search (rank selection, learning
rate, regularization strength) becomes feasible at this scale. A
research group with 2,000 validated enhancer sequences can meaningfully
compare linear probes, LoRA with various ranks, and perhaps cautious
full fine-tuning with aggressive regularization.

With more than 10,000 labeled examples, full fine-tuning becomes viable
and may be necessary when target tasks diverge substantially from
pretraining. The computational investment is justified when the adapted
model will be deployed at scale or when the task is central to a
research program. A consortium with 50,000 annotated regulatory elements
across cell types has sufficient data to reshape pretrained
representations entirely toward their specific predictive objectives.

\subsection{Task Similarity Determines What Is
Necessary}\label{task-similarity-determines-what-is-necessary}

Task similarity provides the second decision axis. When target tasks
closely resemble pretraining patterns (predicting transcription factor
binding after sequence pretraining), frozen features often suffice
because relevant patterns were already learned. When tasks diverge
moderately (tissue-specific expression after genome-wide pretraining),
PEFT enables selective adaptation. When tasks fundamentally differ from
pretraining (three-dimensional chromatin contact prediction from
sequence-only pretraining), full fine-tuning may be necessary to learn
features the pretraining objective never emphasized.

Computational constraints impose practical limits. Linear probing
requires minutes on CPUs. LoRA fine-tuning requires hours on single
GPUs. Full fine-tuning of large models requires days on multiple GPUs or
specialized hardware. When infrastructure is limited, simpler approaches
may be the only feasible option regardless of what would theoretically
be optimal.

Empirical validation supersedes heuristics. No rule reliably predicts
which strategy will succeed for any specific combination of model, task,
and data. The guidelines above indicate which strategies merit trying
first, but final selection requires comparing approaches on held-out
validation data matching the intended deployment distribution.

\section{Domain Shift and Cross-Context
Transfer}\label{domain-shift-and-cross-context-transfer}

The \emph{CYP2D6} gene encodes a cytochrome P450 enzyme metabolizing
approximately 25\% of clinically used drugs, including codeine (where
poor metabolizers experience no analgesic effect) and tamoxifen (where
poor metabolizers show reduced breast cancer treatment efficacy)
(\textbf{gaedigk\_pharmacogene\_2018?}). A foundation model trained on
human genomic data and adapted for \emph{CYP2D6} variant classification
might achieve 90\% accuracy on common variants well-represented in
training data. But the variants most important clinically are rare:
novel star alleles in underrepresented populations, structural variants
creating gene duplications or deletions, population-specific haplotypes
absent from reference databases. \textbf{Domain shift between training
and deployment distributions creates systematic blind spots precisely
where clinical stakes are highest.}

\subsection{Types of Domain Shift in
Genomics}\label{types-of-domain-shift-in-genomics}

Three types of \textbf{domain shift} commonly afflict genomic transfer
learning, each creating different patterns of failure.

\textbf{Cross-species transfer} applies models trained on one organism
to another, facing evolutionary divergence that introduces sequence
differences affecting regulatory syntax, motif grammar, and functional
constraints. Strategies for cross-species success include pretraining on
multi-species data to learn conservation patterns, fine-tuning with
species-specific adapters, and focusing on highly conserved features
(core promoter elements, splice site consensus sequences) that transfer
more readily than species-specific innovations (Kelley 2020b).
Human-to-mouse regulatory prediction works reasonably for conserved
housekeeping genes but fails for rodent-specific enhancers that never
existed in the human training distribution.

\textbf{Cross-tissue transfer} confronts tissue-specific regulatory
programs. Chromatin accessibility varies dramatically across tissues,
with thousands of tissue-specific enhancers and repressors controlling
cell-type identity. Models trained predominantly on one tissue may miss
regulatory logic specific to others. Effective approaches include shared
backbones with tissue-specific prediction heads (each head learns
tissue-specific transformations of shared representations),
tissue-conditional models accepting tissue identity as additional input,
and meta-learning frameworks training across many tissues to extract
general principles applicable to novel tissue types (Å½. Avsec et al.
2021).

\textbf{Cross-assay transfer} handles different molecular readouts of
related biology. ChIP-seq and ATAC-seq both measure chromatin state but
with different biochemistry, resolution, and signal characteristics.
Models trained on one assay may learn assay-specific artifacts rather
than underlying biology, producing predictions that fail when applied to
related assays measuring the same phenomenon differently. Multi-task
pretraining across assays helps models distinguish biological signal
from assay-specific noise.

\subsection{Detecting and Mitigating
Shift}\label{detecting-and-mitigating-shift}

Detecting domain shift before deployment prevents silent clinical
failures. Statistical divergence measures comparing source and target
distributions quantify distribution differences. Embedding
visualizations (t-SNE or UMAP projections) reveal whether target
examples fall within the source distribution or occupy unfamiliar
regions of representation space. Monitoring performance on canary
examples (known easy cases that should always be predicted correctly)
provides early warning of severe shift during deployment.

When domain shift is detected, mitigation strategies include
domain-adaptive fine-tuning, importance weighting of training examples,
and explicit modeling of shift through domain-adversarial training
(\textbf{ganin\_domain-adversarial\_2016?}). When shift is severe and
cannot be mitigated, acknowledging that transfer is inappropriate for
this context prevents overconfident deployment of models that will fail.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-domain-shift-detection}{[}High{]} Three-panel
visualization of domain shift detection. Panel A: UMAP/t-SNE projection
of embeddings showing training distribution (dense cluster) and test
examples at varying distances, with out-of-distribution examples clearly
separated. Panel B: Calibration curves comparing confidence vs.~accuracy
for in-distribution (well-calibrated diagonal) vs.~out-of-distribution
examples (overconfident, curve below diagonal). Panel C: Performance
degradation curve showing accuracy declining as distributional distance
from training data increases.}

\end{figure}%

\section{Few-Shot and Zero-Shot
Learning}\label{few-shot-and-zero-shot-learning}

A geneticist studying a newly characterized neurodevelopmental disorder
has identified 15 patients with variants in a previously unstudied gene.
Functional studies confirm pathogenicity for 8 variants; the remaining 7
are benign. Training a classifier from 15 examples using standard
supervised learning would be absurd, yet the clinical need for variant
interpretation is immediate. Parents are waiting for diagnoses.
\textbf{Few-shot and zero-shot learning} address these extreme data
scarcity scenarios that characterize many genomic applications, where
clinical urgency outpaces data availability.

\subsection{Few-Shot Learning with Minimal
Examples}\label{few-shot-learning-with-minimal-examples}

\textbf{Few-shot learning} operates with 10 to 100 examples per class, a
regime where standard adaptation overfits catastrophically.
\textbf{Meta-learning} approaches train models explicitly for rapid
adaptation by optimizing across many few-shot tasks during meta-training
(\textbf{finn\_model-agnostic\_2017?}). \textbf{Model-Agnostic
Meta-Learning (MAML)} finds parameter initializations that can be
fine-tuned effectively from minimal data; the initialization represents
a point in parameter space from which a few gradient steps reach good
task-specific solutions. At deployment, the meta-trained model adapts to
new tasks from a handful of labeled examples, having learned during
meta-training what features are generally useful and how to adapt
quickly.

\textbf{Prototypical networks} offer a simpler approach well-suited to
genomic classification (\textbf{snell\_prototypical\_2017?}). The model
learns to embed sequences such that examples from the same class cluster
together. At inference, class prototypes are computed as the mean
embedding of the few available examples per class, and novel sequences
are classified based on distance to prototypes. With 10 pathogenic and
10 benign variants as prototypes, novel variants are classified by which
prototype cluster they fall nearest in embedding space. The approach
requires no gradient updates at deployment, only forward passes to
compute embeddings and distances.

\subsection{Zero-Shot Transfer Without Task-Specific
Data}\label{zero-shot-transfer-without-task-specific-data}

\textbf{Zero-shot transfer} eliminates task-specific adaptation
entirely, making predictions using only the pretrained model's outputs.
For protein variant effect prediction, \emph{ESM} log-likelihood ratios
score variants by how much they reduce the model's probability of the
observed sequence (\textbf{meier\_language\_2021?}). Variants that
violate the model's learned expectations for natural proteins
(disrupting conserved residues, introducing destabilizing substitutions)
receive low likelihood ratios, flagging them as potentially deleterious.
This approach proves competitive with supervised methods for
\texttt{ClinVar} pathogenicity prediction because evolutionary
constraint (what masked language modeling learns to predict) correlates
with functional importance (what pathogenicity classification measures).

Zero-shot methods require strong alignment between pretraining
objectives and target tasks. When this alignment exists (evolutionary
constraint predicts pathogenicity), zero-shot approaches provide
immediate predictions without any labeled data. When alignment is weaker
(tissue-specific regulatory activity depends on factors beyond sequence
conservation), few-shot learning with even a handful of examples
typically outperforms zero-shot baselines. For most practical genomic
applications, some labeled data improves predictions; few-shot rather
than true zero-shot represents the realistic minimal-data regime.

\section{When Transfer Fails}\label{when-transfer-fails}

The research team had done everything right. They selected a
state-of-the-art DNA foundation model pretrained on diverse genomic
sequences. They applied LoRA adaptation using 5,000 carefully curated
training examples. Validation accuracy reached 88\%. But when deployed
on prospectively collected samples, performance collapsed to 62\%,
barely better than chance for a binary classification task. Transfer had
failed. For the patients whose variants were misclassified during those
weeks before the failure was detected, the consequences were real:
delayed diagnoses, inappropriate treatments, unnecessary anxiety.
Understanding why transfer fails prevents repeating the mistake.

\subsection{Diagnosing Negative
Transfer}\label{diagnosing-negative-transfer}

\textbf{Negative transfer} occurs when pretraining actively hurts
performance, producing adapted models worse than those trained from
scratch on target data alone. Pretraining on human coding sequences may
encode codon usage patterns and amino acid preferences that create false
expectations when applied to bacterial sequences with different GC
content and codon bias. Pretraining on healthy tissue samples may learn
features of normal cellular function that prove misleading for cancer
samples where regulatory programs are fundamentally altered. The
pretrained initialization, rather than providing a useful starting
point, creates an optimization landscape that leads to poor
task-specific solutions (\textbf{wang\_characterizing\_2019?}).

Diagnostic steps identify whether transfer helps or hurts. First,
compare adapted model performance against a from-scratch baseline
trained on identical target data with equivalent hyperparameter tuning;
if the pretrained model does not meaningfully outperform from-scratch
training, transfer provides no benefit and the computational overhead of
working with large pretrained models is wasted. Second, establish that
simpler adaptation strategies were tried before complex ones; if linear
probing fails, full fine-tuning rarely succeeds unless target data is
very large. Third, visualize embeddings from the pretrained model using
dimensionality reduction; if target task classes are not separated in
embedding space, pretrained representations may lack task-relevant
features. Fourth, ablate pretraining entirely by comparing against
randomly initialized models of identical architecture; this isolates
whether pretrained weights provide value or whether architectural
choices alone drive performance.

\subsection{Remediation When Transfer
Fails}\label{remediation-when-transfer-fails}

When diagnostics reveal fundamental mismatches, several remediation
strategies apply. Task-specific pretraining on data more closely aligned
with target requirements can bridge the gap; pretraining specifically on
regulatory regions for regulatory prediction tasks rather than
genome-wide pretraining may produce more suitable representations.
Hybrid architectures combining pretrained and randomly-initialized
components allow selective use of transfer where it helps while avoiding
its limitations elsewhere. Trying alternative foundation models whose
pretraining objectives better match task requirements may reveal that
the problem was model selection rather than transfer learning generally.
And accepting that transfer provides no benefit for this specific task,
proceeding with from-scratch training, remains a valid conclusion when
evidence supports it.

\section{Case Studies in Transfer
Learning}\label{case-studies-in-transfer-learning}

The contrast between successful and failed transfer illuminates the
principles developed throughout this chapter. Four cases span different
outcomes and reveal the conditions that distinguish success from
failure.

\subsection{DNABERT for Chromatin
Accessibility}\label{dnabert-for-chromatin-accessibility}

Ji et al.~pretrained \emph{DNABERT} using masked language modeling on
k-mer tokenized human genomic sequence (Ji et al. 2021). For ATAC-seq
peak classification, they applied linear probes to {[}CLS{]} token
embeddings without updating backbone parameters. The approach achieved
competitive performance with CNNs trained from scratch while requiring
approximately 10 times less labeled data.

Success reflected strong alignment between pretraining and target task:
both involve recognizing local sequence motifs (transcription factor
binding sites, nucleosome positioning signals) that determine chromatin
state. The pretrained representations already encoded the relevant
patterns; the linear probe simply learned to separate accessible from
inaccessible regions in this well-structured embedding space.

\subsection{ESM for Variant
Pathogenicity}\label{esm-for-variant-pathogenicity}

Rives et al.~pretrained \emph{ESM} on UniRef protein sequences using
masked language modeling (\textbf{rives\_biological\_2021?}). For
\texttt{ClinVar} pathogenicity classification, Meier et al.~showed that
zero-shot scoring based on variant effects on sequence likelihood proved
competitive with supervised methods (\textbf{meier\_language\_2021?}).
Adding a linear probe on \emph{ESM} embeddings improved performance
further, but the zero-shot baseline was already strong.

Success reflected implicit alignment: evolutionary constraint (what
masked language modeling captures) correlates with functional importance
(what pathogenicity measures). The pretraining objective, though never
explicitly targeting variant classification, learned representations
directly relevant to it.

\subsection{Enformer for Cross-Tissue
Expression}\label{enformer-for-cross-tissue-expression}

Avsec et al.~pretrained \emph{Enformer} on thousands of chromatin and
expression tracks spanning dozens of cell types (Å½. Avsec et al. 2021).
Fine-tuning with tissue-specific prediction heads for individual cell
types captured regulatory logic unavailable from frozen features,
outperforming both frozen \emph{Enformer} and from-scratch models
trained on individual tissues.

Success required both the large scale of pretraining (establishing
general sequence-to-function mappings) and extensive fine-tuning data
(enabling tissue-specific adaptation). With smaller fine-tuning
datasets, the approach would have overfit; without diverse pretraining,
the model would have lacked transferable regulatory knowledge.

\subsection{Cross-Species Regulatory
Prediction}\label{cross-species-regulatory-prediction}

Models pretrained on human regulatory sequences and applied to zebrafish
enhancer prediction often underperform zebrafish-specific models despite
the apparent relevance of regulatory sequence patterns (Kelley 2020b).
The failure reflects both sequence divergence (zebrafish regulatory
motifs differ from human) and lineage-specific regulatory innovations
(teleost-specific enhancers have no human homologs from which to
transfer).

\textbf{The boundary between success and failure corresponds to
evolutionary conservation: patterns shared across species transfer;
species-specific patterns do not.} Transfer succeeds for deeply
conserved elements (core promoters, splice sites) but fails for
lineage-specific regulatory logic.

\section{Validation and Common
Pitfalls}\label{validation-and-common-pitfalls}

A research group reports that their foundation model adaptation achieves
95\% accuracy for splice variant classification, far exceeding previous
methods. Six months later, clinical deployment reveals performance
closer to 70\%, with systematic failures on the rare variants that
matter most. The initial evaluation was not wrong, but it was
misleading. \textbf{Proper validation separates genuine transfer success
from evaluation artifacts that dissolve on contact with clinical
reality.}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-validation-pitfalls}{[}High{]} Multi-panel figure
illustrating common validation failures. Panel A: Venn diagram showing
pretraining data / test data overlap creating leakage (inflated
performance). Panel B: Timeline showing temporal leakage when training
on variants annotated after test set creation. Panel C: Bar chart
comparing foundation model against weak baseline (large gap, misleading)
vs.~properly-tuned baseline (small gap, realistic). Panel D: Stratified
performance showing aggregate accuracy (90\%) vs.~rare-variant accuracy
(50\%), revealing hidden failures.}

\end{figure}%

\subsection{Sources of Spurious
Success}\label{sources-of-spurious-success}

Test set overlap with pretraining data creates artificial performance
inflation. Foundation models trained on massive genomic corpora may
inadvertently include sequences later used for evaluation. When
benchmarking on variants that appeared in pretraining data (even if
unlabeled at pretraining time), the model has seen the sequences and may
have memorized relevant patterns. Verifying that test sequences were
excluded from pretraining requires careful provenance tracking, which
published benchmarks often lack (\textbf{lee\_pre-training\_2020?}).
Chromosome-based splits help but do not fully address the problem when
pretraining spans multiple species or includes population-level
diversity.

Temporal leakage uses future information unavailable at prediction time.
Evaluating a variant pathogenicity model on variants annotated after
training data was collected creates an unrealistically favorable
setting; the model may have seen related variants or learned from the
same evidence that later informed annotations. Temporal splits (training
on variants discovered before a cutoff, evaluating on variants
discovered afterward) provide more realistic assessment of prospective
performance (Landrum et al. 2018).

Inappropriate baselines inflate apparent transfer benefits. Comparing
adapted foundation models against weak or poorly-tuned from-scratch
baselines makes transfer look more valuable than warranted. Strong
baselines require equivalent hyperparameter tuning, appropriate
architectures for the task, and sufficient training on the same target
data. When properly-tuned CNNs match or exceed foundation model
performance for a task, the additional complexity of pretrained models
may not be justified.

\subsection{Evaluation Practices That Reveal True
Performance}\label{evaluation-practices-that-reveal-true-performance}

Single-metric reporting obscures important performance characteristics.
A model achieving 90\% overall accuracy may show 95\% accuracy on common
variants and 50\% accuracy on rare variants, with the clinically
important rare cases hidden by aggregate metrics. Stratified evaluation
by allele frequency, variant type, gene family, and other clinically
relevant categories reveals whether transfer benefits generalize or
concentrate in particular subgroups.

Confidence interval reporting and multiple training runs reveal
performance variability. A single training run may produce misleadingly
good or bad results through random initialization effects or data
sampling. Testing on multiple independent datasets rather than a single
benchmark reveals whether gains generalize beyond the specific
evaluation setting.

\section{Emerging Directions}\label{emerging-directions}

A rare disease geneticist encounters a variant in a gene with no
previous pathogenicity annotations. Current approaches require either
zero-shot scoring (which may lack task-specific calibration) or
collecting enough labeled examples to train an adapter (which takes
months). Emerging transfer learning methods may collapse this timeline,
enabling immediate adaptation from a handful of examples provided at
inference time rather than during training.

\subsection{In-Context Learning}\label{in-context-learning}

\textbf{In-context learning} enables predictions by conditioning on
examples provided in the input context without any parameter updates
(\textbf{brown\_language\_2020?}). Very large language models exhibit
this capability, performing tasks by observing demonstrations rather
than through explicit fine-tuning. Early evidence suggests genomic
foundation models at sufficient scale may exhibit similar behavior,
classifying variants based on a few pathogenic and benign examples
included in the input prompt. This could transform deployment: rather
than training adapters or fine-tuning parameters, practitioners would
provide examples of desired behavior at inference time.

\subsection{Test-Time Adaptation}\label{test-time-adaptation}

\textbf{Test-time adaptation} updates models during inference based on
characteristics of test examples rather than freezing parameters after
training (\textbf{wang\_tent\_2021?}). For genomic applications facing
distribution shift between development and deployment populations,
test-time adaptation could adjust model behavior to match
deployment-specific characteristics without requiring labeled examples
from the deployment distribution. A model developed on European-ancestry
data could adapt its uncertainty calibration when encountering
African-ancestry variants that differ from training distributions.

\subsection{Federated Learning Across
Institutions}\label{federated-learning-across-institutions}

\textbf{Federated transfer learning} enables collaborative model
development across institutions without sharing raw genomic data
(\textbf{rieke\_future\_2020?}). Institutions train local models on
private patient data and share only aggregated parameter updates,
enabling foundation models to learn from far more diverse data than any
single institution can access while preserving patient privacy. This
approach could help address the population bias in current genomic
datasets by enabling contributions from institutions serving
underrepresented populations.

\subsection{Toward Theoretical
Foundations}\label{toward-theoretical-foundations}

Theoretical foundations for predicting transfer success based on
measurable properties of source and target domains would reduce
trial-and-error (\textbf{ben-david\_theory\_2010?}). Currently
practitioners must empirically test whether transfer helps; theoretical
guidance specifying when transfer will succeed based on domain
divergence measures, task similarity metrics, or representation analysis
could focus effort on promising applications and avoid wasted investment
in doomed transfer attempts.

\section{Amplification and Its Risks}\label{amplification-and-its-risks}

Transfer learning amplifies the value of pretrained models by connecting
learned representations to specific applications. A foundation model
pretrained on billions of sequences encodes patterns that would require
orders of magnitude more labeled data to learn from scratch. Effective
transfer realizes this investment; ineffective transfer inherits hidden
limitations without the promised benefits.

The risks are concrete. Domain shift between pretraining and deployment
contexts causes silent failures: models trained on research cohorts may
miscalibrate on clinical populations, models trained on one species may
fail unpredictably on another, models trained on one assay technology
may not generalize to its successor. These failures produce confident
predictions that are systematically wrong, often in ways that correlate
with clinically relevant subgroups. Detection through distribution
divergence measures and embedding visualization can identify shift
before deployment, but mitigation requires either domain-adaptive
fine-tuning or acceptance that some shifts cannot be bridged.

Validating transfer claims requires adversarial rigor. Test for
contamination between pretraining and evaluation data through
sequence-level deduplication. Implement temporal splits that respect
real-world prediction scenarios. Compare against properly-tuned
baselines trained from scratch with equivalent effort. Stratify
performance by ancestry, variant type, and other clinically meaningful
categories. The goal is establishing whether transfer provides genuine
benefit under realistic deployment conditions, not demonstrating
impressive numbers on favorable benchmarks. The foundation models
examined in subsequent chapters assume that transfer succeeds; the
methods here determine whether that assumption holds for specific
applications.

\part{Part III: Foundation Model Families}

Each architecture embodies a different set of assumptions about
biological sequence. Convolutional models assume that local motifs and
their short-range combinations are the primary carriers of regulatory
information; they learn to recognize transcription factor binding sites,
splice signals, and chromatin accessibility patterns from the sequence
grammar immediately surrounding each position. Protein language models
treat amino acid sequences as structured compositions whose meaning
emerges from evolutionary context; they learn what substitutions are
tolerated by observing which sequences survived natural selection. DNA
language models extend this paradigm to nucleotides, learning regulatory
grammar through self-supervised objectives that predict masked or next
tokens. Hybrid architectures attempt to reconcile local and global
perspectives, using convolutions to extract features efficiently while
deploying attention to model interactions spanning tens or hundreds of
kilobases. Understanding these assumptions clarifies what each model
family can capture and where each will fail.

This part surveys the major foundation model families in genomic deep
learning. Chapter~\ref{sec-fm-principles} establishes what defines a
foundation model and develops a practical taxonomy for navigating the
rapidly expanding ecosystem. Chapter~\ref{sec-dna-lm} examines DNA
language models, including DNABERT, Nucleotide Transformer, and
HyenaDNA, that apply self-supervised pretraining to genomic sequence,
learning representations that transfer across diverse downstream tasks.
Chapter~\ref{sec-protein-lm} turns to protein language models, where the
foundation model paradigm achieved its earliest and most dramatic
successes; ESM, ProtTrans, and their descendants emerged alongside
AlphaFold2 in 2020, collectively demonstrating that deep learning could
capture protein structure and function from sequence alone. AlphaFold2
revolutionized structure prediction through its Evoformer architecture,
and AlphaMissense subsequently adapted that architecture for
proteome-wide variant effect prediction.

Chapter~\ref{sec-regulatory} examines hybrid architectures, including
Enformer, Borzoi, and AlphaGenome, that combine convolutional processing
with transformer blocks to achieve context windows spanning hundreds of
kilobases, enabling direct prediction of gene expression from sequence.
Chapter~\ref{sec-vep-fm} synthesizes these approaches in the context of
variant effect prediction, showing how foundation model representations
translate into pathogenicity scores across variant types and genomic
contexts. By the end of this part, readers will understand not only how
each model family works but when to deploy each and what limitations to
anticipate.

\chapter{Foundation Model Paradigm}\label{sec-fm-principles}

The deep learning era in genomics began with fragmentation. One
convolutional network predicted transcription factor binding; another
predicted chromatin accessibility; a third classified splice sites. Each
model required its own training data, its own hyperparameter tuning, its
own validation strategy. A laboratory studying gene regulation might
train a dozen specialized models, none of which could inform the others.
Knowledge learned for predicting enhancer activity provided no benefit
for predicting splicing outcomes, even though both tasks depend on
recognizing sequence patterns shaped by the same evolutionary pressures.
The field accumulated tools without accumulating shared knowledge.

Foundation models promise a different approach: train once on vast
genomic data, then adapt to many downstream tasks. A single model
pretrained on billions of nucleotides might provide representations
useful for regulatory prediction, variant interpretation, sequence
design, and cross-species analysis simultaneously. Rather than curating
labeled datasets for each new question, researchers could fine-tune
existing models on modest task-specific data, leveraging knowledge the
model acquired during pretraining. The efficiency gains would be
substantial; the conceptual shift would be larger still. Where
specialized models treat each genomic question as independent,
foundation models assume that shared patterns underlie diverse
biological phenomena and that representations capturing those patterns
should transfer.

This paradigm, which transformed natural language processing and protein
structure prediction, carries both promise and peril for genomics.
Pretraining at scale requires computational resources beyond most
academic budgets. Adaptation to specific tasks demands expertise in
transfer learning techniques that remain poorly understood. Predictions
from general-purpose models may lack the precision of specialized
alternatives trained directly on task-specific data. The decision to
use, adapt, or build foundation models involves tradeoffs that depend on
available resources, target applications, and acceptable uncertainty.
This chapter provides the conceptual framework for navigating these
decisions: what foundation models are, how they scale, what emerges at
scale, and when they represent the right tool for a given genomic
question.

\section{From Task-Specific Models to Foundation
Models}\label{from-task-specific-models-to-foundation-models}

The history of computational genomics reveals a consistent pattern:
models become more general while maintaining or improving task-specific
performance. Hand-crafted scores such as CADD and SIFT established that
integration of diverse genomic annotations could improve variant
pathogenicity prediction (Rentzsch et al. 2019; Schubach et al. 2024).
These approaches relied on expert feature engineering, combining
conservation scores, functional annotations, and population frequency
data through ensemble methods or logistic regression.

Task-specific deep learning models demonstrated that neural networks
could learn relevant features directly from sequence. DeepSEA predicted
chromatin accessibility and transcription factor binding from 1 kb
sequences using convolutional architectures (J. Zhou and Troyanskaya
2015). ExPecto extended this approach to gene expression prediction by
modeling regulatory elements across multiple cell types (J. Zhou et al.
2018). Sei organized regulatory predictions into interpretable sequence
classes through unsupervised clustering (K. M. Chen et al. 2022).
SpliceAI achieved near-perfect splice site prediction through dilated
convolutions over 10 kb contexts (Chapter~\ref{sec-cnn}). Enformer
scaled sequence-to-function modeling to 200 kb windows and thousands of
chromatin tracks through transformer architectures (Å½. Avsec et al.
2021).

These models succeeded within their specific domains but remained
difficult to repurpose. Training a DeepSEA model required chromatin
accessibility data. Using SpliceAI for regulatory prediction would
require complete retraining on different labels. Each application domain
needed its own model, trained from scratch on task-specific data.
\textbf{The fundamental limitation was not model capacity but training
paradigm: supervised learning on narrow tasks produced narrow
capabilities.}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-paradigm-shift}{[}Essential{]} Two-panel comparison
showing the paradigm shift. Panel A (Task-Specific Era): Multiple
independent models (DeepSEA, SpliceAI, ExPecto) shown as separate
pipelines, each with its own training data, architecture,
hyperparameters; arrows showing no knowledge transfer between models;
Label: ``Train separately, no shared learning.'' Panel B (Foundation
Model Era): Single large pretrained backbone at center, multiple
lightweight adaptation heads branching out, downstream tasks
(regulatory, splicing, expression, variant effect) as leaf nodes, shared
representation layer highlighted; Label: ``Pretrain once, adapt many.''}

\end{figure}%

Sequence language models introduced the self-supervised pretraining
paradigm (Chapter~\ref{sec-pretraining}) to genomics. DNABERT applied
masked language modeling to DNA sequences, demonstrating that general
representations could be learned without task-specific labels (Ji et al.
2021). ESM and ESM-2 showed that protein language models pretrained on
sequence alone could transfer effectively to structure prediction,
variant effect prediction, and protein design (Rives et al. 2021; Lin et
al. 2022). The Nucleotide Transformer family scaled DNA language
modeling to cross-species training corpora (Dalla-Torre et al. 2023).
HyenaDNA used implicit convolutions to reach million-token contexts at
single-nucleotide resolution (Nguyen et al. 2023).

The transition from task-specific to foundation models changes the
relationship between model developers and users. Task-specific models
deliver predictions as their primary product. Foundation models deliver
representations that users adapt to their own tasks through the transfer
learning techniques examined in Chapter~\ref{sec-transfer}. This
distinction affects everything from model architecture design to
evaluation strategies to deployment infrastructure.

\section{Defining Genomic Foundation
Models}\label{defining-genomic-foundation-models}

The term ``foundation model'' appears frequently in genomics literature,
sometimes applied to any large neural network trained on biological
sequences. Establishing clear criteria helps separate true genomic
foundation models from ordinary deep learning approaches that happen to
operate on DNA or protein sequences.

\subsection{Essential Properties}\label{essential-properties}

A genomic foundation model satisfies several key properties that
distinguish it from task-specific architectures.

\textbf{Large-scale pretraining with minimal supervision.} Foundation
models train on entire genomes, pan-genomic sequence collections, or
large assay compendia. The pretraining objectives include masked
language modeling, next-token prediction, denoising, or multi-task
sequence-to-function prediction. Critically, these objectives do not
require dense task-specific labels for every training example. A model
that requires annotated enhancers or curated pathogenic variants for
every training instance does not qualify as a foundation model under
this criterion.

\textbf{General-purpose representations.} Foundation models produce
embeddings that prove useful across many downstream tasks. These
representations can be extracted through forward passes and reused with
simple linear probes or lightweight adapter modules rather than
requiring full model retraining. The representations should encode
biological information at multiple scales, from local sequence motifs to
long-range regulatory grammar.

\textbf{Broad transfer capability.} Foundation models support diverse
downstream applications without architectural modifications or complete
retraining. Transfer occurs across multiple dimensions: different assays
(from chromatin accessibility to gene expression), different tissues and
cell types, different species, and different variant types (from SNVs to
structural variants). Evidence of broad transfer requires evaluation
across multiple benchmarks rather than demonstration of performance on a
single task.

\textbf{Scale along at least one dimension.} Foundation models operate
at a scale that would be impractical for task-specific training. Some
scale context length, as HyenaDNA scales to million-token windows at
single-nucleotide resolution. Others scale parameter count, as the ESM
and Nucleotide Transformer families reach billions of parameters. Still
others scale data diversity through pan-genomic pretraining across
hundreds of species or integration of many assays and cell types. The
scaling dimension chosen reflects the model's intended applications and
architectural constraints.

\textbf{Standardized interfaces.} Foundation models typically expose
consistent APIs for common operations. These include embedding
extraction for sequences or variants, sequence probability scoring, and
mask-based in-silico mutagenesis for variant effect prediction. Models
distributed through repositories such as Hugging Face often include
documented recipes for downstream fine-tuning and example notebooks
demonstrating common use cases.

\subsection{What Doesn't Count}\label{what-doesnt-count}

Many excellent genomic models fail one or more of these criteria and
should not be classified as foundation models. Early versions of DeepSEA
trained specifically on chromatin accessibility data from a limited set
of cell types lack the generality and standardized interface of
foundation models, though later iterations that integrate many assays
begin to approach foundation model territory (J. Zhou and Troyanskaya
2015). SpliceAI predicts splicing outcomes exceptionally well but was
designed for that specific task and provides neither general-purpose
embeddings nor easy transfer to other genomic prediction problems
(Jaganathan et al. 2019). Even a very large Enformer-like model trained
solely on human chromatin tracks remains bound to its specific
prediction interface despite its scale and sophistication (Å½. Avsec et
al. 2021).

The distinction matters for several reasons. It affects evaluation
strategy, since foundation models must be assessed across families of
tasks rather than single benchmarks. It affects integration into
existing pipelines, since foundation models serve as feature extractors
while task-specific models typically provide end-to-end predictions. It
affects how we think about model development, since foundation model
training requires different infrastructure and data curation than
task-specific supervised learning.

\subsection{The Metaphor Itself}\label{the-metaphor-itself}

The term ``foundation'' carries implications worth examining.
Architectural foundations are static, load-bearing, and invisible once
construction proceeds. Genomic foundation models share only the
load-bearing property: they support downstream applications that would
otherwise require independent construction. Yet unlike architectural
foundations, these models remain visible and modifiable throughout their
use. Fine-tuning adjusts the foundation itself rather than building atop
an immutable base. The metaphor also implies that foundations precede
and enable all subsequent work, but genomic foundation models often
coexist with task-specific alternatives that outperform them on narrow
benchmarks.

A more accurate metaphor might be ``foundation'' in the educational
sense: a broad base of knowledge that enables specialized learning but
continues to develop alongside it. The pretraining phase establishes
general competence; adaptation refines that competence for specific
purposes without abandoning the original learning. This framing better
captures the dynamic relationship between pretrained representations and
downstream tasks, though the architectural metaphor has become standard
terminology.

\section{Scaling Laws and Compute-Optimal
Training}\label{scaling-laws-and-compute-optimal-training}

The success of foundation models in natural language processing rests
partly on empirical scaling laws: predictable relationships between
model size, training data, computational budget, and performance.
Understanding these relationships guides resource allocation and model
development decisions.

\subsection{The Scaling Law Framework}\label{the-scaling-law-framework}

Scaling laws describe how model performance (typically measured as loss
on held-out data) varies with three key quantities: the number of model
parameters (N), the amount of training data (D), and the total compute
budget (C). The Chinchilla scaling analysis demonstrated that for a
fixed compute budget, there exists an optimal balance between model size
and training data (\textbf{hoffmann\_chinchilla\_2022?}). Training a
model that is too large on too little data, or too small on too much
data, yields worse performance than compute-optimal allocation.

The practical implications are significant. Many early large language
models were undertrained relative to their parameter count. GPT-3's 175
billion parameters were trained on roughly 300 billion tokens, while
Chinchilla-optimal training would suggest either fewer parameters or
more data. The Chinchilla model itself matched GPT-3 performance with
only 70 billion parameters trained on 1.4 trillion tokens.

For genomic foundation models, scaling relationships are less well
characterized but increasingly important. Several key questions arise:
Do genomic sequences exhibit the same scaling behavior as natural
language? How does the finite size of reference genomes constrain data
scaling? What role does sequence diversity (cross-species, population
variation) play in data scaling? Can synthetic data augmentation extend
effective training data beyond natural sequences?

\subsection{Empirical Scaling in Genomic
Models}\label{empirical-scaling-in-genomic-models}

Several genomic foundation model families have reported scaling
experiments, though systematic scaling laws comparable to NLP remain
elusive.

The Nucleotide Transformer family provides perhaps the clearest genomic
scaling data (Dalla-Torre et al. 2023). Performance on downstream
benchmarks improves consistently with parameter count across models from
50 million to 2.5 billion parameters. The largest models (trained on
multi-species data) outperform smaller models trained on human sequences
alone, suggesting that cross-species data provides effective scaling
even when human-specific performance is the target. Training compute
scaled from approximately 10\^{}19 to 10\^{}21 FLOPs across the model
family.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-scaling-laws}{[}Essential{]} Three-panel figure
illustrating scaling relationships. Panel A (Loss vs.~Parameters):
Log-log plot showing validation loss decreasing with parameter count;
data points from Nucleotide Transformer family (50M â†’ 2.5B); ESM-2
scaling curve overlaid for comparison; power law fit line with exponent
annotation. Panel B (Compute-Optimal Frontier): Iso-performance contours
on parameter Ã— data plane; Chinchilla optimal line showing balanced
allocation; undertrained region shaded; genomic data ceiling annotated
(\textasciitilde10\^{}12 nucleotides). Panel C (Emergent Capabilities):
Capability (y-axis) vs.~scale (x-axis, log); multiple curves showing
different capabilities emerging at different scales; threshold markers
where capabilities ``switch on.''}

\end{figure}%

ESM-2 demonstrated similar scaling for protein language models, with
performance on structure prediction and variant effect tasks improving
smoothly from 8 million to 15 billion parameters (Lin et al. 2022). The
largest ESM-2 models approach the structure prediction accuracy of
AlphaFold2 using only single-sequence input, a capability entirely
absent in smaller models. This represents a qualitative capability
threshold crossed through scale.

HyenaDNA focused on context length scaling rather than parameter
scaling, demonstrating that million-token contexts at single-nucleotide
resolution could be achieved through sub-quadratic architectures (Nguyen
et al. 2023). The relationship between context length and downstream
performance is task-dependent: some tasks (local motif recognition) show
saturation at kilobase scales, while others (long-range regulatory
prediction) continue improving with longer contexts.

\subsection{Compute-Optimal Decisions for
Genomics}\label{compute-optimal-decisions-for-genomics}

The scaling law framework has direct implications for model development
decisions in genomics.

\textbf{Data-constrained regimes.} Unlike natural language, where text
data is effectively unlimited, genomic sequence data faces hard
constraints. Reference genomes for well-studied species total perhaps
10\^{}11 to 10\^{}12 nucleotides. Population-level variant data can
expand this somewhat, but the effective diversity may be lower than raw
counts suggest. In data-constrained regimes, smaller models trained to
convergence may outperform larger models that are undertrained.

\textbf{Compute-constrained regimes.} Academic groups typically face
stricter compute constraints than industry labs. Given fixed compute
budgets, the Chinchilla framework suggests allocating resources toward
longer training of smaller models rather than abbreviated training of
larger models. A 500 million parameter model trained for 10 epochs on
diverse genomic data may outperform a 5 billion parameter model trained
for 1 epoch on the same data.

\textbf{Task-specific considerations.} Not all downstream tasks benefit
equally from foundation model scale. Tasks that depend primarily on
local sequence patterns (transcription factor motif recognition, splice
site identification) may show diminishing returns beyond modest model
sizes. Tasks requiring integration of long-range context or rare variant
interpretation may continue benefiting from larger models and longer
contexts.

\textbf{Data curation versus model scaling.} For many practical
applications, investment in data curation and quality may yield better
returns than model scaling. A foundation model trained on carefully
curated, diverse, high-quality sequences may outperform a larger model
trained on noisier data. The relative value of these investments is
task-dependent and not well characterized by current scaling law
frameworks.

\subsection{Emergent Capabilities}\label{emergent-capabilities}

Perhaps the most intriguing aspect of foundation model scaling is the
emergence of qualitatively new capabilities at sufficient scale.
Emergence refers to abilities that are absent or negligible in smaller
models but appear discontinuously as models grow.

In large language models, emergent capabilities include multi-step
reasoning, code generation, and in-context learning. These capabilities
appear at model scales of roughly 10\^{}10 parameters and above, with no
clear precursor in smaller models.

Genomic foundation models exhibit analogous emergence, though the
capability thresholds are less well characterized.

\textbf{Structural understanding from sequence.} ESM-2 at sufficient
scale produces contact maps and secondary structure predictions from
single sequences with accuracy approaching multiple sequence alignment
methods (Lin et al. 2022). Smaller ESM models show no meaningful
structural understanding. This capability emerges at approximately 650
million parameters and continues improving with scale.

\textbf{Cross-species transfer.} Larger Nucleotide Transformer models
transfer more effectively to novel species not seen during training
(Dalla-Torre et al. 2023). The ability to generalize beyond training
species appears to require sufficient model capacity to learn abstract
regulatory principles rather than memorizing species-specific patterns.

\textbf{Zero-shot variant effect prediction.} Foundation models at
sufficient scale can predict variant effects without task-specific
fine-tuning, using only the difference in likelihood between reference
and alternative sequences. This zero-shot capability requires models
large enough to capture subtle sequence dependencies.

\textbf{In-context learning.} The most recent genomic foundation models
show preliminary evidence of in-context learning: the ability to adapt
to new tasks based on examples provided in the input context without
parameter updates. This capability, central to the utility of large
language models, remains nascent in genomic models but may emerge with
further scaling.

The practical implication is that capability thresholds exist: models
below certain scales may be fundamentally incapable of certain tasks
regardless of fine-tuning. Identifying these thresholds helps guide
model selection and prevents wasted effort fine-tuning models that lack
necessary capacity.

\section{A Taxonomy of Genomic Foundation
Models}\label{a-taxonomy-of-genomic-foundation-models}

The landscape of genomic foundation models can be organized into four
broad families. Each family exhibits distinct characteristics,
strengths, limitations, and typical application domains.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER}}
\end{minipage}%

\caption{\label{fig-model-taxonomy}{[}Essential{]} 2Ã—2 grid with
families as quadrants. Quadrant 1 (DNA Language Models, Blue): DNABERT,
Nucleotide Transformer, HyenaDNA, Evo 2. Quadrant 2 (Protein Language
Models, Purple): ESM-2, ProtTrans, ESMFold. Quadrant 3
(Sequence-to-Function Models, Green): Enformer, Borzoi, AlphaGenome.
Quadrant 4 (Multi-Omic Models, Orange): Omni-DNA, cross-modal
architectures. For each: representative models, input/output types,
pretraining approach, strengths, limitations. Connecting lines showing
where models can feed into each other.}

\end{figure}%

\subsection{DNA Language Models}\label{dna-language-models}

DNA language models learn sequence representations from raw nucleotide
strings through self-supervised objectives. These models treat DNA as a
language to be modeled without explicit functional labels, discovering
patterns through statistical regularities in genomic sequence.

\textbf{Core characteristics.} DNA language models typically use masked
language modeling or autoregressive next-token prediction as their
pretraining objective. They train on reference genomes or pan-genomic
sequence collections spanning multiple species. The resulting models
produce per-position or pooled sequence embeddings that can be extracted
and used for downstream tasks. Critically, these embeddings are not tied
to specific assays or cell types, making them applicable to any task
that benefits from general sequence context.

\textbf{Representative models.} DNABERT and DNABERT-2 apply BERT-style
masked language modeling to DNA sequences, using overlapping k-mers as
tokens (Ji et al. 2021; Z. Zhou et al. 2024). The Nucleotide Transformer
family scales this approach to larger parameter counts and cross-species
training (Dalla-Torre et al. 2023). HyenaDNA achieves subquadratic
complexity through implicit convolutions, enabling context lengths up to
one million nucleotides (Nguyen et al. 2023). Caduceus incorporates
bidirectional processing and reverse-complement equivariance as
architectural inductive biases. Evo-2 combines long-range attention with
biological tokenization strategies. GROVER integrates learned BPE-style
tokenization with training on regulatory tracks in addition to raw
sequence (Sanabria et al. 2024).

\textbf{Strengths and limitations.} DNA language models provide truly
general representations not bound to specific assays, cell types, or
experimental conditions. They can process novel sequences not present in
reference genomes. Their self-supervised training requires only genome
sequences, making them scalable. Without explicit functional grounding,
however, they may not capture subtle regulatory patterns that manifest
only under specific cellular conditions. Performance on tasks requiring
fine-grained functional discrimination may lag models trained with
functional supervision.

\textbf{Typical applications.} Sequence classification (promoters,
enhancers, transposons), motif discovery, variant effect prediction
through embedding perturbation, sequence generation for synthetic
biology, and transfer learning to new species with limited labeled data.

\subsection{Sequence-to-Function Foundation
Models}\label{sequence-to-function-foundation-models}

Sequence-to-function models predict molecular readouts directly from
sequence through supervised or semi-supervised training on assay
compendia. These models blur into foundation model territory when their
output space is sufficiently broad and their internal representations
prove useful for tasks beyond the original assay set.

\textbf{Core characteristics.} These models map DNA sequences to
high-dimensional vectors of molecular measurements, including chromatin
accessibility, histone modifications, transcription factor binding, and
gene expression levels. Training uses large collections of functional
genomics assays spanning many cell types. The models learn regulatory
grammar through supervised prediction of molecular phenotypes.

\textbf{Representative models.} Enformer predicts thousands of chromatin
and expression tracks from 200 kb sequence windows through transformer
attention (Å½. Avsec et al. 2021). Borzoi extends this with refined
architectures and expanded coverage. Sei organizes predictions into
interpretable sequence classes through unsupervised clustering (K. M.
Chen et al. 2022). Earlier models including DeepSEA and Basset
established the paradigm at smaller scales.

\textbf{Strengths and limitations.} Explicit functional supervision
provides mechanistic grounding. Predictions can be interpreted through
comparison to experiments. The models naturally support variant effect
prediction by computing reference-alternative differences. Models remain
tied to training assays and cell types. Extension to new contexts
typically requires retraining or new data collection.

\textbf{Typical applications.} Regulatory variant interpretation in
well-studied cell types, eQTL fine-mapping, enhancer identification,
transcription factor binding prediction, and regulatory mechanism
discovery.

\subsection{Variant Effect Prediction
Models}\label{variant-effect-prediction-models}

Models optimized specifically for predicting functional or clinical
consequences of genetic variants. These take a variant and predict its
effect on molecular phenotypes, organismal fitness, or disease risk.

\textbf{Core characteristics.} Variant effect prediction models
integrate sequence context with evolutionary information, population
genetics signals, and sometimes structural or functional annotations.
They output pathogenicity scores, effect size estimates, or functional
consequence predictions. Training combines multiple data sources:
clinical labels from ClinVar, population frequency from gnomAD,
functional assays such as deep mutational scanning, and evolutionary
constraint metrics.

\textbf{Representative examples.} AlphaMissense applies protein language
models to predict pathogenicity of missense variants (J. Cheng et al.
2023). ESM-1v uses evolutionary context for protein variant effect
prediction. EVE combines evolutionary and structural information.
Genomic foundation models like DNABERT and Enformer provide variant
effect predictions through in silico mutagenesis. The architecture,
training, evaluation, and clinical deployment of variant effect
predictors are covered comprehensively in Chapter~\ref{sec-vep-fm}.

\subsection{Multi-Omic Foundation
Models}\label{multi-omic-foundation-models}

Models that natively integrate multiple molecular modalities, jointly
processing DNA sequence, chromatin state, gene expression, protein
abundance, 3D genome structure, or phenotypic descriptions.

\textbf{Core characteristics.} Multi-omic models employ architectures
handling heterogeneous input types: transformer variants with
cross-attention, graph neural networks, or modality-specific encoders
with fusion layers. Training objectives encourage cross-modal alignment
through contrastive learning, joint prediction, or generative modeling
of multiple data types.

\textbf{Representative models.} Omni-DNA uses transformer-based
autoregressive models with vocabulary expansion and multi-task
finetuning, unifying diverse genomic tasks under an instruction-response
paradigm (Z. Li et al. 2025). Models integrating Hi-C data capture 3D
genome organization. Cross-modal architectures align DNA embeddings with
chromatin or expression predictions.

\textbf{Strengths and limitations.} Unified representations enable
cross-modal queries. Joint training can improve performance through
multi-task effects. Data engineering becomes substantially more complex,
with different modalities requiring different measurement technologies
and quality control. The field is early, with few models reaching
production maturity.

\section{Design Dimensions}\label{design-dimensions}

Within and across families, individual models differ along orthogonal
design dimensions that affect suitability for specific tasks.

\subsection{Data Composition}\label{data-composition}

\textbf{Species coverage.} Human-only training focuses on clinically
relevant patterns. Cross-species training encourages learning of
conserved elements and evolutionary constraints, potentially improving
generalization but risking dilution of human-specific signals.

\textbf{Sequence diversity.} Training on reference genomes alone
provides clean sequences but limited exposure to population variation.
Incorporating variant data improves robustness but requires careful
design to avoid learning spurious associations.

\textbf{Annotation integration.} Models may train on raw sequence alone
or incorporate functional annotations. The degree of integration trades
generality against functional grounding.

\subsection{Architecture Choices}\label{architecture-choices}

\textbf{Transformer variants.} Encoder-only models (DNABERT, Nucleotide
Transformer) excel at classification and embedding tasks. Decoder-only
models (GROVER) support generative applications. Full and sparse
attention patterns, linear approximations, and Flash attention
implementations affect computational efficiency.

\textbf{Sub-quadratic architectures.} Hyena-based models and state space
models achieve subquadratic scaling, enabling longer contexts than
standard transformers with comparable parameters.

\textbf{Hybrid architectures.} CNN-transformer combinations use local
convolutions followed by global attention, as in Enformer. Multi-scale
approaches process sequences at multiple resolutions.

\subsection{Context Length}\label{context-length}

\textbf{Short context (under 1 kb)} captures local patterns: motifs,
splice sites, promoter elements. \textbf{Medium context (1-10 kb)} spans
complete genes with proximal regulatory regions. \textbf{Long context
(10-200 kb)} represents enhancer-promoter interactions and TAD-scale
organization. \textbf{Ultra-long context (over 200 kb)} enables
chromosomal domain modeling and complex structural variant
interpretation. The effective use of long context requires appropriate
tokenization and positional encoding.

\subsection{Tokenization}\label{tokenization}

\textbf{Character-level} maintains single-base resolution but imposes
longest sequence lengths. \textbf{K-mer tokenization} reduces length by
a factor approaching k, with vocabulary reaching 4,096 for 6-mers.
\textbf{Learned tokenization} (BPE-style) discovers schemes from data,
potentially allocating vocabulary more efficiently (Medvedev et al.
2025). The choice should align with both computational constraints and
biological resolution requirements.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-design-dimensions}{[}High{]} Multi-axis comparison
(radar/spider plot or parallel coordinates) showing how different models
make different design choices. Axes: (1) Context Length: 512bp â†’ 1Mb;
(2) Parameter Count: 10M â†’ 15B; (3) Species Coverage: Human-only â†’
Pan-genomic; (4) Architecture Type: CNN â†’ Transformer â†’ Hybrid â†’ SSM;
(5) Tokenization: Character â†’ k-mer â†’ BPE â†’ Codon-aware; (6) Pretraining
Objective: MLM â†’ Autoregressive â†’ Multi-task supervised. Models plotted:
DNABERT-2, HyenaDNA, Evo 2, Enformer, Nucleotide Transformer.}

\end{figure}%

\section{Build Versus Use Decisions}\label{build-versus-use-decisions}

The availability of pretrained foundation models creates strategic
choices about when to use existing models, when to adapt them, and when
to train from scratch.

\subsection{When to Use Existing
Models}\label{when-to-use-existing-models}

Existing foundation models provide immediate utility when the target
application aligns with model capabilities, labeled data is limited, and
computational resources are constrained.

\textbf{Embedding extraction.} For tasks where general sequence
representations suffice, frozen foundation model embeddings with simple
downstream classifiers often perform competitively with fine-tuned
alternatives. This approach requires minimal compute (single forward
passes), no gradient computation through large models, and modest
labeled data (hundreds to thousands of examples). Applications include
sequence classification, clustering, and similarity search.

\textbf{Zero-shot inference.} Some foundation models support zero-shot
variant effect prediction through likelihood ratio scoring. This
requires no task-specific training and produces calibrated scores for
novel variants immediately. Zero-shot approaches work well when the
pretraining objective aligns with the target task and when fine-tuning
data is unavailable or unreliable.

\textbf{Rapid prototyping.} Foundation model APIs enable quick
assessment of whether a modeling approach is viable before committing
resources to custom development. Testing variant effect prediction with
ESM-1v takes hours rather than the weeks required to train a custom
model.

\subsection{When to Adapt Existing
Models}\label{when-to-adapt-existing-models}

Adaptation through fine-tuning or lightweight methods (LoRA, adapters,
prefix tuning) makes sense when downstream tasks require specialized
behavior beyond what frozen embeddings provide, sufficient labeled data
exists (typically thousands to tens of thousands of examples), and the
target domain falls within the pretraining distribution.

\textbf{Parameter-efficient fine-tuning.} Methods like LoRA update a
small fraction of model parameters (often under 1\%) while keeping the
foundation model frozen (\textbf{hu\_lora\_2021?}). This preserves
general knowledge while allowing task-specific adaptation. Compute
requirements are modest: a few GPU-hours for most genomic tasks. The
approach works well when the foundation model's representations are
largely appropriate but need refinement for specific applications.

\textbf{Full fine-tuning.} Updating all parameters typically achieves
the best single-task performance but requires more data (tens of
thousands of examples), more compute (GPU-days to weeks), and careful
regularization to prevent overfitting. Full fine-tuning makes sense for
high-stakes applications where maximum accuracy justifies the
investment.

\subsection{When to Train from
Scratch}\label{when-to-train-from-scratch}

Building custom foundation models requires substantial justification
given the resources involved.

\textbf{Novel domains.} When target sequences differ fundamentally from
existing model pretraining data (novel species, synthetic sequences,
non-standard nucleotides), existing models may provide poor transfer.
Custom pretraining on domain-specific data may be necessary.

\textbf{Specialized architectures.} If the application requires
architectural features absent from existing models (specific attention
patterns, custom tokenization, multi-modal inputs), building from
scratch may be unavoidable.

\textbf{Scale requirements.} For applications requiring larger models or
longer contexts than available options, custom training is necessary.
This applies primarily to well-resourced groups with specific
performance requirements.

\textbf{Proprietary data advantages.} Organizations with unique
large-scale datasets (clinical biobanks, pharmaceutical screening data)
may achieve better performance through custom pretraining than public
models allow. The data advantage must be substantial to justify training
costs.

\subsection{Cost-Benefit Analysis}\label{cost-benefit-analysis}

The decision framework involves comparing expected performance against
resource requirements.

\textbf{Compute costs.} Training a foundation model from scratch
requires 10\^{}20 to 10\^{}22 FLOPs, translating to thousands of
GPU-hours and tens of thousands of dollars at current cloud prices.
Fine-tuning requires 10\^{}16 to 10\^{}18 FLOPs, often achievable in
hours on single GPUs. Inference with frozen embeddings requires only
forward passes. Detailed compute estimates for specific model scales and
hardware configurations appear in (\textbf{app-compute?}).

\textbf{Data costs.} Foundation model pretraining requires billions of
tokens. Fine-tuning requires thousands to tens of thousands of labeled
examples. Zero-shot and embedding approaches require only evaluation
data.

\textbf{Performance expectations.} For well-studied tasks with abundant
labeled data, fine-tuned models typically outperform frozen embeddings
by 5-15\% on standard metrics. Zero-shot approaches often achieve
70-90\% of fine-tuned performance. Custom foundation models rarely
outperform existing options by large margins unless the application
involves genuinely novel domains.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-build-vs-use}{[}High{]} Decision flowchart with
cost-benefit annotations. Entry Point: ``New genomic prediction task.''
Decision nodes leading to terminal recommendations: USE (Frozen
embeddings + simple classifier, Hours/\$10, 70-90\% of fine-tuned);
ADAPT (LoRA/adapter fine-tuning, Days/\$100-1000, 95\% of full
fine-tuning); BUILD (Custom pretraining, Months/\$100K+, potentially
best for domain). Annotations: ``Most applications land here'' pointing
to USE/ADAPT paths; ``Rare but sometimes necessary'' pointing to BUILD.}

\end{figure}%

\textbf{Time costs.} Using existing models takes hours to days.
Fine-tuning takes days to weeks. Training from scratch takes weeks to
months. For time-sensitive applications, using existing models often
dominates even if custom training would eventually yield better results.

The practical recommendation for most applications: start with frozen
embeddings from the most appropriate existing foundation model. If
performance is insufficient, try parameter-efficient fine-tuning. Train
from scratch only if adaptation fails and the application justifies the
investment.

\section{Evaluation Principles}\label{evaluation-principles}

Foundation models resist evaluation on single tasks. Their value lies in
transfer across many applications, making comprehensive evaluation
substantially more complex than benchmarking task-specific models.

\subsection{Multi-Task Assessment}\label{multi-task-assessment}

A genomic foundation model should be evaluated across families of
related tasks rather than isolated benchmarks. For DNA language models,
this includes sequence classification tasks, variant effect prediction
across multiple variant types, motif discovery, and cross-species
transfer. For sequence-to-function models, evaluation should span
prediction of held-out assays, transfer to novel cell types, and
consistency with experimental measurements.

The diversity of evaluation tasks complicates comparison across models.
A model excelling at promoter classification may underperform on eQTL
fine-mapping. Direct comparisons require controlling for differences in
training data, model scale, and evaluation protocols.

\subsection{Transfer Versus Pretraining
Performance}\label{transfer-versus-pretraining-performance}

Foundation models are intended for transfer, making pretraining loss
only moderately predictive of downstream utility. A model with slightly
worse masked language modeling loss may produce better embeddings if its
training objective better aligns with useful representations. Evaluation
should explicitly test transfer through zero-shot performance, few-shot
learning, cross-domain transfer, and robustness to distribution shift.

Detailed discussion of benchmark suites, evaluation protocols, and
methodological best practices appears in Chapter~\ref{sec-benchmarks}
and Chapter~\ref{sec-evaluation}.

\section{The Foundation Model
Ecosystem}\label{the-foundation-model-ecosystem}

Genomic foundation models exist within a broader ecosystem of
infrastructure, community resources, and shared practices.

\subsection{Model Distribution}\label{model-distribution}

Most models are distributed through centralized repositories. Hugging
Face hosts many DNA and protein language models with documented APIs.
GitHub repositories accompany publications with weights, code, and
examples. Standardized formats reduce friction in adoption, enabling
rapid benchmarking and experimentation.

\subsection{Documentation
Requirements}\label{documentation-requirements}

Responsible distribution requires comprehensive documentation: training
data provenance, preprocessing procedures, architecture details,
hyperparameters, evaluation protocols, and known limitations. Data
provenance is particularly important given population-specific biases
and use restrictions in genomic datasets.

\subsection{Industry and Academic
Contributions}\label{industry-and-academic-contributions}

Both academic and industry groups develop genomic foundation models.
Academic models emphasize reproducibility and open access. Industry
models may offer superior performance through proprietary data or
compute but with limited transparency. Notable industry contributions
include NVIDIA's BioNeMo platform and Microsoft's Azure genomics
integration. Users should review license terms before clinical or
commercial deployment.

\section{Open Questions}\label{open-questions-1}

Despite rapid progress, fundamental challenges remain.

\textbf{Convergence or divergence.} Whether the field converges toward
unified architectures or maintains specialized families remains unclear.
The diversity of genomic scales, resolution requirements, and functional
contexts may preclude the convergence seen in NLP.

\textbf{Causal reasoning.} Existing models learn correlations without
distinguishing causal from spurious relationships. Integrating causal
structure could improve robustness and enable counterfactual reasoning.

\textbf{Rare variant interpretation.} Models trained on reference
genomes and common variants may not calibrate well for ultra-rare or de
novo variants. Improved integration of structural and evolutionary
constraints could strengthen rare variant interpretation.

\textbf{Clinical deployment.} Translation to clinical use requires
robust cross-population performance, calibrated uncertainty,
interpretability for clinicians, prospective validation, and regulatory
approval. These requirements extend well beyond benchmark performance.

\section{Complementary Tools at
Scale}\label{complementary-tools-at-scale}

Foundation models for genomics divide into families serving different
needs. DNA language models learn general sequence representations from
self-supervised pretraining, capturing evolutionary constraints and
regulatory patterns without explicit functional labels.
Sequence-to-function models predict molecular phenotypes from sequence,
providing quantitative outputs (expression levels, chromatin states,
splice probabilities) that DNA language models alone cannot produce.
Variant effect models integrate sequence representations with
evolutionary information to score the functional impact of genetic
variants. Multi-omic models combine sequence with additional data
modalities to capture regulatory relationships that sequence alone
cannot resolve. No single family dominates; effective genomic AI
requires matching model capabilities to application requirements.

Scale introduces both opportunities and constraints. Scaling laws
describe predictable relationships between parameters, data, compute,
and performance, enabling principled resource allocation. Some
capabilities appear only at sufficient scale, creating thresholds that
cannot be crossed through fine-tuning alone. The practical implication
is that certain applications require institutional-scale investment,
while others can leverage existing pretrained models with modest
adaptation. The build-versus-use framework guides this decision: use
existing models when they suffice, adapt through fine-tuning or feature
extraction when needed, train from scratch only when unique data or
requirements justify the investment.

The chapters that follow instantiate this framework in specific domains.
DNA language models (Chapter~\ref{sec-dna-lm}) and protein language
models (Chapter~\ref{sec-protein-lm}) exemplify self-supervised
pretraining on biological sequence. Regulatory models
(Chapter~\ref{sec-regulatory}) demonstrate sequence-to-function
prediction at long-range scales. Variant effect prediction
(Chapter~\ref{sec-vep-fm}) integrates multiple model families for
clinical interpretation. Throughout, the principles established here
guide model selection: what does this application require, which model
family provides it, and what scale is necessary to achieve it?

\chapter{DNA Language Models}\label{sec-dna-lm}

The transformer revolution in natural language processing rested on a
simple insight: statistical patterns in unlabeled text contain
information about grammar, semantics, and even world knowledge. Train a
model to predict masked words from context, and it learns not just
vocabulary but the structure of language itself. BERT, GPT, and their
successors demonstrated that self-supervised learning on raw text yields
representations useful for tasks the model was never explicitly trained
to perform. Proteins proved amenable to the same approach: models
trained to predict masked amino acids learned evolutionary constraints,
structural properties, and functional relationships without explicit
supervision (Chapter~\ref{sec-protein-lm}). DNA presents the analogous
opportunity. If genomes encode a regulatory language, perhaps
self-supervised learning on raw nucleotide sequence could discover its
grammar.

DNA language models import this paradigm to nucleotide sequences. Rather
than training separate models for each genomic prediction task, as the
CNN era required (Chapter~\ref{sec-cnn}), these approaches learn
general-purpose representations from unlabeled genomes that transfer
across applications. A single pretrained backbone can support regulatory
element classification, variant effect prediction, cross-species
analysis, and sequence generation through different downstream heads or
adaptation strategies. The same model that learns to predict masked
nucleotides can, after fine-tuning, predict chromatin accessibility in
cell types it never saw during pretraining, identify splice sites
without splice-specific training data, and score variant effects using
evolutionary patterns learned from billions of nucleotides.

The opportunity is substantial but not guaranteed to succeed. Protein
sequences have clear functional units (domains, secondary structures,
binding sites) that language model representations can capture. DNA
sequences present a different challenge: regulatory grammar operates at
multiple scales simultaneously, from six-nucleotide transcription factor
binding sites through kilobase-scale enhancers to megabase chromatin
domains. Whether self-supervised learning can discover this multi-scale
grammar remains an empirical question. This chapter traces the
development of DNA language models from early proof-of-concept systems
through current architectures, examining the design choices that
distinguish different approaches, analyzing what these models actually
learn through probing studies, and assessing their performance across
standardized benchmarks.

\section{From Task-Specific CNNs to General-Purpose Language
Models}\label{from-task-specific-cnns-to-general-purpose-language-models}

The convolutional neural networks examined in Chapter~\ref{sec-cnn}
achieved remarkable performance on specific genomic prediction tasks.
DeepSEA predicted chromatin marks from sequence; SpliceAI identified
splice junctions with clinical utility; ExPecto estimated expression
effects of variants. Each model was engineered for its particular
application, with architectural choices (filter sizes, dilation
patterns, pooling strategies) optimized for the task at hand.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-dna-lm-timeline}{[}Essential{]} Horizontal timeline
with milestones and architectural innovations. Key milestones: 2021
DNABERT (512 tokens, proof of concept); 2023 Nucleotide Transformer
(6kb, multi-species, scaling); 2023 HyenaDNA (1 Mb, sub-quadratic); 2024
Caduceus (reverse-complement equivariance, Mamba); 2024-2025 Evo 2 (1
Mb, 7B-40B params, pan-genomic). Upper track: Context length progression
(log scale). Lower track: Key architectural innovations at each stage.}

\end{figure}%

This paradigm succeeded but imposed three constraints that limited
scalability. \textbf{Label dependence} meant that every new assay, cell
type, or phenotype required fresh labeled data. A model trained on
ENCODE chromatin data could not predict histone modifications in a new
cell type without additional labeled examples. \textbf{Task coupling}
bound model architecture to specific prediction problems. SpliceAI's
dilated convolutions were tailored for splice junction detection;
ExPecto's spatial transformation was designed for the distance-dependent
relationship between regulatory elements and transcription start sites.
These choices, while effective, did not transfer naturally to other
problems. \textbf{Limited reuse} meant features learned for one task
could not easily support others. A model that learned to recognize
transcription factor binding sites during chromatin accessibility
training could not directly apply those representations to variant
effect prediction without substantial re-engineering.

Protein language models demonstrated an alternative. ESM and related
models trained on massive corpora of protein sequences using
\textbf{masked language modeling} (predicting held-out amino acids from
context) or autoregressive objectives (predicting the next amino acid).
The resulting representations transferred to structure prediction,
function annotation, and variant effect scoring without architecture
changes. DNA language models import this recipe: pretrain on large
collections of genomic sequences using self-supervised objectives, then
adapt the learned representations to downstream tasks through probing,
fine-tuning, or zero-shot scoring.

The practical workflow involves several steps. First, train a language
model on unlabeled genomic sequences, where the model learns to predict
masked or subsequent nucleotides from context. Second, extract
embeddings from the trained model for sequences of interest (windows
around variants, regulatory elements, or entire genes). Third, apply
these embeddings to downstream tasks through one of several strategies:
train lightweight classifiers on frozen embeddings (probing), update
model parameters for specific applications (fine-tuning), or score
sequence variants by comparing model probabilities (zero-shot
evaluation). The promise is that once a sufficiently powerful backbone
exists, it becomes the default starting point for nearly any DNA-level
prediction problem.

\section{DNABERT: The First DNA Language
Model}\label{dnabert-the-first-dna-language-model}

\emph{DNABERT} applied the BERT masked language modeling framework to
genomic sequences, establishing proof of concept for DNA
self-supervision (Ji et al. 2021). The model used overlapping k-mers
(typically 6-mers) as tokens, creating a vocabulary of 4,096 tokens from
the 4\^{}6 possible hexamers. Training on the human reference genome,
\emph{DNABERT} learned to predict masked tokens from surrounding context
using the standard BERT architecture.

The design choices reflected computational constraints of the time. The
k-mer tokenization provided some sequence compression compared to
single-nucleotide representations, but the overlapping nature (each
nucleotide participates in multiple adjacent k-mers) meant the
compression was modest and created ambiguity about precise variant
positions. Context windows were limited to 512 tokens, corresponding to
a few hundred base pairs of genomic sequence. The standard transformer
architecture with quadratic attention complexity made longer contexts
computationally prohibitive.

Despite these limitations, \emph{DNABERT} demonstrated several important
principles. Fine-tuning on downstream tasks (promoter classification,
splice site prediction, transcription factor binding site
identification) achieved competitive performance with task-specific
models trained from scratch. Learned embeddings captured biologically
meaningful patterns, with similar sequences clustering together in
embedding space even when trained only on the reference genome. The
BERT-style architecture could be reused across multiple tasks with
modest adaptation.

\emph{DNABERT-2} addressed the tokenization limitations through improved
approaches including BPE-style token merging that better compressed
repetitive sequences (Z. Zhou et al. 2024). The resulting model could
represent longer genomic contexts within the same number of tokens,
improving computational efficiency. On standardized benchmarks spanning
sequence classification, regulatory element prediction, and variant
effect scoring, \emph{DNABERT-2} achieved consistent gains over both the
original \emph{DNABERT} and non-pretrained baselines. These improvements
validated the importance of thoughtful tokenization design for genomic
applications (see Chapter~\ref{sec-representations} for detailed
discussion of tokenization strategies).

The DNABERT family collectively established that self-supervision on DNA
works, that tokenization choices substantially affect performance, and
that masked language model training produces reusable representations
for diverse sequence tasks. The foundation model paradigm transfers
effectively from natural language to genomic sequence.

\section{Nucleotide Transformer: Scaling Data and Model
Diversity}\label{nucleotide-transformer-scaling-data-and-model-diversity}

\emph{DNABERT} demonstrated feasibility but operated at modest scale
relative to the size of genomes. The \textbf{Nucleotide Transformer}
family pushed substantially further, emphasizing diversity in both
training data and model architecture (Dalla-Torre et al. 2023).

The training corpus spanned genomic data from multiple species and human
populations, exposing models to diverse sequence patterns, different
regulatory architectures, and evolutionary constraints recurring across
lineages. This cross-species pretraining mirrors the use of large
multi-species alignments in protein language models but operates
directly on raw DNA without explicit alignment. Context length expanded
to approximately 6 kb per input sequence, representing an
order-of-magnitude increase over \emph{DNABERT} while still using dense
transformer attention. The training objective remained masked language
modeling on subsequences sampled from genomes.

The Nucleotide Transformer project introduced a benchmark panel that has
become a standard yardstick for evaluating DNA language models. Tasks
include promoter and enhancer classification, histone mark and chromatin
accessibility prediction, splice site identification, and regulatory
element type classification. Models are evaluated through linear probes
or light fine-tuning on standardized train/validation/test splits. This
benchmark infrastructure enabled systematic comparison across models and
established the evaluation protocols now used throughout the field (see
Chapter~\ref{sec-benchmarks} for comprehensive discussion of genomic
benchmarks).

Scaling experiments revealed predictable relationships between model
size, training data, and performance. Larger models with more
pretraining data and longer context windows achieved better downstream
performance, following patterns observed in natural language and protein
modeling. These scaling trends suggest that continued investment in
larger genomic language models will yield further improvements, though
the optimal allocation between parameters, data, and compute remains an
active research question (Chapter~\ref{sec-fm-principles}).

\section{GPN: Cross-Species Pretraining for Variant Effect
Prediction}\label{gpn-cross-species-pretraining-for-variant-effect-prediction}

While the Nucleotide Transformer demonstrated the value of scaling, the
\textbf{Genomic Pre-trained Network (GPN)} explored a complementary
direction: what can be learned from cross-species pretraining on
relatively small, well-annotated genomes (Benegas, Batra, and Song
2023). Rather than scaling to maximum size, GPN asked whether
self-supervision could yield useful variant effect predictors even in
constrained settings.

GPN was trained on unaligned reference genomes from \emph{Arabidopsis
thaliana} and seven related species within the Brassicales order using
masked language modeling. Despite this modest training corpus, analysis
revealed emergent encoding of gene structure (exon-intron boundaries,
splice sites) and DNA sequence motifs (transcription factor binding
patterns) without explicit supervision. The model discovered these
patterns purely from statistical regularities of genomic sequence across
related species.

For variant effect prediction, GPN used a \textbf{likelihood ratio}
approach. Given reference and alternate alleles at a position, the model
computes the log-likelihood of each under the learned sequence
distribution. Variants that substantially reduce sequence likelihood
(relative to the reference) are inferred to be more disruptive. This
scoring strategy exploits the fact that constrained positions should
have confident predictions for the reference allele, while unconstrained
positions allow more flexibility.

Evaluated on \emph{A. thaliana} variants using allele frequencies from
the 1001 Genomes Project, GPN outperformed traditional conservation
scores including phyloP and phastCons. This was notable because phyloP
and phastCons require explicit multiple sequence alignments and
evolutionary models, while GPN learned its representations from
unaligned sequences through self-supervision alone. The later GPN-MSA
extended this approach to mammalian genomes by incorporating
multi-species alignments, achieving strong performance on human variant
benchmarks as discussed in Chapter~\ref{sec-vep-fm}.

GPN established that cross-species pretraining captures evolutionary
constraints transferable to variant effect prediction, that relatively
small models trained on focused phylogenetic groups can outperform
larger generic conservation measures within that group, and that the
masked language modeling objective naturally produces representations
suitable for variant scoring via likelihood comparisons.

\section{The Long-Context Revolution}\label{the-long-context-revolution}

Quadratic attention complexity limits transformer context to tens of
kilobases at best. Processing a 100 kb sequence with dense attention
requires on the order of 10\^{}10 computations per layer. Yet regulatory
phenomena routinely span larger distances: enhancer-promoter
interactions extend 50-200 kb, topologically associating domains
organize chromatin at the megabase scale, and some gene regulation
involves even longer-range dependencies. \textbf{The mismatch between
biological context and computational context represented a fundamental
architectural limitation.}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-long-context-revolution}{[}Essential{]} Two-panel
comparison. Panel A (Computational Complexity): Log-log plot showing
sequence length (x) vs.~compute/memory (y); standard attention O(LÂ²)
steep curve; Hyena/Mamba O(L) or O(L log L) much flatter; annotated
points at 1kb, 10kb, 100kb, 1Mb showing tractability. Panel B
(Biological Context Coverage): Same x-axis; biological features overlaid
as ranges (TF binding \textasciitilde10-20bp, promoter
\textasciitilde1kb, gene body \textasciitilde10-50kb, enhancer-promoter
\textasciitilde20-200kb, TAD \textasciitilde100kb-1Mb); vertical lines
showing model context limits.}

\end{figure}%

\subsection{HyenaDNA: Megabase Context via Implicit
Convolutions}\label{hyenadna-megabase-context-via-implicit-convolutions}

\emph{HyenaDNA} addressed this limitation by replacing attention with
implicit convolutions that scale sub-quadratically (Nguyen et al. 2023).
The \textbf{Hyena architecture} parameterizes long convolutional filters
through neural networks rather than storing explicit filter weights,
achieving O(L log L) complexity through efficient FFT-based convolution
compared to O(L\^{}2) for standard attention. The result was a 500-fold
increase in context length: \emph{HyenaDNA} processes sequences up to 1
Mb while maintaining single-nucleotide resolution.

Processing megabase-scale windows allows the model to capture entire
gene bodies plus flanking regulatory regions, long-range
enhancer-promoter interactions, and topologically associating domain
structure. Despite the long context, single-nucleotide tokens preserve
maximum resolution for variant effect prediction. Each nucleotide is
independently represented without the ambiguity introduced by k-mer
tokenization.

On Nucleotide Transformer benchmarks, \emph{HyenaDNA} achieved
state-of-the-art results on the majority of tasks with orders of
magnitude fewer parameters. On GenomicBenchmarks, it surpassed prior
state-of-the-art on seven of eight datasets. Perhaps most notably,
\emph{HyenaDNA} demonstrated \textbf{in-context learning} in genomics:
performance improved when examples were included in the input context
without updating model weights. This capability, familiar from large
language models, had not previously been observed for genomic sequences
and suggests that sufficient context length combined with appropriate
architecture enables qualitatively new forms of biological reasoning.

\subsection{Caduceus: Bidirectional Processing with Reverse-Complement
Equivariance}\label{caduceus-bidirectional-processing-with-reverse-complement-equivariance}

DNA is double-stranded, and any sequence can be read from either strand.
The reverse complement of a sequence encodes the same information from
the opposite strand's perspective. For many biological processes,
predictions should be identical or related consistently regardless of
which strand is presented. Standard neural networks can produce
divergent predictions for a sequence and its reverse complement, even
with data augmentation during training.

\emph{Caduceus} addressed this challenge by building
\textbf{reverse-complement equivariance} directly into the architecture
(Schiff et al. 2024). The model extends the Mamba state space
architecture (which achieves O(L) complexity) to support both
bidirectional processing and strand equivariance. The BiMamba component
enables information flow in both directions along the sequence, while
the MambaDNA block ensures mathematically related predictions for
sequences and their reverse complements.

On downstream benchmarks, \emph{Caduceus} outperformed previous
long-range models. On challenging long-range variant effect prediction
tasks, it exceeded models with ten times as many parameters that lacked
bidirectionality or equivariance. \textbf{The key insight was that
incorporating appropriate biological inductive biases can substitute for
raw scale.} Strand symmetry is a known property of DNA; building it into
the architecture avoids wasting model capacity learning what could be
specified directly.

\subsection{Evo 2: Genome-Scale Modeling Across the Tree of
Life}\label{evo-2-genome-scale-modeling-across-the-tree-of-life}

\emph{Evo 2} represents the current frontier: training at genome scale
across all domains of life (Brixi et al. 2025). While previous models
focused on specific organisms (\emph{DNABERT} on human, GPN on plants)
or trained on multi-species corpora at limited scale (Nucleotide
Transformer), \emph{Evo 2} aims to learn universal genomic patterns
spanning bacteria, archaea, eukaryotes, and phages.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-caduceus-equivariance}{[}High{]} Three-panel
diagram. Panel A (The Problem): Input sequence and reverse complement;
standard model gives potentially different predictions; annotation:
``Same biological information, inconsistent predictions.'' Panel B
(Caduceus Architecture): BiMamba component with bidirectional arrows;
MambaDNA block with weight-sharing scheme; mathematical relationship:
f(revcomp(x)) = g(f(x)). Panel C (Performance Impact): Bar chart
comparing models with/without equivariance on long-range tasks;
annotation: ``Appropriate biological inductive biases can substitute for
raw scale.''}

\end{figure}%

The training corpus draws from the OpenGenome2 dataset comprising 9.3
trillion DNA tokens across all domains of life. This massive scale
exposes the model to the full spectrum of genomic organization: compact
prokaryotic gene arrangements, sprawling eukaryotic regulatory
landscapes with extensive noncoding sequence, viral genomes with
overlapping reading frames, and the diversity of regulatory
architectures across evolution. The model comes in 7 billion and 40
billion parameter variants.

The architecture builds on StripedHyena 2, a hybrid design combining
convolutional operations with selective attention mechanisms. This
enables processing of sequences up to 1 million nucleotides while
maintaining computational tractability. The \textbf{autoregressive
training objective} (predicting the next base given all previous bases)
differs from the masked language modeling used in DNABERT and related
models. Autoregressive training may provide complementary strengths for
sequence generation and likelihood-based scoring, since the model learns
to generate plausible sequences in addition to discriminating between
them.

\emph{Evo 2} exhibits several forms of emergent biological knowledge
despite training only on raw sequence. The model learns to identify
exon-intron boundaries without explicit annotation, discovers
transcription factor binding site patterns matching known motifs,
captures aspects of protein secondary and tertiary structure when
processing coding sequences, and identifies prophage insertion regions
in bacterial genomes. These capabilities emerge from pure sequence
statistics, demonstrating that genome-scale pretraining captures
fundamental biological organization.

For variant effect prediction, \emph{Evo 2} enables zero-shot scoring
through likelihood ratios. Variants can be scored for consistency with
learned genomic patterns by comparing model probabilities for reference
versus alternate sequences. On benchmarks of pathogenic versus benign
variants, zero-shot scores achieve competitive performance with
specialized supervised methods, though calibration remains necessary
before clinical application. The model also supports classification of
variants of uncertain significance through simple classifiers trained on
its embeddings.

The pan-species training enables cross-species applications. Variant
interpretation extends naturally to non-model organisms, supporting
conservation genomics and agricultural breeding where labeled training
data is scarce. Model representations cluster sequences by phylogenetic
relationships even without explicit evolutionary modeling. Beyond
discriminative tasks, \emph{Evo 2} demonstrates generative capabilities:
synthesizing plausible mitochondrial genomes, prokaryotic operons, and
eukaryotic regulatory regions with coherence across kilobase to megabase
scales.

\section{Training Data and What Models
Learn}\label{training-data-and-what-models-learn}

DNA language models are trained on diverse corpora ranging from single
reference genomes to pan-genomic collections spanning the tree of life.
Understanding what training data is used and what models learn from it
is essential for anticipating model capabilities and limitations.

\subsection{Training Corpus
Composition}\label{training-corpus-composition}

Early models like \emph{DNABERT} trained primarily on the human
reference genome (GRCh38), providing exposure to approximately 3 billion
nucleotides from a single individual. The Nucleotide Transformer
expanded to include multiple species and human population variation from
resources like the 1000 Genomes Project. \emph{Evo 2} scaled to 9.3
trillion tokens spanning all domains of life, including complete
bacterial chromosomes, eukaryotic genomes, viral sequences, and
metagenomic assemblies.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-evo2-training}{[}High{]} Three-panel figure. Panel A
(Training Corpus): Tree of life visualization with branch widths
proportional to training data; Bacteria, Archaea, Eukaryotes,
Viruses/Phages; total 9.3 trillion DNA tokens; contrast with human-only
models (\textasciitilde3B tokens). Panel B (Architecture): StripedHyena
2 schematic with hybrid attention-convolution blocks; 1 Mb context; 7B
and 40B variants. Panel C (Emergent Cross-Species Capabilities):
Embedding space UMAP colored by taxonomic group; phylogenetic clustering
emerging without explicit evolutionary modeling.}

\end{figure}%

The composition of training data shapes what models learn.
Reference-only training captures the genome's architecture but not
population variation. Multi-individual training exposes models to common
polymorphisms but may underrepresent rare variants. Cross-species
training provides evolutionary context (constrained regions are
conserved, variable regions diverge) but may not capture
species-specific regulatory patterns. Training on functional genomics
data (GROVER-style approaches) teaches regulatory activity patterns but
ties models to specific assays and cell types.

A tension exists between generality and specificity. Models trained on
broader corpora learn more general representations that transfer across
species and contexts, but may underperform narrower models on specific
applications. Models trained on focused datasets may capture
task-relevant patterns more effectively but transfer less well. The
optimal training strategy depends on intended applications.

\subsection{Probing What Models Learn}\label{probing-what-models-learn}

Linear probing experiments reveal what information is encoded in model
representations without task-specific fine-tuning. By training simple
classifiers (logistic regression, single-layer perceptrons) on frozen
embeddings to predict known annotations, researchers can assess whether
models have learned biologically meaningful patterns.

DNA language models consistently learn to recognize several categories
of genomic features. \textbf{Motif recognition} emerges naturally:
models learn patterns corresponding to known transcription factor
binding sites, splice signals, and other sequence motifs without
explicit supervision. Probing for specific motif presence shows that
model embeddings can distinguish sequences containing binding sites from
those lacking them. \textbf{Gene structure} is encoded in
representations: models distinguish coding from noncoding regions,
identify exon-intron boundaries, and recognize splice donor and acceptor
sites. This knowledge emerges from sequence statistics alone, suggesting
that the compositional and structural differences between genomic region
types are learnable from DNA sequence.

\textbf{Evolutionary constraints} are implicitly captured, particularly
in models trained on multi-species data. Positions under purifying
selection (constrained across evolution) show different embedding
patterns than neutral positions. This provides a self-supervised analog
to traditional conservation scoring, though the relationship between
model-learned and alignment-based conservation measures varies across
genomic contexts.

More complex patterns like \textbf{regulatory grammar} (the syntax
governing how transcription factors combine to specify expression) show
mixed evidence. Models capture some aspects of regulatory logic, such as
the spacing preferences between binding sites, but may not fully
represent the combinatorial complexity of enhancer function. Similarly,
long-range dependencies (enhancer-promoter interactions across tens of
kilobases) are accessible to long-context models but require extensive
probing to assess whether they are actually leveraged.

\subsection{What Models Do Not Learn}\label{what-models-do-not-learn}

Equally important is recognizing what current DNA language models
struggle to represent. \textbf{Epigenetic context} is not captured by
sequence-only models: DNA methylation, histone modifications, and
chromatin accessibility all affect gene regulation but are not encoded
in primary sequence. Some models (like GROVER) address this by
incorporating functional genomics data, but this ties them to specific
cell types and experimental conditions.

\textbf{Three-dimensional structure} of chromatin affects which
regulatory elements can physically interact, but linear sequence models
cannot represent folding (see Chapter~\ref{sec-3d-genome}).
\textbf{Cell-type specificity} of gene regulation depends on
transcription factor expression levels and chromatin state, not just
sequence; models trained on sequence alone can predict potential
regulatory activity but not its realization in specific contexts.

\textbf{Complex variant patterns} beyond single nucleotide changes
remain challenging. Indels, structural variants, repeat expansions, and
epistatic interactions between distant loci are either not representable
(depending on tokenization) or poorly predicted. Most benchmark tasks
focus on SNVs, leaving multi-nucleotide effects underexplored.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-dna-lm-probing}{[}High{]} Multi-panel figure. Panel
A (Motif Recognition): First-layer attention patterns aligned with
JASPAR motifs; model learns CTCF pattern from sequence statistics alone.
Panel B (Gene Structure): t-SNE/UMAP of embeddings color-coded by region
type (exon, intron, UTR, intergenic); clear clustering without region
labels during training. Panel C (Evolutionary Constraint): Scatter plot
of model uncertainty vs.~phyloP conservation score; strong correlation.
Panel D (What Models Don't Learn): Icons with X marks for epigenetic
state, 3D structure, cell-type specificity, complex variants.}

\end{figure}%

\section{Benchmark Performance and
Evaluation}\label{benchmark-performance-and-evaluation}

Standardized benchmarks enable systematic comparison across DNA language
models, though each benchmark captures only part of what we care about.
Understanding benchmark construction and limitations is essential for
interpreting performance claims.

\subsection{Major Benchmark Suites}\label{major-benchmark-suites}

\textbf{BEND (Benchmark for Nucleotide Deep learning)} provides a
unified framework with tasks including gene finding, enhancer
annotation, chromatin state prediction, and variant effect scoring
(Marin et al. 2024). Standardized splits and metrics enable fair
comparison. BEND specifically evaluates whether models capture
biologically meaningful features at different resolution scales.

\textbf{Genomic Benchmarks} focus on regulatory element classification
tasks: distinguishing promoters from nonpromoters, identifying active
enhancers, predicting histone mark presence. These tasks test whether
model representations encode basic genomic annotations. Most current DNA
language models achieve high accuracy on these tasks, suggesting
benchmark saturation for simpler classification problems.

\textbf{Long Range Benchmark (LRB)} and \textbf{DNALongBench} evaluate
long-context modeling capabilities (W. Cheng et al. 2024). Tasks include
predicting distal enhancer-promoter interactions, modeling chromatin
structure across hundreds of kilobases, and integrating information over
extended genomic windows. These benchmarks specifically test whether
long-context architectures provide meaningful advantages over
shorter-context models.

Comparative evaluations across model families reveal that no single
architecture dominates all tasks (Manzo, Borkowski, and Ovcharenko
2025). Performance varies substantially depending on task
characteristics (local motif recognition versus long-range integration),
training data composition, and architectural choices. \emph{HyenaDNA}
and \emph{Caduceus} excel on long-range tasks where their architectural
innovations matter; \emph{DNABERT-2} and Nucleotide Transformer perform
well on shorter-range regulatory classification; \emph{Evo 2} shows
advantages on cross-species tasks and variant effect prediction.

\subsection{Benchmark Limitations}\label{benchmark-limitations}

Several systematic issues affect benchmark interpretation.
\textbf{Saturation} occurs when multiple models achieve near-perfect
performance, eliminating discriminative power. This has happened for
simpler classification tasks in Genomic Benchmarks. \textbf{Leakage}
arises when training and test sequences share homology, allowing models
to succeed through memorization rather than generalization. Careful
sequence clustering (using tools like MMseqs2 or CD-HIT) is required to
prevent this, but many older benchmarks lack rigorous split design.

\textbf{Distribution shift} between benchmark data and real-world
applications means strong benchmark performance may not predict
deployment success. Most benchmarks derive from well-studied regions of
well-characterized genomes; performance on understudied regions, rare
variants, or non-European populations may differ substantially (see
Chapter~\ref{sec-confounding} for discussion of ancestry bias).

\textbf{Metric selection} affects what gets optimized. AUROC favors
discrimination regardless of calibration; Spearman correlation measures
rank ordering but not absolute effect size prediction. Clinical
applications may require well-calibrated probability estimates or
accurate quantitative predictions, neither of which standard metrics
directly assess. The gap between benchmark performance and deployment
utility remains substantial for most genomic applications.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-dna-lm-benchmarks}{[}Enhancing{]} Heatmap or grouped
bar chart. Models (rows): DNABERT-2, Nucleotide Transformer (2.5B),
HyenaDNA, Caduceus, Evo 2. Benchmark tasks (columns) grouped:
Short-range (promoter detection, enhancer classification, TF binding);
Long-range (enhancer-promoter interaction, chromatin state); Variant
effect (SNV scoring, splice prediction). Cell values: Relative
performance (color scale). Key observations annotated: ``No single model
dominates all tasks''; ``Long-context models excel on long-range
tasks.''}

\end{figure}%

\section{Annotation-Aware Extensions}\label{annotation-aware-extensions}

Recent work explores enriching DNA language models with explicit
biological structure beyond raw sequence. These approaches represent
early steps toward multi-modal genomic foundation models.

\textbf{Life-Code} proposes central-dogma-informed tokenization,
treating coding and noncoding regions differently (Liu et al. 2025).
Coding regions use codon tokens (three-nucleotide units specifying amino
acids), respecting the genetic code's fundamental structure. Noncoding
regions use learned subword units optimized during training. Knowledge
distillation from protein language models imports protein-level
structural knowledge into DNA representations. Life-Code achieves
competitive results across DNA, RNA, and protein tasks, suggesting that
encoding biological structure into tokenization provides useful
inductive bias.

\textbf{BioToken} extends tokenization to include explicit genomic
annotations (Medvedev et al. 2025). Rather than representing regions
purely as nucleotide strings, BioToken creates composite tokens encoding
sequence content, variant presence, structural annotations (exon,
intron, UTR), and functional context. The associated BioFM model
achieves state-of-the-art performance across genomic benchmarks with
substantially fewer parameters (265M), suggesting that annotation-aware
representations improve parameter efficiency.

These approaches foreshadow the multi-modal foundation models discussed
in Part IV, where sequence is only one of many integrated information
streams.

\section{Using DNA Language Models in
Practice}\label{using-dna-language-models-in-practice}

DNA language models support multiple usage patterns for different
applications.

\subsection{Embeddings as Universal
Features}\label{embeddings-as-universal-features}

The simplest approach extracts embeddings from a pretrained model and
uses them as features for downstream classifiers. The workflow involves
extracting embeddings for windows around loci of interest, pooling or
selecting positions relevant to the task, and training lightweight
downstream models (linear layers, shallow MLPs, gradient boosting) on
the extracted features.

This approach supports diverse applications. Regulatory element
classification distinguishes promoters, enhancers, silencers, and
insulators based on learned representations. Chromatin state prediction
uses sequence embeddings to predict ATAC-seq or histone mark presence.
Variant effect scoring replaces or augments hand-crafted features in
frameworks like CADD with language model features (analogous to CADD
v1.7's incorporation of protein language model features). Splicing
analysis combines embeddings with specialized architectures.

Because the language model remains frozen, this approach is
computationally efficient and avoids catastrophic forgetting when new
tasks are added. The pretrained model serves as a general-purpose
feature extractor supporting many downstream applications.

\subsection{Fine-Tuning and
Adaptation}\label{fine-tuning-and-adaptation}

When sufficient labeled data exists, fine-tuning typically outperforms
frozen embedding approaches. Full fine-tuning updates all language model
parameters for a specific task, allowing representations to specialize.
This achieves highest performance but requires more compute and risks
catastrophic forgetting of general knowledge.

\textbf{Parameter-efficient fine-tuning} methods like LoRA (Low-Rank
Adaptation) insert small trainable modules into each layer while keeping
the backbone mostly frozen. These approaches achieve most of the
performance gains of full fine-tuning while maintaining computational
efficiency and preserving general capabilities. Adapter-based methods
similarly add small bottleneck modules tuned for specific tasks.

\subsection{Zero-Shot and Few-Shot
Scoring}\label{zero-shot-and-few-shot-scoring}

For variant interpretation, language models enable zero-shot scoring
based on sequence likelihood. Compute the model's probability for a
sequence containing the reference allele, compare to probability for the
sequence with the alternative allele, and interpret variants reducing
probability as more disruptive. This approach requires no
variant-specific training and can score any single-nucleotide variant
the model can represent.

Zero-shot scoring quality depends on how well the model's learned
distribution captures biological constraints. Performance tends to
improve with model scale and training data diversity. Few-shot
approaches include task examples in the input context, allowing
in-context learning without parameter updates. \emph{HyenaDNA}
demonstrated this capability for genomic tasks, suggesting that
sufficiently large models with long context can adapt through prompts
rather than training.

\section{Limitations and Open
Challenges}\label{limitations-and-open-challenges}

Despite substantial progress, DNA language models face several
fundamental limitations.

\textbf{Context length versus resolution tradeoffs} persist.
Long-context models like \emph{HyenaDNA} and \emph{Evo 2} can process
megabase sequences but require efficient architectures that may not
capture all the relationships dense attention would learn. Whether these
architectural tradeoffs matter for specific applications remains
task-dependent.

\textbf{Complex variant patterns} beyond SNVs are poorly handled. Most
tokenization schemes represent insertions and deletions awkwardly or not
at all. Structural variants spanning kilobases, repeat expansions, and
complex rearrangements fall outside what current models can process.
Epistatic interactions between variants at distant loci are not captured
even by long-context models.

\textbf{Training data bias} shapes model capabilities in underexplored
ways. Models trained primarily on European-ancestry genomes may perform
poorly on variants common in other populations. Ascertainment bias in
training databases (enrichment for coding regions, well-studied genes,
specific diseases) propagates to learned representations. The field
lacks systematic evaluation of performance disparities across
populations.

\textbf{Interpretability} remains limited. While probing studies reveal
what models encode, explaining why a specific variant receives a
particular score in terms connecting to biological mechanism is
difficult. Attention patterns and gradient-based attribution provide
some insight but often fail to identify the specific sequence features
driving predictions.

\textbf{Integration with other modalities} is nascent. DNA sequence
provides necessary but insufficient information for predicting gene
regulation. Epigenomic state, three-dimensional chromatin structure,
transcription factor concentrations, and cellular context all matter.
Current DNA language models cannot represent these factors; multi-modal
approaches (discussed in Part IV) aim to address this limitation.

\section{Representations Without
Predictions}\label{representations-without-predictions}

DNA language models capture sequence patterns, regulatory motifs, and
evolutionary constraints through self-supervised pretraining on genomic
sequence. The progression from early proof-of-concept models through
architectural innovations enabling megabase context demonstrates that
the paradigm works: models trained to predict masked nucleotides learn
representations that transfer across diverse downstream tasks.
Biological inductive biases (strand symmetry, codon structure,
cross-species training) can substitute for raw scale on appropriate
tasks, creating opportunities for efficient models that encode domain
knowledge.

Yet DNA language models have inherent limitations. They produce
representations, not predictions. A language model can embed a sequence
in a space where similar regulatory elements cluster together, but it
cannot directly output the expression level that sequence will produce
or the chromatin accessibility it will exhibit. The models capture what
patterns exist in genomic sequence but not what those patterns do in
cellular context. They cannot represent epigenomic state,
three-dimensional chromatin organization, or cell-type-specific
regulation without additional inputs beyond sequence.

These limitations define the complementary relationship between language
models and sequence-to-function models. Where DNA language models learn
representations from sequence statistics, regulatory models like
Enformer and Borzoi (Chapter~\ref{sec-regulatory}) predict molecular
phenotypes from sequence context. The regulatory models provide
quantitative outputs (expression levels, chromatin tracks, splice
probabilities) that language models alone cannot produce. For variant
effect prediction (Chapter~\ref{sec-vep-fm}), both representation
quality and phenotypic prediction matter: language model embeddings
capture evolutionary constraint while regulatory models predict
functional consequences. Understanding what each model family provides
is prerequisite to combining them effectively.

\chapter{Protein Language Models}\label{sec-protein-lm}

Evolution is the most thorough experiment ever conducted on protein
sequences. Over billions of years, natural selection tested trillions of
amino acid combinations, ruthlessly eliminating sequences that failed to
fold or function while preserving those that worked. The sequences
populating modern databases are not random strings but successful
solutions to biological problems, each implicitly encoding information
about structure, stability, and function. The central insight of protein
language models is that this evolutionary record, comprising hundreds of
millions of sequences in databases like UniRef, contains sufficient
information to learn the fundamental principles of protein biology
without ever being shown a crystal structure or a functional assay.

This insight transformed computational biology. Traditional approaches
to understanding proteins required either expensive experimental
characterization or physics-based simulations that struggled with the
complexity of protein behavior. Multiple sequence alignments could
extract conservation patterns, but required finding homologs for each
protein of interest and could not generalize beyond specific families.
Protein language models changed the equation by compressing evolutionary
knowledge into neural network parameters that transfer across the entire
protein universe. A model trained to predict masked amino acids learns,
as a byproduct, which residues contact each other in three-dimensional
space, which positions tolerate variation, and which substitutions
disrupt function. The physics of protein folding, selected across
evolutionary time, emerges from the statistics of surviving sequences.

This chapter examines how protein language models pioneered biological
foundation modeling, establishing principles that would later guide DNA
and RNA language models. The ESM family demonstrated that transformers
can learn protein structure and function from sequence alone, achieving
results that rival methods requiring explicit structural supervision.
Evolutionary Scale Modeling, as the name suggests, exploits the scale of
evolutionary data to learn representations that generalize across
proteins regardless of homology or family membership. Understanding
these successes and their limitations provides essential context for
genomic language models, where analogous approaches face distinct
challenges arising from the multi-scale organization of regulatory
information in DNA.

\section{The ESM Model Family}\label{the-esm-model-family}

\subsection{ESM-1b: Establishing the
Paradigm}\label{esm-1b-establishing-the-paradigm}

The Evolutionary Scale Modeling project, developed at Meta AI Research,
demonstrated that transformer language models trained on protein
sequences learn biologically meaningful representations without explicit
supervision (Rives et al. 2021). The approach was strikingly simple:
take the BERT architecture that had revolutionized natural language
processing, replace words with amino acids, and train on protein
sequence databases. The resulting models learned far more than anyone
expected.

ESM-1b was trained on UniRef50, a clustered database of approximately 33
million protein sequences covering the known diversity of protein
families (see Chapter~\ref{sec-data} for details on sequence database
construction and clustering strategies). UniRef50 clusters sequences at
50\% identity, providing broad coverage while reducing redundancy that
would otherwise bias the model toward overrepresented families (Suzek et
al. 2007). This curation strategy ensures the model encounters diverse
evolutionary solutions to protein function rather than memorizing common
motifs.

The architecture follows the BERT-style bidirectional transformer design
with 650 million parameters distributed across 33 layers, a hidden
dimension of 1,280, and 20 attention heads. The maximum sequence length
of 1,024 amino acids accommodates most individual protein domains and
many complete proteins. The training objective is masked language
modeling, the self-supervised strategy introduced in
Chapter~\ref{sec-pretraining}: randomly mask 15\% of amino acids in each
sequence, and train the model to predict the masked positions given
surrounding context. This objective contains no information about
structure, function, or evolution beyond what is implicit in the
sequences themselves.

\subsection{Emergent Biological
Knowledge}\label{emergent-biological-knowledge}

The surprise was not that ESM-1b learned to predict masked amino acids
accurately, but what else it learned in the process. Despite never
seeing structural or functional labels during training, ESM-1b's
internal representations encode information about protein biology at
multiple levels of organization.

Secondary structure emerges in the attention patterns. When researchers
analyzed which sequence positions the model attends to when making
predictions, they found that attention concentrates along patterns
corresponding to alpha helices and beta sheets. The model implicitly
learns that certain amino acid sequences form specific structural
elements, encoding this knowledge without ever being told what secondary
structure is.

More remarkably, ESM-1b captures residue-residue contacts. Amino acids
that are distant in the linear sequence but close in three-dimensional
space attend to each other in the model's attention matrices. This
emergent capability suggests the model learns aspects of protein folding
purely from sequence statistics. When attention weights were converted
to contact predictions, they achieved accuracy approaching dedicated
contact prediction methods that were explicitly trained for that task.

The model's masked token predictions correlate strongly with
position-specific conservation scores derived from multiple sequence
alignments. ESM effectively learns which positions tolerate variation
and which are evolutionarily constrained, extracting this information
from the statistical patterns across 33 million sequences rather than
from explicit conservation annotations. Positions where the model
confidently predicts specific amino acids correspond to positions that
are conserved across protein families.

Perhaps most striking, attention concentrates on functionally important
positions. Catalytic residues, binding sites, and other sites of
biological importance receive elevated attention even without explicit
functional annotation in the training data. The model discovers that
certain sequence positions are more informative about surrounding
context, and these positions frequently correspond to sites where nature
has constrained variation because they perform essential functions.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-plm-emergent}{[}Essential{]} Multi-panel figure.
Panel A (Training Objective): Protein sequence with 15\% masked; model
predicting from context; ``No structure, function, or evolutionary
labels.'' Panel B (Secondary Structure in Attention): Attention heatmap
with alpha helix and beta sheet regions highlighted; attention
concentrating along structural patterns. Panel C (Residue Contacts from
Attention): Attention weights converted to contact map; ground truth
from crystal structure overlaid; strong correspondence. Panel D
(Functional Site Discovery): Protein structure cartoon; positions with
elevated attention highlighted; overlap with catalytic residues, binding
sites.}

\end{figure}%

\subsection{ESM-2: Scaling Up}\label{esm-2-scaling-up}

ESM-2 extended the ESM approach across a range of model scales, from 8
million to 15 billion parameters, enabling systematic study of how
biological knowledge scales with model capacity (Lin et al. 2022). The
results confirmed a pattern familiar from natural language processing:
bigger models learn more.

\begin{longtable}[]{@{}lllll@{}}
\caption{ESM-2 model family spanning four orders of magnitude in
parameter count, with architecture details and relative performance on
structure-related tasks.}\tabularnewline
\toprule\noalign{}
Model & Parameters & Layers & Hidden Dim & Performance Gain \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Model & Parameters & Layers & Hidden Dim & Performance Gain \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ESM-2 (8M) & 8M & 6 & 320 & Baseline \\
ESM-2 (35M) & 35M & 12 & 480 & Modest \\
ESM-2 (150M) & 150M & 30 & 640 & Substantial \\
ESM-2 (650M) & 650M & 33 & 1280 & Large \\
ESM-2 (3B) & 3B & 36 & 2560 & Near-optimal \\
ESM-2 (15B) & 15B & 48 & 5120 & State-of-the-art \\
\end{longtable}

Performance scales smoothly with model size across structure prediction,
contact prediction, and variant effect tasks. The scaling relationship
is not linear: doubling parameters does not double accuracy. But gains
remain consistent through even the largest models, suggesting that the
15-billion parameter ceiling reflects computational constraints rather
than fundamental limits on what sequence statistics can teach.

The scaling behavior mirrors observations in natural language
processing, where larger models consistently capture more nuanced
patterns. This predictable relationship between scale and capability
provides a roadmap for model development: if more biological knowledge
is needed, train a larger model on more data. The practical implications
shaped how the field approached subsequent genomic foundation models.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-esm2-scaling}{[}Essential{]} Scaling analysis. Panel
A (Performance vs.~Parameters): Log-log plot; x-axis parameters (8M â†’
15B); y-axis performance on structure-related tasks; multiple curves; no
sign of saturation. Panel B (Model Family Table): ESM-2 variants stacked
by size; visual encoding of layers, hidden dimension, performance;
annotate where capabilities emerge (\textasciitilde150M useful
embeddings, \textasciitilde650M structural understanding,
\textasciitilde3B near-optimal single-sequence structure,
\textasciitilde15B approaches MSA methods). Panel C (Capability
Thresholds): Specific capabilities as step functions; contact prediction
gradual; zero-shot structure emergent at \textasciitilde650M.}

\end{figure}%

\section{Alternative Architectures}\label{alternative-architectures}

The success of ESM raised a natural question: how much depends on the
specific BERT architecture versus the general approach of
self-supervised learning on protein sequences? The ProtTrans family
explored this question by applying multiple transformer architectures to
protein modeling (Elnaggar et al. 2021).

ProtBERT applies the bidirectional encoder to protein sequences, trained
on the Big Fantastic Database (BFD) comprising approximately 2.1 billion
protein sequences. This training corpus, substantially larger than
UniRef50, provides broader coverage at the cost of including more
redundant and potentially lower-quality sequences. The architectural
choices match ESM closely, enabling direct comparison of training data
effects.

ProtT5 adapts the encoder-decoder architecture from T5, enabling both
understanding and generation tasks (Raffel et al. 2019). The encoder
processes input sequences to produce contextual representations, while
the decoder can generate output sequences conditioned on those
representations. This architecture proved valuable for tasks requiring
sequence generation, such as structure-conditioned design or sequence
completion, though the encoder-only architecture remains dominant for
embedding and classification tasks.

ProtXLNet explores permutation language modeling, capturing
bidirectional context without the artificial {[}MASK{]} token that
BERT-style models require during training (Z. Yang et al. 2020). By
training on all possible token orderings, XLNet-style models learn to
predict each token from any subset of context tokens, potentially
capturing richer dependencies at the cost of more complex training.

These architectural variants demonstrate that the protein language
modeling paradigm generalizes beyond specific design choices. All
architectures learn meaningful representations when trained on
sufficient data, though performance differences emerge for specific
downstream tasks. Encoder-only models excel at classification and
embedding tasks where the entire sequence is available. Encoder-decoder
models enable generation tasks where outputs must be produced token by
token.

\section{Attention and Evolutionary
Coupling}\label{attention-and-evolutionary-coupling}

The emergence of contact information in ESM's attention patterns
connects to a deeper principle: evolutionary coupling. When two residues
must maintain physical contact for a protein to function, mutations at
one position create selective pressure for compensatory mutations at the
other. Over evolutionary time, these correlated mutations leave
statistical signatures in protein families that can be detected through
covariance analysis of multiple sequence alignments.

Direct Coupling Analysis (DCA) and related methods extract these
coevolutionary signals to predict residue-residue contacts
(\textbf{morcos\_dca\_2011?}). The approach requires constructing
multiple sequence alignments, computing covariance matrices, and
applying statistical corrections to distinguish direct from indirect
correlations. The resulting contact predictions enabled the first
accurate structure predictions for proteins lacking homologs in
structural databases.

Protein language models learn to extract similar information through a
different route. Rather than computing covariance explicitly,
transformers learn attention patterns that capture which positions
inform predictions at other positions. When position \(i\) strongly
attends to position \(j\) during masked prediction, the model has
learned that knowing the amino acid at \(j\) helps predict the amino
acid at \(i\). This is precisely the signature of evolutionary coupling:
positions that covary because they must maintain physical contact.

The attention-based approach offers several advantages over traditional
covariance analysis. Language models generalize across protein families,
learning shared principles that transfer to proteins with sparse
evolutionary sampling. They handle the statistical challenge of
distinguishing direct from indirect correlations implicitly through deep
architecture rather than requiring explicit correction. And they provide
rich representations beyond binary contact predictions, encoding
information about the strength and nature of residue relationships.

Rao and colleagues demonstrated this connection directly by extracting
attention weights from ESM and converting them to contact predictions
(\textbf{rao\_transformer\_2021?}). The resulting predictions approached
the accuracy of dedicated contact prediction methods, despite the model
never being trained to predict contacts. The attention mechanism,
optimized purely for masked token prediction, discovers the
coevolutionary structure of protein sequences as a byproduct.

\section{ESMFold: Structure from
Sequence}\label{esmfold-structure-from-sequence}

\subsection{Eliminating the Alignment
Bottleneck}\label{eliminating-the-alignment-bottleneck}

The most dramatic demonstration of protein language model capabilities
came with ESMFold, which predicts protein 3D structure directly from
ESM-2 embeddings without requiring multiple sequence alignments (Lin et
al. 2022). Traditional structure prediction, including AlphaFold2,
relies heavily on MSAs constructed through computationally expensive
searches against sequence databases. These searches can take hours per
protein, and prediction quality depends critically on finding
informative homologs.

ESMFold eliminates this requirement entirely. The architecture couples
ESM-2 (using the 15-billion parameter variant) with a structure module
adapted from AlphaFold2's Evoformer and structure module. The language
model embeddings replace MSA-derived features, providing the
evolutionary context that the structure module needs to predict atomic
coordinates. The model takes a single sequence as input and outputs
predicted 3D coordinates for all atoms.

The computational speedup is substantial: approximately 60-fold faster
than AlphaFold2 for typical proteins. This speed advantage makes it
feasible to predict structures for the millions of protein sequences
emerging from environmental sequencing projects, where computing MSAs
would be prohibitively expensive. Metagenomic proteins, often lacking
close homologs in existing databases, represent exactly the cases where
MSA-based methods struggle and where single-sequence predictions become
essential.

ESMFold achieves atomic-level accuracy for many proteins, though
slightly below AlphaFold2 for proteins that benefit strongly from MSA
information. The accuracy gap is largest for proteins with sparse
evolutionary sampling, where explicit alignments provide information
that single-sequence analysis cannot fully recover. For well-represented
protein families, ESMFold approaches AlphaFold2 accuracy at a fraction
of the computational cost.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-esmfold}{[}High{]} Four-panel figure. Panel A
(Architecture Pipeline): Single sequence â†’ ESM-2 (15B) embeddings â†’
Structure module â†’ 3D coordinates; ``No MSA required.'' Panel B (Speed
Comparison): Bar chart of AlphaFold2 (hours) vs ESMFold (minutes); 60Ã—
speedup. Panel C (Accuracy Comparison): Scatter plot ESMFold vs
AlphaFold2 colored by MSA depth; well-represented proteins both
accurate; sparse MSA proteins ESMFold more robust. Panel D (Metagenomic
Application): Earth Microbiome Project proteins; many lack homologs;
ESMFold enables scale.}

\end{figure}%

\subsection{What ESMFold Reveals About
PLMs}\label{what-esmfold-reveals-about-plms}

ESMFold's success demonstrates that ESM-2's internal representations
encode sufficient information to determine 3D structure. The language
model has learned not merely local sequence patterns but global folding
principles, capturing what makes a sequence fold into a particular
three-dimensional shape.

This has profound implications for understanding what protein language
models learn. The attention patterns that emerge from masked prediction
are, in some sense, learning the physics of protein folding. Residues
that need to be close in 3D space to maintain stability attend to each
other in the transformer's attention matrices. The statistical patterns
in protein sequences, shaped by billions of years of evolution under
physical constraints, encode structural information that sufficiently
powerful language models can decode.

The fundamental insight is that evolution has already solved the
structure prediction problem, millions of times over, and recorded the
solutions in sequence databases. Language models learn to read those
solutions, extracting the implicit structural knowledge that selection
has embedded in surviving sequences.

\section{Function Prediction}\label{function-prediction}

Beyond structure, protein language models enable prediction of protein
function directly from sequence. Function prediction encompasses
multiple tasks: predicting Gene Ontology terms that describe molecular
function, biological process, and cellular component; classifying enzyme
activity; identifying binding sites and interaction partners; and
predicting subcellular localization.

Traditional function prediction relied on homology: proteins similar in
sequence are assumed to share function. This approach fails for orphan
proteins lacking characterized homologs and cannot distinguish
functional differences between closely related sequences. PLM-based
approaches address both limitations by learning representations that
capture functional signatures beyond simple sequence similarity.

For Gene Ontology term prediction, PLM embeddings serve as input
features to classification models that predict which GO terms apply to
each protein. The embeddings capture evolutionary and structural
information relevant to function, enabling accurate predictions even for
proteins with limited homology to characterized sequences. Performance
improves with embedding quality, suggesting that larger language models
capture more functionally relevant information.

Enzyme classification benefits similarly from PLM representations. The
Enzyme Commission hierarchy categorizes enzymes by the reactions they
catalyze, from broad classes (oxidoreductases, transferases) to specific
substrate preferences. PLM embeddings distinguish these categories
effectively, capturing the sequence features that determine catalytic
activity without requiring explicit structural analysis.

Binding site prediction applies attention analysis to identify which
residues participate in ligand binding, protein-protein interactions, or
nucleic acid recognition. Positions that the model identifies as
important for contextual prediction often correspond to functionally
important sites, including binding pockets and catalytic residues. This
capability enables rapid identification of functional sites in newly
sequenced proteins.

\section{Variant Effect Prediction}\label{variant-effect-prediction-1}

A critical clinical application of protein language models is predicting
the effects of amino acid substitutions. Missense variants are the most
common type of protein-coding mutation, and clinical genetics pipelines
must routinely assess whether specific substitutions are likely
pathogenic or benign. The traditional approach required either direct
experimental characterization or computational methods trained on
labeled pathogenicity data, both of which scale poorly to the millions
of possible variants in each human genome.

ESM-1v demonstrated that PLMs can predict variant effects without any
training on variant labels (Meier et al. 2021). The approach exploits
the masked language modeling objective directly: for a variant at
position \(i\) changing amino acid \(a\) to amino acid \(b\), compute
the log-likelihood ratio:

\[\Delta \text{score} = \log P(b \mid \text{context}) - \log P(a \mid \text{context})\]

If the model assigns higher probability to the mutant amino acid than
the wild-type, the variant is predicted benign; if lower, deleterious.
This zero-shot prediction requires no labeled training data. The model's
evolutionary knowledge, learned from sequence databases, directly
informs variant interpretation.

The intuition is straightforward. Evolution has shaped protein sequences
such that certain positions strongly prefer certain amino acids.
Substitutions that violate these preferences are more likely to disrupt
function. The language model captures these preferences through training
on millions of evolutionarily successful sequences. Variants that the
model finds surprising are more likely to be functionally disruptive.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-plm-variant-scoring}{[}High{]} Four-panel figure.
Panel A (The Scoring Mechanism): Protein sequence with variant position;
P(ref\textbar context) vs P(var\textbar context); Score = log P(var) -
log P(ref). Panel B (Intuition): Evolution tested billions of
substitutions; low probability variants = evolutionarily disfavored =
likely disruptive. Panel C (Benchmark Performance): ROC curves ESM-1v vs
classical methods on DMS data; competitive without variant labels. Panel
D (AlphaMissense Enhancement): ESM embeddings + AlphaFold2 structural
features; combined model; 71M precomputed scores; performance boost from
structure.}

\end{figure}%

Brandes and colleagues applied ESM-1b to predict effects for all
approximately 450 million possible missense variants in the human
genome, providing a precomputed resource for clinical variant
interpretation (Brandes et al. 2023). On ClinVar benchmarks, ESM-1b
outperformed existing methods in classifying variants as pathogenic or
benign.

AlphaMissense extended this approach by combining PLM representations
with structural context from predicted protein structures (J. Cheng et
al. 2023). The integration of sequence-based and structure-based signals
improves accuracy, particularly for variants affecting protein stability
or buried residues. AlphaMissense provides predictions for all
approximately 71 million possible single amino acid substitutions in the
human proteome.

The detailed comparison of variant effect prediction methods, including
how PLM-based scores integrate with clinical classification frameworks,
is covered in Chapter~\ref{sec-vep-fm}. Here, the key point is that
protein language models provide the foundational representations that
make accurate zero-shot variant prediction possible.

\section{Integration with Structure
Prediction}\label{integration-with-structure-prediction}

Protein language models exist within a broader ecosystem of
computational methods for protein analysis. Understanding how PLMs
relate to structure prediction systems clarifies their role and
capabilities.

AlphaFold2 achieved breakthrough accuracy in structure prediction by
combining learned representations with explicit geometric modeling
(Jumper et al. 2021). The architecture processes both sequence
information through embeddings and evolutionary information through
multiple sequence alignments, using an attention-based module
(Evoformer) to integrate these signals before predicting atomic
coordinates. AlphaFold2's success depended critically on MSA quality:
proteins with many homologs could be predicted accurately, while orphan
proteins remained challenging.

ESMFold demonstrated that PLM embeddings can replace MSA-derived
features, achieving competitive accuracy without the alignment
bottleneck. This finding clarified the relationship between language
models and structure prediction: PLMs learn to compress evolutionary
information into representations that are functionally equivalent to
explicit alignments, at least for proteins with sufficient
representation in training databases.

AlphaFold3 extended structure prediction to protein complexes, nucleic
acids, and small molecules (Abramson et al. 2024). The architecture
incorporates diffusion-based generation, enabling prediction of binding
poses and complex assemblies. These capabilities complement PLM-based
function prediction by providing structural context for interpreting
functional predictions.

Generative protein design methods including RFDiffusion and ProteinMPNN
leverage both structural and sequence information (Watson et al. 2023;
Dauparas et al. 2022). RFDiffusion generates novel protein backbones
through diffusion processes conditioned on design objectives.
ProteinMPNN designs sequences likely to fold into specified structures.
Both methods benefit from PLM representations when designing sequences
with desired functional properties, demonstrating how language models
integrate into the broader protein engineering pipeline.

The trajectory from ESM to ESMFold to integration with design tools
illustrates how PLMs serve as a foundation for diverse downstream
applications. The representations learned through self-supervised
training transfer across tasks, providing a common language for
structure prediction, function annotation, variant interpretation, and
protein engineering. This pattern of foundation models enabling diverse
applications recurs throughout genomic AI, as discussed in
Chapter~\ref{sec-fm-principles}.

\section{Limitations}\label{limitations}

Despite their success, protein language models face several limitations
that inform the development of genomic models and guide appropriate
application.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-plm-limitations}{[}Enhancing{]} Grid of limitation
categories with visual examples. Orphan Proteins: Phylogenetic tree with
isolated lineage; no homologs = no evolutionary context; performance
degradation curve. Novel Folds: Designed protein with non-natural
topology; predictions unreliable outside training. Conformational
Flexibility: Protein with multiple conformations; PLM produces single
embedding. Epistasis: Two distant positions; individual mutations
benign; combination deleterious; models assume independence.
Interpretability: Attention correlates with biology but mechanism
remains opaque.}

\end{figure}%

\subsection{Orphan and Dark Proteins}\label{orphan-and-dark-proteins}

PLMs learn from evolutionary statistics, performing best for proteins
with rich representation in training databases. Orphan proteins, those
unique to specific lineages without detectable homologs, lack the
evolutionary context that PLMs exploit. For these proteins, the model
has no basis for distinguishing likely from unlikely amino acids at each
position, and predictions degrade accordingly.

The problem extends to ``dark'' proteins that are poorly characterized
despite having homologs. If an entire protein family has escaped
experimental characterization, PLMs may learn statistical patterns
without capturing functional relevance. The model cannot distinguish
constraint imposed by function from constraint imposed by historical
accident.

\subsection{Novel Folds}\label{novel-folds}

Training data shapes what models can predict. PLMs trained on natural
protein databases learn the statistical patterns of naturally occurring
folds, potentially struggling with designed proteins or hypothetical
folds outside the training distribution. When researchers design
proteins with novel topologies not found in nature, PLM predictions
become less reliable because the relevant sequence patterns were never
encountered during training.

\subsection{Conformational
Flexibility}\label{conformational-flexibility}

Most PLM representations assume a single static structure, but many
proteins adopt multiple conformations relevant to function. Allosteric
proteins, intrinsically disordered regions, and proteins that undergo
conformational changes upon binding present challenges for methods that
embed each sequence into a single representation. The language model
learns the average properties of sequences but may not capture the
dynamic range that determines biological behavior.

\subsection{Epistasis}\label{epistasis}

Most variant effect predictions assume independence: the effect of
mutation A does not depend on whether mutation B is present. Real
proteins exhibit epistasis, where variant effects depend on sequence
context. Two individually benign variants may be jointly deleterious if
they disrupt compensatory interactions. Current PLM-based predictors
model marginal effects at each position but do not explicitly capture
higher-order interactions, though the contextual embeddings may
represent some epistatic relationships implicitly.

\subsection{Interpretability}\label{interpretability}

While attention patterns correlate with biological features,
understanding exactly what PLMs learn remains challenging. The field is
developing interpretation methods (Chapter~\ref{sec-interpretability}),
but PLMs remain partially opaque. For clinical applications where
explanations matter, this interpretability gap limits adoption. A
prediction that a variant is pathogenic is more useful when accompanied
by mechanistic insight into why the variant disrupts function.

\section{Lessons for Genomic Foundation
Models}\label{lessons-for-genomic-foundation-models}

The success of protein language models established principles that
guided subsequent development of genomic foundation models. These
lessons transfer with appropriate modifications to DNA and RNA modeling.

\subsection{Self-Supervision Captures Biological
Knowledge}\label{self-supervision-captures-biological-knowledge}

PLMs demonstrated that massive amounts of biological knowledge can be
learned from unlabeled sequences. The same evolutionary pressures that
shape proteins also shape DNA. Purifying selection removes deleterious
variants, leaving statistical signatures in sequence databases that
self-supervised models can exploit. This principle underlies the entire
foundation model paradigm: sufficiently large models trained on
sufficiently large datasets with appropriate objectives will learn
representations that capture biological function.

\subsection{Scale Yields Consistent
Improvements}\label{scale-yields-consistent-improvements}

Performance improves predictably with model size through the range
currently explored. The progression from 8 million to 15 billion
parameters in ESM-2 showed consistent gains across structure prediction,
contact prediction, and variant effect tasks. While scaling cannot
continue indefinitely, current models remain in a regime where
additional capacity yields reliable improvements. This relationship
justified the computational investment in large genomic foundation
models.

\subsection{Transfer Learning is
Effective}\label{transfer-learning-is-effective}

Representations learned for one task (masked token prediction) transfer
to other tasks (structure prediction, variant effects, function
annotation). This suggests that self-supervised pretraining captures
fundamental biological knowledge rather than task-specific shortcuts. A
model trained to predict masked amino acids simultaneously learns about
protein structure, function, evolutionary constraint, and disease
relevance. The same principle motivates genomic language models: models
trained to predict masked nucleotides may simultaneously learn about
regulatory elements, evolutionary conservation, and variant effects.

\subsection{Architecture Choices Must Match Sequence
Properties}\label{architecture-choices-must-match-sequence-properties}

The BERT-style bidirectional encoder proved effective for proteins,
where entire sequences are typically available and lengths rarely exceed
a thousand residues. Genomic sequences present different challenges:
much longer lengths spanning kilobases to megabases, different
information density with coding regions being dense while intergenic
regions are sparser, and structural features including
reverse-complement relationships absent in proteins. These differences
motivate architectural adaptations in genomic language models, as
explored in Chapter~\ref{sec-dna-lm}.

\subsection{Integration Multiplies
Capability}\label{integration-multiplies-capability}

AlphaMissense demonstrated that PLM embeddings combine effectively with
structural and population genetics information, achieving accuracy
beyond what any single information source provides. The most powerful
methods integrate multiple signals, using PLMs as one component of
larger systems. This principle extends to genomic foundation models,
where sequence-based representations complement rather than replace
functional annotations, chromatin data, and clinical information.

\section{The Paradigm That
Generalized}\label{the-paradigm-that-generalized}

Protein language models established that transformer architectures can
learn deep biological knowledge from sequence alone. ESM's ability to
predict structure, function, and variant effects without explicit labels
demonstrated the power of self-supervised learning on evolutionary data.
The framework validated a paradigm: treat biological sequences as
language, train large models to predict masked tokens, and extract
functional knowledge from learned representations. Attention patterns in
these models capture evolutionary constraint, contact prediction, and
structural relationships without requiring multiple sequence alignments
or explicit structural supervision.

This success directly motivated genomic language models. If proteins
constitute a language that transformers can learn, perhaps DNA does too.
The DNA language models examined in Chapter~\ref{sec-dna-lm} adapt
protein language model architectures and training strategies to the
distinct challenges of genomic sequences: longer contexts, different
alphabets, ambiguous tokenization, and the full complexity of gene
regulation beyond protein coding. RNA language models occupy an
intermediate position, sharing features with both protein and DNA
modeling while addressing the unique challenges of RNA structure and
processing.

The integration path extends beyond sequence modeling. Just as protein
language model representations feed into structure prediction (ESMFold)
and variant effect prediction (AlphaMissense), genomic language model
embeddings integrate into regulatory models
(Chapter~\ref{sec-regulatory}) and clinical applications
(Chapter~\ref{sec-rare-disease}, Chapter~\ref{sec-clinical-risk}).
Protein design methods (Chapter~\ref{sec-design}) demonstrate how
generative modeling builds on the representations that language models
provide. Throughout this progression, the principle that ESM established
remains: self-supervised learning on biological sequences captures
knowledge that transfers across diverse applications, providing a
foundation for genomic AI that subsequent chapters build upon.

\chapter{Regulatory Models}\label{sec-regulatory}

An enhancer 80 kilobases from a promoter can determine whether a gene is
expressed in liver or brain. An insulator 50 kilobases downstream can
block inappropriate activation from a neighboring regulatory domain. A
disease-associated variant in an intergenic region may exert its effect
by disrupting a distal element that contacts its target gene through
chromatin looping. Mammalian gene regulation operates across distances
that dwarf the context windows of most sequence models. The
convolutional architectures examined in Chapter~\ref{sec-cnn} excel at
detecting local motifs but cannot span these distances. A model that
processes sequences in kilobase windows treats regulatory elements tens
of kilobases away as if they do not exist. For understanding human gene
regulation, they effectively do not.

The attention mechanisms introduced in Chapter~\ref{sec-attention} could
theoretically model arbitrary-range dependencies, but naive transformer
application to hundred-kilobase windows is computationally prohibitive.
Attention scales quadratically with sequence length: doubling context
length quadruples memory and computation. Processing 200 kilobases at
single-nucleotide resolution would require attending over 200,000
positions simultaneously, far beyond practical limits. The field needed
architectures that could span regulatory distances without the quadratic
penalty.

Hybrid architectures resolve this tension by combining the strengths of
both paradigms. A convolutional front-end efficiently extracts local
sequence features and compresses the input to a manageable length,
reducing 200 kilobases of sequence to a few thousand feature vectors. A
transformer backbone then propagates information across this compressed
representation through attention. The result is a new class of
regulatory models that can capture enhancer-promoter interactions,
predict the effects of distal variants on gene expression, and provide
mechanistic hypotheses about long-range regulation. Enformer, the first
widely adopted model in this class, processes 200-kilobase windows and
predicts chromatin state, transcription initiation, and gene expression
from sequence alone. This chapter examines Enformer and its successors,
tracing the architectural innovations that enabled long-range regulatory
modeling and assessing what these models reveal about the grammar of
gene regulation.

\section{The Long-Range Regulation
Problem}\label{the-long-range-regulation-problem}

Consider a canonical mammalian gene with complex tissue-specific
expression. The promoter sits at the transcription start site, but the
sequences that determine when and where the gene is expressed may be
scattered across a 200 kilobase neighborhood. Multiple enhancers drive
expression in different tissues; silencers suppress expression in
inappropriate contexts; insulators demarcate regulatory domains.
Chromatin looping brings these distal elements into physical proximity
with the promoter, but the loops themselves are dynamic and
cell-type-specific.

Short-context models face an information-theoretic barrier in this
setting. A model with a 2 kilobase receptive field cannot distinguish a
variant in an enhancer 50 kilobases upstream from a variant in neutral
sequence at the same distance. Both fall outside the model's effective
context. Stacking more convolutional layers or using dilated
convolutions can expand the receptive field, but the computational path
between distant positions grows long, and gradients attenuate over many
layers. Models like Basenji2 (Kelley et al. 2018) pushed convolutional
receptive fields to tens of kilobases through aggressive pooling, but
purely convolutional architectures struggle to propagate information
across hundreds of kilobases without impractical depth.

The scale of the problem becomes concrete when examining
enhancer-promoter distances in the human genome. Median
enhancer-promoter distances in many tissues span 20 to 50 kilobases,
with substantial fractions exceeding 100 kilobases
(\textbf{gasperini\_genome-wide\_2019?}). Topologically associating
domains (TADs), which define the neighborhoods within which regulatory
elements typically interact, range from hundreds of kilobases to several
megabases. A model that cannot span these distances cannot fully capture
the regulatory grammar of the genome.

Attention mechanisms offer a direct solution: by computing pairwise
interactions between all positions, attention can model dependencies
across arbitrary distances in a single layer. The cost is quadratic
scaling with sequence length. A naive transformer operating on 200,000
base pairs at single-nucleotide resolution would require attention
matrices with 40 billion entries, far exceeding practical memory limits.
Hybrid architectures sidestep this constraint by using convolutions to
compress the sequence before attention, reducing the effective sequence
length to a few thousand tokens while preserving the information needed
for long-range modeling.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-long-range-regulation}{[}Essential{]} Four-panel
biological motivation. Panel A (Regulatory Element Distribution):
Canonical gene locus with promoter at TSS, enhancers 20-100kb away,
silencer 50kb downstream, CTCF sites; scale bar 200kb. Panel B (Model
Context Windows): Same locus with overlaid windows: DeepSEA (1kb) sees
promoter only; Basenji2 (40kb) sees proximal regulation; Enformer
(200kb) spans most elements; AlphaGenome (1Mb) full domain. Panel C
(Information-Theoretic Barrier): Variant in enhancer 50kb from gene;
short-context cannot distinguish from neutral; long-context can model
relationship. Panel D (Scale of Problem): Distribution of
enhancer-promoter distances; median 20-50kb; substantial fraction
\textgreater100kb.}

\end{figure}%

\section{Enformer: Attention Meets Regulatory
Genomics}\label{enformer-attention-meets-regulatory-genomics}

Enformer (Å½. Avsec et al. 2021) demonstrated that combining
convolutional compression with transformer attention could dramatically
improve expression prediction from sequence. The model processes 200
kilobase windows of DNA and predicts thousands of chromatin and
transcription tracks across cell types and species, establishing a
template that subsequent models have extended and refined.

\subsection{Architecture}\label{architecture}

The Enformer architecture consists of three stages that progressively
transform raw sequence into multi-task predictions.

The convolutional stem takes one-hot encoded DNA (four channels for A,
C, G, T) and applies a series of convolutional blocks with residual
connections. Each block includes convolutions that detect local
patterns, batch normalization and nonlinearities, and pooling operations
that reduce sequence length while increasing channel depth. By the end
of the stem, a 200 kilobase input has been compressed to roughly 1,500
tokens, each representing approximately 128 base pairs of underlying
sequence. This compression is crucial: it reduces the attention
computation from quadratic in 200,000 to quadratic in 1,500, a reduction
of roughly 17,000-fold in memory requirements.

The transformer trunk operates on the compressed sequence through a
stack of self-attention layers. Each layer computes attention scores
between all pairs of positions, allowing information to flow directly
between any two locations in the 200 kilobase window. Relative
positional encodings preserve information about the distances between
elements, which matters for regulatory biology where the spacing between
motifs often carries functional significance. The combination of
multi-head attention and feed-forward layers enables the model to learn
complex, position-dependent relationships across the full window.

Task-specific output heads branch from the shared transformer backbone.
Separate heads predict different types of outputs: DNase accessibility
and ATAC-seq signal (chromatin openness), histone modifications
including H3K4me3, H3K27ac, and other marks, CAGE signal reflecting
transcription initiation, and additional functional genomics readouts
where training data is available. Each head consists of convolutional
and linear layers that transform the shared representation into
track-specific predictions.

The multi-task design serves multiple purposes. Different assays provide
complementary supervision: chromatin accessibility reflects regulatory
potential, histone marks indicate active enhancers and promoters, and
CAGE captures transcriptional output. Training on all assays jointly
encourages the backbone to learn representations that capture the full
regulatory cascade from accessible chromatin through enhancer activation
to transcription initiation.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-enformer-architecture}{[}Essential{]} Detailed
three-stage architecture. Stage 1 (Convolutional Stem): 200kb one-hot
input â†’ conv blocks with residual connections â†’ pooling â†’
\textasciitilde1,500 tokens (128bp resolution); ``Compresses 100Ã—,
detects local features.'' Stage 2 (Transformer Trunk): Stack of
self-attention layers, 8 heads, relative positional encodings; ``Enables
long-range interaction modeling.'' Stage 3 (Task-Specific Heads): Shared
backbone â†’ multiple outputs (DNase, histone marks, CAGE,
\textasciitilde5,000 tracks); ``Multi-task regularizes
representations.'' Inset: Why Hybrid Works---quadratic on 1,500 vs
200,000 tokens.}

\end{figure}%

\subsection{Training Data and Cross-Species
Learning}\label{training-data-and-cross-species-learning}

Enformer trains on functional genomics data from both human and mouse,
spanning hundreds of assays and cell types. The chromatin accessibility,
histone modification, and transcription initiation assays introduced in
Chapter~\ref{sec-data} provide the supervision signals: DNase-seq and
ATAC-seq measure regulatory potential, ChIP-seq for histone marks
identifies active enhancers and promoters, and CAGE captures where
transcription begins. Human training data derives largely from ENCODE
and Roadmap Epigenomics consortia, supplemented by CAGE data from FANTOM
and additional chromatin profiling studies. Mouse data from analogous
consortia provides complementary supervision.

Cross-species training confers several advantages. Regulatory sequences
that are functionally constrained evolve more slowly than neutral
sequence, so mouse and human share many regulatory motifs and principles
despite 80 million years of divergence. Training on both species helps
the model distinguish conserved regulatory logic from species-specific
noise, reduces overfitting to idiosyncrasies of human data, expands the
effective training set without requiring additional human samples, and
implicitly emphasizes evolutionarily conserved patterns that are more
likely to be functionally important.

The training objective combines losses across all tracks, positions, and
species. Count-based likelihoods (Poisson or negative binomial) handle
sequencing-derived signals, while correlation-based objectives ensure
the model captures the overall shape of coverage profiles. Per-track
weighting prevents abundant assays from dominating gradients.

\subsection{Variant Effect
Prediction}\label{variant-effect-prediction-2}

The clinical and scientific value of Enformer lies substantially in its
ability to predict how sequence variants alter regulatory activity. The
procedure follows a straightforward logic: extract a 200 kilobase window
containing the variant, compute predictions for the reference allele,
compute predictions for the alternative allele, and compare the outputs
across all tracks and positions.

The resulting variant effect scores span thousands of dimensions, one
for each assay and cell type. A variant might increase predicted DNase
accessibility in one cell type while decreasing predicted CAGE signal in
another, suggesting context-dependent regulatory effects. By aggregating
predictions around gene promoters, researchers can estimate variant
effects on gene expression in specific tissues.

Validation against GTEx expression quantitative trait loci (eQTLs)
demonstrated that Enformer's predictions correlate with observed genetic
effects on expression (Å½. Avsec et al. 2021). Variants with large
predicted effects on promoter-proximal CAGE signal were enriched among
significant eQTLs. Notably, this correlation extended to distal
variants: sequence changes 50 kilobases or more from a gene's
transcription start site still showed predictive power when they fell in
regions of predicted regulatory activity. This long-range predictive
capacity distinguishes Enformer from short-context models and validates
the architectural investment in extended context windows.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-regulatory-vep-workflow}{[}High{]} Workflow diagram.
Step 1: Variant position â†’ extract 200kb window centered; reference and
alternative versions. Step 2: Parallel forward passes â†’ output 5,000+
tracks Ã— positions. Step 3: Delta computation (alt - ref) â†’ identify
tracks/positions with largest changes. Step 4: Interpretation
examples---DNase â†“ in liver (reduced accessibility), H3K27ac â†“ at
enhancer (weakened activity), CAGE â†“ at target gene (predicted
expression decrease). Validation inset: Correlation with GTEx eQTLs;
performance on distal variants (50+ kb).}

\end{figure}%

\section{Borzoi: From Chromatin to
Transcriptome}\label{borzoi-from-chromatin-to-transcriptome}

While Enformer predicts transcription initiation through CAGE, RNA-seq
captures a richer picture of gene expression: not just where
transcription begins, but how the transcript is spliced, which isoforms
dominate, where transcription terminates, and how stable the resulting
mRNA is. Borzoi (Linder et al. 2025) extends the hybrid architecture
paradigm to predict full RNA-seq coverage profiles, enabling a unified
view of how sequence variation affects the entire transcriptional
program.

\subsection{Motivation}\label{motivation}

A single gene can produce multiple transcript isoforms through
alternative promoter usage, alternative splicing, and alternative
polyadenylation. These isoforms may have different stabilities,
different translation efficiencies, and different functions. A variant
that shifts isoform ratios without changing total expression could have
substantial phenotypic consequences: a switch from a cytoplasmic to a
nuclear isoform, for instance, or inclusion of a premature stop codon in
the predominant transcript.

CAGE and chromatin assays cannot capture these complexities. They
measure where transcription might begin and what the chromatin
environment looks like, but they do not reveal how RNA polymerase
traverses the gene body, where splicing occurs, or which 3' end is
selected. RNA-seq coverage profiles encode all of this information: exon
boundaries appear as coverage drops at intron junctions, alternative
splicing manifests as variable junction usage, and polyadenylation site
choice appears in the coverage pattern near gene 3' ends.

\subsection{Architecture and
Training}\label{architecture-and-training-2}

Borzoi builds on an Enformer-style backbone with modifications tailored
to RNA-seq prediction. The convolutional stem and transformer trunk
follow similar principles, compressing long input windows and
propagating information through attention. Output heads predict stranded
RNA-seq coverage across the window, with additional heads for
complementary signals like PRO-seq (nascent transcription), CAGE, and
other assays when available.

Training on RNA-seq coverage imposes different demands than training on
chromatin marks. Coverage varies over orders of magnitude between
introns and exons; the model must capture both the overall expression
level and the fine structure of the coverage profile. Junction reads
that span splice sites provide particularly informative supervision, as
they directly constrain the model to learn splicing patterns. The loss
function balances accurate prediction of coverage levels with faithful
reproduction of the coverage shape, including sharp transitions at exon
boundaries.

\subsection{Applications Beyond Expression
Level}\label{applications-beyond-expression-level}

By predicting full RNA-seq coverage, Borzoi enables analyses that go
beyond simple expression quantification. Splicing variant effects can be
assessed by comparing predicted coverage at exons and junctions under
reference and alternative alleles. A variant that reduces predicted
junction reads for a particular exon suggests exon skipping; increased
junction reads to a cryptic splice site suggests aberrant splicing.
These predictions complement specialized splicing models like SpliceAI
(Chapter~\ref{sec-cnn}), providing additional context about how splicing
changes fit within the broader transcriptional program.

Alternative promoter usage becomes visible through coverage patterns
near transcription start sites. A variant that increases coverage
downstream of one TSS while decreasing it downstream of another suggests
a shift in promoter preference. Such shifts can alter the 5' UTR of the
resulting transcript, affecting translation efficiency and regulatory
motif content.

Polyadenylation site choice affects 3' UTR length and content. Shorter
3' UTRs may escape microRNA-mediated repression; longer ones may include
additional regulatory elements. Borzoi's coverage predictions around
annotated polyadenylation sites can reveal variants that shift site
usage, potentially explaining effects on mRNA stability and translation
that would be invisible to chromatin-based models.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-borzoi-rnaseq}{[}High{]} Four-panel figure. Panel A
(What RNA-seq Captures): Gene with alternative isoforms; CAGE sees only
TSS; RNA-seq coverage shows exon structure, splice junctions,
polyadenylation. Panel B (Borzoi Predictions): Example gene with complex
splicing; predicted stranded coverage profile; exon boundaries visible.
Panel C (Beyond Expression Level): Splicing effects (changed junction
usage), alternative promoters (shifted TSS), polyadenylation (altered 3'
UTR length); each with clinical relevance. Panel D (Integration): Borzoi
predictions + SpliceAI scores; complementary information.}

\end{figure}%

\section{Sei: A Regulatory Vocabulary from
Sequence}\label{sei-a-regulatory-vocabulary-from-sequence}

While Enformer and Borzoi predict continuous coverage tracks, Sei
(\textbf{chen\_sequence\_2022?}) takes a complementary approach:
learning a discrete vocabulary of sequence classes that capture distinct
regulatory activities. Rather than predicting thousands of individual
assays, Sei maps sequences to a reduced set of regulatory states, each
associated with characteristic chromatin and transcription patterns.

\subsection{Approach}\label{approach}

Sei builds on observations that chromatin states cluster into
interpretable categories: active promoters, strong enhancers, poised
enhancers, heterochromatin, and so forth. Previous methods like ChromHMM
defined such states from observed chromatin marks in specific cell
types. Sei learns to predict sequence class membership directly from
DNA, asking what regulatory identity a sequence carries based on its
intrinsic properties.

The model predicts 40 sequence classes derived from clustering patterns
across chromatin accessibility, histone modifications, and transcription
factor binding. Each class corresponds to a recognizable regulatory
state: promoter-like sequences, enhancer-like sequences, CTCF binding
sites, repressed regions, and various intermediate states. The output is
not a single class assignment but a probability distribution over
classes, reflecting the observation that many sequences have
context-dependent regulatory potential.

\subsection{Complementary to Track
Prediction}\label{complementary-to-track-prediction}

Sei and Enformer-style models serve complementary purposes. Enformer
provides detailed, quantitative predictions across specific assays and
cell types; Sei provides a compressed, interpretable summary of
regulatory identity. For variant interpretation, both perspectives can
be valuable. Enformer might reveal that a variant reduces predicted
H3K27ac signal in liver but not heart; Sei might reveal that the same
variant shifts sequence class membership from ``strong enhancer'' toward
``weak enhancer,'' a more immediately interpretable characterization.

The regulatory vocabulary approach also facilitates systematic analysis
across many variants. Rather than tracking changes in thousands of
individual tracks, researchers can ask how a set of variants affects the
distribution of regulatory classes, identifying patterns that might be
obscured in high-dimensional track space.

\section{AlphaGenome: Unifying Modalities at Megabase
Scale}\label{alphagenome-unifying-modalities-at-megabase-scale}

AlphaGenome (Z. Avsec, Latysheva, and Cheng 2025) extends the hybrid
modeling paradigm in two directions: longer context windows
(approximately one megabase) and broader output modalities spanning
chromatin, expression, splicing, and three-dimensional contacts. The
goal is a single model that provides a comprehensive view of how
sequence determines regulatory state.

\subsection{Architectural Extensions}\label{architectural-extensions}

The megabase context window pushes against computational limits even
with hybrid architectures. AlphaGenome addresses this through efficient
attention mechanisms that reduce the quadratic cost, hierarchical
processing that handles different output modalities at appropriate
resolutions, and architectural refinements accumulated from Enformer and
Borzoi development.

The output repertoire spans chromatin accessibility and histone
modifications (following Enformer), gene expression and RNA coverage
(following Borzoi), splicing predictions including exon inclusion and
junction usage, and contact predictions reflecting three-dimensional
chromatin organization.

Unifying these modalities in a single model offers several advantages.
The backbone representation must capture information relevant to all
outputs, encouraging learning of features that connect chromatin state
to transcription to RNA processing. Variant effect predictions become
coherent across modalities: a single forward pass reveals how a variant
affects chromatin, expression, splicing, and contacts, rather than
requiring separate runs through independent models.

\subsection{Access and Practical
Considerations}\label{access-and-practical-considerations}

AlphaGenome is primarily available through an API interface rather than
as a downloadable model. This arrangement simplifies use for many
applications: researchers can score variants without managing large
model weights or specialized hardware. It also introduces constraints
around data privacy, customization, and integration with local
pipelines. Clinical applications that cannot send patient sequence data
to external services may be unable to use API-only models directly,
motivating interest in openly available alternatives.

From the perspective of variant interpretation workflows, AlphaGenome
provides a comprehensive set of predictions from a single query. A
variant can be assessed for effects on local chromatin state, expression
of nearby genes, splicing of overlapping transcripts, and potential
disruption of chromatin contacts, all from the same underlying model.
The challenge lies in synthesizing these multiple outputs into
actionable conclusions, a topic addressed further in
Chapter~\ref{sec-vep-fm}.

\section{What Hybrid Architectures
Accomplish}\label{what-hybrid-architectures-accomplish}

The progression from DeepSEA through Enformer, Borzoi, and AlphaGenome
reflects accumulating solutions to specific limitations. Each model
addresses constraints that bounded its predecessor's utility.

\subsection{Spanning Enhancer-Promoter
Distances}\label{spanning-enhancer-promoter-distances}

The most direct contribution is enabling long-range interaction
modeling. A 200 kilobase context window encompasses the distances over
which most cis-regulatory interactions occur. Attention mechanisms allow
the model to learn direct relationships between enhancers and promoters
without requiring information to propagate through many intermediate
layers. Empirically, this translates to improved prediction of
expression and better correlation with eQTLs, particularly for variants
in distal regulatory elements.

\subsection{Multi-Task Regularization}\label{multi-task-regularization}

Training on hundreds of assays jointly constrains the model to learn
representations that generalize across regulatory modalities. A feature
useful only for predicting H3K4me3 in one cell type provides less
gradient signal than a feature useful across chromatin, transcription,
and accessibility. This multi-task pressure steers the model toward
learning fundamental regulatory logic rather than assay-specific
artifacts.

\subsection{Cross-Species Constraints}\label{cross-species-constraints}

Training on human and mouse together further regularizes the model.
Species-specific binding site variants, repetitive elements, and
technical artifacts in training data affect one species but not the
other. Features that generalize across species are more likely to
reflect conserved regulatory mechanisms. This provides a form of
evolutionary validation built into the training process.

\subsection{Unified Variant Effect
Prediction}\label{unified-variant-effect-prediction}

Perhaps most practically valuable, hybrid models provide a unified
framework for variant effect prediction on expression and related
phenotypes. Rather than assembling scores from multiple specialized
models, researchers can query a single model for comprehensive
predictions. The outputs span cell types and assays, enabling
tissue-specific interpretation of regulatory variants.

\section{Limitations and Open
Challenges}\label{limitations-and-open-challenges-1}

Despite their power, long-context regulatory models face fundamental
limitations that bound their current utility and define directions for
future development.

\subsection{Training Data Constraints}\label{training-data-constraints}

Functional genomics data is biased in coverage, overrepresenting
well-studied cell types (embryonic stem cells, K562, HepG2,
lymphoblastoid cell lines) while leaving many tissue types and
disease-relevant cell states poorly covered. Models trained on available
data will perform better in represented contexts and may fail silently
in underrepresented ones. Ancestry bias compounds the problem: most
functional genomics studies derive from individuals of European descent,
limiting the diversity of haplotypes and regulatory variants represented
in training data.

These biases propagate to variant effect predictions. A variant in a
regulatory element active primarily in pancreatic beta cells may receive
poor predictions if beta cell data is sparse in training. A variant on a
haplotype common in African populations but rare in Europeans may fall
outside the model's effective training distribution. Users must
recognize that prediction confidence varies with representation in
training data, a consideration that current models do not explicitly
communicate.

\subsection{Finite Context}\label{finite-context}

Even megabase-scale windows capture only local regulation. Trans-acting
factors, three-dimensional contacts spanning multiple megabases, and
whole-chromosome organization fall outside model context. Structural
variants that rearrange large genomic segments, duplicate enhancers, or
create novel fusion genes cannot be modeled within fixed-window
architectures. The reference genome assumption underlying these models
further limits their applicability to complex haplotypes and populations
with substantial structural variation relative to the reference.

\subsection{Missing Three-Dimensional
Context}\label{missing-three-dimensional-context}

Linear sequence models treat DNA as a one-dimensional string, but gene
regulation occurs in three-dimensional nuclear space. Chromatin loops
bring distal elements into proximity; nuclear compartmentalization
segregates active and repressed regions; phase-separated condensates
concentrate regulatory factors. While AlphaGenome predicts some contact
features, current hybrid models do not fully integrate three-dimensional
chromatin organization. The relationship between linear sequence,
three-dimensional structure, and regulatory output remains incompletely
captured. We return to this gap in Chapter~\ref{sec-3d-genome}, which
examines models that explicitly address chromatin architecture.

\subsection{Correlation Versus
Causation}\label{correlation-versus-causation}

Hybrid models learn correlations between sequence and functional
readouts, not causal mechanisms. A variant might receive a high
predicted effect score because it disrupts a motif correlated with
expression in training data, not because the motif causally drives
expression. Attribution methods can identify which sequence features
contribute to predictions, but attribution is not validation.
High-confidence predictions require experimental confirmation through
approaches like massively parallel reporter assays, CRISPR perturbation,
or allelic series analysis.

\subsection{Interpretability
Challenges}\label{interpretability-challenges}

The scale of these models (hundreds of millions of parameters) makes
mechanistic interpretation difficult. Attention patterns provide some
insight into which positions the model considers related, but attention
weights are not guaranteed to reflect the model's actual computational
strategy. Attribution methods (saliency maps, integrated gradients) can
highlight important input positions, but the features the model
constructs from those positions remain opaque. Chapter
Chapter~\ref{sec-interpretability} examines these interpretability
methods and their limitations in detail.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-regulatory-limitations}{[}Enhancing{]} Honest
assessment. Panel A (Training Data Bias): Pie chart of ENCODE/Roadmap
cell types; overrepresented ESCs, K562, HepG2; underrepresented
disease-relevant states. Panel B (Missing 3D Context): Linear sequence
model vs actual looped, compartmentalized chromatin; distant elements in
3D proximity. Panel C (Correlation vs.~Causation): Variant with high
predicted effect; is it because motif disruption causes change, or
correlation in training? Panel D (Finite Context): 200kb-1Mb windows;
trans-acting factors genome-wide; SVs span megabases.}

\end{figure}%

\section{Relationship to Foundation
Models}\label{relationship-to-foundation-models}

Long-context regulatory models occupy an interesting position in the
genomic foundation model landscape. They share key characteristics with
foundation models: large scale, broad training data, strong performance
across tasks, and utility as feature extractors for downstream
applications. Yet they differ from self-supervised DNA language models
in their heavy reliance on supervised, task-specific training signals.

Enformer and its descendants can be viewed as highly specialized
foundation models, pre-trained on the specific task of regulatory
prediction and adaptable to related applications. Their representations
encode regulatory logic learned from functional genomics supervision,
complementing the sequence patterns learned by self-supervised models
from raw DNA. In practice, the two approaches may prove most powerful in
combination: self-supervised models provide sequence representations
from evolutionary context, while supervised regulatory models provide
representations from functional genomics context. Integrating these
representations for tasks like variant effect prediction is an active
area of development, explored further in Chapter~\ref{sec-vep-fm}.

From a practical standpoint, hybrid regulatory models remain among the
most directly useful genomic deep learning systems for variant
interpretation. They provide quantitative, tissue-specific predictions
for regulatory variants, outperform short-context alternatives on distal
regulatory elements, and integrate naturally into variant prioritization
workflows. Their limitations are real but understood; their strengths
are substantial and empirically validated.

\section{Prediction Without
Explanation}\label{prediction-without-explanation}

The models examined in this chapter demonstrate that long-range
regulatory prediction from sequence is tractable. Enformer established
that hybrid CNN-transformer architectures could span 200 kilobases and
predict expression-related chromatin features. Borzoi extended coverage
to the full transcriptome with improved quantitative accuracy.
AlphaGenome unified multiple regulatory modalities at megabase scale,
predicting chromatin accessibility, histone modifications, transcription
factor binding, and gene expression from a single architecture. Each
generation captures more of the regulatory landscape with greater
fidelity to experimental measurements.

Yet these models predict regulatory outcomes without explaining
regulatory mechanism. They learn that certain sequence patterns
associate with certain expression levels, but they do not represent
enhancer-promoter contacts, transcription factor cascades, or the causal
chain from sequence to phenotype. The attention patterns that span long
distances may correspond to genuine regulatory interactions or may
reflect confounded sequence features that happen to predict expression.
Interpretability methods (Chapter~\ref{sec-interpretability}) can probe
what patterns models have learned, but high prediction accuracy does not
guarantee mechanistic insight.

This distinction shapes how regulatory model predictions should be used.
For variant effect prediction (Chapter~\ref{sec-vep-fm}), regulatory
models provide one input among several: they predict whether a variant
alters chromatin accessibility or expression, while protein language
models assess coding consequences and evolutionary models quantify
constraint. The clinical integration of these signals
(Chapter~\ref{sec-rare-disease}) requires understanding what each model
contributes and where each is likely to fail. Regulatory models excel at
predicting noncoding variant effects when the relevant cell type is
represented in training data; they struggle with cell types absent from
training and with variants acting through mechanisms not captured by the
output tracks they predict.

\chapter{Variant Effect Prediction}\label{sec-vep-fm}

Classical variant effect prediction required labels: pathogenic variants
to define one class, benign variants to define another, and enough
examples of each to train a classifier. This requirement created a
fundamental bottleneck. The variants most important to classify, those
that are rare, never before observed, and located in poorly
characterized genes, were precisely those for which labels did not
exist. Foundation models offer a different paradigm: score variants
using patterns learned from unlabeled sequences, without ever seeing a
pathogenic/benign label during pretraining. A protein language model
trained only to predict masked amino acids can distinguish damaging
substitutions from benign polymorphisms because evolution has already
encoded this distinction in the sequences that survived. A DNA language
model can identify regulatory disruptions because it learned the grammar
of functional elements from billions of nucleotides. The variants that
violate learned patterns are the variants that disrupt function.

This zero-shot capability does not eliminate the need for labeled data
but changes its role. Rather than training classifiers from scratch,
practitioners fine-tune foundation models on modest variant datasets,
leveraging pretrained knowledge to achieve performance impossible for
models that start from random initialization. The combination of
self-supervised pretraining and supervised fine-tuning produces variant
effect predictors that outperform classical methods across most
benchmarks while requiring far less task-specific data. AlphaMissense,
ESM-1v, and similar systems demonstrate that foundation model
representations capture variant effects across protein families,
including families with no labeled variants in training data.

Yet significant challenges remain. Foundation models predict that
variants are damaging without explaining why. Calibration varies across
variant types, protein families, and populations, creating uncertainty
about when predictions can be trusted. The distinction between
``evolutionarily unusual'' and ``clinically pathogenic'' is real: not
every rare substitution causes disease, and not every disease-causing
variant appears evolutionarily constrained. This chapter examines how
foundation models transform variant effect prediction, what they add
beyond classical methods, and where the gaps between computational
prediction and clinical interpretation remain substantial.

\section{The Foundation Model Paradigm for Variant
Interpretation}\label{the-foundation-model-paradigm-for-variant-interpretation}

Classical variant effect predictors operate by aggregating hand-crafted
features: conservation scores computed from multiple sequence
alignments, amino acid property changes, protein domain annotations, and
regulatory marks at genomic loci (Chapter~\ref{sec-vep-classical}).
Methods like CADD train machine learning models to distinguish
pathogenic from benign variants using these features, achieving useful
discrimination but ultimately limited by what features the developers
chose to include. When a variant falls in a region poorly covered by
existing annotations, classical methods have little to offer.

Foundation models invert this relationship. Rather than engineering
features, they learn representations from raw sequence data during
pretraining, then apply those representations to variant interpretation.
A protein language model trained to predict masked amino acids
implicitly learns which substitutions violate evolutionary constraints.
A DNA language model trained to predict nucleotides in genomic context
learns which changes disrupt sequence grammar. The representations
encode information about structure, function, and constraint that was
never explicitly labeled during training.

This paradigm shift has practical consequences. Coverage extends to any
variant in any gene, not just those with extensive prior annotation.
Representations capture subtle patterns (co-evolution between distant
residues, context-dependent motif strength) that resist manual feature
engineering. Transfer learning enables rapid adaptation to new tasks and
variant classes. The cost is interpretability: understanding why a
foundation model assigns a particular score requires specialized
analysis techniques rather than simple inspection of feature weights.

Three architectural families dominate current VEP applications. Protein
language models (Chapter~\ref{sec-protein-lm}) encode amino acid
sequences and score missense variants by measuring likelihood changes.
DNA language models (Chapter~\ref{sec-dna-lm}) operate on nucleotide
sequences and can score variants of any type. Regulatory models
(Chapter~\ref{sec-regulatory}) predict molecular phenotypes (chromatin
accessibility, gene expression, splicing) and score variants by their
predicted impact on these phenotypes. The most powerful systems combine
elements from multiple families.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-fm-vep-paradigm}{[}Essential{]} Paradigm comparison.
Panel A (Classical Approach): Variant â†’ hand-crafted features
(conservation, Grantham, domains, regulatory marks) â†’ feature vector â†’
classifier â†’ score; limitation: coverage limited by annotations. Panel B
(Foundation Model Approach): Same variant â†’ pretrained model â†’ learned
representations â†’ adaptation â†’ score; strength: extends to any position.
Panel C (What Changes): Classical features
explicit/interpretable/manual; foundation features
learned/opaque/automatic; trade-off arrow. Panel D (Three Families):
Protein LMs â†’ missense; DNA LMs â†’ all variants; Regulatory models â†’
noncoding mechanism; arrow: ``Most powerful combine families.''}

\end{figure}%

\subsection{Zero-Shot and Supervised
Approaches}\label{zero-shot-and-supervised-approaches}

Foundation model VEP methods divide into two paradigms. Zero-shot
approaches apply pretrained models directly without task-specific
training: ESM-1v scores variants by comparing amino acid likelihoods,
requiring no pathogenicity labels. The model's pretraining objective
(masked token prediction) implicitly teaches which substitutions violate
evolutionary constraints. Supervised approaches like AlphaMissense add
task-specific training layers and optimize explicitly for pathogenicity
prediction using labeled examples.

The choice involves tradeoffs. Zero-shot methods avoid label bias
entirely; they cannot learn to recapitulate existing predictor scores
because they never see those scores during training. Supervised methods
achieve stronger discrimination when high-quality labels exist but risk
inheriting biases from training data. Zero-shot approaches generalize
more reliably to novel proteins outside training distributions;
supervised methods may overfit to well-studied gene families. In
practice, the strongest current systems (AlphaMissense, popEVE) combine
foundation model representations with some supervised adaptation,
attempting to capture benefits of both paradigms.

\section{Protein-Based Variant Effect
Prediction}\label{protein-based-variant-effect-prediction}

Missense variants (single amino acid substitutions) account for
approximately half of known pathogenic variants in ClinVar, making
protein-level prediction a central challenge. Foundation model
approaches exploit a simple insight: evolution has already tested
billions of amino acid substitutions across millions of years; variants
that repeatedly survive natural selection are likely tolerable, while
those never observed in homologous proteins likely disrupt function.

\subsection{Zero-Shot Scoring with Protein Language
Models}\label{zero-shot-scoring-with-protein-language-models}

The simplest foundation model approach to missense VEP requires no
task-specific training. A protein language model trained on masked token
prediction assigns probabilities to each amino acid at each position
given surrounding context. Variant effect scores emerge from comparing
the probability of the reference amino acid to the probability of the
variant amino acid.

ESM-1v operationalizes this approach using the ESM-2 architecture
fine-tuned for single-sequence variant effect prediction (Meier et al.
2021). For a variant substituting amino acid \(a_\text{ref}\) with
\(a_\text{var}\) at position \(i\), the score is computed as:

\[\Delta \text{LLR} = \log P(a_\text{var} | \text{context}) - \log P(a_\text{ref} | \text{context})\]

Negative scores indicate that the variant amino acid is less probable
than reference in learned evolutionary context, suggesting potential
deleteriousness. The model sees only the single query sequence, not
multiple sequence alignments, yet achieves discrimination competitive
with alignment-based methods on deep mutational scanning benchmarks.

This zero-shot capability reflects what protein language models learn
during pretraining: structural constraints (buried positions are
hydrophobic), functional constraints (active sites are conserved), and
co-evolutionary patterns (compensating mutations at contacting
residues). The model has never seen pathogenicity labels, yet its
predictions correlate with disease association because evolution and
disease share underlying biology.

\subsection{Alignment-Based Models: EVE and
popEVE}\label{alignment-based-models-eve-and-popeve}

An alternative approach explicitly models multiple sequence alignments
rather than relying on implicit evolutionary information in
single-sequence representations. EVE (Evolutionary Model of Variant
Effect) fits a variational autoencoder to the MSA for each protein,
learning a generative model that captures position-specific and pairwise
constraints (Frazer et al. 2021). Variant scores derive from the change
in sequence probability under this model.

The EVE architecture consists of an encoder that maps sequences to a
latent space and a decoder that reconstructs sequences from latent
representations. Training maximizes a lower bound on sequence likelihood
across the MSA. For variant scoring, EVE computes the log-likelihood
ratio between mutant and wild-type sequences, capturing how surprising
the substitution appears given the evolutionary record for that specific
protein.

popEVE extends this framework with improved training procedures and
explicit modeling of population allele frequencies (Orenbuch et al.
2025). By incorporating frequency information, popEVE better separates
rare deleterious variants from common benign polymorphisms. The model
achieves strong performance on ClinVar classification while providing
uncertainty estimates through ensemble disagreement.

The tradeoff between single-sequence and MSA-based approaches involves
coverage versus depth. ESM-1v scores any protein sequence without
requiring alignment construction. EVE provides stronger performance when
high-quality MSAs are available but cannot score proteins lacking
sufficient homologs. For well-studied protein families with deep
evolutionary sampling, MSA-based methods remain competitive; for orphan
proteins or rapidly evolving sequences, single-sequence models offer the
only foundation model option.

\subsection{AlphaMissense: Structure-Informed Pathogenicity
Prediction}\label{alphamissense-structure-informed-pathogenicity-prediction}

AlphaMissense represents the current state of the art for proteome-wide
missense pathogenicity prediction, combining protein language model
representations with structural information from AlphaFold2 (J. Cheng et
al. 2023). The system provides precomputed scores for 71 million
possible missense variants across the human proteome, enabling instant
lookup for any variant in any protein-coding gene.

The architecture integrates multiple information sources. Sequence
representations come from a protein language model encoding the
wild-type sequence and mutation position. Structural representations
derive from AlphaFold2 predictions, capturing local geometry (secondary
structure, solvent accessibility, packing density) and longer-range
contacts. A neural network combines these representations to produce a
pathogenicity probability between 0 and 1.

Training uses a carefully constructed dataset that avoids the
circularity plaguing earlier predictors. Rather than training on ClinVar
labels (which themselves derive from computational predictions),
AlphaMissense uses population frequency as a proxy for pathogenicity:
variants common in gnomAD are likely benign, while variants absent from
large population samples and observed in disease contexts are likely
pathogenic. This approach reduces the risk of learning features that
simply recapitulate existing predictor scores.

Calibration receives explicit attention. Raw model outputs undergo
isotonic regression calibration against held-out ClinVar variants,
ensuring that predicted probabilities correspond to observed pathogenic
proportions. A score of 0.8 should mean that 80\% of variants with
similar scores are pathogenic, enabling meaningful clinical
interpretation. AlphaMissense reports calibrated scores along with
discrete classifications (likely pathogenic, likely benign, uncertain)
at thresholds chosen to achieve specific precision targets.

Performance on independent benchmarks substantially exceeds classical
predictors. On deep mutational scanning datasets (where experimental
fitness measurements provide ground truth independent of clinical
labels), AlphaMissense achieves correlations of 0.5 to 0.7 depending on
the assay, compared to 0.3 to 0.5 for CADD or PolyPhen-2. On ClinVar
expert-reviewed variants held out from training, AlphaMissense achieves
AUROC values above 0.9, representing a meaningful improvement over the
0.85 to 0.88 typical of classical methods.

The structural component proves essential for this performance. Ablation
experiments removing AlphaFold2 features degrade performance
substantially, particularly for variants at protein-protein interfaces
and buried core positions where local geometry determines functional
impact. The protein language model captures evolutionary constraint;
structural information explains why that constraint exists.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-alphamissense}{[}Essential{]} Four-panel figure.
Panel A (Input Integration): Protein sequence â†’ ESM-like embeddings;
mutation position â†’ AlphaFold2 structural context (secondary structure,
solvent accessibility, contact density); combined features. Panel B
(Training Strategy): NOT trained on ClinVar; instead population
frequency as pathogenicity proxy; common in gnomAD â†’ likely benign;
absent + disease context â†’ likely pathogenic; reduces circularity risk.
Panel C (Calibration): Raw outputs â†’ isotonic regression â†’ calibrated
probabilities; reliability diagram; score 0.8 = 80\% pathogenic;
thresholds annotated. Panel D (Performance): DMS benchmarks (Ï = 0.5-0.7
vs CADD 0.3-0.5); ClinVar (AUROC \textgreater0.9 vs 0.85-0.88);
structural component essential.}

\end{figure}%

\section{DNA-Based Variant Effect
Prediction}\label{dna-based-variant-effect-prediction}

Approximately 98\% of the human genome lies outside protein-coding
regions, yet noncoding variants contribute substantially to disease risk
through effects on gene regulation, splicing, and genome stability.
Predicting the impact of these variants requires models that operate
directly on DNA sequence rather than translated protein.

\subsection{Splice Variant Prediction with
SpliceAI}\label{splice-variant-prediction-with-spliceai}

Splicing variants illustrate both the promise and current limitations of
deep learning for noncoding VEP. Approximately 10\% of pathogenic
variants in ClinVar act through splicing mechanisms, disrupting the
precise excision of introns from pre-mRNA. Classical approaches relied
on position weight matrices matching consensus splice site sequences,
achieving limited sensitivity for variants outside the core GT-AG
dinucleotides.

SpliceAI applies the dilated convolutional architecture introduced in
Chapter~\ref{sec-cnn} to predict splice site usage from raw DNA sequence
(\textbf{jaganathan\_predicting\_2019?}). The architecture processes
10,000 nucleotides of context through 32 residual blocks with dilated
convolutions (dilation rates increasing from 1 to 128), enabling the
receptive field to span several kilobases while maintaining nucleotide
resolution. Output heads predict splice donor probability, splice
acceptor probability, and junction usage at each position.

For variant effect prediction, SpliceAI compares predictions between
reference and alternate sequences. The delta score quantifies the change
in splice site probability, with positive values indicating gained
splice sites and negative values indicating lost sites. Scores exceeding
0.2 correlate with experimentally validated splicing changes; scores
above 0.5 have high specificity for pathogenic splicing variants.

Clinical deployment has validated SpliceAI's utility. Illumina
integrated the model into their clinical interpretation pipeline, and
multiple diagnostic laboratories use SpliceAI scores as supporting
evidence for ACMG classification. The model identifies pathogenic
splicing variants missed by classical methods, particularly deep
intronic variants that create novel splice sites through cryptic
activation.

Limitations reflect the model's training data. SpliceAI learned from
annotated transcripts representing major isoforms in common tissues.
Tissue-specific alternative splicing, rare isoforms, and developmental
stage-specific patterns fall outside the training distribution. The
model also does not capture downstream consequences: whether a predicted
splicing change produces a functional protein, triggers
nonsense-mediated decay, or has no phenotypic effect requires additional
analysis.

\subsection{Regulatory Variant Prediction with
Enformer}\label{regulatory-variant-prediction-with-enformer}

While SpliceAI addresses one specific noncoding mechanism, regulatory
variants that alter enhancer activity, promoter function, or chromatin
organization require different approaches. Enformer
(Chapter~\ref{sec-regulatory}) predicts multiple molecular phenotypes
(histone modifications, transcription factor binding, chromatin
accessibility, gene expression) from 196,608 base pairs of DNA sequence,
providing a substrate for regulatory VEP (Å½. Avsec et al. 2021).

Variant effect prediction with Enformer compares predicted tracks
between reference and alternate sequences. For a variant in an enhancer,
the model might predict reduced H3K27ac signal and decreased CAGE
expression at the target promoter. These molecular predictions can be
aggregated into variant effect scores, with larger predicted changes
indicating greater functional impact.

Several challenges complicate Enformer-based VEP. The model predicts
relative effects (fold changes in predicted signal) rather than absolute
deleteriousness. Calibrating these predictions against pathogenicity
labels requires additional supervised training. Cell-type specificity
adds complexity: a variant might strongly affect predictions in cardiac
tissue while showing no effect in liver, requiring prior knowledge of
relevant tissues for clinical interpretation.

Sei extends this approach by learning a regulatory vocabulary: clusters
of predicted effects that correspond to interpretable categories like
``active promoter,'' ``strong enhancer,'' or ``CTCF binding site''
(\textbf{chen\_sei\_2022?}). Variant scores reflect shifts between these
categories, providing more interpretable outputs than raw track changes.
A variant that converts an enhancer prediction to a quiescent state has
clearer implications than one that reduces H3K27ac by 0.3 log-fold.

\subsection{DNA Language Models: GPN-MSA and Evo
2}\label{dna-language-models-gpn-msa-and-evo-2}

DNA language models provide an alternative to phenotype prediction:
scoring variants by how unexpected they appear in learned sequence
context, analogous to protein language model approaches for missense
variants.

GPN-MSA combines DNA language modeling with multi-species sequence
alignments (Benegas et al. 2024). The model processes aligned sequences
from dozens of vertebrate species, learning which positions are
conserved and which tolerate variation. Variant scores derive from
likelihood ratios: how much less probable is the variant allele compared
to reference given the alignment context? This approach captures deep
evolutionary constraint missed by simple conservation scores while
providing genome-wide coverage including noncoding regions.

Evo 2 pushes context length to approximately one megabase, enabling
single models to capture local motifs and long-range dependencies
simultaneously (Brixi et al. 2025). The StripedHyena architecture
provides computational efficiency at this scale through
state-space-based sequence modeling rather than quadratic attention.
Training on diverse genomes across the tree of life teaches general
principles of sequence organization that transfer to human variant
interpretation.

Zero-shot variant scoring with Evo 2 follows the standard likelihood
ratio approach. Initial benchmarks show performance competitive with
conservation-based scores for coding variants and potentially superior
performance for noncoding variants where local sequence context matters
more than position-specific conservation. The extremely long context
enables modeling of effects mediated by distal elements, though whether
this theoretical capability translates to improved VEP remains under
investigation.

\subsection{AlphaGenome: Unified Multi-Omic Variant Effect
Prediction}\label{alphagenome-unified-multi-omic-variant-effect-prediction}

AlphaGenome (Chapter~\ref{sec-dna-lm}) represents the most ambitious
current attempt at comprehensive VEP, predicting multiple molecular
phenotypes from megabase-scale DNA sequence and using those predictions
to assess variant effects across modalities (Z. Avsec, Latysheva, and
Cheng 2025). The architecture and training procedure are detailed in
Chapter 13; here we focus on VEP-specific applications.

Variant effect prediction with AlphaGenome provides mechanistically
interpretable outputs. A promoter variant might show reduced
accessibility and decreased expression prediction. An enhancer variant
might show weakened contact with its target promoter in addition to
reduced local histone acetylation. A splicing variant triggers
SpliceAI-like splice site changes while also affecting regulatory track
predictions near the affected exon.

The multi-omic approach enables variant prioritization that considers
multiple mechanisms simultaneously. A variant in a regulatory element
that affects accessibility, expression, and chromatin contacts
represents stronger evidence than one affecting only a single predicted
phenotype. Conversely, variants with no predicted effect across
modalities can be deprioritized despite proximity to disease genes.

Practical deployment involves tradeoffs. Evaluating a single variant
requires forward passes through the full model, incurring substantial
computational cost compared to lookup-based approaches like
AlphaMissense. The model may exhibit overconfidence when extrapolating
beyond training cell types. Calibrating multi-dimensional predictions
into single pathogenicity scores remains an open problem. These
constraints position AlphaGenome as a tool for detailed mechanistic
investigation of prioritized variants rather than genome-wide screening.

\section{Combining Evidence Across
Modalities}\label{combining-evidence-across-modalities}

No single model addresses all variant types and mechanisms. Missense
variants in protein-coding regions call for protein-level predictors;
splicing variants require splice-specific models; regulatory variants
benefit from long-context DNA models. Practical VEP workflows combine
multiple predictors to achieve comprehensive coverage.

\subsection{Integration Strategies}\label{integration-strategies}

The simplest integration approach applies different models to different
variant classes. Missense variants receive AlphaMissense scores;
synonymous and intronic variants near splice sites receive SpliceAI
scores; promoter and enhancer variants receive Enformer or AlphaGenome
predictions. This modular strategy ensures that each variant type
receives predictions from an appropriate model.

More sophisticated integration aggregates scores across models for the
same variant. A missense variant might receive both AlphaMissense
(protein impact) and Enformer (regulatory impact, relevant if the codon
overlaps a regulatory element) predictions. Combining these requires
decisions about weighting and potential double-counting of shared
information.

Bayesian approaches offer principled integration. Priors encode beliefs
about variant mechanism proportions; likelihoods incorporate model
predictions given mechanism; posteriors combine evidence across models
while respecting uncertainty. REVEL demonstrated this approach for
classical predictors; extending it to foundation model outputs requires
careful calibration of each component score.

\subsection{Avoiding Double-Counting}\label{avoiding-double-counting}

Foundation models trained on overlapping data risk capturing correlated
rather than independent information. AlphaMissense and ESM-1v both
encode evolutionary constraint; combining their scores as independent
evidence overweights evolutionary signal. Similarly, conservation-based
DNA models like GPN-MSA share information with phyloP scores already
incorporated in classical predictors.

Correlation analysis helps quantify redundancy. If two model scores
correlate above 0.8 across a benchmark dataset, they likely provide
similar information and should not be counted as independent evidence.
Residual analysis can identify what unique signal each model contributes
beyond shared components.

For ACMG classification, guidelines specifically address computational
evidence weighting. The PP3 (computational evidence supporting
pathogenicity) and BP4 (computational evidence supporting benign)
criteria apply when multiple tools agree. Using five correlated
predictors that all derive from evolutionary conservation should not
count as five independent pieces of evidence. Clinical laboratories
develop local policies for which tools to consult and how to weight
their outputs, ideally based on validation against known variants in
their patient population.

\subsection{Practical Workflow Design}\label{practical-workflow-design}

An effective VEP workflow balances comprehensiveness against efficiency.
Genome-wide screening might use fast, zero-shot models (DNA language
model likelihood scores) to identify variants deviating from expected
sequence patterns. Prioritized variants then receive detailed evaluation
with computationally expensive models (AlphaGenome multi-omic
predictions). Final interpretation combines computational scores with
population frequency, gene-level constraint metrics, segregation data,
and clinical phenotype.

The ordering matters for efficiency. Filtering the majority of variants
with fast models before applying expensive models reduces computational
cost by orders of magnitude. The choice of filtering threshold trades
sensitivity against specificity: strict thresholds miss true pathogenic
variants; lenient thresholds burden downstream analysis with false
positives. Threshold selection should match intended use: diagnostic
applications prioritize sensitivity while research screening may
prioritize specificity.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-multimodel-integration}{[}Essential{]} Practical
integration workflow. Panel A (Model Selection by Variant Type):
Missense â†’ AlphaMissense, ESM-1v; splice-proximal â†’ SpliceAI; deep
intronic â†’ Enformer, cryptic splice; promoter/enhancer â†’ Enformer,
AlphaGenome; synonymous â†’ splicing + regulatory; structural â†’ limited
coverage flag. Panel B (Evidence Integration): Same variant scored by
multiple models; example missense near exon boundary; AlphaMissense
0.85, SpliceAI 0.45, Enformer regulatory effect; combining without
double-counting. Panel C (Correlation and Redundancy): Correlation
matrix between scores; high correlation = shared info (don't count
twice); residual analysis. Panel D (Practical Workflow): Tier 1 fast
screening (DNA-LM likelihood); Tier 2 detailed (AlphaMissense,
SpliceAI); Tier 3 mechanistic (AlphaGenome); cost/time at each tier.}

\end{figure}%

\section{Calibration and Clinical
Categories}\label{calibration-and-clinical-categories}

Model scores become clinically useful only when they map to actionable
categories. A score of 0.73 means nothing without context; knowing that
73\% of variants with similar scores are pathogenic enables
interpretation. Calibration ensures this correspondence between scores
and outcomes.

\subsection{Assessing Calibration}\label{assessing-calibration}

Calibration plots (reliability diagrams) visualize the relationship
between predicted probabilities and observed frequencies. Variants are
binned by predicted score; the proportion of pathogenic variants in each
bin is plotted against the bin's mean predicted probability. Perfect
calibration falls on the diagonal: predicted 0.8 pathogenicity
corresponds to 80\% observed pathogenic rate.

Most raw model outputs are poorly calibrated. Neural networks trained
with cross-entropy loss tend toward overconfidence, predicting
probabilities near 0 or 1 more often than warranted. Protein language
model likelihood ratios produce unbounded scores requiring
transformation before probability interpretation. Calibration procedures
address these systematic biases.

Expected calibration error (ECE) quantifies miscalibration as the
weighted average absolute difference between predicted and observed
frequencies across bins. Lower ECE indicates better calibration.
Comparing ECE across models identifies which provide more reliable
probability estimates independent of discrimination performance (AUROC).

\subsection{Calibration Methods}\label{calibration-methods}

Temperature scaling applies a learned scalar divisor to logits before
softmax, effectively ``softening'' overconfident predictions
(\textbf{guo\_calibration\_2017?}). The temperature parameter is
optimized on a held-out calibration set to minimize negative
log-likelihood. This simple approach often substantially improves ECE
with no change to discrimination.

Isotonic regression learns a monotonic mapping from raw scores to
calibrated probabilities. The method fits a step function that preserves
ranking while adjusting probability estimates to match observed
frequencies. Isotonic regression handles more complex miscalibration
patterns than temperature scaling but requires larger calibration sets
to avoid overfitting.

Platt scaling fits a logistic regression from raw scores to binary
outcomes, learning slope and intercept parameters. This produces
well-calibrated probabilities when the relationship between scores and
log-odds is approximately linear. For foundation model outputs with more
complex score distributions, isotonic regression typically outperforms
Platt scaling.

Calibration should use data representative of deployment conditions.
Calibrating on ClinVar expert-reviewed variants produces good
performance on similar variants but may not transfer to novel genes,
rare populations, or variant classes underrepresented in ClinVar.
Stratified calibration by gene function, variant class, or population
improves reliability at the cost of increased data requirements.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-calibration-clinical}{[}High{]} Four-panel figure.
Panel A (The Calibration Problem): Raw model score distribution; neural
networks often overconfident (cluster near 0/1); reliability diagram
showing miscalibration. Panel B (Calibration Methods): Temperature
scaling, isotonic regression, Platt scaling; before/after reliability
diagrams. Panel C (Mapping to ACMG Categories): Continuous score 0-1;
thresholds defining PP3, VUS, BP4; threshold selection trade-off. Panel
D (What Uncertainty Looks Like): Intermediate scores genuinely reflect
uncertainty; example variants at different score levels; ``A score of
0.73 means nothing without calibration context.''}

\end{figure}%

\subsection{Mapping to ACMG
Categories}\label{mapping-to-acmg-categories}

The ACMG-AMP variant classification framework defines five categories:
pathogenic, likely pathogenic, uncertain significance, likely benign,
and benign (\textbf{richards\_standards\_2015?}). Computational evidence
contributes to classification through specific criteria: PP3
(computational evidence for pathogenicity) and BP4 (computational
evidence for benignity).

Mapping continuous foundation model scores to these discrete criteria
requires threshold selection. Conservative thresholds ensure high
precision at the cost of low recall: only variants with very high (or
very low) scores receive computational evidence designation. Lenient
thresholds increase recall but admit more false positives, potentially
inflating pathogenicity classifications.

ClinGen sequence variant interpretation working groups have developed
model-specific recommendations for several classical predictors. Similar
guidance for foundation models remains under development. In the
interim, laboratories should validate thresholds against local truth
sets and document threshold choices in variant reports.

The uncertain significance category deserves special attention. Variants
with intermediate foundation model scores genuinely reflect uncertainty:
the models cannot confidently distinguish pathogenic from benign.
Forcing these variants into discrete categories by applying arbitrary
cutoffs misrepresents the actual evidence. Reporting calibrated
probabilities alongside discrete classifications preserves information
for downstream decision-making.

\section{Uncertainty Quantification}\label{uncertainty-quantification}

Calibration addresses systematic bias in probability estimates;
uncertainty quantification addresses the confidence of individual
predictions. A well-calibrated model might correctly estimate that 70\%
of variants in some category are pathogenic, but for any individual
variant, we want to know whether the model's prediction is reliable or
whether the variant falls outside the model's competence.

\subsection{Sources of Uncertainty}\label{sources-of-uncertainty}

Epistemic uncertainty reflects gaps in the model's knowledge: regions of
input space with sparse training data, variant types rarely observed
during training, or proteins from understudied families. This
uncertainty is reducible in principle by collecting more data and can be
estimated by measuring model disagreement across training variations.

Aleatoric uncertainty reflects inherent noise in the prediction target:
variants whose pathogenicity genuinely varies across individuals or
contexts, or cases where the same score corresponds to both pathogenic
and benign variants for biological rather than modeling reasons. This
uncertainty is irreducible by additional training and represents
fundamental limits on predictability.

Distinguishing these uncertainty types matters for interpretation. High
epistemic uncertainty suggests caution: the model has not seen similar
variants and may be extrapolating unreliably. High aleatoric uncertainty
suggests that the variant's effect genuinely depends on factors not
captured by sequence alone.

\subsection{Uncertainty Estimation
Methods}\label{uncertainty-estimation-methods}

Ensemble methods train multiple models on different data subsets or with
different random initializations. Prediction variance across ensemble
members estimates epistemic uncertainty. Large disagreement indicates
that the prediction depends strongly on training specifics rather than
robust learned patterns. Deep ensembles provide well-calibrated
uncertainty estimates but multiply computational cost linearly with
ensemble size.

Monte Carlo dropout approximates Bayesian inference by applying dropout
at test time and averaging predictions across multiple stochastic
forward passes. Variance across passes estimates uncertainty without
training multiple models. This approach adds modest computational
overhead and can be applied to any dropout-containing architecture.

Conformal prediction provides distribution-free uncertainty
quantification with coverage guarantees
(\textbf{angelopoulos\_conformal\_2023?}). Given a calibration set,
conformal methods construct prediction sets guaranteed to contain the
true label with specified probability (e.g., 90\%). For variant
classification, this might produce sets like \{pathogenic, uncertain\}
or \{benign\} depending on the variant and desired coverage. Larger
prediction sets indicate greater uncertainty; single-element sets
indicate confident predictions.

\subsection{Out-of-Distribution
Detection}\label{out-of-distribution-detection}

Beyond quantifying uncertainty for in-distribution predictions,
responsible deployment requires detecting when inputs fall outside the
model's training distribution. A protein language model trained on
natural proteins may produce confident but unreliable predictions for
synthetic sequences or fragments. A regulatory model trained on common
cell types may fail on rare developmental stages.

Likelihood-based detection uses the model's own representations to
identify unfamiliar inputs. Sequences with low embedding density or
anomalous attention patterns may fall outside the training distribution
regardless of predicted scores. Flagging these inputs for manual review
prevents automated classification of cases the model cannot reliably
assess.

Distance-based methods compare new inputs to training examples in
representation space. Variants far from any training example in
embedding space warrant skepticism even if the model produces confident
predictions. Maintaining summary statistics of training representations
enables efficient distance computation at deployment.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-vep-uncertainty}{[}High{]} Four-panel figure. Panel
A (Sources of Uncertainty): Epistemic (model hasn't seen similar,
reducible) vs aleatoric (inherent noise, irreducible); training
distribution vs query position. Panel B (Ensemble Methods): Multiple
models trained differently; prediction variance; high disagreement =
high epistemic uncertainty. Panel C (Conformal Prediction): Calibration
set â†’ conformal scores; prediction sets with coverage guarantee;
confident \{Benign\} vs uncertain \{P, VUS, B\}. Panel D (OOD
Detection): Embedding space; training examples dense region; query
variant isolated = OOD; flag for manual review.}

\end{figure}%

Chapter Chapter~\ref{sec-uncertainty} develops uncertainty
quantification methods in detail, including practical implementation
guidance and evaluation metrics. For VEP applications, the key insight
is that uncertainty estimates complement point predictions:
high-confidence predictions can inform clinical decisions;
low-confidence predictions should prompt additional evidence gathering
rather than blind acceptance of model outputs.

\section{What Foundation Models Add}\label{what-foundation-models-add}

Having surveyed current foundation model approaches, we can now directly
address what they contribute beyond classical methods
(Chapter~\ref{sec-vep-classical}). The answer is nuanced: substantial
improvements in some domains, modest gains in others, and persistent
blind spots that new architectures have not yet resolved.

\subsection{Improved Discrimination}\label{improved-discrimination}

On standard benchmarks, foundation model VEP methods consistently
outperform classical predictors. AlphaMissense achieves AUROC of 0.91 on
held-out ClinVar missense variants compared to 0.85 for CADD. SpliceAI
detects pathogenic splicing variants with sensitivity of 0.90 compared
to 0.60 for MaxEntScan. GPN-MSA scores correlate more strongly with deep
mutational scanning measurements than phyloP or GERP.

These improvements reflect richer representations. Classical methods
aggregate independent features (conservation, amino acid properties,
domain annotations); foundation models learn nonlinear interactions
among positions and capture patterns too subtle for manual feature
engineering. The gap is largest for variants where context matters:
buried core missense variants where structural environment determines
impact, splice variants where cryptic site activation depends on
flanking sequence, regulatory variants where motif disruption interacts
with chromatin context.

\subsection{Extended Coverage}\label{extended-coverage}

Classical methods often fail silently on understudied genes, rare
variant classes, or poorly annotated regions. SIFT and PolyPhen require
protein alignments; variants in singleton genes without homologs receive
no prediction. CADD depends on annotation features; variants in regions
lacking regulatory marks receive uninformative scores.

Foundation models degrade more gracefully. Protein language models score
any amino acid sequence regardless of available homologs. DNA language
models score any genomic position regardless of existing annotation.
This extended coverage matters for clinical sequencing of rare diseases,
where pathogenic variants often reside in less-studied genes precisely
because their severe effects are incompatible with population frequency.

\subsection{Mechanistic
Interpretability}\label{mechanistic-interpretability}

AlphaGenome and similar multi-output models provide predictions about
mechanism rather than bare pathogenicity scores. A variant flagged as
deleterious might also show predicted effects on chromatin
accessibility, contact frequency, and downstream gene expression. These
mechanistic predictions enable hypothesis generation and targeted
experimental validation (Chapter~\ref{sec-interpretability}).

Classical methods offer limited mechanistic insight. CADD provides a
single score without indicating whether it derives from conservation,
protein impact, regulatory disruption, or other features. Decomposing
the score into component contributions requires separate analysis.
Foundation models that predict molecular phenotypes naturally provide
this decomposition.

\subsection{Persistent Limitations}\label{persistent-limitations}

Foundation models have not solved several fundamental challenges.
Ancestry bias persists because training data remain skewed toward
European populations; performance degrades for variants common in
African or Asian populations but rare in training sets
(Chapter~\ref{sec-confounding}). Calibration requires substantial
labeled data that inherit existing biases. Rare variant classes
(structural variants, complex indels, repeat expansions) lack sufficient
training examples for reliable prediction.

The comparison to classical methods reveals diminishing returns on
certain axes. For well-conserved active site variants in thoroughly
studied proteins, PolyPhen-2 already achieves near-optimal performance;
AlphaMissense improves marginally. The largest foundation model gains
appear for difficult cases where classical features are uninformative or
misleading.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-vep-gains-gaps}{[}Enhancing{]} Balanced assessment.
Panel A (Performance Improvements): Benchmark comparison table (Method
\textbar{} ClinVar AUROC \textbar{} DMS Correlation \textbar{}
Coverage). Panel B (Where Improvements Largest): Difficult cases (sparse
annotations, orphan genes), context-dependent effects, novel genes.
Panel C (Persistent Gaps): Population bias, complex variants (SVs,
repeats, phase), combinatorial effects (epistasis, compound het), tissue
specificity. Panel D (The Bottom Line): Foundation models = tools for
interpretation, not oracles; best use = combined with population data,
functional evidence, clinical judgment.}

\end{figure}%

\section{Clinical Integration
Considerations}\label{clinical-integration-considerations}

Foundation model VEP tools require thoughtful integration into clinical
workflows. Their impressive benchmark performance does not automatically
translate to improved patient outcomes without attention to deployment
context, validation requirements, and human factors.

\subsection{Laboratory Validation}\label{laboratory-validation}

Before clinical use, laboratories should validate foundation model tools
against local truth sets representing their patient population.
Published benchmark performance on ClinVar may not generalize to a
laboratory's specific case mix. Validation should assess discrimination
(can the tool distinguish pathogenic from benign?), calibration (do
probability estimates match observed frequencies?), and utility (does
incorporating the tool improve variant classification compared to
existing workflows?).

Validation requires variants with known pathogenicity independent of the
computational predictions being tested. Using ClinVar variants whose
classifications already incorporated CADD scores to validate CADD
creates circular reasoning. Gold-standard variants from functional
studies, segregation data, or expert review provide cleaner validation
targets.

\subsection{Workflow Integration}\label{workflow-integration}

Foundation model predictions represent one evidence type among many.
ACMG guidelines specify how computational evidence combines with
population frequency, functional data, segregation, and clinical
phenotype. Computational evidence alone rarely suffices for pathogenic
or benign classification; it supports or weakens classifications
established by other evidence types.

Laboratory information systems require modification to display and store
foundation model outputs alongside existing annotations. Analyst
training ensures appropriate interpretation: understanding that high
scores indicate deleteriousness without establishing causation,
recognizing when scores fall outside validated ranges, and knowing when
to request additional evidence for uncertain cases.

\subsection{Communication to
Clinicians}\label{communication-to-clinicians}

Variant reports communicated to ordering clinicians should present
foundation model evidence appropriately. Reporting raw scores without
context confuses non-specialist clinicians. Reporting discrete
classifications without uncertainty may convey false confidence.
Effective reporting might state: ``Computational tools (AlphaMissense,
SpliceAI) concordantly predict this variant is likely to affect protein
function, supporting the PP3 criterion for pathogenicity
classification.''

When foundation model predictions conflict with other evidence, reports
should acknowledge the discrepancy rather than suppressing inconvenient
results. A variant segregating with disease in a family but receiving a
benign computational prediction warrants explicit discussion, not quiet
exclusion of the computational evidence.

\section{Open Challenges}\label{open-challenges}

Current foundation model approaches leave substantial problems unsolved.
These open challenges define directions for future research and areas
where clinical caution remains warranted.

\subsection{Complex Variant Types}\label{complex-variant-types}

Most current models address single nucleotide variants and small indels.
Structural variants (deletions, duplications, inversions spanning
kilobases to megabases) remain largely outside foundation model
capabilities. Copy number variation, repeat expansions, and complex
rearrangements alter genome architecture in ways current sequence models
cannot represent. Extending foundation model paradigms to these variant
classes requires architectural innovations beyond current approaches.

\subsection{Combinatorial Effects}\label{combinatorial-effects}

Genomes contain multiple variants that may interact. Compound
heterozygosity (two variants affecting both copies of a gene) creates
pathogenic states from individually tolerable variants. Modifier
variants in other genes modulate penetrance. Haplotype effects mean
variants on the same chromosome have different consequences than
variants on opposite chromosomes. Current models score variants
independently, ignoring these interactions that determine clinical
presentation.

\subsection{Phenotype Specificity}\label{phenotype-specificity}

A variant pathogenic for one phenotype may be benign for another.
\emph{SCN5A} variants cause distinct cardiac arrhythmia syndromes
depending on their specific functional effects. Foundation models
trained on pathogenic/benign labels average across phenotypes,
potentially obscuring clinically relevant specificity.
Phenotype-specific training requires much larger datasets than currently
available.

\subsection{Temporal and Environmental
Context}\label{temporal-and-environmental-context}

Variant effects often depend on age, environmental exposures, or
physiological state. A variant pathogenic under metabolic stress may be
tolerable at baseline. Foundation models capture sequence context but
not the dynamic biological context determining phenotypic expression.
Integrating longitudinal clinical data with sequence-level predictions
remains an unsolved challenge.

\subsection{Equity and Access}\label{equity-and-access}

State-of-the-art foundation models require substantial computational
resources for training and sometimes for inference. Laboratories in
resource-limited settings may lack access to cutting-edge tools.
Precomputed scores (like AlphaMissense's proteome-wide release)
partially address this, but equity concerns extend beyond compute access
to representation in training data and validation cohorts. Ensuring that
foundation model VEP benefits all populations requires deliberate effort
beyond technical development.

\section{Tools for Interpretation, Not
Oracles}\label{tools-for-interpretation-not-oracles}

Foundation models have transformed variant effect prediction from
feature engineering to representation learning. Protein language models
capture evolutionary constraint at resolution that multiple sequence
alignments cannot match. DNA language models and regulatory models
extend coverage to noncoding variants across the genome. Multi-omic
architectures provide mechanistic predictions enabling hypothesis
generation beyond bare deleteriousness scores. The best current methods
substantially outperform classical approaches on established benchmarks,
particularly for rare variants and novel genes where training data are
sparse.

Yet benchmark performance does not automatically translate to clinical
utility. Calibration requires careful attention: a model may
discriminate pathogenic from benign variants while systematically
overestimating or underestimating probabilities. Uncertainty
quantification remains immature; models often produce confident
predictions for inputs that fall outside their training distribution.
Population bias persists despite foundation model advances; improvements
over classical methods are smallest for ancestry groups underrepresented
in training data. Complex variant types, combinatorial effects, and
tissue-specific consequences remain beyond current capabilities.

Clinical deployment demands humility alongside enthusiasm. Foundation
model VEP tools are aids to human interpretation, not autonomous
classifiers. Their predictions inform rather than determine variant
classification, complementing population frequency data, functional
assay evidence, segregation analysis, and clinical judgment. Used
appropriately, they accelerate diagnosis and reduce missed findings.
Used as oracles, they create false confidence and may perpetuate
existing inequities in genomic medicine. The chapters that follow
examine how these predictions integrate into clinical workflows
(Chapter~\ref{sec-rare-disease}, Chapter~\ref{sec-clinical-risk}), the
broader challenges of uncertainty quantification
(Chapter~\ref{sec-uncertainty}), and the interpretability methods that
probe what foundation models have learned
(Chapter~\ref{sec-interpretability}). Variant effect prediction sits at
the center of genomic medicine; foundation models have raised its
ceiling while the work of achieving its potential continues.

\part{Part IV: Systems and Scale}

Biology operates across scales that sequence alone cannot capture. Cells
of different types read the same genome differently, activating distinct
gene programs that produce neurons, hepatocytes, and immune cells from
identical DNA. Genes function not in isolation but within networks of
regulation and interaction, where perturbing one node propagates effects
throughout the system. The three-dimensional folding of chromatin brings
distal elements into contact, creating regulatory logic invisible to
models that treat genomes as one-dimensional strings. Sequence
foundation models ask what a genome encodes; the models in this part ask
what that sequence becomes in particular cellular contexts, interaction
networks, and spatial architectures.

This transition from sequence-centric to systems-scale modeling demands
new data modalities and new computational approaches. Single-cell
transcriptomics reveals the cellular heterogeneity that bulk
measurements average over. Hi-C and related methods expose the spatial
organization that determines which enhancers contact which promoters.
Protein interaction networks and gene regulatory graphs capture
relational structure that no amount of sequence analysis can infer.
Foundation model principles extend naturally to these modalities: learn
representations from large-scale data, then transfer to specific
prediction tasks.

Five chapters develop this theme. Chapter~\ref{sec-rna} examines RNA
structure and function, covering models that predict secondary
structure, capture splicing regulation, and represent the emerging
frontier of RNA foundation models. Chapter~\ref{sec-single-cell}
addresses foundation models for single-cell transcriptomics and
epigenomics, showing how transformer architectures adapt to the
sparsity, noise, and scale of single-cell data.
Chapter~\ref{sec-3d-genome} turns to three-dimensional genome
organization, examining models that predict chromatin contacts from
sequence, connect spatial structure to gene regulation, and extend to
spatial transcriptomics methods that preserve tissue context.
Chapter~\ref{sec-networks} introduces graph neural networks that
represent biological entities and their interactions as structured
graphs, integrating foundation model embeddings with relational
reasoning. Chapter~\ref{sec-multi-omics} broadens the view to
multi-omics integration, exploring how models jointly represent genomic,
transcriptomic, proteomic, and clinical information to trace the path
from genotype to phenotype.

\chapter{RNA Structure and Function}\label{sec-rna}

A synonymous mutation changes the DNA codon but preserves the amino
acid. By the logic of protein-centric biology, such mutations should be
functionally neutral: same protein sequence, same structure, same
function. Yet synonymous variants can dramatically alter gene
expression, affect protein folding, and cause disease. The mechanisms
operate at the RNA level: altered codon optimality changes translation
speed, modified mRNA secondary structure affects ribosome processivity,
disrupted regulatory motifs change transcript stability. A model that
sees only DNA sequence or only protein sequence misses these effects
entirely. DNA foundation models learn regulatory sequence patterns;
protein language models learn amino acid constraints. Neither captures
RNA-level biology: secondary structure stability, RBP binding
accessibility, or the coupling between splicing and decay.

RNA occupies a distinct position in the central dogma, essential to
every step from transcription to translation, yet historically receiving
less computational attention than its neighbors. The disparity reflects
data availability more than biological importance. Protein sequences
accumulate over billions of years of evolution, providing the massive
corpora that enabled ESM to learn structure from sequence. DNA benefits
from reference genomes, population sequencing, and functional genomics
consortia generating petabytes of data. RNA databases remain
comparatively sparse, structural annotations cover only
well-characterized families, and no equivalent of AlphaFold's
crystallographic training set exists for RNA tertiary structure. The
result is a modeling landscape where RNA foundation models exist but
remain immature relative to protein and DNA counterparts.

This chapter examines RNA-specific modeling as a bridge between the
sequence-level foundation models of Part 3 and the cellular and systems
contexts that follow. The foundation models examined previously all
manifest their predictions through RNA intermediates: Enformer predicts
RNA-seq coverage, protein models predict translation products, SpliceAI
models spliceosome recognition of RNA. RNA-specific models add a
distinct layer, treating RNA not merely as a readout of DNA or a
precursor to protein, but as a structured molecule with its own sequence
constraints, folding landscapes, and functional roles. We examine
secondary structure prediction, RNA foundation models, codon-level mRNA
models, and noncoding RNA classification, while confronting the data
limitations that constrain current approaches.

\section{RNA as Molecule Versus Transcriptome
Readout}\label{rna-as-molecule-versus-transcriptome-readout}

Two complementary perspectives frame computational approaches to RNA.
The molecular view treats RNA as a physical object with primary
sequence, secondary structure through base pairing, tertiary
organization in three-dimensional space, and chemical modifications that
alter its properties. In this view, modeling goals include predicting
which bases pair with which, how the molecule folds, which proteins bind
to it, and how synthetic RNAs might be designed with desired properties.
The transcriptomic view treats RNA as a cellular readout: coverage
profiles along the genome, splice junction usage, isoform abundances,
expression levels that vary across cell types and conditions. Here the
goal is explaining how genomic sequence and chromatin state give rise to
these measurements.

Models that predict transcriptomic signals from DNA sequence (Enformer,
Borzoi, and related architectures covered in
Chapter~\ref{sec-regulatory}) operate in the second paradigm. They take
genomic sequence as input and output RNA-seq or CAGE profiles as
predictions. These models never see RNA sequence directly; they learn
the mapping from DNA context to transcriptional output. This chapter
focuses instead on the molecular perspective: models whose input is RNA
sequence itself and whose outputs concern RNA structure, function, or
design.

The distinction parallels the difference between protein language models
and proteomics prediction models. ESM takes amino acid sequences and
learns structural representations (Chapter~\ref{sec-protein-lm}). A
model predicting protein abundance from genomic features would be
solving a different problem. Both perspectives are valuable, and both
ultimately concern RNA, but they operate at different levels of the
biological hierarchy and require different architectures and training
strategies.

\section{Why Secondary Structure Creates a Distinct Modeling
Challenge}\label{why-secondary-structure-creates-a-distinct-modeling-challenge}

\subsection{The Flat Energy Landscape
Problem}\label{the-flat-energy-landscape-problem}

RNA's defining computational challenge emerges from thermodynamics.
Proteins fold into stable three-dimensional structures because their
energy landscapes contain deep minima: the native state sits in a
pronounced funnel that guides the folding process. RNA energy landscapes
are remarkably flatter. Multiple conformations compete for occupancy,
with free energy differences often smaller than thermal fluctuations at
cellular temperatures. A given RNA sequence may adopt several
alternative structures with similar stabilities, and the dominant
conformation can shift in response to ion concentrations, temperature,
protein binding, or chemical modifications.

This conformational plasticity has biological functions (riboswitches
that change structure in response to ligand binding, RNA thermometers
that regulate translation at different temperatures) but creates
modeling difficulties. Minimum free energy predictions, which identify
the single lowest-energy structure, may miss functionally relevant
alternative conformations. Partition function calculations that consider
the full ensemble are more complete but computationally expensive and
harder to interpret. Deep learning models that predict structure from
sequence must somehow capture this many-to-many relationship between
sequence and conformation, a challenge that protein structure prediction
largely avoided because the sequence-to-structure mapping for most
proteins is effectively one-to-one.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-rna-energy-landscape}{[}Essential{]} Two-panel
comparison. Panel A (Protein Folding): 3D surface with deep funnel
topology; clear global minimum (native state); steep energy barriers;
folding trajectory descending into funnel; ``Deep minimum---single
stable structure.'' Panel B (RNA Folding): Flatter surface with multiple
shallow minima at similar energy levels; small energy differences;
multiple arrows showing alternative folding paths; ``Flat
landscape---multiple competing structures.'' Bottom panel: Same RNA
sequence adopting different structures; riboswitch example; RNA
thermometer.}

\end{figure}%

\subsection{Base Pairing and Long-Range
Dependencies}\label{base-pairing-and-long-range-dependencies}

Secondary structure arises from Watson-Crick base pairing (A-U, G-C) and
wobble pairs (G-U) that create stems, loops, bulges, and internal loops.
Unlike protein secondary structure, where alpha helices and beta sheets
are local motifs determined by nearby residues, RNA secondary structure
involves long-range contacts. A base at position \(i\) may pair with a
base at position \(j\) hundreds of nucleotides away. The intervening
sequence must accommodate this pairing without introducing steric
clashes or thermodynamically unfavorable arrangements.

This long-range dependency structure differs fundamentally from protein
contact prediction, where most important contacts occur between residues
close in primary sequence. RNA structure prediction must consider all
possible pairings across the entire sequence, evaluate their
compatibility, and identify the globally optimal (or near-optimal)
arrangement. The number of possible secondary structures grows
exponentially with sequence length, making exhaustive enumeration
intractable for long RNAs.

\subsection{Pseudoknots and Tertiary
Complexity}\label{pseudoknots-and-tertiary-complexity}

Pseudoknots occur when bases in a loop pair with bases outside that
loop, creating interleaved base-pairing patterns that violate the nested
structure assumed by standard secondary structure algorithms. A typical
pseudoknot involves two stem regions whose base pairs cross each other
when drawn in standard notation. These structures are functionally
important (the telomerase RNA catalytic core contains a pseudoknot
essential for activity) but algorithmically challenging. Standard
dynamic programming approaches for secondary structure prediction
exclude pseudoknots because their inclusion increases computational
complexity from \(O(n^3)\) to \(O(n^6)\) or worse.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-rna-structure-elements}{[}Essential{]} Comprehensive
diagram. Panel A (Basic Elements): Stem, hairpin loop, internal loop,
bulge, multi-loop/junction. Panel B (Long-Range Pairing): Linear
sequence with arc diagram showing base pairs; contrast with protein
local secondary structure (\textasciitilde10 residues); ``RNA base pairs
span hundreds of nucleotides.'' Panel C (Pseudoknot): Interleaved
pairing in notation and 3D; ``Increases complexity from O(nÂ³) to
O(nâ¶)''; telomerase RNA example. Panel D (Notation Systems):
Dot-bracket, arc diagram, 2D graphical representation.}

\end{figure}%

Tertiary structure involves the three-dimensional arrangement of
secondary structure elements in space, including long-range interactions
mediated by non-Watson-Crick base pairs, metal ion coordination, and
RNA-RNA kissing loops. Predicting RNA tertiary structure remains far
less developed than protein tertiary structure prediction. No RNA
equivalent of AlphaFold exists, and the training data situation is dire:
the Protein Data Bank contains over 200,000 protein structures but fewer
than 2,000 RNA structures, many of which are ribosomal RNA fragments or
tRNA variants from the same structural families.

\section{Classical Approaches to Structure
Prediction}\label{classical-approaches-to-structure-prediction}

\subsection{Thermodynamic Folding
Models}\label{thermodynamic-folding-models}

The dominant classical paradigm for RNA secondary structure prediction
relies on nearest-neighbor thermodynamic models. These approaches assign
free energy contributions to each base pair and structural element
(loops, bulges, internal loops, multiloops) based on experimentally
calibrated parameters. Given these parameters, dynamic programming
algorithms identify the minimum free energy structure or compute the
partition function over all possible structures.

Mfold and the ViennaRNA package represent the most widely used
implementations. They achieve reasonable accuracy for short,
well-behaved RNAs where the thermodynamic parameters are most reliable.
Limitations emerge for longer RNAs where the flat energy landscape means
many structures have similar energies, for RNAs in complex cellular
environments where proteins and other factors alter folding, and for
RNAs with modifications or non-canonical interactions not captured by
standard parameter sets. These methods also assume equilibrium
conditions that may not hold for co-transcriptional folding or
kinetically trapped states.

\subsection{Comparative and Covariation
Methods}\label{comparative-and-covariation-methods}

For RNAs with sufficient homologous sequences, comparative approaches
provide an orthogonal route to structure inference. If two positions
exhibit compensatory mutations (G-C changing to A-U while maintaining
complementarity), those positions likely base-pair. Databases like Rfam
curate consensus secondary structures for RNA families based on these
covariation signals.

Comparative methods are powerful but require multiple sequence
alignments of homologous RNAs. Novel RNAs, rapidly evolving regulatory
elements, or species-specific transcripts may lack sufficient homologs
for reliable inference. The approach also assumes that structure is
conserved across the aligned sequences, which breaks down for RNAs that
have diverged in function or that adopt condition-specific alternative
structures.

\section{Deep Learning for Secondary Structure
Prediction}\label{deep-learning-for-secondary-structure-prediction}

\subsection{From Thermodynamics to Learned
Patterns}\label{from-thermodynamics-to-learned-patterns}

Deep learning models for RNA structure prediction frame the task as
sequence-to-structure mapping, analogous to protein contact prediction.
Given an RNA sequence, the model predicts base-pairing probabilities for
all position pairs, contact maps indicating which bases interact, or
per-nucleotide structural states (paired, unpaired, in loop, in stem).

Models like SPOT-RNA use convolutional or attention-based architectures
to capture long-range dependencies in sequence. Some approaches directly
predict pairing matrices as dense outputs; others output per-position
classifications that are post-processed into structures. Training
typically uses experimentally determined structures from databases like
RNAstralign or bpRNA, supplemented by computationally predicted
structures from thermodynamic models.

Performance on benchmark datasets often exceeds classical thermodynamic
methods, particularly for RNAs with complex structures or pseudoknots
where dynamic programming approaches struggle. The learned models can
capture patterns beyond nearest-neighbor rules, potentially encoding
longer-range sequence dependencies that contribute to folding but were
not parameterized in classical approaches.

\subsection{Structure Probing as
Supervision}\label{structure-probing-as-supervision}

High-throughput structure probing experiments provide an alternative
source of supervision. SHAPE (selective 2'-hydroxyl acylation analyzed
by primer extension), DMS-seq, and icSHAPE measure nucleotide
accessibility or flexibility across entire transcriptomes. Positions
that are base-paired or buried in tertiary structure show lower
reactivity than exposed positions.

These data offer several advantages for model training. They cover far
more RNAs than crystal structures, extending beyond well-characterized
families to regulatory elements and novel transcripts. They capture
structure in cellular context, reflecting the influence of proteins,
modifications, and physiological conditions. And they provide soft
constraints rather than binary pairing assignments, potentially better
matching the conformational heterogeneity of real RNA populations.

Models trained on structure probing data learn to predict accessibility
profiles from sequence. These predictions can be integrated with
thermodynamic models (using predicted accessibility as constraints) or
used directly for downstream tasks like predicting RNA-protein binding
or designing stable constructs.

\section{RNA Foundation Models}\label{rna-foundation-models}

\subsection{The Scale Gap with Protein Language
Models}\label{the-scale-gap-with-protein-language-models}

RNA foundation models attempt to replicate the protein language model
paradigm: train large transformers on massive sequence corpora using
self-supervised objectives, then transfer learned representations to
downstream tasks. The approach has produced working models, but the
results lag substantially behind protein LMs in both scale and
demonstrated capabilities.

The comparison with ESM illustrates the gap. ESM-2 trained on over 65
million protein sequences from UniRef, spanning the known diversity of
protein families. RNA-FM, one of the more successful RNA foundation
models, trained on approximately 23 million noncoding RNA sequences from
RNAcentral (J. Chen et al. (2022)). While not a trivial corpus, this
represents an order of magnitude fewer sequences, and the RNA sequences
span a narrower range of structural and functional diversity than
proteins. The consequences appear in downstream performance: RNA-FM
improves over baselines on secondary structure prediction and family
classification, but shows nothing like the emergent structure prediction
that made ESM-2's attention patterns predict contact maps without
supervision.

Several factors explain the disparity. Protein sequences have
accumulated over 4 billion years of evolution across all domains of
life, with each functional protein family represented by thousands of
homologs. RNA databases are biased toward well-characterized structural
families (tRNAs, rRNAs, ribozymes) with sparser coverage of regulatory
ncRNAs and lineage-specific transcripts. The epitranscriptomic
modifications that alter RNA function are invisible in sequence
databases, unlike protein post-translational modifications that at least
occur at predictable sequence motifs.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-rna-plm-gap}{[}High{]} Scale comparison. Panel A
(Scale Comparison Table): Training sequences (65M vs 23M, 3Ã—);
Structural diversity (all protein families vs mainly tRNA/rRNA);
Parameters (15B vs \textasciitilde100M, 150Ã—); Emergent structure
(contacts/folding vs limited). Panel B (Training Data Composition):
Protein pie chart (diverse families) vs RNA (dominated by tRNA, rRNA,
ribozymes). Panel C (Emergent Capabilities Comparison): Protein LMs
(structure prediction âœ“, variant effects âœ“) vs RNA LMs (secondary
partial âš , tertiary missing). Panel D (The Data Challenge): PDB proteins
\textgreater200,000 vs RNA \textless2,000.}

\end{figure}%

\subsection{Architectures and
Objectives}\label{architectures-and-objectives}

Most RNA foundation models follow the masked language modeling (MLM)
paradigm established by BERT. RNA-FM uses a transformer encoder with
nucleotide-level tokenization, predicting masked bases from surrounding
context. The learned embeddings show some correspondence to secondary
structure when probed with downstream tasks, though the correspondence
is weaker than the structure-function relationship learned by protein
LMs.

Alternative architectures explore different design choices. Some models
incorporate explicit structure tokens or operate on sequence-structure
graphs, learning joint representations over both modalities. Others use
codon-level tokenization for coding RNAs (discussed in the next section)
or explore state-space models and other efficient attention variants to
handle longer sequences. RNAErnie and related models experiment with
multi-task objectives that combine MLM with auxiliary predictions for
structural features or family classification.

The field remains in active development, with no clear consensus on
optimal architecture, tokenization, or training strategy. Unlike protein
modeling, where ESM established a dominant paradigm that subsequent work
has refined, RNA modeling still explores fundamental design choices.

\subsection{Downstream Applications}\label{downstream-applications}

RNA foundation model embeddings support various downstream tasks.
Secondary structure prediction fine-tunes the model to output pairing
probabilities or SHAPE reactivity profiles. RNA-protein binding
prediction uses CLIP-seq data to predict interactions with RNA-binding
proteins. Family classification assigns sequences to Rfam families or
functional categories (tRNA, rRNA, miRNA, lncRNA). Expression and
stability tasks predict transcript half-life or steady-state levels from
UTR sequences.

Performance varies substantially across tasks. For structurally
constrained RNAs like tRNAs and rRNAs, where sequence motifs strongly
determine structure and function, foundation model embeddings provide
useful features. For regulatory lncRNAs that often lack stable secondary
structures and conserved motifs, improvement over baseline methods is
more modest. The diversity of RNA types and tasks complicates
benchmarking, and models that excel on one task may struggle on others.

\section{Codon-Level Models for Coding
RNA}\label{codon-level-models-for-coding-rna}

\subsection{Beyond Nucleotide
Tokenization}\label{beyond-nucleotide-tokenization}

Coding sequences occupy a special niche where protein and nucleic acid
constraints intersect. The genetic code assigns 61 sense codons to 20
amino acids, creating synonymous redundancy where multiple codons encode
the same amino acid. This redundancy is not functionally neutral:
synonymous codons differ in tRNA availability, translation speed,
co-translational folding effects, and mRNA stability. Protein language
models, which operate on amino acid sequences, cannot capture these
codon-level signals.

Codon-level foundation models address this gap by tokenizing mRNA into
codons rather than nucleotides. Models like cdsFM, EnCodon, and DeCodon
treat each three-nucleotide codon as a single token, training on masked
codon prediction and related objectives (Naghipourfar et al. (2024)).
This tokenization encodes a biological prior: codons are the fundamental
units of translation, and mutations at the codon level determine amino
acid changes while mutations within synonymous codons affect expression
without changing protein sequence.

The codon vocabulary contains 61 tokens (excluding stop codons) plus
special tokens for noncoding regions and boundaries. This intermediate
vocabulary size (between character-level nucleotide tokenization and
typical BPE vocabularies of thousands of tokens) balances resolution
with context length. A 300-amino-acid protein corresponds to 900
nucleotides or 300 codons, making whole-gene modeling tractable within
standard transformer context windows.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-codon-tokenization}{[}High{]} Codon-level modeling.
Panel A (Same Protein, Different mRNA): Amino acid sequence; multiple
mRNA sequences encoding same protein; highlight synonymous codon
choices. Panel B (What Codon Choice Affects): tRNA availability,
translation speed, mRNA structure, stability (GC content). Panel C
(Tokenization Comparison): Character-level (900 tokens for 300 AA),
codon-level (300 tokens); ``Encodes biological prior.'' Panel D (Model
Capabilities): Protein LM sees amino acids only; DNA LM sees nucleotides
but no codon boundaries; Codon LM sees both.}

\end{figure}%

\subsection{What Codon Models Add}\label{what-codon-models-add}

Compared to protein language models, codon-level models enable direct
modeling of mRNA design problems where amino acid sequence is fixed but
codon choice is variable. They capture codon usage bias and its
relationship to expression, model translation elongation dynamics that
affect co-translational folding, and distinguish synonymous variants
that are neutral at the protein level but affect mRNA properties.

Life-Code extends this approach into a central-dogma-wide framework,
linking DNA, RNA, and protein representations through shared or aligned
embedding spaces (Liu et al. (2025)). CodonBERT specifically targets
mRNA design for vaccines and therapeutics, training on over 10 million
mRNA sequences to learn representations that predict expression,
stability, and immunogenicity (S. Li et al. (2023)).

However, codon models typically ignore mRNA secondary structure and
modifications. Local structure affects ribosome access and translation
rate; modifications like m6A influence stability and localization.
Combining codon-aware tokenization with structure-aware representations
remains an open direction, less mature than the parallel integration of
sequence and structure in protein modeling.

\section{UTR Models and Translation
Regulation}\label{utr-models-and-translation-regulation}

\subsection{Why UTRs Dominate Expression
Control}\label{why-utrs-dominate-expression-control}

The protein output of an mRNA depends as much on its untranslated
regions as on its coding sequence. A transcript's 5' UTR determines
whether ribosomes find and engage the start codon; its 3' UTR controls
how long the message survives and where in the cell it localizes. Two
mRNAs encoding identical proteins can differ by orders of magnitude in
expression if their UTRs differ. This regulatory leverage makes UTR
modeling essential for both understanding endogenous gene regulation and
designing synthetic mRNAs for therapeutic applications. The 5' UTR spans
from the transcription start site to the start codon, typically 50 to
200 nucleotides in human mRNAs. Within this region, secondary structure
can occlude the start codon and impede ribosome scanning, upstream open
reading frames (uORFs) can capture ribosomes before they reach the main
coding sequence, and internal ribosome entry sites (IRES) can enable
cap-independent translation under stress conditions. The Kozak consensus
sequence surrounding the start codon influences initiation efficiency,
but context extending dozens of nucleotides in either direction
modulates this effect. Predicting translation efficiency from 5' UTR
sequence requires integrating these overlapping signals. The 3' UTR
extends from the stop codon to the poly-A tail, ranging from under 100
nucleotides to over 10 kilobases. This region harbors binding sites for
RNA-binding proteins and microRNAs that collectively determine mRNA
half-life, localization, and translational status. AU-rich elements
(AREs) recruit decay machinery in response to cellular signals. Pumilio
and other RNA-binding proteins recognize specific motifs to repress or
activate translation. The density and arrangement of miRNA binding sites
create combinatorial regulatory logic that varies across cell types
depending on which miRNAs are expressed.

\subsection{Sequence-to-Expression
Models}\label{sequence-to-expression-models}

High-throughput reporter assays have enabled systematic modeling of UTR
function. Massively parallel reporter assays (MPRAs) measure expression
driven by thousands of UTR variants in a single experiment, providing
training data at scales previously unavailable. Sample et al.~used such
data to train Optimus 5-Prime, a convolutional model that predicts
ribosome load from 5' UTR sequence with accuracy sufficient to guide
synthetic UTR design (\textbf{sample\_human\_2019?}). The model learned
interpretable features corresponding to known regulatory elements (uORF
presence, Kozak strength, secondary structure) while also capturing
context-dependent interactions invisible to element-counting approaches.

For 3' UTRs, models must contend with greater length and combinatorial
complexity. A 2-kilobase 3' UTR may contain dozens of potential
regulatory sites whose effects depend on spacing, secondary structure
context, and the expression levels of cognate binding proteins.
Approaches range from motif-based models that score individual elements
and sum contributions, to deep learning architectures that process
entire UTR sequences and learn nonlinear interactions. Agarwal and
Kelley trained models on endogenous mRNA stability measurements,
demonstrating that 3' UTR sequence features explain substantial variance
in half-life across the transcriptome
(\textbf{agarwal\_predicting\_2022?}).

Transfer learning from RNA foundation models offers a complementary
approach. Rather than training UTR-specific models from scratch,
pretrained representations from RNA-FM or similar models can be
fine-tuned on expression prediction tasks. The pretrained embeddings
encode sequence context and potential structural features that may
transfer to UTR function prediction, though systematic comparisons
between foundation model transfer and task-specific training remain
limited.

\subsection{Integration with mRNA
Design}\label{integration-with-mrna-design}

UTR optimization represents a distinct component of therapeutic mRNA
design, complementing the codon optimization discussed in the previous
section. For a vaccine or protein replacement therapy, the coding
sequence determines what protein is made while the UTRs determine how
much protein is made and for how long. Current mRNA therapeutics
typically use UTRs borrowed from highly expressed endogenous genes
(human alpha-globin and beta-globin UTRs are common choices) rather than
computationally optimized sequences.

Model-guided UTR design could improve on this approach by optimizing for
specific objectives: maximizing expression in target tissues, extending
mRNA half-life to reduce dosing frequency, or minimizing immunogenicity
by avoiding sequences that trigger innate immune sensors. The challenge
lies in the combinatorial interaction between UTRs and coding sequence.
Secondary structures can span the UTR-CDS boundary, and the optimal 5'
UTR for one coding sequence may perform poorly for another. Integrated
models that jointly optimize UTRs and coding sequence represent an
active research direction, though experimental validation of
computationally designed UTRs remains limited compared to the extensive
optimization of coding sequences.

\section{mRNA Design and
Optimization}\label{mrna-design-and-optimization}

\subsection{Design Objectives and
Trade-offs}\label{design-objectives-and-trade-offs}

mRNA sequence design selects nucleotide sequences that encode a desired
protein while optimizing expression, stability, safety, and
manufacturability. For a 300-amino-acid protein, there are approximately
\(3^{300}\) possible synonymous mRNA sequences (roughly the number of
synonymous codons raised to the protein length). This astronomical space
cannot be exhaustively searched, motivating both classical heuristics
and modern machine learning approaches.

Key objectives include high protein expression in target tissues, mRNA
stability during manufacturing and in vivo, controlled translation
kinetics that influence co-translational folding, and low immunogenicity
for therapeutic applications. These objectives often conflict:
increasing GC content may improve stability but introduce unwanted
secondary structure, while avoiding rare codons may reduce expression if
tRNA pools are limiting.

\subsection{Lessons from COVID-19
Vaccines}\label{lessons-from-covid-19-vaccines}

The COVID-19 mRNA vaccines provided a high-profile demonstration of mRNA
design principles at unprecedented scale. The Pfizer-BioNTech and
Moderna vaccines incorporated several design elements:
N1-methylpseudouridine modification throughout the sequence to reduce
innate immune activation, codon optimization to enhance expression in
human cells, optimized 5' and 3' UTRs from highly expressed genes, and
sequence modifications to stabilize the prefusion spike conformation.
These choices drew on decades of basic research but were refined through
empirical optimization rather than systematic model-based design.

The vaccines' success demonstrated that rationally designed mRNAs can
achieve therapeutic efficacy at scale. It also revealed limitations in
current understanding: the optimal combination of modifications, codons,
and UTRs for a given protein target remains partly empirical, and
transferring designs across proteins or therapeutic applications
requires substantial optimization.

\subsection{Model-Based Design
Strategies}\label{model-based-design-strategies}

RNA and codon foundation models enable several approaches to systematic
design. Scoring and screening use pretrained models to evaluate large
candidate sets for predicted expression or stability, selecting top
designs for experimental validation. When models are differentiable with
respect to input embeddings, gradient-based methods can guide sequence
optimization toward desired objectives. Generative approaches sample
diverse high-scoring sequences subject to constraints like fixed amino
acid sequence or avoided motifs.

Empirical results suggest that deep models trained on high-throughput
reporter assays or ribosome profiling can outperform classical codon
adaptation indices like CAI or tAI, particularly for context-specific
expression prediction. Classical indices rely on genome-wide codon
frequencies that may not reflect the relevant cellular context, while
deep models can learn local effects of codon pairs, mRNA structure, and
regulatory elements. However, these models require substantial training
data and may not generalize across organisms or synthetic constructs far
from natural sequences.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-mrna-design-pipeline}{[}High{]} End-to-end pipeline.
Stage 1 (Target Protein): Desired sequence and structural requirements.
Stage 2 (Codon Optimization): \textasciitilde3\^{}300 possibilities;
objectives (expression, stability, immunogenicity); model-based scoring.
Stage 3 (5' UTR Design): Translation initiation, Kozak, secondary
structure. Stage 4 (3' UTR Design): Stability elements, miRNA avoidance,
poly-A tail. Stage 5 (Modification Selection): N1-methylpseudouridine.
Output: Optimized construct. Inset: COVID-19 vaccine design choices.}

\end{figure}%

\section{Noncoding RNA Classification and
Function}\label{noncoding-rna-classification-and-function}

\subsection{The Diversity of Noncoding
RNA}\label{the-diversity-of-noncoding-rna}

RNA that does not encode protein spans an enormous functional and
structural range. Housekeeping RNAs (tRNAs, rRNAs, snRNAs, snoRNAs)
perform essential cellular functions with well-characterized structures.
Regulatory RNAs (miRNAs, siRNAs, piRNAs, lncRNAs) control gene
expression through diverse mechanisms. Structural and catalytic RNAs
(ribozymes, riboswitches) adopt complex folds that enable enzymatic
activity or ligand sensing. Circular RNAs (circRNAs) and other
noncanonical species continue to expand the catalog of RNA diversity.

Each class has characteristic lengths, structural motifs, genomic
contexts, and functional mechanisms. tRNAs are approximately 76
nucleotides with a conserved cloverleaf structure. miRNAs are
approximately 22 nucleotides processed from longer hairpin precursors.
lncRNAs span thousands of nucleotides with poorly conserved sequence and
often no stable secondary structure. Unifying these classes under a
single modeling framework is challenging, and models that excel on one
class may fail on others.

\subsection{From Handcrafted Features to Learned
Representations}\label{from-handcrafted-features-to-learned-representations}

Classical ncRNA classification relied on engineered features: k-mer
frequencies, GC content, minimum free energy of predicted secondary
structure, structural motif counts, and genomic context features like
proximity to coding genes or chromatin marks. These features fed
conventional classifiers (SVMs, random forests, shallow neural networks)
that achieved reasonable performance for well-studied classes with
strong sequence and structure signatures.

The limits of handcrafted features emerge most clearly for lncRNAs.
These transcripts are defined partly by what they lack (no long open
reading frame) rather than what they possess. Many lncRNAs show poor
conservation, lack stable secondary structures, and have diverse, poorly
characterized functions. Distinguishing functional lncRNAs from
transcriptional noise remains difficult, and classical feature sets
often collapse to generic statistics like length and GC content.

Foundation model embeddings offer a more flexible approach.
Per-nucleotide representations can be pooled into fixed-dimensional
vectors that support classification with simple downstream heads. For
ncRNAs without strong sequence motifs, the pretrained embeddings may
capture subtle distributional patterns learned during self-supervised
training. Few-shot learning becomes possible: given a handful of newly
characterized RNAs, their embeddings can seed new clusters in
representation space, guiding annotation of related sequences.

\section{miRNA Target Prediction}\label{mirna-target-prediction}

MicroRNAs regulate gene expression by guiding the RNA-induced silencing
complex (RISC) to complementary sites in target mRNAs, typically in the
3' UTR. A single miRNA can regulate hundreds of transcripts, and a
single transcript can harbor binding sites for dozens of miRNAs. This
regulatory network influences virtually every cellular process, and
dysregulation of miRNA-target interactions contributes to cancer,
cardiovascular disease, and neurodegeneration. Predicting which
transcripts a given miRNA targets (and vice versa) has been a persistent
computational challenge since the discovery of miRNA-mediated
regulation.

The dominant paradigm centers on seed complementarity. Nucleotides 2
through 7 of the miRNA (the seed region) typically form perfect
Watson-Crick pairs with target sites, while the remaining nucleotides
contribute variably to binding affinity and regulatory effect. Classical
algorithms like TargetScan identify conserved seed matches in 3' UTRs
and rank targets by evolutionary conservation, site type (8mer, 7mer-m8,
7mer-A1), and local sequence context
(\textbf{agarwal\_predicting\_2015?}). Additional features including AU
content flanking the site, position within the UTR, and proximity to
other miRNA sites improve prediction accuracy.

Despite decades of refinement, target prediction remains noisy.
Experimental validation rates for top predictions rarely exceed 50\%,
and many functional targets lack canonical seed matches. The disconnect
arises partly from context dependence: a site may be accessible in one
cell type but occluded by RNA structure or competing protein binding in
another. It arises partly from the limitations of reporter assays that
measure binding in artificial contexts rather than endogenous regulatory
effects. And it arises from the biology itself, where weak individual
sites combine additively and miRNA-target interactions are probabilistic
rather than deterministic.

Deep learning approaches attempt to improve on seed-based methods by
learning complex sequence features from high-throughput binding data.
Models trained on CLIP-seq experiments (which crosslink miRNA-target
complexes and identify bound sites transcriptome-wide) can capture
non-canonical binding modes and context effects invisible to
seed-matching algorithms. However, these models often overfit to
cell-type-specific binding patterns and generalize poorly across
contexts. The fundamental challenge is that miRNA targeting depends on
factors beyond sequence: miRNA and target abundance, competition among
targets for limiting RISC, and cellular state variables that no
sequence-based model can capture.

For clinical applications, target prediction informs both the mechanism
of disease-associated miRNAs and the design of therapeutic
interventions. AntimiR oligonucleotides that sequester specific miRNAs
have entered clinical trials for hepatitis C (targeting miR-122) and
other indications. Predicting off-target effects of such therapeutics
requires understanding the full network of targets that will be
derepressed when a miRNA is inhibited. Similarly, miRNA mimics designed
to replace lost tumor-suppressor miRNAs must be evaluated for potential
regulation of unintended targets. In both cases, computational target
prediction provides a starting point that experimental validation must
refine.

\section{Splicing and Transcript Processing
Models}\label{splicing-and-transcript-processing-models}

\subsection{Beyond SpliceAI}\label{beyond-spliceai}

SpliceAI demonstrated that deep convolutional networks could predict
splice sites with near-spliceosomal precision (Chapter~\ref{sec-cnn}).
The model's success in identifying cryptic splice variants has made it a
standard tool in clinical variant interpretation. However, splicing
involves more than splice site recognition, and several extensions
address aspects that SpliceAI does not fully capture.

Tissue-specific splicing patterns vary substantially across cell types
and developmental stages. A splice site may be used in brain but skipped
in liver due to differential expression of splicing factors. Models like
Pangolin extend splice prediction by training on tissue-specific RNA-seq
data, learning to predict not just whether a site is splice-competent
but whether it is used in specific cellular contexts. These models
enable variant interpretation that accounts for tissue-relevant splicing
patterns rather than generic predictions.

Branchpoint prediction identifies the adenosine residue where the lariat
intermediate forms during splicing. While SpliceAI focuses on donor and
acceptor sites, branchpoint recognition involves distinct sequence
features (typically a degenerate YURAY motif 18-40 nucleotides upstream
of the acceptor) that specialized models can capture. Combined analysis
of donor, acceptor, and branchpoint predictions provides more complete
characterization of splice-altering variants.

Alternative splicing prediction moves beyond binary splice site
identification to model exon inclusion rates and isoform usage. Models
in this space attempt to predict not just whether an exon can be
included but quantitative measures of inclusion across conditions,
enabling analysis of splicing quantitative trait loci (sQTLs) and their
effects on transcript diversity.

\section{Limitations and Open
Challenges}\label{limitations-and-open-challenges-2}

\subsection{Sparse Structural Data}\label{sparse-structural-data}

The fundamental limitation of RNA modeling is data scarcity. Protein
structure prediction benefits from over 200,000 experimentally
determined structures; RNA has fewer than 2,000, heavily biased toward
ribosomal RNA and tRNA. This scarcity limits supervised learning for
tertiary structure prediction and constrains the emergence of structural
knowledge from self-supervised pretraining. Until high-throughput
methods generate RNA structures at scale comparable to protein
crystallography and cryo-EM, RNA tertiary structure prediction will
remain a frontier problem rather than a solved one.

Secondary structure data is more abundant but still limited.
Experimentally validated structures cover mainly well-characterized
families, while computational predictions for novel sequences rely on
thermodynamic models whose accuracy degrades for long RNAs and complex
folds. Structure probing experiments provide genome-wide coverage but
measure accessibility rather than pairing directly, requiring inference
to convert reactivity profiles into structural models.

\subsection{Functional Annotation
Gaps}\label{functional-annotation-gaps}

For many ncRNA classes, function remains poorly characterized. LncRNA
annotations often specify only genomic location and expression pattern
without mechanistic understanding. Circular RNA functions are emerging
but incompletely cataloged. Even for better-characterized classes like
miRNAs, target prediction remains noisy and context-dependent.

This annotation gap limits supervised learning for function prediction
and complicates evaluation. When ground truth is uncertain, it becomes
difficult to assess whether a model's predictions reflect genuine
biological insight or artifacts of incomplete training data. The field
needs both experimental advances to characterize ncRNA function and
computational approaches that can learn from weak or partial
supervision.

\subsection{The Maturity Gap}\label{the-maturity-gap}

RNA foundation models exist but have not achieved the transformative
impact of protein language models. ESM-2 enabled ESMFold, providing
structure prediction from single sequences that nearly matches
AlphaFold. No comparable RNA breakthrough has occurred. The reasons
include data scarcity, the conformational complexity of RNA, and the
diversity of RNA classes that makes unified modeling difficult.

This maturity gap represents both a limitation and an opportunity. The
techniques that succeeded for proteins (large-scale self-supervised
learning, attention mechanisms, scaling laws) provide a roadmap.
Applying that roadmap to RNA requires addressing the data challenge
through structure probing, synthetic data generation, or more efficient
use of limited experimental structures. It requires architectural
innovations that handle RNA's long-range base pairing and conformational
flexibility. And it requires benchmarks and evaluation frameworks that
cover the full diversity of RNA types and tasks.

\section{The Bridge Between Sequence and
Cell}\label{the-bridge-between-sequence-and-cell}

RNA occupies a distinctive position in genomic AI, bridging the
sequence-level models of Part III with the cellular perspectives that
follow. Splicing models like SpliceAI operate on pre-mRNA and predict
transcript processing outcomes. Codon-level models capture translation
dynamics invisible to protein language models. mRNA therapeutic design
demonstrates practical value through codon optimization, UTR
engineering, and stability prediction. These applications proceed
despite the absence of the structure prediction breakthrough that
transformed protein modeling; secondary structure prediction has
advanced through deep learning, but tertiary structure accuracy lags
protein structure by a wide margin.

The relationship between RNA models and other modalities reflects RNA's
position in the central dogma. RNA is the product of transcription that
regulatory models predict, the substrate for translation that protein
models assume, and the primary measurement that single-cell models use
to represent cellular state. Foundation models that learn from RNA
sequence capture patterns distinct from those in DNA or protein: codon
usage biases, secondary structure constraints, and post-transcriptional
regulatory elements that neither genomic nor protein models directly
represent.

The chapters that follow extend from sequence to cellular and tissue
context. Single-cell models (Chapter~\ref{sec-single-cell}) treat RNA
expression as the primary readout of cellular state, learning
representations that capture cell type identity and perturbation
response. Three-dimensional genome models (Chapter~\ref{sec-3d-genome})
add spatial context that influences transcription. Network models
(Chapter~\ref{sec-networks}) integrate gene relationships that transcend
individual sequences. RNA models provide sequence-level representations
that feed into these higher-level frameworks, completing the molecular
arc from DNA through RNA to protein while opening the path to
systems-level integration.

\chapter{Single-Cell Models}\label{sec-single-cell}

The foundation models in Part 3 operate on sequence: DNA nucleotides,
amino acids, RNA bases. They learn what the genome encodes, what
proteins it produces, how regulatory elements respond to transcription
factors. Yet sequence alone cannot explain why a neuron and a
hepatocyte, carrying identical genomes, perform utterly different
functions. The answer lies not in sequence but in state: which genes are
active, which regulatory elements are accessible, which epigenetic marks
are present. A neuron expresses synaptic genes and silences metabolic
pathways; a hepatocyte does the reverse. The genome is the same; the
cellular interpretation differs. Capturing this interpretation at
single-cell resolution has become possible only in the past decade, and
the resulting data now approach the scale that enabled language models
in text.

Single-cell technologies decompose cellular mixtures that bulk assays
average over. A tumor biopsy contains malignant cells, immune
infiltrates, stromal fibroblasts, and endothelial cells in varying
proportions. Bulk RNA-seq reports average expression across this
mixture, potentially masking the drug-resistant subpopulation that will
cause relapse. Single-cell RNA-seq profiles each cell individually,
revealing which cells express which genes and how composition shifts
during disease progression. The challenge is that single-cell data are
sparse, with most genes showing zero counts in most cells; noisy, as
technical dropout obscures biological signal; and high-dimensional,
spanning tens of thousands of features across millions of cells.
Traditional analysis methods struggle with this combination; foundation
models offer a path through.

This chapter examines foundation models for single-cell and epigenomic
data across three interconnected scales. Cellular language models treat
gene expression profiles as documents and learn the grammar of which
genes co-occur in different cellular contexts. Epigenomic models capture
regulatory state encoded in DNA methylation and chromatin accessibility.
Integration methods align cells across modalities when different assays
are performed on different cells from the same tissue. Throughout, the
central question remains: can models trained on cellular state
representations learn regulatory logic that generalizes across tissues,
conditions, and species? The answer determines whether single-cell
foundation models can achieve the transfer learning successes that
protein and DNA models have demonstrated.

\section{The Single-Cell Data
Landscape}\label{the-single-cell-data-landscape}

\subsection{From Bulk to Single-Cell
Resolution}\label{from-bulk-to-single-cell-resolution}

Traditional transcriptomic studies measure gene expression in bulk
tissue, producing a single measurement per gene that represents the
average across thousands to millions of cells. This averaging is both a
strength and a limitation. It provides robust, reproducible measurements
that have powered decades of biological discovery. It also fundamentally
limits what questions can be asked. If a gene appears moderately
expressed in bulk, is it uniformly expressed across all cells, or highly
expressed in a rare subpopulation while silent elsewhere? Bulk data
cannot distinguish these scenarios.

Single-cell RNA sequencing (scRNA-seq) resolves this ambiguity by
measuring gene expression in individual cells. The technology has
evolved rapidly since its introduction in 2009. Early methods captured
hundreds of cells per experiment; current platforms routinely profile
hundreds of thousands of cells, with some studies exceeding a million.
Public repositories now contain tens of millions of single-cell
transcriptomes spanning diverse tissues, developmental stages, disease
states, and species. This scale approaches the data volumes that enabled
large language models in natural language processing.

The analogy between cells and documents runs deeper than dataset size.
In language, words combine according to grammatical rules to form
sentences that convey meaning. In cells, genes combine according to
regulatory programs to form expression profiles that define cellular
identity. A hepatocyte expresses genes for drug metabolism, albumin
synthesis, and bile production; a neuron expresses genes for synaptic
transmission, ion channels, and neurotransmitter receptors. These
expression programs are not random: transcription factors activate
coherent sets of target genes, signaling pathways coordinate cellular
responses, and developmental programs establish cell type identities
through cascades of regulatory events. Just as language models learn
syntax and semantics by predicting masked words, single-cell foundation
models might learn regulatory logic by predicting masked genes.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-cellular-lm-analogy}{[}Essential{]} Visual analogy.
Panel A (Language Model): Sentence ``The cat sat on the mat''; words as
tokens; grammar rules; BERT predicts masked words. Panel B (Cellular
Language Model): Cell expression profile as ``sentence''; genes as
tokens (\textasciitilde20,000 vocabulary); regulatory programs as
``grammar''; Geneformer/scGPT predict masked genes. Panel C (Parallel
Structure Table): NLP â†” Single-Cell mapping. Panel D (What Models
Learn): Language (syntax, semantics) â†” Cellular (co-expression modules,
cell type programs, perturbation effects).}

\end{figure}%

\subsection{Technical Challenges and Data
Characteristics}\label{technical-challenges-and-data-characteristics}

Single-cell data present distinctive challenges that shape how
foundation models must be designed. Dropout is pervasive: due to
inefficiencies in RNA capture and amplification, many genes that are
actually expressed in a cell register as zero in the measurement. A gene
with true expression may appear as zero in 50\% to 90\% of cells where
it is actually transcribed. This zero-inflation means that absence of
signal is not absence of expression.

Sparsity compounds the interpretation challenge. A typical single-cell
transcriptome measures 20,000 genes, but any individual cell might have
detectable expression for only 1,000 to 5,000 of them. The resulting
data matrices are more than 90\% zeros, requiring specialized
computational approaches.

Batch effects arise because technical variation between experiments
often exceeds biological variation within them. Cells processed on
different days, by different operators, or with different reagent lots
may cluster by batch rather than by biological type. A model that learns
batch-specific patterns rather than biological ones will fail to
generalize.

Dynamic range spans orders of magnitude, from highly expressed
housekeeping genes to rare transcription factors present at a few copies
per cell. Normalizing across this range while preserving biologically
meaningful variation requires careful preprocessing choices that can
affect downstream results.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-single-cell-challenges}{[}Essential{]} Data
challenges. Panel A (Dropout/Sparsity): Gene Ã— Cell matrix mostly zero
(90\%+); ``Zero can mean silent OR undetected.'' Panel B (Batch
Effects): UMAP colored by cell type (mixed) vs batch (clustered);
``Technical variation can exceed biological.'' Panel C (Dynamic Range):
Histogram spanning orders of magnitude; few copies for some genes. Panel
D (How Foundation Models Address): Rank-based encoding handles range;
large corpus learns robust patterns; contrastive objectives for
batch-invariance.}

\end{figure}%

Despite these challenges, the scale of available data creates
opportunities. Tens of millions of cells, spanning hundreds of cell
types across dozens of tissues and multiple species, provide training
corpora large enough to learn general representations. The question is
whether foundation model architectures can extract biological signal
from noisy, sparse, high-dimensional measurements.

\section{Cellular Language Models}\label{cellular-language-models}

\subsection{Geneformer: Learning Network
Biology}\label{geneformer-learning-network-biology}

Geneformer exemplifies the cellular language model approach, treating
each cell as a sentence where genes serve as tokens (Theodoris et al.
2023). The model was pretrained on approximately 30 million single-cell
transcriptomes to learn context-aware representations that capture how
genes function within cellular regulatory networks. The key insight was
that during pretraining, the model gained understanding of network
dynamics in a completely self-supervised manner, encoding network
hierarchy in its attention weights without explicit supervision on
network structure.

Rather than using raw expression counts, Geneformer employs rank-based
encoding that emphasizes relative expression. For each cell, genes are
ranked by their expression level compared to their typical expression
across the training corpus. This transformation highlights which genes
are unusually active or silent in each cellular context. A gene ranked
highly in a given cell is one whose expression deviates from its
baseline, potentially indicating context-specific regulatory activation.
The representation discards absolute counts, which vary with sequencing
depth and capture efficiency, while preserving the relative ordering
that reflects cellular state.

Pretraining uses a masked gene prediction objective analogous to
BERT-style language modeling. A fraction of genes are masked in each
cell, and the model learns to predict which genes were masked based on
the remaining expression context. This forces the model to learn
co-expression patterns: which genes tend to appear together at high
ranks in the same cells, and which genes predict each other's presence.
The objective implicitly captures regulatory modules, signaling
pathways, and cell-type-specific programs.

After pretraining, Geneformer supports diverse downstream applications
through fine-tuning or feature extraction. Cell type annotation achieves
high accuracy even with limited labeled examples, leveraging general
biological knowledge acquired during pretraining. The model identified
candidate therapeutic targets for cardiomyopathy by analyzing how
disease-associated genes fit within learned network structure,
demonstrating potential for accelerating discovery in rare diseases
where large disease-specific datasets are unavailable (Theodoris et al.
2023).

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-geneformer-architecture}{[}High{]} Geneformer
innovations. Panel A (Rank-Based Encoding): Raw expression â†’ ranks
relative to corpus baseline â†’ tokens; ``Highlights unusually
active/silent genes.'' Panel B (Architecture): Ranked gene sequence â†’
transformer encoder (BERT-style) â†’ masked gene prediction â†’ gene/cell
embeddings. Panel C (What Attention Learns): Network diagram showing
gene-gene attention; correspondence to regulatory relationships;
``Network hierarchy emerges without supervision.'' Panel D (Transfer
Applications): Cell type annotation, therapeutic target ID, disease gene
prioritization.}

\end{figure}%

\subsection{scGPT: Generative Pretraining for Single-Cell
Analysis}\label{scgpt-generative-pretraining-for-single-cell-analysis}

scGPT extends the foundation model paradigm with a generative
architecture trained on over 33 million cells (Cui et al. 2024). The
model functions as a generalist backbone for single-cell analysis
pipelines, supporting applications from cell type annotation to
perturbation response prediction within a unified framework.

The architecture incorporates several innovations tailored to
single-cell data characteristics. Gene tokens combine learnable
embeddings with position encodings that can capture genomic location
when relevant. Expression values are discretized into bins to handle the
wide dynamic range and zero-inflation characteristic of single-cell
measurements; rather than predicting continuous values, the model
predicts which expression bin a gene falls into. Special tokens mark
cell boundaries and indicate modality when multi-omic data are
available.

scGPT uses multiple pretraining objectives simultaneously. Masked gene
prediction encourages learning of co-expression patterns, similar to
Geneformer. Autoregressive generation predicts expression of one set of
genes conditioned on others, enabling the model to generate synthetic
expression profiles or impute missing values. Contrastive objectives
push cells from the same type to cluster in embedding space while
separating different types, providing discriminative signal that
complements the generative objectives.

The combination of objectives enables scGPT to excel across multiple
applications. Cell type annotation benefits from rich pretrained
representations, including identification of fine-grained subtypes that
might be missed by simpler methods. Multi-batch integration aligns cells
from different experiments while preserving genuine biological
variation, addressing the pervasive batch effect problem. Perturbation
response prediction anticipates how cells will respond to genetic
knockouts or drug treatments, providing a foundation for in silico
experimentation.

\subsection{scFoundation and Scaling Single-Cell
Models}\label{scfoundation-and-scaling-single-cell-models}

scFoundation pushes the scale of single-cell foundation models further,
training on over 50 million cells with an architecture designed for both
representation learning and generation
(\textbf{hao\_scfoundation\_2024?}). The model explores how scaling laws
observed in language models translate to cellular data, finding that
larger models trained on more diverse data produce embeddings that
transfer better across tasks and contexts.

The pretraining corpus spans diverse tissues, developmental stages, and
disease states, including both human and mouse data. This diversity
proves essential: models trained on narrow datasets (a single tissue or
condition) learn representations that capture that specific context but
fail to generalize. Models trained on diverse corpora learn more
abstract representations of cellular state that transfer across
biological contexts.

scFoundation emphasizes the importance of tokenization and normalization
choices for downstream performance. The model systematically compared
different approaches to handling zero-inflation, normalization across
sequencing depth, and gene vocabulary selection. These preprocessing
decisions, often treated as implementation details, significantly affect
what biological signals the model can capture.

\subsection{TranscriptFormer: Cross-Species Cellular
Models}\label{transcriptformer-cross-species-cellular-models}

TranscriptFormer extends single-cell foundation models across
evolutionary time, training on over 112 million cells spanning 1.5
billion years of evolution across 12 species (Pearce et al. 2025). This
cross-species approach tests whether regulatory principles learned from
one organism generalize to others.

The model uses a novel autoregressive architecture that jointly predicts
genes and their expression levels. Rather than treating gene identity
and expression as separate prediction problems, TranscriptFormer
generates them together, enabling it to produce synthetic cells
conditioned on prompts specifying species, tissue, or cell type. Because
the vocabulary spans multiple species with ortholog mappings, the model
can transfer cell type annotations across evolutionary distances.

In zero-shot settings, TranscriptFormer demonstrates strong performance
on both in-distribution and out-of-distribution cell type
classification. Remarkably, models trained predominantly on mouse and
human data can annotate cell types in zebrafish and other species
separated by hundreds of millions of years of evolution. This
cross-species transfer reveals that core principles of cellular
regulation are deeply conserved, and that foundation models can capture
these conserved principles when trained on evolutionarily diverse data.

\section{Perturbation Response
Prediction}\label{perturbation-response-prediction}

\subsection{The In Silico Experiment
Promise}\label{the-in-silico-experiment-promise}

One of the most compelling applications of cellular foundation models is
predicting how cells will respond to perturbations. If a model truly
understands regulatory logic, it should be able to anticipate the
transcriptional consequences of knocking out a gene, activating a
pathway, or treating with a drug. Such predictions could accelerate drug
discovery by prioritizing candidates before expensive wet-lab
validation, identify synthetic lethal interactions for cancer therapy,
and suggest targets for diseases without known interventions.

The perturbation prediction task requires more than memorizing
co-expression patterns. The model must understand directional
relationships: if gene A activates gene B, then knocking out A should
reduce B's expression. It must capture network effects: perturbations
propagate through regulatory cascades, producing secondary and tertiary
effects beyond direct targets. It must recognize context dependence: the
same perturbation may have different effects in different cell types or
states.

\subsection{Perturb-seq and Foundation Model
Training}\label{perturb-seq-and-foundation-model-training}

Perturb-seq combines CRISPR-based genetic perturbations with single-cell
RNA sequencing, measuring the transcriptional consequences of gene
knockouts across thousands of cells
(\textbf{dixit\_perturb-seq\_2016?}). These datasets provide supervised
signal for perturbation prediction: given the pre-perturbation state and
the identity of the perturbed gene, predict the post-perturbation
expression profile.

Foundation models approach this task through transfer learning. A model
pretrained on tens of millions of unperturbed cells learns general
representations of cellular state and gene-gene relationships.
Fine-tuning on Perturb-seq data teaches the model to map these
representations to perturbation outcomes. The hope is that general
biological knowledge from pretraining will enable accurate predictions
for perturbations not seen during fine-tuning, including knockouts of
genes never directly perturbed in training data.

scGPT and Geneformer both demonstrate perturbation prediction
capabilities, though performance varies across perturbation types and
cellular contexts. Predictions are most accurate for well-characterized
genes with many training examples and clear regulatory relationships.
Performance degrades for poorly characterized genes, complex
combinatorial perturbations, and cell types underrepresented in training
data.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-perturbation-prediction}{[}High{]} Perturbation
response. Panel A (The Task): Unperturbed cell + perturbation identity â†’
predicted post-perturbation profile â†’ comparison to Perturb-seq. Panel B
(Training Data): CRISPR knockout library + single-cell readout;
thousands of genes Ã— cells; ``Supervised signal.'' Panel C (What Models
Must Learn): Directional relationships (A activates B â†’ KO A reduces B);
network cascades; context dependence. Panel D (Current Performance):
Strong for well-characterized genes; weak for poorly characterized;
``Predictions most accurate where we need them least.''}

\end{figure}%

\subsection{Limitations of Current
Approaches}\label{limitations-of-current-approaches}

Despite promising results, current perturbation prediction models face
fundamental limitations. Most training data come from immortalized cell
lines that may not reflect primary tissue biology. Perturbations are
typically single-gene knockouts; combinatorial perturbations involving
multiple genes remain challenging. The models predict average responses
across perturbed cells rather than the heterogeneity of individual
responses.

More fundamentally, correlation-based learning from expression data
cannot reliably distinguish correlation from causation. A gene that is
always co-expressed with another may be co-regulated rather than
directly regulating. Training on observational data (unperturbed cells)
and interventional data (perturbed cells) provides complementary
signals, but even Perturb-seq data have limited coverage of the
regulatory network. Foundation models capture patterns in data; whether
those patterns reflect causal regulatory relationships remains an
empirical question that requires experimental validation.

\section{Epigenomic Foundation
Models}\label{epigenomic-foundation-models}

\subsection{DNA Methylation and CpGPT}\label{dna-methylation-and-cpgpt}

DNA methylation occupies a privileged position in the regulatory
hierarchy, sitting at a junction between genotype, environment, and
phenotype. Methylation patterns integrate genetic influences, since
sequence context affects which CpG sites can be methylated and
polymorphisms can create or destroy CpG dinucleotides. They also
integrate developmental programs, since methylation landscapes are
extensively remodeled during differentiation and establish
cell-type-specific regulatory states. Environmental exposures including
diet, smoking, and stress leave lasting methylation signatures that
persist long after the exposure ends.

Beyond serving as an integrative readout, methylation encodes rich
information about cellular identity and state. Epigenetic clocks built
from methylation data predict chronological age with striking accuracy,
and deviations from predicted age (epigenetic age acceleration)
correlate with mortality risk and disease burden. Cell types can be
distinguished by their methylation profiles, and disease states often
manifest as characteristic methylation changes.

CpGPT (Cytosine-phosphate-Guanine Pretrained Transformer) treats
methylation as a sequence-like object amenable to transformer-based
pretraining (Camillo et al. 2024). The model was pretrained on over
1,500 DNA methylation datasets encompassing more than 100,000 samples
from diverse tissues and conditions. Each sample is tokenized as a
sequence of CpG sites with their methylation values (beta values ranging
from 0 to 1) and genomic positions. The model learns to predict masked
methylation values from surrounding context, capturing both local
correlations between neighboring CpG sites and global patterns that
distinguish different tissues or conditions.

After pretraining, CpGPT supports several capabilities with minimal
additional supervision. The model can impute methylation levels at CpG
sites not directly measured on a given array platform, effectively
enabling conversion between different array technologies such as EPIC
and 450K. For biological age prediction, fine-tuned CpGPT models match
or exceed purpose-built epigenetic clocks while using a more general
architecture. The learned embeddings cluster by tissue type without
explicit supervision during pretraining, suggesting that the model
captures biologically meaningful variation. For disease-associated
methylation patterns, CpGPT can be adapted to distinguish cases from
controls across multiple disease contexts through transfer learning.

\subsection{Chromatin Accessibility
Models}\label{chromatin-accessibility-models}

Chromatin accessibility, measured by ATAC-seq and related assays,
provides a complementary view of regulatory state. Accessible chromatin
regions mark active regulatory elements: promoters, enhancers, and
insulators where transcription factors can bind. The accessibility
landscape varies across cell types and conditions, reflecting the
regulatory programs that define cellular identity.

Foundation models for chromatin accessibility face the challenge of
representing accessibility peaks, which are genomic intervals of
variable width rather than single values at fixed positions. Different
approaches tokenize this data differently: some treat peaks as binary
features (accessible or not), others use continuous accessibility
scores, and some operate directly on the underlying sequence to predict
accessibility.

Models that predict chromatin accessibility from DNA sequence, such as
those built on Enformer-style architectures (see
Chapter~\ref{sec-regulatory}), learn how sequence motifs and their
arrangements determine accessibility. These models complement
single-cell accessibility measurements by providing a mechanistic link
between genotype and epigenetic state. Variants that alter predicted
accessibility become candidates for regulatory function even when they
fall outside coding regions.

Single-cell ATAC-seq (scATAC-seq) provides cell-type-resolved
accessibility profiles, revealing which regulatory elements are active
in which cells. Foundation models for scATAC-seq face similar challenges
to scRNA-seq models (sparsity, dropout, batch effects) with the
additional complexity that the feature space (accessibility peaks)
varies across datasets depending on peak calling procedures. Models that
operate on fixed genomic coordinates can integrate across datasets more
readily than those that rely on dataset-specific peak sets.

\section{Cross-Modality Integration}\label{cross-modality-integration}

\subsection{The Unpaired Integration
Challenge}\label{the-unpaired-integration-challenge}

Single-cell experiments often profile different modalities in different
cells. A study might include scRNA-seq data from one set of cells,
scATAC-seq data from another set, and perhaps a small subset with both
modalities measured simultaneously through multiome protocols.
Integrating these data into a unified atlas requires aligning cells
across modalities when the feature spaces are entirely different.

This problem is harder than standard batch correction because there is
no direct correspondence between features. RNA-seq measures expression
across roughly 20,000 genes. ATAC-seq measures accessibility across
hundreds of thousands of peaks. A gene is not the same object as a peak.
Simple approaches assign peaks to nearby genes and use gene-level
summaries for alignment, but this conversion loses information about the
detailed structure of accessibility within regulatory regions and
introduces arbitrary choices about assignment rules.

\subsection{GLUE: Graph-Linked Unified
Embedding}\label{glue-graph-linked-unified-embedding}

GLUE (Graph-Linked Unified Embedding) addresses unpaired integration by
combining modality-specific encoders with a graph of biological prior
knowledge linking features across omics (Cao and Gao 2022). Rather than
converting features between modalities, GLUE explicitly encodes
regulatory relationships into a guidance graph and learns cell
embeddings that are consistent with this graph.

The architecture has three key components. Modality-specific variational
autoencoders provide encoders that map cells to a shared low-dimensional
latent space and decoders that reconstruct modality-specific features.
Generative distributions are tailored to each modality: negative
binomial for count data, appropriate alternatives for accessibility.

The feature graph encodes biological prior knowledge about relationships
between features across modalities. Nodes represent genes, peaks, and
other genomic features. Edges connect ATAC peaks to genes they might
regulate based on genomic proximity or chromatin conformation data.
Edges connect genes to transcription factors that bind their promoters.
This graph is provided as input rather than learned, allowing
incorporation of external knowledge from databases and literature.

A graph variational autoencoder learns feature embeddings from the
guidance graph. These embeddings are used in the decoders, tying
different modalities to a common regulatory backbone. Biologically
related features (a gene and its putative enhancer) have similar
representations, helping align the latent spaces.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-glue-architecture}{[}High{]} Multi-omics
integration. Panel A (Unpaired Integration Problem): RNA-seq in cells
A,B,C; ATAC-seq in D,E,F; no direct correspondence; goal: unified
embedding. Panel B (GLUE Architecture): Modality-specific VAE encoders;
feature graph linking genes â†” peaks; shared latent space; adversarial
alignment. Panel C (Feature Graph as Prior): Nodes (genes, peaks, TF
sites); edges (proximity, regulatory); graph VAE learns embeddings;
``Biological knowledge constrains alignment.'' Panel D (Applications):
Cross-modal prediction, regulatory inference, triple-omics integration.}

\end{figure}%

Adversarial alignment ensures that cell embeddings from different
modalities are truly integrated. A discriminator tries to distinguish
which modality produced each embedding, and encoders are trained to fool
the discriminator. This forces the encoders to produce
modality-invariant embeddings where cells from different assays occupy a
shared manifold reflecting biological rather than technical variation.

\subsection{Applications of Cross-Modal
Integration}\label{applications-of-cross-modal-integration}

GLUE enables several applications beyond basic integration. Triple-omics
integration combines gene expression, chromatin accessibility, and DNA
methylation measured in different cells, producing unified cell type
annotations that leverage all data types. Regulatory inference uses
learned feature embeddings to identify candidate enhancer-gene links,
providing a principled alternative to simple distance-based assignment.

Cross-modal prediction becomes possible once cells are aligned. The
model can predict chromatin accessibility from expression or vice versa,
enabling imputation of missing modalities. If a new dataset contains
only scRNA-seq, the integrated model can predict which accessibility
peaks would likely be active in each cell type based on expression
patterns.

SCGLUE extends the framework with optimizations for single-cell scale
and sparsity (Cao and Gao 2022). The adversarial alignment handles batch
effects common in single-cell experiments, and the graph structure
incorporates tissue-specific regulatory relationships. The model scales
to millions of cells while maintaining biological grounding from the
guidance graph.

The success of graph-guided integration demonstrates that biological
prior knowledge can regularize learning and improve alignment. The
feature graph constrains what the model learns, ensuring consistency
with known regulatory relationships while allowing discovery of new
patterns. This combination of learned representations with structured
biological knowledge provides a template for integrating foundation
model embeddings with domain expertise (see Chapter~\ref{sec-networks}
for further discussion of graph-based approaches).

\section{Practical Challenges and
Limitations}\label{practical-challenges-and-limitations}

\subsection{Batch Effects and Technical
Artifacts}\label{batch-effects-and-technical-artifacts}

Batch effects remain the dominant challenge in single-cell analysis.
Technical variation between experiments, protocols, and platforms can
exceed biological variation, causing cells to cluster by batch rather
than by type. Foundation models pretrained on diverse data may be more
robust to batch effects than models trained on narrow datasets, but
robustness is not guaranteed.

The problem is particularly acute when applying pretrained models to new
data from platforms or protocols not represented in pretraining. A model
trained predominantly on 10x Genomics data may perform poorly on
Smart-seq2 data, not because of biological differences but because of
systematic technical differences in capture efficiency, amplification
bias, and gene detection. Evaluation must carefully distinguish genuine
biological generalization from memorization of technical signatures.

\subsection{Cell Type Imbalance}\label{cell-type-imbalance}

Training corpora overrepresent common cell types while rare populations
are poorly captured. Immune cells, particularly from blood, dominate
many datasets. Rare cell types that may be disease-relevant, such as
specific neuronal subtypes or tissue-resident stem cells, appear
infrequently. Models may excel at distinguishing well-represented types
while struggling with rare or novel populations.

This imbalance has equity implications when certain tissues or
conditions are systematically undersampled. Neurological and psychiatric
diseases involve cell types less represented in current atlases than
blood or epithelial cells. Diseases affecting underrepresented
populations may be modeled less accurately if training data come
predominantly from European ancestry cohorts.

\subsection{Evaluation Complexity}\label{evaluation-complexity}

Evaluating single-cell foundation models is complicated by uncertain
ground truth. Cell type labels in training data reflect current
annotations that may be incomplete or inconsistent. Different studies
use different annotation schemes, different levels of granularity, and
different evidence standards. Performance metrics conflate model quality
with annotation quality.

Perturbation predictions face similar challenges. The ``correct''
transcriptional response to a perturbation depends on cell type,
context, and measurement technology. Even well-characterized
perturbations produce variable responses across replicates. Evaluation
protocols must acknowledge these uncertainties rather than treating
benchmarks as definitive ground truth.

\subsection{Causality and Mechanism}\label{causality-and-mechanism}

The most fundamental limitation is that correlation-based learning
cannot establish causation. Foundation models learn patterns of
co-occurrence: which genes appear together, which accessibility peaks
associate with which expression changes. These patterns may reflect
regulatory relationships, but they may also reflect confounding factors,
indirect associations, or artifacts of data processing.

The perturbation prediction task illustrates this limitation. A model
that accurately predicts perturbation outcomes for well-characterized
genes may be learning genuine regulatory logic, or it may be exploiting
superficial correlations that happen to work for genes with abundant
training data. Distinguishing these possibilities requires experimental
validation and careful analysis of model behavior on held-out
perturbations.

\section{From Sequence to State}\label{from-sequence-to-state}

Single-cell and epigenomic foundation models learn what states cells
occupy, complementing the sequence-based models that learn what
sequences encode. DNA and protein language models capture the
information content of genomic and protein sequence; cellular models
capture the configurations that cells assume in development,
homeostasis, and disease. These perspectives address different
biological questions: sequence determines the possible states a cell can
achieve, while cellular state reflects which possibilities are realized
in a given context. A complete understanding of gene regulation requires
both.

The representations learned by cellular foundation models enable
integration across scales and modalities. Cell embeddings serve as node
features in graph-based reasoning systems (Chapter~\ref{sec-networks}),
connecting expression profiles to protein interaction networks and
regulatory pathways. Three-dimensional genome organization
(Chapter~\ref{sec-3d-genome}) provides spatial context that constrains
which regulatory relationships can operate. Multi-omics integration
(Chapter~\ref{sec-multi-omics}) extends beyond expression to proteomics,
epigenomics, and clinical measurements. In each case, foundation model
embeddings provide the representational substrate that downstream
methods refine.

The ultimate goal extends beyond prediction to explanation: models that
identify the regulatory mechanisms underlying cellular state, the
variants that perturb those mechanisms, and the interventions that might
restore normal function. Current foundation models capture patterns in
cellular data with remarkable fidelity, enabling accurate cell type
classification, perturbation response prediction, and cross-dataset
integration. Whether those patterns reflect the causal structure of
biological regulation, or merely correlations useful for prediction,
remains open. Resolving this question requires continued integration of
computational modeling with experimental validation, connecting the
patterns that models learn to the mechanisms that biology employs.

\chapter{3D Genome Organization}\label{sec-3d-genome}

The human genome spans approximately two meters of linear DNA, yet it
must fit within a nucleus roughly ten micrometers in diameter: a
compaction ratio of nearly 200,000 to one. This folding is not random.
Specific sequences contact each other across vast genomic distances
while others remain isolated, and these contact patterns determine which
enhancers can activate which genes. An enhancer 500 kilobases from its
target can drive transcription only because intervening chromatin folds
to bring them into physical proximity. The regulatory models predict
expression from sequence within a fixed window, treating the genome as a
one-dimensional string. They cannot explain why an enhancer activates
one gene and not another when multiple promoters lie within range.

Disruptions to 3D genome architecture cause disease through mechanisms
that sequence alone cannot predict. When structural variants delete a
boundary between chromatin domains, enhancers can contact genes they
normally never reach, a phenomenon called enhancer hijacking that
underlies developmental disorders and cancer. The clinical consequences
depend entirely on which contacts are disrupted. A deletion that removes
a domain boundary may be pathogenic; an identical-sized deletion
preserving boundaries may be benign. Current variant effect prediction
tools largely ignore this spatial dimension, creating systematic blind
spots for structural variant interpretation.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-tad-disruption}{[}High{]} Clinical example. Panel A
(Normal Configuration): TAD with EPHA4 and limb enhancers; adjacent TAD
with WNT6/PAX3; boundary separating; ``Enhancers contact EPHA4, not
WNT6.'' Panel B (Boundary Deletion): Boundary removed; TADs merge;
enhancers can contact both genes. Panel C (Pathogenic Outcome): Limb
enhancers activate WNT6 ectopically; abnormal expression; brachydactyly.
Panel D (Interpretation Implications): Same-size deletions, different
outcomes; boundary-preserving may be benign; boundary-disrupting
pathogenic; ``Current VEP tools miss 3D effects.''}

\end{figure}%

This chapter examines how the genome folds, what determines that
folding, and how computational models predict 3D structure from
sequence. We begin with the hierarchy of chromatin organization, from
megabase-scale compartments through topologically associating domains to
kilobase-scale loops. We introduce the experimental methods (Hi-C,
Micro-C, imaging approaches) that measure these structures. The
technical core covers models that predict chromatin contacts from
sequence, including Akita, Orca, and C.Origami, examining what
architectural features they learn and where prediction fails. We extend
to spatial transcriptomics, where tissue architecture provides cellular
context for gene expression, and connect throughout to regulatory
models: what does spatial context add that sequence alone cannot
provide?

\section{Chromatin Organization
Hierarchy}\label{sec-chromatin-hierarchy}

The genome folds through multiple organizational levels, each with
distinct functional consequences and arising from different molecular
mechanisms. Understanding this hierarchy is essential for interpreting
both normal gene regulation and how structural variants cause disease.
The levels are not independent; they interact in complex ways that
computational models must capture to predict 3D structure accurately.

At the largest scale, chromosomes occupy distinct nuclear volumes called
chromosome territories. Gene-rich chromosomes tend toward the nuclear
interior while gene-poor chromosomes associate with the nuclear
periphery. This territorial organization limits which chromosomes can
exchange material during translocations: recurrent cancer-associated
translocations occur preferentially between chromosomes that occupy
neighboring territories. While chromosome territory organization has
clear functional implications, most computational models focus on
finer-scale structures where sequence determinants are more tractable.

Within chromosome territories, chromatin partitions into two major
compartment types distinguished by their transcriptional activity and
chromatin state. A compartments contain gene-rich, transcriptionally
active chromatin with open, accessible structure. B compartments contain
gene-poor, transcriptionally silent regions often associated with the
nuclear lamina at the nuclear periphery. This compartmentalization is
visible in Hi-C contact maps as a characteristic checkerboard pattern: A
compartment regions preferentially contact other A regions even when
separated by megabases, while B regions contact other B regions.
Compartment identity correlates strongly with histone modifications
(H3K27ac marks active A compartments; H3K9me3 marks repressive B
compartments) and changes during cellular differentiation as
lineage-specific genes shift between active and inactive states. The
molecular mechanism underlying compartmentalization appears to involve
phase separation: regions with similar chromatin states aggregate
through weak multivalent interactions, creating nuclear
microenvironments with distinct biochemical properties.

Below the megabase scale of compartments, the genome organizes into
topologically associating domains, or TADs: sub-megabase regions (median
approximately 800 kilobases in mammals) within which sequences contact
each other more frequently than with sequences outside the domain. TAD
boundaries appear as sharp transitions in contact frequency, visible in
Hi-C maps as triangular domains along the matrix diagonal. These
boundaries show remarkable conservation across mammalian species and
across cell types within a species, suggesting strong selective pressure
to maintain domain organization. The prevailing model holds that TADs
constrain enhancer-promoter interactions: regulatory elements within a
TAD can contact genes in the same domain, but boundaries prevent
crosstalk with genes in adjacent domains. This insulation function has
clear clinical relevance. Deletions that remove TAD boundaries allow
enhancers to contact genes they normally cannot reach. In a
well-characterized example, deletions removing the boundary between the
\emph{EPHA4} locus and the \emph{WNT6/PAX3} region allow limb enhancers
to ectopically activate \emph{WNT6}, causing brachydactyly and other
limb malformations (\textbf{lupiaÃ±ez\_disruptions\_2015?}).

The molecular basis of TAD formation is now well understood through the
loop extrusion model. The cohesin protein complex loads onto chromatin
and extrudes DNA bidirectionally, progressively enlarging the extruded
loop until it encounters an obstacle. The key obstacle is CTCF protein
bound to DNA in a specific orientation. When cohesin encounters CTCF
sites oriented toward each other (convergent orientation), extrusion
halts and a stable loop forms with the convergent CTCF sites at the loop
anchors. This model explains several key observations: TAD boundaries
are enriched for CTCF binding sites; CTCF motif orientation predicts
which sites will anchor loops (convergent pairs form loops while
divergent pairs do not); and acute degradation of cohesin eliminates
TADs within hours while leaving compartments intact. The distinction
between compartment and TAD formation mechanisms has important
implications for prediction. Models that capture CTCF binding and
orientation can predict TAD boundaries; predicting compartments requires
learning different sequence features associated with chromatin state.

At the finest scale, chromatin forms specific loops between defined
loci. Enhancer-promoter loops bring distal regulatory elements into
physical proximity with their target genes, while structural loops
between convergent CTCF sites establish the TAD framework. Most
enhancer-promoter contacts span less than 200 kilobases, but some extend
over a megabase. Detecting these fine-scale contacts requires
high-resolution data; the Micro-C method uses micrococcal nuclease
digestion to achieve nucleosome-level resolution, revealing contact
patterns invisible in standard Hi-C. The functional significance of
individual loops remains debated. Some loops appear essential for gene
activation; others may be structural features without direct regulatory
consequences.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-chromatin-hierarchy}{[}Essential{]} Four-level
visualization. Panel A (Chromosome Territories, Nuclear Scale): Nucleus
with distinct territories; gene-rich interior, gene-poor periphery.
Panel B (A/B Compartments, Megabase Scale): Hi-C checkerboard pattern; A
active interior, B inactive lamina-associated. Panel C (TADs,
Sub-Megabase Scale): Triangular domains; sharp boundaries; median
\textasciitilde800 kb; CTCF at boundaries. Panel D (Loops, Kilobase
Scale): Focal Hi-C enrichments; enhancer-promoter contacts; CTCF-CTCF
structural loops.}

\end{figure}%

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-loop-extrusion}{[}Essential{]} Loop extrusion
mechanism. Panel A (Cohesin Loading): Ring loading onto chromatin;
initial state no loop. Panel B (Bidirectional Extrusion): Cohesin
extruding DNA; loop growing progressively; intermediate states. Panel C
(CTCF Arrest): Cohesin encountering convergent CTCF; extrusion halted;
stable loop. Panel D (The Orientation Rule): Convergent â†’ â† (loop
anchors âœ“); divergent â† â†’ (no stable loop âœ—); tandem â†’ â†’ (extrusion
continues \textasciitilde). Inset: Acute cohesin degradation eliminates
TADs but compartments remain.}

\end{figure}%

\section{Measuring the 3D Genome}\label{sec-3d-measurement}

Predicting 3D genome structure requires training data: measurements of
which sequences contact which other sequences in real cells. Chromosome
conformation capture methods provide these measurements through a common
biochemical principle. Cells are crosslinked with formaldehyde to freeze
chromatin contacts in place; DNA is digested with restriction enzymes;
free DNA ends are ligated, preferentially joining fragments that were
spatially proximate; and the ligated junctions are identified through
sequencing. The frequency of junction reads between two genomic regions
reflects how often those regions were in physical contact across the
cell population.

Hi-C extends this principle genome-wide by incorporating biotinylated
nucleotides at ligation junctions, enabling purification of chimeric
fragments from the entire genome. The output is a contact matrix where
rows and columns represent genomic bins (typically 1 to 50 kilobases
depending on sequencing depth) and values represent contact frequencies
between bin pairs. Resolution depends directly on sequencing depth:
achieving 1 kilobase resolution requires billions of reads, while 10
kilobase resolution requires hundreds of millions. Raw contact
frequencies require extensive normalization to correct for biases from
GC content, restriction site density, and mappability. The ICE
(iterative correction and eigenvector decomposition) method and related
approaches remove these technical artifacts while preserving biological
signal.

The contact matrix encodes all levels of chromatin organization.
Compartments appear as the checkerboard pattern when viewing
megabase-scale interactions; TADs appear as triangular domains of
enriched contacts along the diagonal; and loops appear as focal
enrichments at specific off-diagonal positions. The matrix is dominated
by the polymer effect: sequences that are close in linear distance
contact each other frequently regardless of specific 3D structure,
creating strong signal along the diagonal that can obscure biologically
meaningful contacts at greater distances.

Beyond standard Hi-C, several technologies address specific limitations.
Micro-C achieves nucleosome-level resolution by using micrococcal
nuclease instead of restriction enzymes, revealing fine-scale contact
patterns invisible at standard Hi-C resolution. Single-cell Hi-C
measures contacts in individual cells, revealing that any two loci
contact each other in only 5 to 15 percent of cells, but the resulting
matrices are extremely sparse (most possible contacts are unmeasured in
any single cell). Imaging methods such as DNA FISH directly visualize
genomic loci in the nucleus, providing ground truth for computational
predictions but at much lower throughput than sequencing-based
approaches.

Training data for 3D prediction models comes primarily from a small
number of well-characterized cell lines. The lymphoblastoid cell line
GM12878 and the leukemia cell line K562 have deep Hi-C coverage across
multiple laboratories, making them the default training sets for most
models. Primary tissues and rare cell types have sparse coverage,
creating a significant gap between where models are trained and where
clinical applications require predictions. The 4D Nucleome Data Portal
and ENCODE provide the most comprehensive repositories of 3D genome
data, though coverage remains heavily biased toward common cell lines
and human samples.

\section{Predicting 3D Structure from Sequence}\label{sec-3d-prediction}

If 3D genome structure determines which enhancers contact which genes,
can we predict that structure directly from DNA sequence? Success here
would enable predicting how mutations affect chromatin folding even when
no Hi-C data exists for the relevant cell type or genetic variant. The
core prediction task takes DNA sequence surrounding a genomic region
(typically 1 to 2 megabases) as input and produces a predicted contact
matrix at some resolution (typically 2 to 10 kilobase bins) as output.
The training signal comes from experimentally measured Hi-C contact
frequencies, with models learning to map sequence features to the
patterns of contacts observed in real data.

\textbf{Akita} established the foundational approach for sequence-based
3D prediction (\textbf{fudenberg\_predicting\_2020?}). The architecture
uses dilated convolutions similar to SpliceAI (Chapter~\ref{sec-cnn}),
processing the one-dimensional input sequence through convolutional
layers that progressively expand the receptive field to approximately
one megabase. A symmetric output layer transforms the processed sequence
into a two-dimensional contact matrix. Training uses mean squared error
loss on log-transformed, distance-normalized contact frequencies, where
distance normalization removes the strong diagonal signal from the
polymer effect. Akita demonstrated that sequence alone contains
sufficient information to predict Hi-C contact maps with meaningful
accuracy. The model correctly predicts TAD boundaries, capturing the
enrichment of CTCF binding sites and transcription start sites at domain
edges. In silico mutagenesis (systematically introducing mutations and
observing predicted changes) reveals which sequence features most
strongly influence predicted contacts, providing interpretable links
between sequence and structure. The primary limitations are fixed
resolution (2 kilobase bins), training on a single cell type (GM12878),
and lack of explicit CTCF binding modeling.

\textbf{Orca} extended this approach through multi-scale architecture
that predicts at multiple resolutions simultaneously
(\textbf{zhou\_orca\_2022?}). Rather than predicting directly at fine
resolution, Orca first generates coarse predictions at megabase scale,
then progressively refines to kilobase scale. This cascaded approach
better captures the hierarchical nature of chromatin organization,
improving compartment predictions while maintaining TAD and loop
accuracy. Orca also introduced virtual 4C analysis: extracting the
predicted contact profile from any genomic viewpoint, enabling focused
analysis of specific loci without rerunning the model. The multi-scale
training objective combines losses at different resolutions, allowing
the model to learn both compartment-scale and loop-scale features from
the same training data.

\textbf{C.Origami} addressed the cell-type specificity problem by
incorporating CTCF ChIP-seq as auxiliary input
(\textbf{tan\_corigami\_2023?}). While Akita and Orca predict from
sequence alone, C.Origami takes both sequence and CTCF binding profiles,
enabling cell-type-specific predictions based on cell-type-specific CTCF
patterns. This design reflects the loop extrusion model: TAD formation
depends on where CTCF binds and in what orientation, and CTCF binding
varies across cell types. The practical advantage is transfer learning:
the model can be trained on cell types with expensive Hi-C data, then
applied to cell types where only CTCF ChIP-seq is available (a much
cheaper assay). C.Origami substantially outperforms sequence-only models
for cross-cell-type prediction, though it requires CTCF data as input
rather than predicting purely from sequence.

Other approaches explore different architectural and methodological
choices. DeepC uses transfer learning to adapt models trained on one
cell type to another, partially addressing the limited training data
problem. Epiphany applies diffusion models to 3D structure prediction,
treating contact matrix generation as a denoising process. Higashi
addresses single-cell Hi-C data specifically, using variational
autoencoders to impute missing contacts in the sparse single-cell
matrices. Emerging transformer-based models may better capture the very
long-range dependencies inherent in chromatin organization, though the
computational cost of attention over megabase sequences presents
challenges.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-3d-prediction-models}{[}High{]} Model comparison.
Panel A (Akita Architecture): 1 Mb DNA input â†’ dilated convolutions â†’
symmetric output â†’ 2 kb Hi-C. Panel B (Orca Multi-Scale): Coarse Mb
prediction â†’ progressive refinement â†’ multi-resolution loss. Panel C
(C.Origami with CTCF): Sequence + CTCF ChIP-seq â†’ cell-type-specific
predictions; transfer from Hi-C-rich to CTCF-only cells. Panel D
(Prediction vs.~Ground Truth): Example region; TAD boundaries correctly
positioned; loop anchors identified; ``Sequence contains substantial 3D
information.''}

\end{figure}%

Interpretability analysis reveals what these models learn about sequence
determinants of 3D structure. Attribution methods consistently identify
CTCF motifs as the strongest predictors of contact patterns, with
convergent CTCF pairs (motifs oriented toward each other) most strongly
associated with loop anchors. Transcription start sites contribute to
boundary predictions, consistent with the observation that active
promoters often coincide with domain edges. GC content correlates with
compartment identity (GC-rich regions tend toward A compartment), and
repetitive element composition shows systematic associations (LINE
elements with B compartment; Alu elements with A compartment). The
orientation rule for CTCF emerges naturally from training: models learn
that CTCF motif orientation, not just presence, predicts which sites
will anchor loops. This learned relationship matches the mechanistic
understanding from the loop extrusion model, providing validation that
models capture biologically meaningful features.

Despite these advances, significant limitations remain. Resolution is
constrained by training data; predicting nucleosome-level contacts
requires Micro-C training data that exists for few cell types. The
single-cell variation problem is fundamental: models trained on bulk
Hi-C predict population averages, but gene regulation may depend on the
stochastic 3D configurations in individual cells. Causality cannot be
established from prediction alone; a model may correctly predict that
two regions contact each other without revealing whether that contact
causes any functional consequence. Generalization to cell types distant
from training data remains uncertain, and the computational cost of
processing megabase sequences limits practical applications for
genome-wide analysis.

\section{3D Structure and Gene Regulation}\label{sec-3d-regulation}

Enformer (Chapter~\ref{sec-regulatory}) predicts gene expression from
sequence within a 200 kilobase window, sufficient to capture many
enhancer-promoter relationships but fundamentally limited by its
treatment of the genome as a one-dimensional string. This representation
cannot distinguish an enhancer that loops to a distant gene from one
blocked by a TAD boundary, nor can it explain cell-type-specific
contacts that activate different genes from the same enhancer in
different contexts. The 3D genome provides this missing context:
physical proximity through chromatin loops determines which regulatory
elements can communicate.

Consider an enhancer located 300 kilobases from two genes, one upstream
and one downstream. Linear models would predict similar regulatory
influence on both genes based on comparable distances. But if a TAD
boundary lies between the enhancer and the upstream gene, 3D structure
predicts that only the downstream gene receives regulatory input. The
boundary insulates the upstream gene from enhancer activity regardless
of linear proximity. This insulation function explains why TAD
boundaries show such strong evolutionary conservation: disrupting
boundaries allows regulatory crosstalk that can dysregulate gene
expression with pathogenic consequences.

The clinical significance is clearest in structural variant
interpretation. Deletions that remove TAD boundaries cause enhancer
hijacking, where regulatory elements gain access to genes in adjacent
domains. The \emph{EPHA4} locus provides the canonical example: limb
enhancers normally activate \emph{EPHA4} expression in developing limbs.
When deletions remove the TAD boundary separating \emph{EPHA4} from the
adjacent \emph{WNT6/PAX3} domain, these enhancers ectopically activate
\emph{WNT6}, causing limb malformations including brachydactyly and
polydactyly. Different deletion sizes produce different phenotypes
depending on which boundaries are removed and which new enhancer-gene
contacts form. Similar mechanisms operate in cancer, where structural
variants create novel enhancer-oncogene contacts that drive tumor
growth. The diagnostic challenge is substantial: predicting
pathogenicity of structural variants requires understanding which 3D
contacts will be disrupted and what new contacts will form, predictions
that sequence-only models cannot provide.

Integrating 3D predictions with expression models remains technically
challenging. Hybrid approaches use predicted contacts to weight enhancer
contributions: rather than treating all enhancers within a window
equally, weights reflect predicted contact frequency with the target
promoter. This activity-by-contact framework (expression proportional to
the sum of enhancer activities weighted by contact frequencies) captures
some of the regulatory logic that 1D models miss. Graph-based
representations (Chapter~\ref{sec-networks}) can encode genes and
enhancers as nodes with contacts as edges, enabling graph neural
networks to reason about regulatory relationships in 3D space.
End-to-end training of combined 3D and expression models remains
difficult; most current approaches train the components separately and
combine predictions post hoc.

The causality question complicates interpretation. Do enhancer-promoter
contacts cause gene activation, or does gene activation cause contacts?
Transcription itself can influence chromatin organization: active
transcription may stabilize enhancer-promoter contacts that would
otherwise be transient. Perturbation experiments provide cleaner causal
tests than correlational analysis. Acute degradation of cohesin
eliminates TADs within hours, yet most genes show minimal expression
changes, suggesting that many TAD structures are permissive rather than
deterministic for gene regulation. CRISPR-based deletion of specific TAD
boundaries similarly produces more modest effects than the structural
disruption would suggest. The emerging view is nuanced: 3D structure
constrains which enhancer-promoter interactions are possible, but
whether those interactions occur depends on additional factors including
transcription factor availability and chromatin state.

\section{Spatial Transcriptomics}\label{sec-spatial-transcriptomics}

Single-cell RNA sequencing (Chapter~\ref{sec-single-cell}) reveals
cellular heterogeneity but discards spatial information: we learn which
genes each cell expresses but not where that cell sits within the
tissue. For understanding tumor microenvironments, developmental
gradients, or tissue architecture, spatial context is essential. A T
cell adjacent to a tumor cell experiences a different microenvironment
than one in the surrounding stroma, and this spatial context shapes gene
expression programs in ways that dissociated single-cell data cannot
capture.

Spatial transcriptomics technologies fall into two broad categories with
complementary strengths. Spot-based methods like Visium (10x Genomics)
capture polyadenylated RNA at arrayed positions on a slide, providing
transcriptome-wide measurement at approximately 55 micrometer resolution
(typically 1 to 10 cells per spot). These methods offer comprehensive
gene coverage but limited spatial resolution. Imaging-based methods like
MERFISH use sequential rounds of fluorescent hybridization to identify
RNA molecules in situ, achieving subcellular resolution but limited to
pre-selected gene panels (hundreds to thousands of genes rather than
transcriptome-wide). Newer technologies like Stereo-seq achieve
near-cellular resolution with transcriptome-wide coverage through
spatial barcoding, though they remain less validated than established
methods.

Computational challenges in spatial transcriptomics mirror and extend
those in single-cell analysis. Spot deconvolution addresses the
multiple-cells-per-spot problem in Visium data: inferring the cell type
composition within each spot by comparing spot expression profiles to
reference single-cell atlases. Imputation methods predict expression of
genes not measured in imaging-based assays, leveraging correlations
learned from reference datasets. Integration aligns spatial data with
single-cell references, mapping reference cell types onto spatial
coordinates. Domain correction handles batch effects that manifest in
spatial patterns as well as expression levels. The sparsity problem is
even more severe than in standard scRNA-seq; gene detection rates in
spatial methods often fall below 10 percent.

Spatial foundation models remain much less mature than sequence-based
models. The fundamental challenge is the lack of an equivalent to
evolutionary pretraining: DNA and protein models learn from billions of
years of evolutionary experiments encoded in sequence databases, but no
comparable natural augmentation exists for spatial organization. Current
approaches include graph neural networks that encode spatial
relationships as edges between neighboring cells or spots, transformer
architectures that treat spatial positions as tokens with positional
encodings derived from coordinates, and generative models that learn
spatial patterns from atlases of reference tissues. Models like
Nicheformer apply transformer architectures to spatial niches (local
cellular neighborhoods), learning representations that capture cell-cell
communication patterns and tissue microenvironment signatures. SpaGCN
uses graph convolutional networks with spatial graphs, propagating
information between spatially adjacent regions to identify spatial
domains with coherent expression patterns.

Other approaches address different aspects of the spatial modeling
problem. CellPLM pretrains on millions of spatial transcriptomics cells,
learning representations that transfer across tissue types and
experimental platforms. STACI combines spatial coordinates with
morphological features from histology images, enabling joint reasoning
about molecular and visual tissue properties. GraphST uses graph
attention networks to propagate expression signals across spatial
neighborhoods while preserving local heterogeneity. These methods remain
early in development compared to sequence foundation models; no spatial
equivalent of DNABERT or ESM-2 has achieved broad adoption, and
benchmark comparisons across methods remain limited by the diversity of
spatial platforms and tissue types.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-spatial-transcriptomics}{[}Enhancing{]} Spatial data
and models. Panel A (Visium Spot-Based): Tissue section with spot
overlay; 1-10 cells per spot; \textasciitilde55 Î¼m resolution. Panel B
(Imaging-Based MERFISH/Xenium): Single-cell resolution; limited gene
panel; subcellular localization. Panel C (Deconvolution Challenge):
Multi-cell spot â†’ infer composition; reference atlas required. Panel D
(Spatial Foundation Models): Cell as node, proximity as edge; GNNs over
spatial graphs; cell-cell communication; examples (Nicheformer, SpaGCN,
GraphST).}

\end{figure}%

The clinical applications motivating spatial foundation model
development center on tumor microenvironment characterization. The
spatial organization of immune cells relative to tumor cells predicts
treatment response: tumors with immune cells infiltrating the tumor core
respond better to immunotherapy than those with immune exclusion at the
tumor periphery. Spatial models aim to learn these prognostic patterns
from training data, enabling prediction of treatment response from
spatial organization alone. Similar applications exist in developmental
biology (understanding morphogen gradients and cell fate decisions),
neuroscience (mapping brain region organization), and pathology
(characterizing disease architecture in tissue sections).

\section{Limitations and Open Questions}\label{sec-3d-limitations}

Current 3D genome and spatial models face limitations that constrain
their utility for clinical and research applications. Resolution remains
a fundamental constraint: most Hi-C prediction models operate at 2 to 10
kilobase resolution, while functionally relevant enhancer-promoter
contacts involve specific sequences within those bins. Predicting which
specific kilobases within a TAD contact each other requires resolution
that exceeds current training data in most cell types. The resolution
needed for accurate prediction may exceed the resolution achievable from
bulk Hi-C, creating a data ceiling that computational methods cannot
overcome.

The population averaging problem is more fundamental than a mere
technical limitation. Bulk Hi-C measurements average over millions of
cells, each with a different 3D configuration. Any two loci contact each
other in only a minority of cells at any given time, yet the averaged
contact frequency appears as a single value in the training data.
Single-cell Hi-C reveals this heterogeneity but produces extremely
sparse data (most possible contacts unmeasured in each cell). Models
trained on population averages cannot predict single-cell behavior, yet
gene regulation may depend on the stochastic dynamics of contact
formation in individual cells. Whether the population average or the
single-cell distribution matters more for predicting gene expression
remains unclear.

Causality represents the deepest conceptual challenge. Predicting that
two regions contact each other does not establish that the contact
causes any biological consequence. Many TAD disruptions produce minimal
expression changes; many enhancer-promoter contacts may be bystanders
rather than drivers of transcription. The loop extrusion machinery that
creates TADs operates continuously, but the transcriptional machinery
that reads out enhancer-promoter communication operates on different
timescales and with different requirements. Computational predictions of
3D structure are correlational; establishing which predicted contacts
matter functionally requires experimental validation that computational
methods cannot replace.

For clinical applications, the sparse training data creates systematic
blind spots. Models trained on GM12878 and K562 may not transfer to the
primary cells, developmental stages, or disease states where predictions
matter most. A structural variant affecting 3D organization in neural
progenitor cells cannot be reliably interpreted using models trained
only on lymphoblastoid cells. The cell types most relevant for clinical
interpretation are often those with the least 3D characterization data
available.

\section{Structure as Context, Not
Cause}\label{structure-as-context-not-cause}

The genome's three-dimensional organization provides context that
one-dimensional sequence models cannot capture. Enhancer-promoter
contacts explain regulatory relationships spanning hundreds of
kilobases; TAD boundaries constrain which elements can interact; tissue
architecture determines the cellular neighborhoods where gene expression
programs execute. Models like Akita, Orca, and C.Origami demonstrate
that sequence contains substantial information about chromatin folding,
predicting contact maps from DNA sequence with accuracy sufficient to
identify structural variants and disease-associated changes.

Yet the functional role of 3D structure remains more modest than early
enthusiasm implied. Experimental perturbation studies show that TAD
boundary disruption often has limited expression consequences. Many
chromatin contacts appear permissive rather than instructive: they
establish the possibility of regulatory communication without
determining whether that communication occurs. A predicted
enhancer-promoter contact indicates that interaction could happen, not
that it does happen or that it matters when it does. The 3D genome may
constrain the regulatory landscape without specifying regulatory
outcomes.

This distinction shapes how 3D structure should be integrated with other
modalities. Chromatin contacts become edges in gene regulatory networks
(Chapter~\ref{sec-networks}), providing structural priors for
graph-based reasoning. Spatial expression patterns integrate with
multi-omics approaches (Chapter~\ref{sec-multi-omics}), adding tissue
architecture alongside genomics and transcriptomics. For
interpretability (Chapter~\ref{sec-interpretability}), 3D structure
offers mechanistic hypotheses that require experimental validation.
Whether a predicted regulatory effect operates through chromatin
proximity, or whether proximity merely correlates with regulation
through shared causes, remains a question that computational models can
motivate but not answer. The integration of 3D information into genomic
AI proceeds with appropriate uncertainty about what that information
contributes.

\chapter{Graph and Network Models}\label{sec-networks}

Graph neural networks are not alternatives to foundation models; they
are consumers of them. The foundation models explored in earlier
chapters produce rich representations of individual biological entities:
protein language models encode evolutionary constraint and structural
propensity, DNA models capture regulatory grammar, RNA models represent
transcript-level features. These representations are powerful but
operate on isolated sequences. A protein embedding captures what ESM
learned about that protein's sequence; it says nothing about which other
proteins it binds, which pathways it participates in, or which disease
phenotypes result from its disruption. Graph neural networks operate at
a higher level of abstraction, taking foundation model representations
as node features and learning to propagate information across relational
structure. The combination yields capabilities that neither approach
achieves alone.

This architectural relationship reflects a biological reality: organisms
are not collections of independent molecules but systems of interacting
components. A transcription factor affects its target genes through
regulatory edges. Proteins assemble into functional complexes through
physical binding. Signaling cascades propagate perturbations across
cellular networks. These relationships exist at a level of abstraction
above sequence, requiring a different mathematical framework to
represent. Graphs provide precisely this framework. In a protein-protein
interaction network, proteins become nodes and physical binding creates
edges. In a gene regulatory network, directed edges connect
transcription factors to their targets. In spatial transcriptomics data,
cells become nodes with edges capturing physical proximity. Each graph
encodes relational structure that sequence models cannot directly
capture.

The practical implications are substantial. Disease gene prioritization
leverages the observation that genes causing similar diseases cluster in
network neighborhoods. A GNN can learn to propagate disease signals
across protein interaction networks, but effectiveness depends
critically on node feature quality. When those features come from
protein language models encoding evolutionary constraint and structural
propensity, the GNN gains access to sequence-level biological knowledge
unavailable from simpler features like expression levels alone.
Drug-target interaction prediction similarly benefits: ESM embeddings
capture what makes a protein druggable, while network context reveals
which targets sit in therapeutically relevant pathways. This chapter
examines how to combine foundation model representations with graph
structure across genomic applications.

\section{Biological Networks and Data
Resources}\label{biological-networks-and-data-resources}

\subsection{The Landscape of Biological
Graphs}\label{the-landscape-of-biological-graphs}

Before examining graph neural network architectures, it is essential to
understand what biological networks exist and where they come from. The
choice of network fundamentally shapes what a model can learn, and the
biases inherent in network construction propagate through all downstream
analyses.

\textbf{Protein-protein interaction networks} represent physical
associations between proteins. Major databases include STRING, which
integrates experimental data with computational predictions and text
mining to assign confidence scores to interactions; BioGRID, which
focuses on curated experimental interactions; and IntAct, which provides
detailed interaction metadata from direct molecular experiments. These
networks are incomplete (current estimates suggest only 20-30\% of human
PPIs are catalogued) and biased toward well-studied proteins in
well-characterized pathways. A gene involved in cancer or a common
disease may have hundreds of documented interactions, while an
uncharacterized protein in a specialized tissue may have none, not
because it lacks interactions but because no one has looked.

\textbf{Gene regulatory networks} encode transcriptional control
relationships. Unlike PPIs, regulatory networks are inherently directed:
a transcription factor activates or represses its targets, not vice
versa. Sources include ChIP-seq experiments that identify transcription
factor binding sites, chromatin accessibility data (ATAC-seq, DNase-seq)
that reveals active regulatory regions, and chromosome conformation
capture (Hi-C) that maps enhancer-promoter contacts. Databases like
ENCODE and the Roadmap Epigenomics Project provide regulatory
annotations across cell types, though coverage varies dramatically by
tissue. Computational methods infer regulatory edges from expression
correlations or sequence motifs, but such predictions contain
substantial false positives and miss context-specific interactions.

\textbf{Pathway and metabolic networks} organize biochemical knowledge
into structured representations. KEGG, Reactome, and WikiPathways curate
reactions, enzymatic steps, and signaling cascades into hierarchical
graphs where nodes can represent genes, proteins, metabolites, or
abstract pathway concepts. These networks encode decades of molecular
biology knowledge but reflect historical research priorities: metabolism
and signal transduction are well-characterized, while more recently
discovered processes like autophagy or RNA modification have sparser
coverage.

\textbf{Knowledge graphs} extend beyond molecular interactions to encode
relationships among genes, diseases, drugs, phenotypes, and other
biomedical entities. Unlike protein interaction networks, which contain
a single node type and edge type, knowledge graphs are inherently
heterogeneous: nodes represent diverse entity classes, and edges capture
semantically distinct relationship types. This heterogeneity enables
richer reasoning but demands architectures capable of handling multiple
node and edge embeddings.

Several large-scale biomedical knowledge graphs have become standard
resources. Hetionet integrates 47,031 nodes across 11 types (genes,
diseases, compounds, anatomies, and others) with 2.25 million edges
spanning 24 relationship types, providing a comprehensive substrate for
computational drug repurposing. The Unified Medical Language System
(UMLS) aggregates over 200 biomedical vocabularies into a metathesaurus
linking millions of concepts through hierarchical and associative
relationships. PrimeKG consolidates 17 biological databases into a
precision medicine knowledge graph with over 4 million relationships
connecting diseases, drugs, genes, pathways, and phenotypes, explicitly
designed for machine learning applications.

Disease-gene association databases provide critical edges for clinical
applications. DisGeNET curates over one million gene-disease
associations from expert-reviewed sources, GWAS catalogs, and text
mining, assigning evidence scores that enable confidence-based
filtering. OMIM (Online Mendelian Inheritance in Man) provides
authoritative curation of Mendelian disease genes, while OrphaNet
focuses on rare diseases with detailed phenotypic annotations. The
Clinical Genome Resource (ClinGen) adds expert-reviewed gene-disease
validity assessments using standardized evidence frameworks.

Drug-centric resources complete the translational picture. DrugBank
provides comprehensive drug-target annotations with mechanism and
pharmacology details. ChEMBL aggregates bioactivity data from medicinal
chemistry literature, linking compounds to protein targets through
binding affinity measurements. The Drug Gene Interaction Database
(DGIdb) consolidates druggable gene categories and known interactions to
support target prioritization.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-biological-networks}{[}Essential{]} Network
landscape. Panel A (PPI Networks): Undirected edges (physical binding);
STRING, BioGRID, IntAct; ``20-30\% catalogued.'' Panel B (Gene
Regulatory Networks): Directed TF â†’ target edges; ChIP-seq, ATAC-seq,
motifs; cell-type specific. Panel C (Knowledge Graphs): Multiple node
types (genes, diseases, drugs, pathways); multiple edge types; multi-hop
reasoning; Hetionet example. Panel D (Spatial Graphs): Nodes as
cells/spots; edges as proximity/communication; from spatial
transcriptomics; emerging and sparse.}

\end{figure}%

The power of knowledge graphs lies in their support for multi-hop
reasoning. A query asking whether a drug might treat a disease can
traverse multiple edge types: drug inhibits protein A, protein A
interacts with protein B, protein B is implicated in disease. Each hop
contributes evidence, and the combination of paths provides signal that
no single edge contains. Graph neural networks learn to aggregate across
such paths, weighting different relationship types and path lengths
according to their predictive value for specific tasks.

\textbf{Spatial and cell-cell interaction graphs} arise from spatially
resolved transcriptomics and imaging data. Nodes represent cells or
spatial locations, edges encode physical proximity or inferred
ligand-receptor communication. These graphs capture tissue organization
invisible to bulk or even single-cell measurements, enabling questions
about how spatial context influences cell behavior.

\subsection{Biases and Limitations}\label{biases-and-limitations}

All biological networks share systematic biases that affect downstream
modeling. Well-studied genes appear as highly connected hubs not
necessarily because they have more interactions but because researchers
have investigated them more thoroughly. This ascertainment bias means
that GNNs trained on network structure may primarily learn to propagate
signals toward well-characterized genes, potentially missing novel
biology in peripheral network regions.

Network incompleteness creates particular challenges for message passing
algorithms. If a critical interaction is missing, information cannot
flow across that gap. If a spurious interaction is present, noise
propagates where it should not. These issues are especially acute for
less-studied organisms, tissues, or disease contexts where network
coverage is sparse.

The distinction between physical and functional associations matters for
interpretation. A protein-protein interaction might represent stable
complex membership, transient signaling, or indirect association through
shared binding partners. Different edge types may warrant different
treatment by graph models, but many databases conflate these categories
or provide insufficient metadata to distinguish them.

\section{Graph Neural Network
Fundamentals}\label{graph-neural-network-fundamentals}

\subsection{Why Message Passing?}\label{why-message-passing}

The challenge of learning from graph-structured data lies in the
irregular topology: unlike images (regular grids) or sequences (linear
chains), graphs have variable-degree nodes, no inherent ordering, and
complex connectivity patterns. Classical approaches computed
hand-crafted features such as degree centrality, clustering
coefficients, or shortest path statistics, then fed these to standard
machine learning models. Such features capture useful properties but
cannot adapt to task-specific patterns.

\textbf{Message passing} provides a learnable alternative. The core
intuition is local information exchange: each node should update its
representation based on what its neighbors know. By iterating this
process across multiple layers, information propagates across the graph,
allowing nodes to incorporate signals from increasingly distant parts of
the network.

Formally, at layer \(\ell\), each node \(i\) maintains a hidden state
\(\mathbf{h}_i^{(\ell)}\). A message passing layer computes, for each
edge from neighbor \(j\) to node \(i\), a message:

\[
\mathbf{m}_{ij}^{(\ell)} = \phi_m\left(\mathbf{h}_i^{(\ell)}, \mathbf{h}_j^{(\ell)}, \mathbf{e}_{ij}\right)
\]

where \(\phi_m\) is a learned function and \(\mathbf{e}_{ij}\)
represents edge features. The node then aggregates messages from all
neighbors and updates its state:

\[
\mathbf{h}_i^{(\ell+1)} = \phi_h\left(\mathbf{h}_i^{(\ell)}, \bigoplus_{j \in \mathcal{N}(i)} \mathbf{m}_{ij}^{(\ell)}\right)
\]

where \(\mathcal{N}(i)\) denotes neighbors of node \(i\) and
\(\bigoplus\) is a permutation-invariant aggregation (sum, mean, max, or
attention-weighted combination). The aggregation must be
permutation-invariant because neighbors have no inherent ordering.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-message-passing}{[}High{]} Step-by-step message
passing. Panel A (Initial State): 5-node graph; each with initial
features from FM. Panel B (Message Computation): For each edge, compute
message m\_ij = Ï†(h\_i, h\_j, e\_ij); visualize as arrows. Panel C
(Aggregation): Each node aggregates incoming (sum, mean, max,
attention); new representation. Panel D (After L Layers): Each embedding
incorporates L-hop neighborhood; ``Gene embedding now reflects pathway
context.'' Mathematical summary at bottom.}

\end{figure}%

After \(L\) layers, a node's representation incorporates information
from all nodes within \(L\) hops. For biological networks, this means a
gene's learned embedding can reflect not only its own features but
signals from interaction partners, their partners, and so on, capturing
pathway-level and module-level context.

\subsection{Canonical Architectures}\label{canonical-architectures}

Several GNN architectures have become standard tools for biological
applications, each with distinct design choices.

\textbf{Graph Convolutional Networks (GCN)} perform normalized
neighborhood averaging followed by linear transformation and
nonlinearity. GCNs are computationally efficient and conceptually
straightforward but suffer from \textbf{over-smoothing} when stacked
deeply: repeated averaging causes node representations to converge,
losing the discriminative signal that distinguishes different network
positions.

\textbf{GraphSAGE} addresses scalability by learning aggregation
functions that operate on sampled neighborhoods rather than the full
neighbor set. This enables mini-batch training on large graphs and
provides inductive capability: the model can generate embeddings for
nodes not seen during training by applying learned aggregators to their
neighborhoods. For biological networks that grow as new genes are
characterized, this generalization is valuable.

\textbf{Graph Attention Networks (GAT)} introduce attention mechanisms
to weight neighbors differentially based on relevance. Rather than
treating all interactions equally, GAT learns compatibility scores
between node pairs, allowing the model to focus on the most informative
relationships. In biological contexts, attention weights are often
interpreted as highlighting key regulatory connections or critical
binding partners, though such interpretations require careful
validation.

\textbf{Graph Transformers} extend transformer architectures to graphs,
replacing local message passing with structured or global attention.
Some variants attend over all node pairs with positional encodings
derived from graph structure (shortest paths, Laplacian eigenvectors);
others restrict attention to k-hop neighborhoods. These architectures
blur the boundary between sequence and graph models, potentially
capturing long-range dependencies that multi-layer message passing
struggles to propagate.

The expressiveness of GNNs is bounded by their ability to distinguish
different graph structures. Theoretical analysis connects standard
message passing to the Weisfeiler-Lehman graph isomorphism test,
revealing that certain graph structures remain indistinguishable
regardless of the number of layers. For most biological applications,
this theoretical limitation is less constraining than practical issues
of data quality, training efficiency, and interpretability.

\section{Foundation Model Embeddings as Node
Features}\label{foundation-model-embeddings-as-node-features}

\subsection{The Integration Principle}\label{the-integration-principle}

The central architectural insight for genomic graph learning is that
foundation models and graph neural networks operate at complementary
levels of abstraction. Sequence-based foundation models excel at
extracting biological information from linear sequences: ESM-2 learns
evolutionary constraints and structural propensities from protein
sequences; DNABERT and its successors capture regulatory motifs and
sequence grammar; single-cell foundation models like scGPT learn cell
state representations from expression profiles. These representations
encode rich biological knowledge but operate on individual entities
without explicit relational information.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-gnn-integration}{[}Essential{]} Central concept.
Panel A (Foundation Models Produce Representations): ESM-2 â†’ protein
embeddings; DNABERT â†’ DNA embeddings; scGPT â†’ cell embeddings; ``Rich
representations of individual entities.'' Panel B (Biological Networks
Encode Relationships): PPI, regulatory, cell-cell communication;
``Relational structure not in sequence.'' Panel C (GNNs Integrate Both):
Node features from FMs; edges from networks; message passing refines
representations; context-aware outputs. Panel D (Capabilities Neither
Achieves Alone): FM can't model interactions; networks alone limited
features; combined enables disease gene prioritization, drug-target
prediction.}

\end{figure}%

Graph neural networks excel at learning from relational structure but
require informative node features to propagate. When node features are
uninformative (simple one-hot encodings or scalar expression values),
message passing can only learn from network topology. When node features
carry substantial biological signal, message passing can refine and
contextualize that information based on network position.

Combining these approaches follows a natural two-stage pattern. First,
apply a foundation model to each entity in the graph to generate initial
node embeddings. For a protein-protein interaction network, run ESM-2 on
each protein sequence; for a gene regulatory network, use DNA embeddings
for regulatory elements and protein embeddings for transcription
factors; for a cell graph, apply scGPT to generate cell state
representations. Second, train a GNN on these embeddings using the
biological graph structure, allowing message passing to integrate
entity-level representations with relational context.

This combination yields capabilities that neither component achieves
alone. The foundation model provides rich, transferable features that
would require massive labeled datasets to learn from scratch. The GNN
provides relational reasoning that sequence models cannot perform. A
protein's druggability depends both on intrinsic properties (binding
pocket geometry, expression pattern) that ESM captures and on network
context (pathway position, interaction partners) that the GNN
integrates.

\subsection{Practical Integration
Patterns}\label{practical-integration-patterns}

Several integration patterns have emerged in practice. The simplest
approach freezes foundation model weights and treats embeddings as fixed
features, training only the GNN layers. This is computationally
efficient and prevents catastrophic forgetting of pretrained knowledge
but limits the model's ability to adapt representations to the specific
task.

\textbf{Joint fine-tuning} allows gradients to flow through both the GNN
and (parts of) the foundation model, enabling end-to-end optimization.
This typically requires careful learning rate scheduling, with smaller
updates to foundation model parameters and larger updates to GNN layers.
The approach can improve performance when sufficient task-specific data
is available but risks overfitting and requires substantially more
computation.

\textbf{Adapter-based integration} inserts small trainable modules
between foundation model layers or at the interface between foundation
model outputs and GNN inputs. This provides task adaptation with modest
parameter overhead, avoiding full fine-tuning costs while retaining
flexibility.

\textbf{Multi-scale integration} uses foundation model representations
at multiple granularities. For proteins, one might extract both
per-residue embeddings (capturing local structure) and sequence-level
embeddings (capturing global properties), concatenating these as node
features. For regulatory networks, one might combine nucleotide-level
DNA embeddings with region-level chromatin accessibility predictions.

The choice of integration pattern depends on data availability,
computational resources, and the degree of distribution shift between
foundation model pretraining and the target application. For
well-characterized systems with substantial labeled data, joint
fine-tuning may be warranted. For novel organisms or rare diseases with
limited labels, frozen embeddings with simple GNN layers often
generalize better.

\subsection{Evidence for the Integration
Benefit}\label{evidence-for-the-integration-benefit}

Empirical studies consistently demonstrate that foundation model
embeddings improve GNN performance on biological tasks. In protein
function prediction, ESM embeddings combined with PPI network GNNs
substantially outperform either sequence-only or network-only baselines
(\textbf{gligorijevic\_structure\_2021?}). The improvement is
particularly pronounced for proteins with few characterized interaction
partners, where network structure alone provides limited signal but
sequence features carry evolutionary information.

For disease gene prioritization, combining DNA and protein foundation
model embeddings with multi-relational GNNs over heterogeneous
biological networks improves ranking of causal genes from GWAS loci
(\textbf{schulte\_schrepping\_analysis\_2020?}). The foundation model
features help distinguish genes with similar network positions based on
sequence-level functional signals.

In single-cell analysis, scGPT embeddings combined with cell-cell
communication graphs enable more accurate prediction of perturbation
effects than either component alone (Cui et al. 2024). The cell
embeddings capture transcriptional state, while the graph structure
encodes spatial and molecular interaction context.

These results suggest that the integration principle generalizes across
biological domains. The specific foundation models and graph types vary,
but the architectural pattern (rich entity embeddings + relational
structure + message passing) consistently outperforms simpler
alternatives.

\section{Applications}\label{applications}

\subsection{Disease Gene
Prioritization}\label{disease-gene-prioritization}

Genome-wide association studies identify genomic loci associated with
disease risk but rarely pinpoint causal genes (Chapter~\ref{sec-gwas}).
A typical GWAS locus contains dozens of genes, most of which are
passengers linked to the true causal variant through linkage
disequilibrium. Identifying which gene(s) mediate the association
requires integrating functional evidence with genetic signal.

Network-based prioritization leverages the observation that disease
genes cluster in biological networks. If a GWAS locus contains genes A,
B, and C, and gene B interacts with five known disease genes while A and
C interact with none, gene B becomes a stronger causal candidate. Graph
neural networks formalize and extend this intuition, learning to
propagate disease labels through networks and score candidate genes
based on their network context.

The integration with foundation models strengthens this approach. Rather
than relying solely on network topology, which favors well-studied hub
genes, the model can assess each candidate's intrinsic functional
properties through sequence embeddings. A gene with protein features
characteristic of disease-relevant functions (membrane localization, DNA
binding, signaling domains) receives higher scores even if its network
position is peripheral. This helps mitigate the ascertainment bias
toward well-characterized genes that plagues purely topological methods.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-disease-gene-prioritization}{[}High{]} GWAS
follow-up. Panel A (The GWAS Challenge): Locus with genes A,B,C,D; lead
SNP in LD with all; which is causal? Panel B (Network Context): Same
genes in PPI; gene B connected to 5 known disease genes; A,C,D
peripheral. Panel C (GNN Scoring): FM embeddings as node features;
disease labels propagate; B receives higher network score. Panel D
(Integration with Sequence Features): Gene C strong protein features;
gene B network + moderate protein; combined identifies both candidates.}

\end{figure}%

Clinical applications include rare disease diagnosis, where patient
exome sequencing identifies hundreds of candidate variants and
network-based scoring helps prioritize which genes to investigate
further (Chapter~\ref{sec-rare-disease}). The approach also supports
drug target identification by highlighting genes whose network position
and functional properties make them amenable to therapeutic modulation
(Chapter~\ref{sec-drug-discovery}).

\subsection{Drug-Target Interaction
Prediction}\label{drug-target-interaction-prediction}

Identifying which proteins a drug binds is fundamental to understanding
mechanism and predicting side effects. Experimental screening of
drug-target pairs is expensive and incomplete; computational prediction
can prioritize candidates for validation.

Drug-target interaction prediction naturally fits a graph framework.
Construct a heterogeneous graph with drug nodes, protein nodes, and
edges representing known interactions. Node features for proteins come
from sequence foundation models; node features for drugs come from
molecular encodings (fingerprints, learned representations from
molecular graphs). Train a GNN to predict missing edges, learning which
drug and protein features, combined with network context, indicate
likely binding.

The foundation model integration is critical here. Protein embeddings
from ESM capture binding pocket characteristics, domain structure, and
evolutionary constraint that influence druggability. The graph structure
provides context: if a drug binds protein A, and protein A participates
in complex with protein B, then the drug may also affect protein B's
function. Multi-relational GNNs can learn different propagation patterns
for different edge types (physical binding versus pathway membership
versus sequence similarity), improving prediction accuracy.

This application connects to broader drug discovery workflows
(Chapter~\ref{sec-drug-discovery}), where target identification is one
component of a multi-stage pipeline. GNN-based predictions provide
hypotheses for experimental validation, accelerating the search for
novel therapeutic targets.

\subsection{Knowledge Graph Reasoning and Drug
Repurposing}\label{knowledge-graph-reasoning-and-drug-repurposing}

Drug repurposing seeks new therapeutic applications for existing
compounds, exploiting the observation that drugs often affect multiple
targets and pathways beyond their original indication. Knowledge graphs
provide a natural framework for repurposing by encoding the
relationships through which a drug's effects might propagate to new
disease contexts. The repurposing problem can be framed as link
prediction in a heterogeneous graph: given a knowledge graph with drugs,
diseases, genes, and pathways as nodes, predict missing
drug-treats-disease edges. Unlike direct drug-target prediction, this
task requires reasoning across multiple relationship types. A candidate
repurposing hypothesis might involve a chain such as: drug D binds
protein P1, P1 regulates pathway W, pathway W is dysregulated in disease
X, therefore D may treat X. Graph neural networks designed for
heterogeneous graphs learn to aggregate evidence across such chains,
weighting different metapaths (sequences of edge types) according to
their predictive reliability.

Foundation model embeddings strengthen knowledge graph reasoning in
several ways. For gene and protein nodes, ESM embeddings encode
functional properties that influence druggability and pathway
membership. For disease nodes, embeddings derived from clinical text or
phenotype ontologies capture symptom patterns and comorbidity
relationships. For drug nodes, molecular representations from chemical
language models or graph neural networks over molecular structure encode
binding properties and pharmacokinetics. These rich node features allow
the GNN to assess not just whether a path exists but whether the
entities along that path have compatible functional characteristics.

Empirical results demonstrate the value of this integration. Models
combining knowledge graph structure with foundation model embeddings
outperform both topology-only approaches (which ignore node semantics)
and embedding-only approaches (which ignore relational structure) on
standard drug repurposing benchmarks. The improvement is particularly
pronounced for drugs and diseases with sparse direct evidence, where
multi-hop reasoning through well-characterized intermediate entities
provides the primary signal.

Clinical translation of knowledge graph predictions requires careful
interpretation. A high-scoring drug-disease prediction indicates that
multiple lines of computational evidence converge, not that efficacy has
been established. The paths contributing to predictions provide
mechanistic hypotheses that can guide experimental validation: if the
model relies heavily on a drug-protein-pathway-disease chain, that
pathway becomes a candidate biomarker for patient selection or treatment
response monitoring. Several repurposing candidates identified through
knowledge graph methods have entered clinical trials, though the
approach remains most valuable for hypothesis generation rather than
definitive target validation.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-kg-drug-repurposing}{[}High{]} Knowledge graph
reasoning. Panel A (KG Structure): Heterogeneous graph with drugs,
proteins, diseases, pathways; multiple edge types. Panel B (Multi-Hop
Reasoning Path): Drug D â†’ binds â†’ Protein P1 â†’ participates\_in â†’
Pathway W â†’ disrupted\_in â†’ Disease X. Panel C (Link Prediction): Learn
embeddings; predict missing edges; score drug-disease pairs; new
indication discovery. Panel D (FM Enhancement): Replace node features
with FM embeddings; capture functional similarity beyond database
annotations.}

\end{figure}%

The integration of knowledge graphs with foundation models exemplifies
the broader theme of this chapter: relational reasoning over rich entity
representations yields capabilities that neither component achieves
alone. As biomedical knowledge graphs continue to expand through text
mining, database integration, and experimental annotation, their
combination with increasingly powerful foundation models will enable
more sophisticated multi-hop reasoning across the full landscape of
biological and clinical relationships.

\subsection{Pathway and Module
Analysis}\label{pathway-and-module-analysis}

Understanding complex diseases often requires moving beyond individual
genes to identify dysregulated pathways or functional modules. Graph
neural networks provide natural tools for learning such modular
structure.

Given a pathway graph with nodes annotated by patient-specific
multi-omic measurements, a GNN can learn to predict clinical outcomes
(disease subtype, treatment response, survival). After training,
attention weights or gradient-based attribution highlight which edges
and nodes most influence predictions. These highlighted subgraphs often
correspond to known disease-relevant pathways but can also reveal novel
modules whose coherence was not previously appreciated.

Hierarchical GNNs extend this approach by explicitly learning
multi-scale structure. Pooling operations coarsen the graph by grouping
nodes into super-nodes, creating a hierarchy from individual genes to
modules to pathways to biological processes. Each level of this
hierarchy provides a different granularity of analysis, aligning with
biological intuition that disease perturbations can occur at multiple
scales.

For rare diseases with patient-specific perturbation patterns,
module-level analysis is particularly valuable. Rather than asking
whether a patient's variants affect canonical pathways, a GNN can score
pathway activity based on the patient's specific genetic background,
enabling more personalized interpretation
(Chapter~\ref{sec-rare-disease}).

\subsection{Cell Type and State
Annotation}\label{cell-type-and-state-annotation}

Single-cell foundation models generate rich representations of
individual cells (Chapter~\ref{sec-single-cell}), but many biological
questions involve relationships between cells: which cells communicate,
how spatial neighborhoods influence behavior, which cell types co-occur
in disease states.

Graph neural networks over cell-cell interaction graphs enable several
applications. Cell type annotation propagates labels from
well-characterized cells to ambiguous ones based on expression
similarity and spatial proximity. Perturbation response prediction
models how signals from perturbed cells propagate to neighbors. Tissue
region classification identifies coherent spatial domains (tumor,
stroma, immune infiltrate) based on local cell compositions.

The foundation model integration follows the standard pattern: scGPT or
similar models generate cell embeddings, spatial proximity or inferred
ligand-receptor interactions define edges, and GNN message passing
refines cell representations based on neighborhood context. The
resulting embeddings capture both intrinsic cell state and extrinsic
spatial/communicative context, enabling predictions that purely
expression-based or purely spatial models cannot make.

\section{Practical Considerations}\label{practical-considerations}

\subsection{Graph Construction
Quality}\label{graph-construction-quality}

The impact of graph construction choices cannot be overstated. A GNN can
only learn from relationships encoded in its input graph; missing edges
prevent information flow, spurious edges introduce noise, and biased
edge sets propagate ascertainment artifacts.

Source selection involves tradeoffs between precision and completeness.
Curated databases like BioGRID provide high-confidence interactions but
miss most true relationships. Computational predictions from STRING or
co-expression analysis are more comprehensive but noisier. The
appropriate choice depends on the downstream task: high-precision
networks may be preferable when false positives are costly, while
high-recall networks enable discovery of novel biology at the risk of
chasing artifacts.

Thresholding decisions determine network density. Confidence scores or
distance metrics allow continuous edge weights, but many GNN
implementations require discrete edges or work better with relatively
sparse graphs. Cross-validation over threshold values or principled
selection criteria (target edge density, ensure graph connectivity) help
navigate this choice.

For heterogeneous graphs, schema design (which node types exist, which
edge types connect them) encodes strong assumptions about relevant
biology. A knowledge graph that separates genes, transcripts, and
proteins as distinct node types enables fine-grained reasoning but
requires more training data than a simpler gene-only representation.

\subsection{Scalability and
Mini-Batching}\label{scalability-and-mini-batching}

Biological graphs range from thousands of nodes (a single-patient cell
graph) to millions (a comprehensive knowledge graph or large spatial
transcriptomics dataset). Full-batch training, where the entire graph is
processed simultaneously, becomes infeasible at scale due to memory
constraints.

Mini-batching strategies partition computation into manageable pieces.
Neighborhood sampling (GraphSAGE-style) restricts message passing to a
fixed sample of neighbors per node, enabling node-level mini-batches.
Subgraph sampling trains on induced subgraphs corresponding to
meaningful units (individual pathways, tissue regions, patient subsets).
Cluster-based training partitions the graph into communities, processes
each independently, and handles cross-cluster edges in a second pass.

For foundation model integration, computational cost compounds:
generating embeddings for millions of proteins or cells may itself be
expensive. Pre-computing and caching embeddings is often practical,
decoupling the foundation model forward pass from GNN training. When
embeddings must be computed on-the-fly (for dynamic features or joint
fine-tuning), careful batching and gradient checkpointing become
essential.

\subsection{Robustness to Noise and
Missingness}\label{robustness-to-noise-and-missingness}

All biological networks contain errors. Experimental methods for
detecting interactions have false positive and false negative rates;
computational predictions rely on imperfect proxies; even curated
databases contain mistakes. GNNs must tolerate this noise to be
practically useful.

\textbf{Edge dropout} during training randomly masks edges, forcing the
model to not rely on any single interaction. This improves robustness to
missing or incorrect edges and serves as a form of regularization.
\textbf{Node dropout} similarly masks node features or entire nodes,
preventing overfitting to well-connected hubs.

Ensemble methods train multiple GNNs on different network subsamples or
with different random initializations, aggregating predictions to reduce
variance from network noise. Bayesian GNNs provide uncertainty estimates
that flag low-confidence predictions for manual review.

Evaluation should explicitly assess robustness by testing on held-out
edges, nodes from poorly characterized network regions, or networks
constructed from different data sources than training. A model that
performs well only on hub genes or well-characterized interactions may
fail in precisely the scenarios where computational prediction is most
needed.

\subsection{Interpretation and
Validation}\label{interpretation-and-validation}

A key advantage of graph models is interpretability: the graph structure
itself provides a scaffold for understanding predictions. Several
techniques extract biological insight from trained GNNs.

\textbf{Attention weight analysis} in GAT and graph transformer models
indicates which neighbors most influenced each node's prediction.
Aggregating attention across predictions can highlight critical edges or
subgraphs, suggesting which interactions drive model behavior.

\textbf{Gradient-based attribution} computes how predictions change with
respect to node or edge features, identifying which parts of the input
most affect outputs. Integrated gradients and similar methods provide
smoother, more reliable attributions than raw gradients.

\textbf{Counterfactual analysis} systematically removes edges, masks
nodes, or perturbs features and observes prediction changes. This
reveals which graph elements are necessary for specific predictions and
can identify model vulnerabilities.

\textbf{Embedding visualization} projects learned node representations
into two dimensions using UMAP or t-SNE, revealing clusters that may
correspond to functional categories, cell types, or disease subtypes.
Comparing embeddings across conditions identifies network regions that
show context-specific changes.

Interpretation is not an afterthought but a central goal. The most
impactful applications are those where GNN predictions generate testable
hypotheses about biological mechanism, ultimately validated by
experiment. Attention weights highlighting a regulatory edge or gradient
attribution implicating a signaling pathway should prompt follow-up
experiments, not immediate clinical action.

\section{Limitations and Open
Challenges}\label{limitations-and-open-challenges-3}

\subsection{The Study Bias Problem}\label{the-study-bias-problem}

Network-based methods inherit the biases of their input networks.
Well-studied genes appear as hubs; poorly characterized genes are
peripheral or disconnected. GNNs trained on such networks learn to
propagate signals toward well-characterized genes, effectively
recapitulating rather than extending existing knowledge.

This creates particular problems for disease gene discovery, where the
goal is often to identify previously unrecognized genes. A model that
consistently ranks known disease genes highly may simply be exploiting
their network prominence rather than learning generalizable disease
biology. Careful evaluation on temporal holdouts (genes characterized
after training data was assembled) or stratified by network degree can
reveal whether models truly generalize.

Mitigation strategies include degree-corrected training objectives,
explicit modeling of ascertainment bias, or alternative network
constructions that reduce dependence on historical research focus. None
fully solves the problem, which reflects fundamental data limitations
rather than algorithmic shortcomings.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-network-bias}{[}Enhancing{]} Study bias
visualization. Node degree vs.~publication count showing strong
correlation; well-studied genes (TP53, BRCA1) highly connected;
understudied genes peripheral; risk: GNN prioritizes well-studied genes;
mitigation strategies (degree normalization, attention to edge
confidence).}

\end{figure}%

\subsection{Causality Versus
Association}\label{causality-versus-association}

Network edges typically represent associations (two proteins bind, two
genes correlate) rather than causal relationships (perturbing gene A
changes gene B). GNNs learn to exploit correlational patterns, which may
not correspond to causal mechanisms.

For applications like drug target identification, this distinction
matters enormously. A gene that correlates with disease through
confounding may be a poor target despite high network-based
prioritization scores. Integrating causal inference methods with graph
learning is an active research area, but current GNN applications should
be interpreted as identifying associations worthy of experimental
follow-up rather than establishing causal relationships.

\subsection{Negative Data and Class
Imbalance}\label{negative-data-and-class-imbalance}

Most biological network datasets encode only positive relationships:
known interactions, confirmed regulatory edges, documented associations.
The absence of an edge may indicate true non-interaction or simply lack
of evidence. This creates severe class imbalance for edge prediction
tasks and makes negative sampling strategies critical.

Random negative sampling (assuming absent edges represent
non-interactions) is common but biologically unrealistic. More
sophisticated approaches sample negatives with matched properties (same
degree distribution, similar node features) to create harder and more
meaningful contrasts. Evaluation should report performance separately on
different negative sampling schemes to assess whether models generalize
beyond easily discriminated negatives.

\subsection{Distribution Shift}\label{distribution-shift}

A GNN trained on one biological network (human PPI from STRING) may not
transfer to another (mouse regulatory network, patient-specific spatial
graph). Foundation model embeddings help by providing transferable
features, but network structure differences can still break performance.

Cross-species transfer is particularly challenging: network topology,
edge type distributions, and gene function may all differ. Cross-tissue
or cross-disease transfer poses similar challenges. Explicit domain
adaptation methods, multi-task training across related networks, or
foundation model fine-tuning on target domains can help but add
complexity.

\section{Relational Reasoning at a Different
Scale}\label{relational-reasoning-at-a-different-scale}

Graph neural networks operate at a complementary level of abstraction to
sequence-based foundation models. Foundation models learn rich
representations of biological entities from sequence data; graph neural
networks learn to reason about relationships between those entities.
Combining them follows a natural pattern: generate embeddings with
foundation models, then refine them through message passing over graph
structure. This integration yields capabilities that neither component
achieves alone, propagating information across protein interaction
networks, regulatory pathways, and spatial neighborhoods in ways that
sequence models cannot represent.

The central insight is that biological knowledge exists at multiple
scales. Sequence encodes what individual genes and proteins can do;
networks encode how they interact to produce cellular function. GNNs
translate the relational structure of biological networks into learnable
inductive biases, enabling disease gene prioritization through network
propagation, drug target prediction through pathway context, and spatial
analysis through tissue graphs. The improvements over sequence-only
approaches are consistent across applications, demonstrating that
relational context adds genuine information beyond what sequence
representations capture.

Yet network structure carries its own biases. Protein interaction
databases are enriched for well-studied genes and disease-relevant
pathways; less-characterized genes have fewer annotated interactions
regardless of their biological importance. Correlation between genes
does not imply regulatory relationship. Class imbalance between known
disease genes and the genome-wide background reflects research history
as much as biology. These biases propagate through GNN predictions,
creating systematic patterns in what the models emphasize and what they
miss. The multi-omics integration examined in
Chapter~\ref{sec-multi-omics} extends graph-based reasoning to
additional modalities, while the clinical applications in
Chapter~\ref{sec-clinical-risk} and Chapter~\ref{sec-rare-disease}
depend on understanding where network-derived predictions are
trustworthy and where they inherit the limitations of their inputs.

\chapter{Multi-Omics Integration}\label{sec-multi-omics}

Combining data types should improve prediction. If genomic variants
provide one signal and transcriptomic measurements provide another,
their combination ought to be more informative than either alone. This
intuition, while reasonable, proves frequently wrong in practice. Naive
concatenation of multi-omics features often degrades performance
relative to single-modality models. Noise from uninformative features
overwhelms signal from informative ones. Batch effects between
modalities create spurious correlations that models exploit. The curse
of dimensionality intensifies when features from multiple assays are
stacked without principled integration. The paradox is real: more data
can mean worse predictions, and understanding why is prerequisite to
making multi-omics integration work.

Each molecular layer captures part of the biological story but not all
of it. Genomic variants identify predisposition; transcriptomics reveals
which genes respond; proteomics shows which proteins change;
metabolomics measures downstream biochemical consequences. A patient
with a BRCA1 variant may show altered DNA repair gene expression,
deficient homologous recombination protein activity, and characteristic
metabolic signatures. No single layer provides the complete picture.
Effective integration traces this causal cascade from genetic variation
through molecular intermediates to clinical phenotype, distinguishing
primary effects from downstream consequences and noise from signal.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-integration-paradox}{[}Essential{]} Central tension.
Panel A (The Promise): Different modalities capture different aspects;
genomics (heritable potential), transcriptomics (current state),
proteomics (functional complement), metabolomics (downstream
biochemistry); integration should improve prediction. Panel B (The
Paradox): More modalities = more features = more overfitting risk;
missing data compounds problem; when does integration actually help?
Panel C (When Integration Helps): Complementary information (genomics +
expression for complex traits), cross-modality validation, mechanistic
interpretation. Panel D (When Integration Hurts): Redundant information,
batch effects across modalities, sample size dilution.}

\end{figure}%

The integration strategy matters as much as the data itself. Early
fusion concatenates features before modeling, intermediate fusion learns
joint representations across modalities, and late fusion combines
predictions from modality-specific models. Each approach carries
distinct tradeoffs for different applications and data characteristics.
Multi-omics foundation models attempt to learn unified representations
across genomics, transcriptomics, proteomics, and other modalities
simultaneously, while clinical integration extends further still,
combining electronic health records, imaging data, and molecular
measurements for patient-level prediction. The practical challenges are
substantial: missing modalities when not every patient has every assay,
batch effects from technical variation between measurement platforms,
and a persistent gap between multi-omics potential and deployment
reality.

\section{The Limits of Single-Modality
Models}\label{the-limits-of-single-modality-models}

Each molecular layer tells an incomplete story. DNA sequence is static;
it encodes potential but not state. A variant's presence says nothing
about whether the gene is expressed, whether the protein is active, or
whether the pathway is perturbed. Transcriptomic data captures
expression state but misses post-transcriptional regulation, protein
modifications, and metabolic flux. Proteomic measurements reveal protein
abundance but not necessarily activity or localization. Methylation
profiles indicate epigenetic state but require expression data to
understand functional consequences.

The incompleteness becomes concrete when modeling complex traits.
Genome-wide association studies explain perhaps 10-20\% of heritability
for most common diseases through identified variants. Adding expression
quantitative trait loci (eQTLs) improves fine-mapping by suggesting
which variants affect gene expression, but many causal mechanisms
operate through splicing, translation, or post-translational
modification rather than expression level. Single-cell RNA sequencing
reveals cellular heterogeneity invisible to bulk measurements, but the
same cell cannot simultaneously undergo RNA-seq and ATAC-seq, forcing
computational integration across modalities measured in different cells.

Consider the challenge of predicting drug response. Germline variants in
drug-metabolizing enzymes explain some inter-individual variation, but
tumor-specific somatic mutations, expression programs, and
microenvironment all influence therapeutic efficacy. A genomics-only
model sees the inherited component; a transcriptomics-only model sees
the current expression state; neither captures the full picture.
Multi-omics integration promises to bridge these gaps by learning
representations that span molecular layers.

The foundation models examined in preceding chapters address each layer
individually: sequence models predict regulatory effects from DNA
(Chapter~\ref{sec-regulatory}), expression models capture
transcriptional programs (Chapter~\ref{sec-rna}), and protein language
models predict structure and function from amino acid sequence
(Chapter~\ref{sec-protein-lm}). Multi-omics integration asks how these
modality-specific representations can be combined into unified patient
or cell representations.

The promise comes with caveats. Adding modalities increases the number
of parameters that must be estimated, potentially worsening overfitting
when sample sizes are limited. Different modalities have different noise
characteristics, batch structures, and missingness patterns. The same
patient's measurements across platforms may not align perfectly due to
sample handling, timing, or technical variation. Naive concatenation of
features often performs worse than single-modality models because the
signal-to-noise ratio degrades when noisy features outnumber informative
ones.

These challenges motivate careful consideration of integration strategy.
The question is not whether to integrate, but how.

\section{Integration Strategies and Their
Tradeoffs}\label{integration-strategies-and-their-tradeoffs}

Three broad strategies have emerged for combining multi-omics data, each
with distinct strengths and limitations.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-fusion-strategies}{[}Essential{]} Strategy
comparison. Panel A (Early Fusion): All features concatenated â†’ single
model; pros: can learn cross-modal interactions; cons: curse of
dimensionality, requires complete data. Panel B (Late Fusion): Separate
models â†’ predictions combined; pros: handles missing, modality-specific
architectures; cons: cannot learn feature interactions. Panel C
(Intermediate Fusion): Modality-specific encoders â†’ shared latent space
â†’ task head; pros: flexibility + robustness; cons: alignment complexity.
Summary table comparing cross-modal interactions, missing data handling,
compute.}

\end{figure}%

\subsection{Early Fusion}\label{early-fusion}

Early fusion concatenates features from multiple modalities before any
modeling, creating a single high-dimensional input vector that contains
genomic variants, expression values, methylation levels, and any other
available measurements. A classifier or regressor then learns directly
from this concatenated representation.

The appeal of early fusion lies in its simplicity and flexibility. Any
downstream model architecture can operate on concatenated features, from
linear regression to deep neural networks. The model can learn arbitrary
interactions between features from different modalities, since all
information is present in the input. Implementation requires only
normalization and alignment of features across samples.

The limitations become apparent at scale. Dimensionality explodes when
combining genome-wide variants (millions of features), gene expression
(tens of thousands of genes), methylation (hundreds of thousands of CpG
sites), and protein abundance (thousands of proteins). Most samples have
far fewer observations than features, creating severe overfitting risk.
Regularization helps but cannot fully compensate when the ratio of
features to samples exceeds practical bounds.

Missing data creates additional complications. If any modality is
missing for a sample, early fusion requires either excluding that sample
(reducing effective sample size) or imputing the missing modality
(introducing noise and potential bias). Since multi-omics studies often
have incomplete overlap between modalities, with some patients having
genomics and transcriptomics but not proteomics, early fusion frequently
operates on substantially reduced cohorts.

Scale differences between modalities pose another challenge. Expression
values span orders of magnitude; methylation beta values range from zero
to one; variant encodings are typically binary. Without careful
normalization, modalities with larger variance can dominate the learned
representation regardless of biological relevance. Batch effects within
each modality add further complexity, since batch correction must
precede concatenation but may interact with cross-modal relationships.

Despite these limitations, early fusion remains appropriate when sample
sizes are large relative to feature counts, when all modalities are
available for all samples, and when the downstream task is well-defined
enough to guide feature selection. Biobank-scale studies with thousands
of participants and focused feature sets can succeed with early fusion
approaches.

\subsection{Late Fusion}\label{late-fusion}

Late fusion trains separate models for each modality and combines their
predictions at the output level. A genomics model produces a risk score;
a transcriptomics model produces another risk score; an ensemble method
or meta-learner combines these modality-specific predictions into a
final output.

This approach handles missing modalities gracefully. If a patient lacks
proteomic data, the proteomics model simply does not contribute to the
ensemble. Sample sizes for each modality-specific model can differ,
since training requires only samples with that modality rather than
complete multi-omics profiles. Each modality can use whatever
architecture works best for its data type: deep networks for imaging,
gradient boosting for tabular omics, convolutional architectures for
sequence.

Late fusion cannot capture cross-modal interactions at the feature
level. If a variant's effect on disease depends on expression level of a
regulatory gene, neither the genomics model nor the transcriptomics
model alone can detect this interaction. The ensemble sees only the
modality-specific predictions, not the underlying features. This
limitation is fundamental: late fusion assumes that each modality
provides independent signal that can be additively combined.

The assumption of independence often fails in biological systems. Gene
expression depends on genetic variants through eQTLs. Protein levels
depend on both transcription and post-transcriptional regulation.
Methylation states influence and are influenced by transcription. The
molecular layers are not independent information sources but coupled
components of a dynamic system. Late fusion ignores this coupling.

Calibration presents a practical challenge. For ensemble predictions to
be meaningful, the modality-specific models must produce well-calibrated
probability estimates. If the genomics model is overconfident and the
transcriptomics model is underconfident, naive averaging produces biased
predictions. Calibration techniques help but add complexity to the
modeling pipeline.

Late fusion works well when modalities genuinely provide independent
signals, when sample sizes for each modality differ substantially, or
when interpretability requires understanding each modality's
contribution separately. Clinical deployment often favors late fusion
because it gracefully handles the reality that not all patients will
have all measurements.

\subsection{Intermediate Fusion}\label{intermediate-fusion}

Intermediate fusion learns modality-specific encoders that map each data
type into a shared latent space, then operates on the aligned
representations for downstream tasks. This approach combines the
flexibility of early fusion with the robustness of late fusion.

Each modality has its own encoder architecture tailored to its
characteristics. A variational autoencoder might encode single-cell
expression data, handling sparsity and dropout noise. A convolutional
network might process methylation profiles along chromosomal
coordinates. A graph neural network might encode protein interaction
data. These diverse architectures share nothing except their output
dimensionality: all encoders produce embeddings in a common latent
space.

Alignment between modalities is encouraged through multiple mechanisms.
Reconstruction losses require each encoder's latent representation to
support decoding back to the original features, ensuring that the
embeddings retain modality-specific information. Contrastive terms pull
together representations of the same biological entity across
modalities: the expression embedding for a cell should be similar to the
ATAC-seq embedding for the same cell. Graph constraints enforce
consistency with known biological relationships: genes connected in
interaction networks should have similar embeddings.

The shared latent space enables cross-modal reasoning. A classifier
operating on the shared space can learn interactions between genomic and
transcriptomic features, since both are present in the same
representation. Transfer becomes possible: a model trained on expression
data can be applied to samples with only ATAC-seq by encoding through
the ATAC-seq encoder into the shared space.

Missing modalities no longer require imputation or exclusion. If a
sample lacks proteomics, only the available encoders fire, producing a
partial representation in the shared space. The downstream model
operates on whatever representation is available, degrading gracefully
as modalities are missing rather than failing entirely.

GLUE, introduced in Chapter~\ref{sec-single-cell} for single-cell
multi-omics integration, exemplifies this approach. Separate variational
autoencoders encode RNA-seq and ATAC-seq data into a shared cell
embedding space. A feature graph links ATAC-seq peaks to genes based on
genomic proximity and transcription factor binding, providing biological
constraints on the alignment. The result enables integration of
measurements from different cells, not just different modalities in the
same cell.

Intermediate fusion dominates modern multi-omics deep learning because
it balances flexibility with robustness. The modality-specific encoders
can be pretrained on large single-modality datasets, then fine-tuned for
alignment. New modalities can be added by training new encoders without
retraining existing components. The shared space provides a natural
target for interpretation and visualization.

The approach is not without limitations. The quality of alignment
depends heavily on the training objective and the availability of paired
samples where multiple modalities are measured in the same biological
entity. Without sufficient anchoring, the shared space may fail to
capture true biological correspondence. Hyperparameter choices for
balancing reconstruction against alignment losses require careful
tuning.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-intermediate-fusion}{[}High{]} Detailed
architecture. Panel A (Modality-Specific Encoders): Expression VAE,
methylation CNN, proteomics MLP; each tailored to modality. Panel B
(Shared Latent Space): All encoders output same dimensionality;
alignment via reconstruction loss, contrastive loss, graph constraints.
Panel C (Downstream Task Head): Classifier/regressor on latent
representation; can use partial representations. Panel D (Missing
Modality Handling): Complete sample all encoders fire; partial sample
available encoders only; graceful degradation.}

\end{figure}%

\section{Multi-Omics Foundation
Models}\label{multi-omics-foundation-models}

The foundation model paradigm, introduced in
Chapter~\ref{sec-fm-principles}, extends naturally to multi-omics
settings. Rather than training task-specific models that integrate
modalities for a single downstream application, multi-omics foundation
models learn general-purpose representations that transfer across tasks.

\subsection{Factor-Based Integration}\label{factor-based-integration}

Multi-Omics Factor Analysis (MOFA and its successor MOFA+) provides a
probabilistic framework for learning shared and modality-specific
factors from multi-omics data. The approach assumes that observed
measurements across modalities can be explained by a small number of
latent factors, some shared across modalities and others specific to
individual data types.

MOFA+ extends this framework to handle multiple sample groups (such as
different tissues or conditions), non-Gaussian likelihoods appropriate
for count data, and scalable inference for large datasets. The factors
learned by MOFA+ capture sources of variation that span modalities,
enabling biological interpretation: a factor that loads heavily on
inflammatory genes in expression data and on hypomethylation at immune
loci in methylation data suggests coordinated epigenetic-transcriptional
regulation of inflammation.

While MOFA+ is not a deep learning method in the strict sense, its
factor-based decomposition provides a foundation for understanding what
multi-omics integration should capture. The shared factors correspond to
biological processes that manifest across molecular layers; the
modality-specific factors capture technical variation or layer-specific
biology.

\subsection{Deep Generative Multi-Omics
Models}\label{deep-generative-multi-omics-models}

totalVI (Total Variational Inference) integrates protein abundance from
CITE-seq with gene expression in single-cell data through a hierarchical
Bayesian model. The approach learns a joint latent space that captures
cell state while properly modeling the distinct noise characteristics of
RNA and protein measurements. Protein abundance follows a negative
binomial distribution with technical factors including background
binding; RNA counts follow a zero-inflated negative binomial accounting
for dropout.

The generative model structure enables imputation of missing modalities.
Given RNA expression alone, totalVI can predict expected protein
abundance by sampling from the learned joint distribution. This
imputation is not mere correlation-based prediction but reflects the
full posterior distribution over protein levels given expression.

MultiVI extends this framework to integrate gene expression with
chromatin accessibility. The model learns to align measurements from
different cells, enabling construction of unified cell atlases from
studies that measured different modalities. The alignment relies on the
biological assumption that gene expression and chromatin state reflect
the same underlying cell state, even when measured in different cells.

These Bayesian deep generative models exemplify intermediate fusion with
principled uncertainty quantification. The posterior distributions over
latent variables capture not just point estimates but confidence in the
learned representations. This property becomes important for clinical
applications where prediction uncertainty must inform decision-making.

\subsection{Contrastive Multi-Modal
Learning}\label{contrastive-multi-modal-learning}

Contrastive learning provides another path to multi-omics integration.
The CLIP model for vision-language demonstrated that contrastive
objectives can align embeddings from fundamentally different data types
(images and text) into a shared space. Similar approaches apply to
biological modalities.

The contrastive objective is straightforward: embeddings of the same
biological entity across modalities should be similar, while embeddings
of different entities should be dissimilar. A cell's expression
embedding should be close to its methylation embedding and far from
other cells' methylation embeddings. A patient's genomic embedding
should be close to their transcriptomic embedding across the cohort.

This objective requires paired samples for training: the same cells or
patients measured across modalities. Anchor pairs define the positive
examples; negative examples come from non-matching pairs within a batch.
The encoders learn to produce embeddings where cross-modal
correspondence emerges from training dynamics rather than explicit
feature engineering.

Contrastive approaches scale well and can incorporate foundation model
encoders pretrained on single modalities. An expression encoder
pretrained on millions of cells via masked gene prediction can be
fine-tuned with contrastive objectives to align with an ATAC-seq
encoder. The pretraining provides rich initial representations; the
contrastive fine-tuning establishes cross-modal correspondence.

\section{Clinical Integration: EHR, Imaging, and Molecular
Data}\label{clinical-integration-ehr-imaging-and-molecular-data}

The ultimate goal of multi-omics modeling for many applications is
patient-level prediction: disease risk, treatment response, prognosis.
Achieving this goal requires integrating molecular measurements with
clinical data that directly captures patient state and outcomes.

\subsection{Electronic Health Records as a
Modality}\label{electronic-health-records-as-a-modality}

Electronic health records contain decades of longitudinal observations
for millions of patients: diagnoses, procedures, medications, laboratory
values, vital signs, clinical notes. This wealth of phenotypic
information complements molecular data by capturing disease
manifestation rather than molecular mechanism.

Integrating EHR with genomics poses distinct challenges. The data types
differ fundamentally: structured codes, continuous lab values, free-text
notes, and time-stamped events versus static or slowly-changing
molecular measurements. Temporal structure matters: the sequence of
diagnoses and treatments contains prognostic information that static
snapshots miss. Missingness is informative: the absence of a laboratory
test may indicate that a clinician deemed it unnecessary, which itself
conveys information about patient state.

Foundation models for EHR data learn representations from the
longitudinal event sequences. These models, often based on transformer
architectures that process sequences of medical codes, capture temporal
dependencies and co-occurrence patterns in clinical trajectories. The
resulting patient embeddings encode disease state and prognosis in a
form amenable to integration with molecular data.

Combining EHR embeddings with genomic features requires handling the
different temporal scales. Genetic variants are constant throughout
life; EHR observations accumulate over years. The integration must
determine which clinical observations are relevant to a given molecular
measurement, accounting for the time between sample collection and
clinical events.

\subsection{Imaging Integration}\label{imaging-integration}

Medical imaging provides spatial information that molecular assays lack.
A CT scan reveals tumor location, size, and heterogeneity;
histopathology slides show cellular morphology and tissue architecture;
MRI captures organ structure and function. These spatial data complement
molecular measurements that aggregate over dissected tissue regions.

Radiogenomics links imaging features to genetic and molecular
characteristics. Glioblastoma tumors with specific imaging signatures
have distinct methylation patterns and expression programs. Radiomic
features extracted from CT scans correlate with mutational burden and
immune infiltration in lung cancer. These associations enable prediction
of molecular state from non-invasive imaging, potentially guiding
treatment decisions when biopsy is impractical.

Foundation models for medical imaging, pretrained on millions of scans
through self-supervised objectives, provide rich representations for
downstream tasks. Integrating these imaging embeddings with molecular
data follows the intermediate fusion paradigm: modality-specific
encoders produce representations in a shared latent space where
multi-modal classifiers operate.

The integration must account for correspondence between imaging regions
and molecular samples. A tumor may be molecularly heterogeneous, with
different subclones in different spatial locations. A biopsy samples one
location; imaging captures the entire lesion. Alignment requires either
spatial registration of biopsy location to imaging coordinates or
acceptance that the correspondence is imperfect.

\subsection{Multi-Modal Clinical Prediction
Models}\label{multi-modal-clinical-prediction-models}

Combining EHR, imaging, and molecular data for clinical prediction
follows the intermediate fusion pattern. Each data type has a
specialized encoder: a transformer for longitudinal EHR events, a vision
encoder for imaging, domain-specific encoders for expression,
methylation, and other molecular modalities. All encoders produce
embeddings in a common patient representation space.

The training objective typically combines modality-specific
reconstruction losses with alignment terms that encourage consistency
across data types. A patient's EHR embedding should be predictive of
their molecular state; their imaging embedding should be consistent with
their clinical trajectory. Downstream classifiers for outcomes like
survival, treatment response, or disease progression operate on the
combined representation.

Missing modalities are common in clinical settings. Not all patients
have genomic data; imaging may be unavailable for some conditions; the
depth of EHR history varies by healthcare system and patient engagement.
Multi-modal clinical models must handle this missingness gracefully,
producing useful predictions from whatever data are available while
leveraging cross-modal information when present.

The clinical deployment path for such models requires validation on
external cohorts, prospective evaluation, and regulatory clearance.
These practical considerations, addressed in
Chapter~\ref{sec-clinical-risk}, shape model development from the
outset. A model that performs well on a research cohort but requires
modalities unavailable in clinical workflows provides little value.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-clinical-multimodal}{[}High{]} Patient-level
integration. Panel A (Data Modalities): EHR (diagnoses, procedures,
medications, labs---longitudinal), imaging (CT, MRI,
histopathology---spatial), molecular (genomics, expression,
proteomics---static). Panel B (Modality-Specific Encoders): EHR
transformer over event sequence; imaging vision encoder; molecular FM
embeddings. Panel C (Patient Representation Space): All â†’ shared patient
embedding; EHR predicts molecular; imaging consistent with trajectory.
Panel D (Clinical Prediction Tasks): Risk stratification, treatment
response, prognosis; missing modality handling (not all patients have
all data). Practical challenges callout: batch effects, temporal
alignment, cost constraints.}

\end{figure}%

\section{The Systems View: From Variant to
Phenotype}\label{the-systems-view-from-variant-to-phenotype}

Multi-omics integration gains conceptual clarity from a systems biology
perspective that traces information flow from genetic variation through
molecular intermediates to clinical phenotypes. This cascade view
organizes the molecular layers into a causal hierarchy and identifies
where integration should occur.

\subsection{The Information Cascade}\label{the-information-cascade}

Genetic variants are the starting point: heritable differences in DNA
sequence that perturb downstream molecular processes. Some variants
directly alter protein structure through missense or nonsense mutations.
Others affect regulation: promoter variants change expression level,
splice site variants alter transcript isoforms, enhancer variants
modulate tissue-specific expression.

These primary effects propagate through molecular layers. Expression
changes alter the cellular protein complement. Protein level changes
affect enzyme activity, signaling cascades, and transcriptional
feedback. Metabolic flux shifts in response to enzyme availability. Cell
behavior changes as the integrated molecular state crosses thresholds
for proliferation, differentiation, or death.

Tissue-level phenotypes emerge from cellular behavior aggregated across
the organ. Tumor growth reflects altered cell proliferation; fibrosis
reflects aberrant extracellular matrix deposition; inflammation reflects
immune cell recruitment and activation. These tissue phenotypes manifest
as clinical symptoms, laboratory abnormalities, and imaging findings.

The cascade view suggests where different modalities provide
information. Genomics captures the inherited potential and somatic
alterations. Transcriptomics and epigenomics capture the current
regulatory state. Proteomics and metabolomics capture the functional
molecular complement. Clinical data captures the phenotypic
consequences.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-information-cascade}{[}High{]} Systems biology view.
Panel A (The Causal Cascade): Genetic variant â†’ (cis-regulation) â†’
Expression change â†’ (translation) â†’ Protein change â†’ (metabolism) â†’
Metabolite change â†’ (cellular) â†’ Cell behavior â†’ (tissue) â†’ Clinical
phenotype. Panel B (Where Each Modality Provides Information): Genomics
(starting point), transcriptomics (current state), proteomics
(functional complement), metabolomics (downstream), clinical
(manifestation). Panel C (Bottleneck Modalities): Coding variants
bottleneck at protein; regulatory at expression; some phenotypes
bottleneck downstream. Panel D (Integration Implications): Model should
trace causal chain; not all modalities equally informative for all
questions.}

\end{figure}%

\subsection{Bottleneck Modalities}\label{bottleneck-modalities}

Not all modalities are equally informative for all questions. The
concept of bottleneck modalities identifies which molecular layers most
directly mediate the relationship between genetic variation and
phenotype.

For many coding variants, protein structure is the bottleneck. A
missense variant's effect on disease depends primarily on how it alters
protein function, which depends on how the amino acid substitution
affects folding, stability, and activity. Expression level matters less
than structural consequence. Protein language models that predict
structural effects from sequence directly address this bottleneck.

For regulatory variants, expression is closer to the bottleneck. An
enhancer variant affects disease through its effect on target gene
expression, which affects downstream processes. Chromatin accessibility
and transcription factor binding are intermediate steps; expression
level is the more proximal readout. Models that predict expression
effects from sequence address this bottleneck.

For some phenotypes, the bottleneck may lie downstream of molecular
measurements entirely. Behavioral traits depend on neural circuit
function that emerges from complex cellular and network dynamics.
Metabolic traits depend on flux through interconnected pathways that may
not be apparent from enzyme abundance alone. These cases suggest that
molecular measurements provide incomplete information regardless of
integration sophistication.

\subsection{Causal vs.~Correlational
Integration}\label{causal-vs.-correlational-integration}

Multi-omics data are pervasively correlated. Genes in the same pathway
have correlated expression. Methylation and expression are
anti-correlated at many promoters. Clinical variables cluster by disease
category. These correlations can improve prediction even without causal
understanding.

Causal integration seeks to identify the mechanistic relationships
between molecular layers. If a variant causes reduced expression, which
causes protein deficiency, which causes metabolic dysfunction, this
causal chain suggests intervention targets: expression restoration or
enzyme supplementation might address the downstream effects.
Correlational integration might achieve the same predictive performance
without identifying this chain, since all layers correlate with the
phenotype.

Distinguishing causal from correlational relationships requires
experimental perturbation or careful causal inference from observational
data. Mendelian randomization uses genetic variants as instruments to
infer causal effects of expression on outcomes. CRISPR screens directly
perturb gene function and measure consequences. Multi-omics integration
methods increasingly incorporate causal assumptions or validation
against perturbation data.

The distinction matters for interpretation and intervention. A
predictive model based on correlations may fail when the data
distribution shifts or when interventions alter the causal structure. A
causally informed model captures mechanism that persists across
contexts.

\section{Handling Missing Modalities}\label{handling-missing-modalities}

Real-world multi-omics data are incomplete. Different studies measure
different modalities. Within studies, technical failures, sample
limitations, and cost constraints create missing data. Clinical
deployment must handle patients with incomplete molecular profiles.
Robust multi-omics methods must address missingness directly.

\subsection{Training with Incomplete
Data}\label{training-with-incomplete-data}

Intermediate fusion architectures handle missing modalities naturally
during inference: only the available encoders contribute to the shared
representation. Training is more complex because alignment terms require
paired measurements across modalities.

One approach trains on the subset of samples with complete data, then
applies the trained encoders to samples with partial data during
inference. This wastes information from the samples with incomplete
profiles and may learn representations that fail to generalize to the
missing-modality setting.

A better approach incorporates missingness into training. Modality
dropout randomly masks modalities during training, forcing the model to
learn representations robust to missing inputs. The reconstruction and
alignment losses are computed only for available modalities, so samples
with partial data can still contribute to training.

Curriculum learning strategies may first train with complete data to
establish alignment, then gradually increase modality dropout to improve
robustness. The balance between alignment quality (which benefits from
complete data) and robustness (which requires training on partial data)
requires empirical tuning.

\subsection{Cross-Modal Imputation}\label{cross-modal-imputation}

Intermediate fusion enables principled imputation of missing modalities.
Given a sample's available modalities encoded into the shared latent
space, decoders for missing modalities can predict expected values. If a
patient has expression data but not methylation, the expression encoder
produces a latent embedding, and the methylation decoder generates
predicted methylation values from that embedding.

The imputation quality depends on how well the shared space captures the
biological factors underlying both modalities. If expression and
methylation reflect the same cell state, the imputation may be accurate.
If they capture distinct aspects of biology, imputation will smooth over
true variation.

Uncertainty in imputation matters for downstream use. Point estimates of
missing values provide no indication of confidence. Generative models
that produce distributions over missing values enable propagation of
uncertainty through downstream analyses. A risk prediction that depends
heavily on imputed values should have wider confidence intervals than
one based entirely on measured data.

\subsection{Zero-Shot Cross-Modal
Transfer}\label{zero-shot-cross-modal-transfer}

The most ambitious application of multi-omics integration is zero-shot
prediction across modalities: using a model trained on one set of
modalities to make predictions for samples measured with entirely
different modalities.

This transfer relies on the shared latent space capturing biological
state independently of measurement modality. If the space truly
represents cell state, then a classifier trained on expression-derived
embeddings should work on ATAC-seq-derived embeddings, since both
encoders map to the same biological meaning. The alignment training
enables this transfer by ensuring that the same biological entity maps
to the same latent location regardless of which modality was measured.

Zero-shot transfer is rarely perfect. The modalities may capture
somewhat different aspects of biology, and the alignment may be
imprecise. But partial transfer can still be valuable: a model achieving
80\% of supervised performance without any labeled examples in the new
modality saves substantial annotation effort.

\section{Practical Challenges}\label{practical-challenges}

\subsection{Batch Effects Across
Modalities}\label{batch-effects-across-modalities}

Batch effects, systematic technical variation between experimental
batches, are endemic in high-throughput biology. Multi-omics integration
faces compounded batch effects: each modality may have its own batch
structure, batches may be correlated or anti-correlated across
modalities, and batch correction methods designed for single modalities
may not extend to multi-modal settings.

Consider a study where expression data were generated at three
sequencing centers and proteomics data were generated at two mass
spectrometry facilities. The batch effects in each modality are
independent. Samples from expression batch 1 are spread across
proteomics batches. Correcting expression batch effects does not address
proteomics batch effects, and vice versa.

Integration must either correct batch effects within each modality
before combining (risking removal of real biology that correlates with
batch) or incorporate batch as a covariate in the integrated model
(requiring that batch structure be known and modeled correctly). Domain
adaptation techniques treat batches as domains and learn representations
invariant to domain while retaining biological signal.

\subsection{Sample Size and Power}\label{sample-size-and-power}

Multi-omics studies typically have smaller sample sizes than
single-modality studies due to cost constraints. Each additional
modality increases per-sample cost, trading breadth for depth. This
tradeoff has implications for statistical power and model complexity.

The effective sample size for multi-omics integration may be smaller
than for any single modality. If 1000 patients have expression data and
800 have methylation data but only 600 have both, intermediate fusion
sees 600 fully informative samples. Late fusion can use all 1000
expression samples and all 800 methylation samples, avoiding the
intersection penalty.

Power analyses for multi-omics studies must account for the specific
integration strategy and the expected missingness pattern. A study
designed for early fusion needs larger sample sizes (relative to feature
count) than one designed for late fusion. Grant applications and study
planning should explicitly consider how integration choices affect
required sample sizes.

\subsection{Interpretability Across
Modalities}\label{interpretability-across-modalities}

Multi-omics models compound the interpretability challenges inherent in
deep learning. When a model predicts disease risk from integrated
genomic, transcriptomic, and proteomic features, clinicians need to
understand which modalities and which features drive the prediction. A
black-box risk score, however accurate, provides little guidance for
understanding mechanism or identifying intervention targets.

Attribution methods that work for single-modality models do not
automatically extend to multi-modal settings. Gradient-based attribution
can identify important features within each modality, but comparing
importance across modalities requires careful normalization. A genomic
variant and an expression value operate on different scales with
different effect size distributions; raw attribution scores are not
directly comparable.

The intermediate fusion architecture offers some interpretability
advantages. The shared latent space can be visualized to understand how
samples cluster and which modalities contribute to separation. Attention
weights in cross-modal transformers indicate which features from each
modality the model considers when making predictions. Modality ablation
studies quantify each data type's contribution to overall performance.

Biological interpretability requires connecting learned representations
to known biology. Do the latent dimensions correspond to pathways, cell
types, or disease processes? Are cross-modal attention patterns
consistent with known regulatory relationships? These questions demand
validation against external biological knowledge, not just introspection
of model parameters. The interpretability methods developed in
Chapter~\ref{sec-interpretability} provide starting points, but
multi-modal settings require extensions that account for the distinct
semantics and scales of each data type.

\subsection{Evaluation Complexity}\label{evaluation-complexity-1}

Evaluating multi-omics models is more complex than evaluating
single-modality models. Multiple dimensions of performance matter:
prediction accuracy, calibration, cross-modality transfer, robustness to
missing modalities, biological plausibility of learned representations,
and clinical utility.

A model might achieve high prediction accuracy by memorizing batch
effects or leveraging shortcuts in the data. Evaluation should include
cross-batch and cross-cohort validation to assess generalization.
Ablation studies that remove each modality quantify the contribution of
each data type and identify whether the model genuinely integrates
information or relies predominantly on one modality.

Biological validation through comparison to known biology provides
another evaluation axis. Do the learned factors correspond to known
pathways? Are attention patterns consistent with regulatory
relationships? Do imputed values match held-out measurements? These
checks assess whether the model captures biological signal rather than
technical artifacts. The evaluation frameworks for multi-modal models,
including strategies for ablation studies, calibration assessment, and
cross-cohort validation, are developed systematically in
Chapter~\ref{sec-evaluation}.

Clinical evaluation, addressed in Chapter~\ref{sec-clinical-risk},
requires prospective validation in real deployment settings. A model
that improves prediction in research cohorts may not improve clinical
decisions if the predictions do not change management or if the required
modalities are unavailable in clinical workflows.

\section{Integration as Means, Not
End}\label{integration-as-means-not-end}

Multi-omics integration is not an end in itself but a means to improved
prediction, understanding, and intervention. The integration strategies
and foundation models surveyed here produce representations; downstream
applications convert those representations to actionable outputs. Risk
prediction combines multi-omic embeddings with clinical variables for
individualized prognosis. Treatment response models predict which
patients will benefit from specific therapies based on their integrated
molecular profiles. Drug discovery uses multi-omics to inform target
identification and patient stratification for clinical trials
(Chapter~\ref{sec-drug-discovery}). In each case, integration provides
the substrate; clinical or scientific goals provide the purpose.

The systems view that multi-omics enables shapes how predictions should
be interpreted. A risk prediction based on integrated features inherits
explanatory power from the causal relationships linking molecular layers
to phenotype. Understanding which modalities drive predictions, and how
those modalities relate to underlying biology, supports clinical
reasoning about mechanism and intervention. This explanatory capacity
distinguishes multi-omics from single-modality approaches that may
predict equally well but provide less insight into why predictions
succeed or fail.

The path from research models to clinical deployment requires addressing
practical challenges that intensify with integration: batch effects
across modalities and institutions, missing measurements that differ
systematically across patients, sample size limitations that grow with
feature dimensionality, and evaluation complexity when outcomes depend
on multiple data types. The clinical applications examined in
Chapter~\ref{sec-clinical-risk} and Chapter~\ref{sec-rare-disease}
confront these realities. As the field advances toward whole-patient
foundation models that jointly encode genomics, transcriptomics,
proteomics, imaging, and clinical data, the integration principles
established here provide the foundation. The trade-offs between fusion
strategies, the importance of shared latent spaces, the challenge of
missing modalities, and the systems biology perspective on information
flow will remain relevant as scale and scope expand.

\part{Part V: Evaluation and Trust}

Evaluating genomic models presents challenges that distinguish this
domain from natural language processing or computer vision. Biological
sequences contain evolutionary history: a model tested on homologous
sequences may appear to generalize when it has merely memorized.
Population structure creates spurious associations: a variant predictor
may learn ancestry rather than pathogenicity. Nested functional
hierarchies obscure what models actually capture: strong performance on
common variants provides no guarantee of accuracy on the rare variants
that drive most clinical decisions. Standard machine learning evaluation
practices, developed for domains where training and test examples are
approximately independent and identically distributed, become actively
misleading when applied to genomic data without careful adaptation.

This part develops the frameworks and methodologies that determine
whether genomic foundation models deliver on their promises.
Chapter~\ref{sec-benchmarks} surveys established benchmarks across
protein, DNA, regulatory, and clinical domains, examining their
construction and limitations while distinguishing meaningful signal from
benchmark-specific artifacts. Chapter~\ref{sec-evaluation} addresses how
to use benchmarks appropriately, developing principles for data
splitting, metric selection, and statistical testing that produce
trustworthy conclusions rather than inflated claims.

Chapter~\ref{sec-confounding} confronts the systematic biases that
pervade genomic datasets: population stratification that confounds
genotype with ancestry, batch effects that encode technical rather than
biological variation, and hidden correlations that models exploit
without learning genuine biology. Chapter~\ref{sec-uncertainty} examines
calibration and uncertainty quantification, the methods that determine
whether model outputs can inform decisions or require careful
reinterpretation. Chapter~\ref{sec-interpretability} explores how to
move beyond black-box prediction toward mechanistic understanding,
distinguishing faithful explanations that accurately reflect model
computation from plausible explanations that merely satisfy human
intuition. Together, these chapters provide the critical toolkit needed
to evaluate claims about genomic AI and to deploy models responsibly in
research and clinical settings.

\chapter{Benchmarks}\label{sec-benchmarks}

Every benchmark measures a proxy. ClinVar pathogenicity labels proxy
clinical impact. AUROC on held-out variants proxies discrimination
ability in deployment. Chromatin accessibility predictions proxy
regulatory function. The gap between proxy and target varies across
benchmarks, across variant types, and across populations. A model
achieving state-of-the-art performance on ClinVar may systematically
miscalibrate predictions for the rare variants that matter most
clinically, because ClinVar's composition does not reflect the
distribution of variants clinicians actually encounter. A DNA language
model excelling at enhancer classification may have learned GC content
rather than regulatory grammar, because the benchmark's negative
examples differ from positives in ways that have nothing to do with
enhancer function.

Understanding what benchmarks actually measure, and how that differs
from what we need to know, is prerequisite to interpreting any
leaderboard result. The genomic AI field has accumulated substantial
evaluation infrastructure. Dozens of benchmark suites target different
modalities: protein structure and function, DNA regulatory elements,
variant pathogenicity, gene expression prediction, and more. Hundreds of
individual tasks probe specific capabilities. Thousands of models have
reported results, creating leaderboards that rank approaches and track
progress over time. This infrastructure enables comparison and drives
methodological improvement. Yet the relationship between benchmark
success and deployment value remains poorly characterized. A foundation
model that dominates protein benchmarks may fail on the specific protein
family relevant to a drug discovery campaign. A variant effect predictor
that leads regulatory benchmarks may provide no clinical utility for the
variant classes that lack representation in evaluation data.

This chapter surveys the benchmark landscape for genomic foundation
models, organized by biological modality. We examine protein benchmarks
first, as the most mature evaluation ecosystem, then DNA and regulatory
benchmarks, variant effect prediction benchmarks, and trait-level
benchmarks linking genetics to phenotypes. Throughout, we attend not
only to what these benchmarks measure but to their construction, their
biases, and their limitations. The methodological principles for using
benchmarks properly are covered in Chapter~\ref{sec-evaluation}. Here we
focus on cataloging what exists and developing the critical perspective
necessary to interpret benchmark claims.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-benchmark-landscape}{[}Essential{]} Navigational map
organized as matrix. Rows (modalities): Protein, DNA/Regulatory, Variant
Effect, Trait-Level. Columns (benchmark types): Structure, Function,
Effect Prediction, Clinical Relevance. Include: TAPE, FLIP, ProteinGym,
Genomic Benchmarks, BEND, ClinVar, CAGI, DMS/MaveDB, TraitGym, EmbedGEM.
Visual encoding: icons per modality, color intensity for maturity, size
for scale. Annotations: typical evaluation progression, prospective vs
retrospective benchmarks.}

\end{figure}%

\section{Protein Language Model
Benchmarks}\label{protein-language-model-benchmarks}

Protein language models (Chapter~\ref{sec-protein-lm}) benefit from the
longest-established and most systematic evaluation ecosystem in genomic
AI. The maturity of protein benchmarks reflects both the longer history
of computational protein science and the relative tractability of
protein structure and function prediction compared to regulatory
genomics.

\subsection{TAPE: Tasks Assessing Protein
Embeddings}\label{tape-tasks-assessing-protein-embeddings}

The Tasks Assessing Protein Embeddings (TAPE) benchmark, introduced in
2019, established the template for systematic protein representation
evaluation (\textbf{rao\_evaluating\_2019?}). TAPE frames protein
language model assessment as transfer learning evaluation: pretrained
models generate embeddings, which are then used as features for
supervised prediction on downstream tasks. This framework decouples
representation quality from task-specific modeling, enabling comparison
across architectures that may have very different inductive biases.

TAPE comprises five tasks spanning different aspects of protein biology.
Secondary structure prediction requires classifying each residue as
helix, sheet, or coil, testing whether embeddings capture local
structural preferences. Contact prediction asks whether residue pairs
are spatially proximate in the folded structure, probing the
representation's ability to encode tertiary structure information from
sequence alone. Remote homology detection requires classifying proteins
into structural superfamilies, testing whether embeddings capture
evolutionary relationships that transcend sequence similarity.
Fluorescence prediction and stability prediction use data from deep
mutational scanning experiments to assess whether embeddings encode
fitness landscapes.

The benchmark's design reflects deliberate methodological choices.
Train, validation, and test splits enforce sequence identity thresholds
to prevent homology-based leakage. Evaluation uses simple linear or
shallow neural network heads rather than complex task-specific
architectures, isolating representation quality from modeling capacity.
Standardized preprocessing and data loading eliminate confounds from
inconsistent implementation.

TAPE's influence extended beyond its specific tasks. The benchmark
established norms for protein representation evaluation: systematic
coverage of diverse prediction targets, controlled transfer learning
protocols, and explicit attention to data splitting. Subsequent
benchmarks adopted and extended this framework.

\subsection{FLIP: Function-Linked Protein
Benchmark}\label{flip-function-linked-protein-benchmark}

The FLIP (Function-Linked Integrated Protein) benchmark addresses gaps
in TAPE's coverage by focusing on experimentally measured functional
properties (\textbf{dallago\_flip\_2021?}). Where TAPE includes
structurally derived labels and computational annotations, FLIP
emphasizes high-throughput experimental assays that directly measure
protein fitness.

FLIP aggregates deep mutational scanning datasets across diverse
proteins and functional readouts. The benchmark includes assays
measuring enzymatic activity, binding affinity, thermostability, and
expression level. Each dataset provides quantitative measurements for
thousands of single-point mutations, enabling evaluation of fine-grained
variant effect prediction.

The benchmark's value lies in its experimental grounding. Computational
structure predictions and evolutionary conservation scores, while
useful, are indirect proxies for function. Deep mutational scanning
provides direct measurements of how sequence changes affect the property
of interest. Models that perform well on FLIP demonstrate the ability to
predict experimentally validated functional consequences rather than
computationally inferred annotations.

FLIP also introduced systematic evaluation of different splitting
strategies. Random splits, where training and test variants are sampled
uniformly from the same protein, represent the easiest setting.
Contiguous splits, where training and test variants occupy different
sequence regions, test spatial generalization. Modulo splits, which
interleave training and test positions along the sequence, provide
intermediate difficulty. Performance typically degrades from random to
contiguous splits, revealing how much models rely on local sequence
context versus genuine functional understanding.

\subsection{ProteinGym: Comprehensive Variant Effect
Evaluation}\label{proteingym-comprehensive-variant-effect-evaluation}

ProteinGym has emerged as the most comprehensive benchmark for protein
variant effect prediction, compiling 217 deep mutational scanning assays
across diverse protein families (\textbf{notin\_proteingym\_2024?}). The
benchmark's scale enables statistically robust comparison across
modeling approaches while its diversity reveals where different methods
excel or struggle.

The primary evaluation metric is Spearman correlation between predicted
and experimentally measured fitness effects. This rank-based metric is
appropriate for deep mutational scanning data, where absolute fitness
values depend on assay-specific calibration but relative rankings are
more comparable across experiments. ProteinGym reports correlations for
each assay individually and aggregated across the full benchmark,
enabling both global comparison and identification of task-specific
strengths.

ProteinGym distinguishes between zero-shot and supervised evaluation
regimes. In zero-shot evaluation, models predict variant effects without
any task-specific training, relying entirely on representations learned
during pretraining. Models like ESM-1v (Chapter~\ref{sec-protein-lm})
compute effects as log-likelihood ratios under the pretrained language
model, while structure-based methods like AlphaMissense
(Chapter~\ref{sec-vep-fm}) incorporate predicted structural
consequences. In supervised evaluation, models are fine-tuned on a
subset of measured variants before predicting held-out effects. The gap
between zero-shot and supervised performance indicates how much
task-specific information improves over general-purpose representations.

The benchmark reveals systematic patterns in model performance. Protein
language models generally outperform conservation-based methods,
particularly for variants in regions with sparse evolutionary sampling.
Structure-aware models show advantages for variants affecting protein
stability or buried residues. Ensemble methods that combine multiple
predictors often achieve the highest correlations, suggesting that
different approaches capture complementary information.

ProteinGym's limitations mirror those of its constituent datasets. Deep
mutational scanning experiments are biased toward well-studied proteins
amenable to high-throughput screening. Assay-specific selection
pressures affect which variants appear deleterious: a variant may
strongly affect enzymatic activity while leaving thermostability
unchanged, or vice versa. The benchmark measures correlation with
specific experimental readouts rather than clinical pathogenicity, which
integrates multiple functional consequences in complex ways.

\subsection{Structure Prediction
Benchmarks}\label{structure-prediction-benchmarks}

Protein structure prediction benchmarks derive from the Critical
Assessment of protein Structure Prediction (CASP) tradition, which has
evaluated computational methods against experimentally determined
structures since 1994. The dramatic success of AlphaFold2 at CASP14 in
2020 transformed the field, but structure prediction benchmarks remain
relevant for evaluating single-sequence methods and assessing whether
language model pretraining improves structural accuracy.

Structure prediction quality is typically assessed using the Global
Distance Test (GDT-TS) and Template Modeling score (TM-score). GDT-TS
measures the percentage of residues that can be superimposed within
various distance thresholds, providing a single number between 0 and 100
that correlates well with visual assessment of structural similarity.
TM-score normalizes by protein length, enabling comparison across
proteins of different sizes.

For protein language models, the relevant evaluation setting is
single-sequence structure prediction, where the model receives only the
target sequence without multiple sequence alignments. This tests whether
pretraining on evolutionary sequence databases enables structure
prediction without explicit evolutionary analysis at inference time.
ESMFold (Chapter~\ref{sec-protein-lm}) demonstrated that single-sequence
prediction can approach MSA-based methods for many proteins, though
performance gaps remain for sequences with sparse evolutionary coverage.

Structure prediction benchmarks complement sequence-based evaluations by
testing whether learned representations encode biophysical constraints.
A model that achieves high accuracy on contact prediction or secondary
structure classification may still fail to integrate these local
predictions into globally consistent structures. The emergence of
accurate single-sequence structure prediction from language model
embeddings suggests that pretraining captures substantial structural
information, even without explicit structural supervision.

\section{DNA and Regulatory
Benchmarks}\label{dna-and-regulatory-benchmarks}

DNA foundation models (Chapter~\ref{sec-dna-lm}) and regulatory models
(Chapter~\ref{sec-regulatory}) face a less mature but rapidly developing
benchmark landscape. Early deep learning work in genomics focused on
individual tasks derived from ENCODE-style assays. Recent efforts have
introduced benchmark suites that attempt to standardize evaluation
across multiple tasks, tissues, and species.

\subsection{Classical Regulatory Prediction
Tasks}\label{classical-regulatory-prediction-tasks}

The earliest deep learning benchmarks for genomics framed regulatory
prediction as classification over short sequence windows. Transcription
factor binding prediction asks whether a specific TF ChIP-seq peak
overlaps a given sequence window, typically around 1 kilobase centered
on the binding site. Open chromatin prediction requires classifying
regions as accessible or inaccessible based on DNase-seq or ATAC-seq
signal. Histone mark prediction asks whether a chromatin modification
peak (H3K27ac, H3K4me3, etc.) is present at each position.

These tasks derive from consortia like ENCODE and Roadmap Epigenomics,
which systematically profiled chromatin states across cell types.
Benchmark construction typically involves defining positive regions from
called peaks and sampling negative regions from elsewhere in the genome,
extracting fixed-length sequences centered on each region, and
evaluating binary classification using AUROC or average precision.

Models such as DeepSEA, Basset, and DanQ established baseline
performance on these tasks (see Chapter~\ref{sec-cnn} for architectural
details). Their success demonstrated that convolutional networks could
learn sequence features predictive of regulatory state without
hand-crafted motifs. Modern foundation models still report performance
on similar tasks as sanity checks, though these classical benchmarks
have significant limitations.

The primary limitation is that binary classification over short windows
fails to capture the quantitative, cell-type-specific, and long-range
nature of transcriptional regulation. A region may be weakly accessible
in some cell types and strongly accessible in others; binary labels
collapse this continuous variation. Short windows cannot assess whether
models capture distal regulatory interactions that span tens to hundreds
of kilobases. Evaluation on curated peak regions may overestimate
performance relative to genome-wide prediction, where the vast majority
of positions are regulatory ``background.''

\subsection{Quantitative Regulatory
Prediction}\label{quantitative-regulatory-prediction}

Beyond binary classification, benchmarks increasingly require prediction
of quantitative regulatory readouts. Signal regression asks models to
predict per-base or per-bin signal intensity from ChIP-seq, ATAC-seq, or
related assays. Gene expression prediction requires predicting
transcript abundance (TPM, counts) from promoter sequences or larger
genomic contexts. Massively parallel reporter assays (MPRAs) provide
systematic measurements of enhancer or promoter activity for thousands
of sequences, enabling evaluation of quantitative activity prediction.

Hybrid architectures like Enformer (Chapter~\ref{sec-regulatory})
popularized benchmarks combining large receptive fields with dense
quantitative targets across many assays and cell types. Evaluation
metrics shift from AUROC to Pearson or Spearman correlation between
predicted and observed profiles. Some benchmarks report correlation
relative to replicate concordance, establishing an upper bound set by
experimental reproducibility.

Quantitative benchmarks better reflect the continuous nature of
regulatory activity but introduce new challenges. Heterogeneous noise
across assays and laboratories complicates aggregation: should a model
be penalized equally for poor performance on a low-quality assay versus
a high-quality one? Cell-type diversity raises questions about how to
weight performance across tissues: is accurate prediction in a rare cell
type more or less important than in a common one? The relationship
between predicted and observed signal depends on assay-specific
calibration that may not transfer across experimental batches.

\subsection{Genomic Benchmarks}\label{genomic-benchmarks}

The Genomic Benchmarks resource provides standardized classification
datasets for DNA sequence models (GreÅ¡ovÃ¡ et al. 2023). The benchmark
compiles tasks including enhancer identification, promoter recognition,
splice site detection, and coding sequence classification across
multiple species. Standardized train, validation, and test splits enable
direct comparison of different architectures without confounds from
inconsistent data processing.

Genomic Benchmarks emphasizes accessibility and reproducibility.
Datasets are available in a unified format with documented
preprocessing. Baseline results for multiple architectures provide
reference points for new models. The benchmark includes tasks of varying
difficulty, from relatively easy (distinguishing coding from non-coding
sequence) to challenging (identifying tissue-specific enhancers).

The benchmark's limitations reflect its design priorities. Focus on
classification rather than regression excludes quantitative prediction
tasks. Task difficulty varies substantially, with some tasks approaching
saturation where gains become difficult to measure. Species coverage,
while broader than many benchmarks, remains biased toward well-studied
model organisms.

\subsection{BEND: Benchmark for DNA Language
Models}\label{bend-benchmark-for-dna-language-models}

BEND (Benchmark for Evaluating DNA Models) provides a unified framework
for evaluating genomic foundation models across diverse tasks
(\textbf{de\_almeida\_bend\_2024?}). The benchmark includes regulatory
element classification, chromatin accessibility prediction, variant
effect scoring, and gene expression prediction. Standardized splits and
evaluation protocols enable fair comparison across model families.

BEND's design reflects lessons learned from earlier benchmarks. Tasks
span multiple biological scales, from nucleotide-level variant effects
to kilobase-scale regulatory elements. Evaluation includes both
zero-shot settings (using pretrained representations directly) and
fine-tuned settings (adapting models to specific tasks). Performance is
reported separately for each task rather than aggregated into a single
score, acknowledging that different models may excel at different
aspects of genomic prediction.

Comparative evaluations using BEND reveal that no single model dominates
across all tasks. Architecture choices (CNN versus transformer versus
state space model), tokenization schemes (single nucleotide versus k-mer
versus BPE), and pretraining corpora all influence task-specific
performance. These patterns inform model selection for specific
applications while highlighting the limitations of aggregate benchmarks
that obscure such variation.

\subsection{Long-Range Benchmarks}\label{long-range-benchmarks}

Long-range regulatory interactions, where enhancers tens to hundreds of
kilobases from their target genes influence expression, require
benchmarks that specifically test extended context modeling. The Long
Range Benchmark (LRB) evaluates models' ability to integrate information
across large genomic distances, with tasks including predicting distal
enhancer-promoter interactions, modeling TAD boundary effects, and
identifying long-range regulatory dependencies.

DNALongBench extends evaluation to ultra-long contexts spanning up to
millions of base pairs. Tasks at this scale test whether models can
leverage chromosome-level context for regulatory prediction, potentially
capturing effects from 3D chromatin organization and large-scale
chromatin domains.

These benchmarks are particularly relevant for evaluating efficient
attention mechanisms, state space models, and other architectures
designed to extend effective context length. Performance on long-range
benchmarks does not necessarily correlate with short-range task
performance, indicating that different architectural choices optimize
for different aspects of sequence modeling.

\subsection{Cross-Species Evaluation}\label{cross-species-evaluation}

GenBench and related resources test whether models trained on one
organism generalize to related species. Cross-species evaluation is
important for several reasons. Many applications require predictions in
non-human organisms (agricultural genomics, model organism research,
comparative genomics). Multi-species training may improve within-species
performance by providing additional evolutionary signal. The ability to
transfer across species indicates that models have learned general
principles of genome organization rather than species-specific
artifacts.

Cross-species benchmarks typically evaluate models on held-out species
not seen during training. Performance degradation from training to
held-out species indicates the degree to which learned representations
depend on species-specific features. Some architectures show better
cross-species transfer than others, suggesting differences in how well
they capture conserved regulatory principles.

\section{Variant Effect Prediction
Benchmarks}\label{variant-effect-prediction-benchmarks}

Variant effect prediction (VEP) benchmarks connect sequence changes to
molecular or phenotypic consequences, addressing the clinically central
question of which variants matter. These benchmarks span multiple
biological levels, from molecular function to clinical pathogenicity.

\subsection{Clinical Variant
Databases}\label{clinical-variant-databases}

ClinVar provides the most widely used labels for clinical variant effect
prediction, aggregating pathogenicity assertions from clinical
laboratories and researchers worldwide. Benchmarks derived from ClinVar
frame variant interpretation as classification: given a variant, predict
whether it is pathogenic, likely pathogenic, benign, or likely benign.

ClinVar's value as a benchmark stems from its clinical relevance.
Variants classified in ClinVar represent the actual population of
variants encountered in clinical testing. Performance on ClinVar
directly addresses whether a model can assist variant interpretation
workflows. The database's scale (over 2 million variant submissions as
of 2024) enables statistically robust evaluation.

ClinVar's limitations as a benchmark are equally important. Submission
heterogeneity means that label quality varies dramatically:
expert-curated panels provide high-confidence classifications while
single-laboratory submissions may reflect limited evidence. Version
sensitivity means that benchmark composition changes over time as new
submissions arrive and old classifications are updated. Most
consequentially, circularity with computational predictors creates
feedback loops: variants may have been classified using the very tools
being evaluated, inflating apparent performance.

Ancestry and gene coverage biases profoundly shape what ClinVar
benchmarks measure. Variants from European ancestry individuals and
well-studied disease genes are heavily overrepresented. High performance
on ClinVar demonstrates accuracy for this specific population rather
than robust generalization across human genetic diversity. Benchmarks
stratified by ancestry reveal substantial performance gaps, with models
typically performing worse on variants from underrepresented
populations.

Best practices for using ClinVar as a benchmark include specifying the
exact database version and download date, excluding variants with
conflicting assertions, stratifying performance by evidence level and
ancestry, and comparing to baselines using only allele frequency to
detect circularity. These practices are detailed in
Chapter~\ref{sec-evaluation}.

\subsection{CAGI: Critical Assessment of Genome
Interpretation}\label{cagi-critical-assessment-of-genome-interpretation}

The Critical Assessment of Genome Interpretation (CAGI) challenges
provide prospective evaluation of variant effect predictors on
unpublished datasets. Unlike retrospective benchmarks that evaluate
models on historical data, CAGI distributes prediction targets before
ground truth is available, preventing any possibility of overfitting to
known labels.

CAGI challenges cover diverse prediction targets. Some challenges focus
on molecular phenotypes: predicting the effect of variants on protein
stability, binding affinity, or enzymatic activity. Others target
clinical phenotypes: predicting disease risk, drug response, or clinical
severity from individual genomes. The diversity of challenges tests
whether models generalize across different types of variant effects.

The prospective design provides several advantages over retrospective
benchmarks. Predictions must be made before labels are known,
eliminating leakage from any source. The timeline forces models to
commit to predictions rather than post-hoc optimization. Community
participation enables comparison across many approaches under identical
conditions.

CAGI's limitation is scale: challenges include hundreds to thousands of
variants rather than the millions available in databases like ClinVar.
Statistical power to detect small performance differences is
correspondingly limited. The challenges also depend on experimental
collaborators willing to withhold data until after the prediction
deadline, limiting the range of phenotypes that can be assessed.

\subsection{Deep Mutational Scanning
Benchmarks}\label{deep-mutational-scanning-benchmarks}

Deep mutational scanning (DMS) provides systematic experimental
measurement of variant effects across entire proteins or regulatory
elements. DMS benchmarks test whether models can predict these
experimentally determined effects, providing direct validation against
measured functional consequences rather than inferred clinical
classifications.

MaveDB aggregates DMS datasets in a standardized format, enabling
systematic benchmarking across diverse proteins and assays. ProteinGym's
DMS component (discussed above) represents the most comprehensive
benchmark in this space. For non-coding variants, MPRA datasets provide
analogous systematic measurements of regulatory activity.

DMS benchmarks have distinct strengths and limitations compared to
clinical databases. The experimental grounding means that labels reflect
actual measured effects rather than clinical inference that may involve
multiple assumptions. However, the relationship between DMS fitness and
clinical pathogenicity is complex: a variant may substantially affect
enzymatic activity without causing disease if the residual activity
suffices for normal physiology. DMS benchmarks measure one component of
the variant interpretation puzzle rather than the full clinical picture.

\subsection{Regulatory and Non-Coding Variant
Benchmarks}\label{regulatory-and-non-coding-variant-benchmarks}

Non-coding variants require specialized benchmarks because their effects
operate through different mechanisms than coding variants. MPRA-based
benchmarks test whether models can predict the quantitative effect of
variants on enhancer or promoter activity measured in reporter assays.
eQTL-based benchmarks use naturally occurring variants associated with
expression changes, treating the statistical evidence for eQTL status as
a proxy for regulatory impact.

The challenge for non-coding benchmarks is connecting molecular effects
to phenotypic consequences. A variant may alter chromatin accessibility
without affecting any gene's expression. A variant may affect expression
without influencing disease risk. This gap between molecular and
clinical effects complicates interpretation: high performance on MPRA
prediction does not necessarily translate to accurate regulatory disease
variant interpretation.

Fine-mapped GWAS variants provide another benchmark source for
non-coding VEP. Statistical fine-mapping identifies putatively causal
variants within associated loci, and models can be evaluated on their
ability to prioritize these variants over nearby non-causal variants.
Performance on fine-mapping tasks more directly assesses clinical
relevance than molecular phenotype prediction, though fine-mapping
itself has substantial uncertainty.

\section{Trait and Population-Level
Benchmarks}\label{trait-and-population-level-benchmarks}

At the individual and population level, benchmarks assess whether models
improve prediction of complex traits and disease risk.

\subsection{Polygenic Score
Evaluation}\label{polygenic-score-evaluation}

Polygenic score (PGS) benchmarks evaluate how well genotype-derived
scores predict disease risk or quantitative traits. Common evaluation
settings include within-biobank evaluation, where a single large cohort
is partitioned into training and test sets, and cross-biobank
evaluation, where models trained in one population are tested in
another.

Metrics depend on the phenotype. For quantitative traits, benchmarks
report the coefficient of determination (RÂ²) or incremental RÂ² over
non-genetic covariates. For binary disease outcomes, AUROC and AUPRC
quantify discrimination. Calibration metrics assess whether predicted
risks match observed event rates. The clinical utility of PGS, discussed
in Chapter~\ref{sec-clinical-risk}, depends on all these properties: a
score may discriminate well (high AUROC) while being poorly calibrated
(predicted risks don't match actual event rates).

Cross-population evaluation is particularly important because PGS
portability is a major limitation of current methods
(Chapter~\ref{sec-gwas}). Benchmarks stratified by ancestry typically
reveal substantial performance degradation from European ancestry (where
most GWAS have been conducted) to other populations. This degradation
stems from multiple sources: different linkage disequilibrium patterns
mean that tag SNPs identify different causal variants,
population-specific variants are absent from training data, and effect
sizes may differ across populations due to gene-environment
interactions.

\subsection{TraitGym}\label{traitgym}

TraitGym provides a framework specifically designed to assess complex
trait prediction using genomic foundation models. The benchmark
evaluates whether foundation model embeddings or variant scores improve
prediction beyond traditional polygenic score methods.

TraitGym's design addresses several limitations of standard PGS
benchmarks. Ancestry stratification is built into the evaluation
protocol, requiring models to report performance separately for
different population groups. Multiple phenotypes spanning different
genetic architectures (highly polygenic versus more oligogenic) test
generalization across trait types. Comparison to appropriate baselines
(standard PGS methods, clinical covariates alone) isolates the
contribution of foundation model features.

The benchmark is particularly relevant for assessing claims that genomic
foundation models add predictive value beyond classical statistical
genetics. Foundation models incur substantial computational costs
compared to linear PGS models; TraitGym helps determine whether these
costs are justified by improved prediction.

\subsection{EmbedGEM Framework}\label{embedgem-framework}

The EmbedGEM framework evaluates whether foundation model embeddings
capture biologically meaningful genetic signal, as opposed to technical
artifacts or confounders (Mukherjee et al. 2024). The framework assesses
embeddings along two axes: heritability and disease relevance.

The heritability axis measures how much genetic signal an embedding
captures. EmbedGEM counts the number of genome-wide significant loci
associated with embedding components and quantifies the strength of
association through mean chi-squared statistics. Higher values indicate
that the embedding reflects heritable biology rather than noise.

The disease relevance axis measures whether embedding-associated
variants predict clinically meaningful outcomes. Polygenic scores
constructed from embedding GWAS hits are evaluated for their ability to
predict disease in independent cohorts. Incremental predictive value
over standard clinical models indicates that the embedding captures
disease-relevant genetic information.

This two-axis evaluation addresses a critical question for foundation
model deployment: do learned representations discover novel biology or
merely recapitulate known associations with additional computational
overhead? Embeddings that show high heritability but low disease
relevance may capture biological signal that is not clinically
actionable. Embeddings that show disease relevance without novel genetic
discoveries may not add value beyond existing PGS methods.

\section{Benchmark Construction and Hidden
Assumptions}\label{benchmark-construction-and-hidden-assumptions}

Beyond cataloging benchmark suites, understanding how benchmarks are
constructed reveals assumptions that shape what they measure and what
they miss.

\subsection{Data Sources and Label
Provenance}\label{data-sources-and-label-provenance}

Benchmark labels derive from diverse sources with different properties.
Experimental assays (ChIP-seq, DMS, MPRA) provide direct measurements
but are limited by assay-specific artifacts and selection pressures.
Computational annotations (gene calls, functional predictions,
conservation scores) provide broader coverage but introduce circular
dependencies if models are trained and evaluated on overlapping sources.
Clinical classifications aggregate expert judgment but reflect the
evidence available at classification time, which may include the very
predictors being benchmarked.

The provenance of benchmark labels determines what success on that
benchmark actually means. High performance on experimentally derived
labels suggests the model captures the specific molecular process
assayed. High performance on clinical labels may indicate genuine
clinical utility or may reflect circularity with existing prediction
tools. Understanding label provenance is prerequisite to interpreting
benchmark results.

\subsection{Splitting Strategies and
Leakage}\label{splitting-strategies-and-leakage}

How benchmarks partition data into training and test sets determines
whether evaluation measures generalization or memorization. Random
splitting, where examples are assigned to splits uniformly at random,
represents the weakest form of evaluation. In genomics, random splits
often permit homology-based leakage: training and test sequences may
share sufficient similarity that memorization suffices for good
performance.

Homology-aware splitting clusters sequences by similarity before
assigning clusters to splits, ensuring that test sequences are
evolutionarily distant from training sequences. This approach is
standard for protein benchmarks (using tools like CD-HIT or MMseqs2) but
less consistently applied for DNA benchmarks.

Chromosome-based splitting holds out entire chromosomes for testing,
preventing any position-based leakage within chromosomes. This approach
is common for regulatory benchmarks but does not account for homologous
sequences on different chromosomes. Temporal splitting reserves recent
data for testing, appropriate when benchmarks derive from databases with
submission timestamps. Each splitting strategy tests different aspects
of generalization; the choice should match the intended deployment
scenario.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-leakage-pathways}{[}High{]} Flow diagram showing
data flow with leakage highlighted. Central pipeline: Pretraining â†’ FM â†’
Fine-tuning â†’ Benchmark. Leakage pathways (red): Direct overlap,
homology leakage, label circularity, resource sharing, community
iteration. Specific examples: ESM/UniRefâ†’ProteinGym, ClinVar/CADD
circularity, ENCODE in both. Visual encoding: legitimate flow blue,
leakage red dashed.}

\end{figure}%

\subsection{Metric Selection and
Aggregation}\label{metric-selection-and-aggregation}

Benchmark metrics determine what aspects of model performance are
measured. Discrimination metrics (AUROC, AUPRC, correlation) assess
whether models rank predictions correctly. Calibration metrics (ECE,
reliability diagrams) assess whether predicted probabilities match
observed frequencies. Clinical utility metrics (net benefit, decision
curves) assess whether predictions improve decisions compared to
treating all patients the same.

Different metrics can yield different rankings of models. A model with
superior discrimination may have poor calibration, predicting the right
relative order but wrong absolute probabilities. Choosing which metric
to optimize, and how to aggregate across multiple tasks or datasets,
involves implicit decisions about what matters for downstream use.

Aggregation across tasks raises additional issues. Mean performance
across many tasks weights each task equally, regardless of clinical
importance or dataset quality. Median performance is robust to outliers
but obscures variation. Reporting full distributions of task-level
performance provides more information but complicates comparison. The
choice of aggregation method can substantially affect which model
appears best.

\subsection{Goodhart's Law and Benchmark
Gaming}\label{goodharts-law-and-benchmark-gaming}

Benchmarks create incentive structures, and incentive structures invite
optimization. Goodhart's Law, that a measure ceases to be a good measure
once it becomes a target, applies with particular force to machine
learning evaluation. When model development prioritizes leaderboard
position, the benchmark becomes the optimization target rather than a
proxy for the underlying capability it was designed to measure.

Gaming takes multiple forms in genomic AI. Architectural choices may be
tuned specifically to benchmark characteristics: receptive fields sized
to match benchmark sequence lengths, output heads designed for benchmark
label distributions, hyperparameters selected through extensive
benchmark-specific search. Such tuning improves benchmark performance
without necessarily improving generalization to deployment scenarios
that differ from benchmark conditions.

More subtle gaming arises from selective reporting. Models may be
evaluated on many benchmarks with only favorable results published.
Benchmark versions may be chosen to maximize apparent performance.
Evaluation protocols may deviate from published standards in ways that
inflate metrics. The cumulative effect is a literature where reported
performance systematically overestimates deployment capability.

The circularity between predictors and databases creates particularly
insidious gaming dynamics. When ClinVar classifications incorporate
computational predictions, and those predictions are then benchmarked
against ClinVar, the benchmark rewards models that resemble their
predecessors rather than models that provide independent information.
This circularity is rarely acknowledged in benchmark reporting, yet it
fundamentally compromises the validity of performance claims.

Mitigating gaming requires structural changes to evaluation practice:
prospective benchmarks like CAGI where predictions precede labels,
held-out evaluation consortia that resist optimization pressure, and
reporting standards that require disclosure of all benchmarks attempted
rather than only those where performance was favorable. The field's
maturation depends on developing evaluation cultures that reward honest
assessment over leaderboard position.

\section{Benchmark Saturation and
Staleness}\label{benchmark-saturation-and-staleness}

Benchmarks have finite useful lifetimes. As models improve, benchmarks
saturate; as data and methods evolve, benchmarks become stale.

\subsection{Saturation: When Benchmarks Stop
Discriminating}\label{saturation-when-benchmarks-stop-discriminating}

A benchmark saturates when the best models achieve performance that
cannot be meaningfully improved. Saturation may reflect fundamental
limits (the benchmark approaches the Bayes error rate), measurement
noise (the benchmark's labels are too noisy to support finer
discrimination), or ceiling effects (the metric itself cannot
distinguish between excellent and perfect performance).

Saturation is problematic because it removes the benchmark's value for
model selection. When all reasonable models achieve 0.97 AUROC,
differences between 0.970 and 0.975 are unlikely to reflect meaningful
capability differences. Yet benchmark reporting conventions often
emphasize such decimal places, creating an illusion of progress.

Detecting saturation requires estimating the irreducible error. For
benchmarks with replicate measurements, comparing model performance to
replicate concordance provides an upper bound: models cannot
systematically outperform the reproducibility of the underlying assay.
For benchmarks without replicates, saturation is harder to diagnose. One
heuristic is tracking the rate of improvement: when new methods provide
diminishing gains despite substantial architectural innovations,
saturation is likely.

The response to saturation should be moving to harder benchmarks that
still discriminate between methods, developing new benchmarks that
capture aspects of performance that existing benchmarks miss, and
retiring saturated benchmarks from active leaderboard competition while
retaining them as sanity checks.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-benchmark-saturation}{[}High{]} Two-panel figure.
Panel A (Saturation curve): Time vs benchmark performance; multiple
lines; ceiling line; saturation zone annotation. Panel B (Staleness
timeline): Benchmark creation dates above; data collection/annotation
dates; current year below; growing gap; examples (ENCODE 2012, ClinVar
evolving).}

\end{figure}%

\subsection{Staleness: When Benchmarks Diverge from
Practice}\label{staleness-when-benchmarks-diverge-from-practice}

Benchmarks become stale when they no longer reflect current data,
methods, or clinical practice. Assays evolve: a benchmark constructed
from early ENCODE data may not represent current experimental protocols.
Annotations improve: gene models, variant classifications, and
functional element maps are continuously updated. Clinical practice
shifts: treatment guidelines and diagnostic criteria change the meaning
of historical labels.

Staleness is insidious because it erodes benchmark validity gradually
rather than abruptly. A benchmark that accurately represented regulatory
prediction in 2015 may systematically misrepresent it in 2025, yet the
benchmark's continued use perpetuates optimization for an outdated
target.

Addressing staleness requires periodic benchmark refresh with updated
data and annotations, version control that documents exactly what each
benchmark version contains, and awareness that performance on historical
benchmarks may not predict performance on current data.

\subsection{Leakage from Scale}\label{leakage-from-scale}

Modern foundation models are pretrained on corpora that may include most
publicly available genomic data. This creates novel leakage risks
distinct from classical train-test overlap. A model pretrained on all
ENCODE data may effectively have seen the exact experiments used in many
regulatory benchmarks. A model pretrained on all UniRef may have seen
sequences highly similar to protein benchmark test sets. This
pretraining-benchmark overlap inflates performance in ways that are
difficult to detect and even more difficult to correct.

Leakage from scale is particularly problematic because it is often
undocumented. Model papers rarely enumerate exactly which datasets were
included in pretraining corpora, and benchmark papers rarely specify
which datasets should be excluded. The result is ambiguity about whether
benchmark success reflects genuine generalization or memorization from
pretraining.

Mitigating leakage from scale requires explicit documentation of
pretraining corpora, tools or hashes that help identify overlap between
pretraining data and benchmark test sets, and held-out evaluation
consortia that reserve data specifically for assessment without any use
in pretraining.

\section{The Benchmark-Deployment
Gap}\label{the-benchmark-deployment-gap}

High benchmark performance does not guarantee deployment success.
Understanding why requires examining the systematic differences between
benchmark settings and real-world applications.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-proxy-target-gap}{[}Essential{]} Conceptual diagram
with layers. Left column (What we measure): ClinVar labels, held-out
AUROC, DMS correlation, expression prediction accuracy. Right column
(What we want): True clinical impact, deployment discrimination, actual
protein function, regulatory mechanism. Center: Arrows with gap
indicators. Gap factors: label quality, distribution shift, metric
mismatch, temporal drift. Width of arrows indicates strength of proxy.
Key insight: gap not uniform.}

\end{figure}%

\subsection{Distribution Shift}\label{distribution-shift-1}

Benchmark test sets sample from the same distribution as training sets.
Deployment populations may differ systematically. For variant effect
prediction, benchmark variants are typically common enough to appear in
multiple databases, while deployment often targets rare variants seen in
single individuals. For regulatory prediction, benchmarks derive from
well-studied cell types and tissues, while deployment may require
prediction in understudied contexts.

Distribution shift manifests as degraded performance, but the pattern of
degradation varies. Some models degrade gracefully, maintaining
reasonable accuracy across the distribution shift. Others degrade
catastrophically, with confident predictions that prove systematically
wrong. Benchmarks that include held-out subpopulations or
out-of-distribution test sets provide some information about robustness,
but cannot anticipate every deployment scenario.

\subsection{Calibration Requirements}\label{calibration-requirements}

Clinical deployment requires not just accurate rankings but accurate
probability estimates. A variant classifier that achieves 0.95 AUROC by
assigning probability 0.9 to all pathogenic variants and 0.3 to all
benign variants discriminates well but provides miscalibrated
uncertainty. Clinical decisions that depend on thresholded predictions
(reporting variants above a certain probability) will perform poorly if
those probabilities don't reflect actual pathogenicity rates.

Most benchmark metrics emphasize discrimination over calibration. AUROC
is invariant to monotonic transformations of predicted probabilities.
Correlation measures rank preservation. As a result, models may be
optimized for benchmark success through strategies that damage
calibration. The benchmark-deployment gap for calibration can be large
even when discrimination metrics are excellent.

\subsection{Metric Mismatch}\label{metric-mismatch}

Benchmarks optimize specific metrics that may not align with deployment
objectives. AUROC weights errors equally regardless of where they occur
on the score distribution, but clinical utility may depend primarily on
performance at specific operating points. Correlation rewards getting
the overall pattern right but may not penalize systematic errors in
clinically important regions.

The gap between optimized metrics and deployment objectives creates
misaligned incentives. Model developers optimize for benchmark success,
which rewards specific metric improvements. Deployment success may
require different tradeoffs: prioritizing calibration over
discrimination, minimizing false negatives over false positives, or
performing well on specific subpopulations rather than overall.

\subsection{Practical Constraints}\label{practical-constraints}

Deployment environments impose constraints that benchmarks typically
ignore. Inference speed matters when predictions must be returned in
clinical timescales. Model size matters when deployment hardware has
limited memory. Interpretability matters when predictions must be
explained to clinicians or patients. Benchmarks that evaluate only
accuracy miss these dimensions of deployment fitness.

The benchmark-deployment gap is not merely a technical inconvenience. It
represents a fundamental tension between evaluation tractability and
deployment validity. Benchmarks are valuable precisely because they are
standardized, reproducible, and comparable across methods. Deployment is
valuable precisely because it addresses the specific needs of real-world
applications. Bridging this gap requires benchmark designs that better
approximate deployment conditions and deployment evaluations that
provide feedback to benchmark development.

\section{Systematic Gaps in Current
Benchmarks}\label{systematic-gaps-in-current-benchmarks}

Despite the proliferation of benchmark suites, systematic gaps remain in
the genomic evaluation landscape.

\textbf{Variant types}: Structural variants, inversions, copy number
variants, and complex rearrangements are rarely evaluated despite
accounting for substantial genomic variation and disease burden. Repeat
regions are often excluded or masked. Multi-variant effects and
haplotype-specific phenomena receive minimal attention.

\textbf{Populations}: Non-European ancestry groups remain severely
underrepresented. Performance stratified by ancestry reveals gaps that
aggregate metrics conceal. Environmental diversity (lifestyle,
exposures, treatments) that shapes phenotypic expression is rarely
incorporated.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-cross-population-performance}{[}High{]} Grouped bar
chart or heatmap. Rows: Different tasks/models. Columns: Ancestry groups
(European, African, East Asian, South Asian, Admixed). Values:
Performance metrics. Specific comparisons: PGS 40-75\% reduction
non-European; ClinVar-trained models across ancestries. Annotations:
sample sizes, significance, training data representation. Key insight:
aggregate conceals systematic failures.}

\end{figure}%

\textbf{Modalities}: Long-read sequencing data is scarce in benchmarks
despite its advantages for structural variants and phasing. Single-cell
benchmarks are emerging but remain limited compared to bulk assay
benchmarks. Spatial transcriptomics and other emerging modalities have
minimal coverage.

\textbf{Clinical endpoints}: Most benchmarks use molecular surrogates
rather than hard clinical endpoints. Disease incidence, progression,
treatment response, and patient-reported outcomes are rarely the direct
prediction target. The gap between molecular proxy accuracy and clinical
utility remains poorly characterized.

These gaps mean that strong benchmark performance may not predict
utility for underserved populations, understudied variant classes, or
clinical applications that depend on endpoints the benchmarks don't
measure.

\section{Incentives and Their Limits}\label{incentives-and-their-limits}

Benchmarks structure the incentives of genomic AI development. The
specific tasks, metrics, and leaderboards that the community adopts
determine what models are optimized for, what claims of progress are
evaluated against, and what capabilities receive attention versus
neglect. A benchmark that emphasizes European-ancestry variants produces
models tuned for European-ancestry performance. A benchmark that rewards
discrimination (AUROC) over calibration produces models that rank
variants well but estimate probabilities poorly. A benchmark that reuses
training data from widely available resources creates indirect leakage
that inflates apparent performance. The benchmark landscape is not
neutral infrastructure but an active force shaping what the field
builds.

The landscape surveyed here spans protein benchmarks (TAPE, FLIP,
ProteinGym), DNA and regulatory benchmarks (Genomic Benchmarks, BEND),
variant effect benchmarks (ClinVar, CAGI, DMS), and trait-level
benchmarks (TraitGym, EmbedGEM). Across all categories, persistent
challenges emerge: saturation that reduces discriminative power as
models approach ceiling performance, staleness that erodes validity as
benchmarks age, leakage risks that inflate apparent capabilities, and
systematic gaps in population diversity, variant type coverage, and
clinical endpoint representation.

The benchmark-deployment gap represents perhaps the most consequential
limitation. Strong performance on established benchmarks does not
guarantee that models will behave reliably when deployed in clinical or
research settings with different data distributions, patient
populations, or outcome definitions. The methodological principles for
using benchmarks properly, including experiment design, metric
selection, and common pitfalls, are examined in
Chapter~\ref{sec-evaluation}. The confounding issues that plague both
benchmark construction and model training receive dedicated treatment in
Chapter~\ref{sec-confounding}. Together with this catalog of what
benchmarks exist, those chapters provide the critical apparatus for
evaluating genomic foundation model claims.

\chapter{Evaluation Principles}\label{sec-evaluation}

Genomic data makes it exceptionally easy to fool yourself. Sequences
share evolutionary history, so a model that memorizes training sequences
may appear to generalize when tested on homologs. Variants cluster in
families and populations, so ancestry-stratified performance can
masquerade as genuine prediction. Experimental measurements carry batch
effects invisible to the untrained eye, so a model can learn to
distinguish sequencing centers rather than biological states. Training
labels often derive from the very databases used for evaluation,
creating circular validations that inflate performance without testing
genuine predictive power. Every shortcut that simplifies evaluation in
other machine learning domains becomes an opportunity for false
confidence in genomics.

Random data splits that work perfectly well for natural images become
actively misleading when applied to biological sequences. A protein held
out for testing may share 90\% sequence identity with a training
protein, allowing the model to succeed through memorization rather than
generalization. A variant classified as pathogenic in the test set may
come from the same gene family as training variants, letting the model
exploit gene-level signals rather than learning variant-specific
effects. A cell line in the test set may have been processed at the same
sequencing center as training samples, enabling the model to recognize
batch signatures rather than biological patterns. These leakages are not
hypothetical; they have inflated reported performance across the genomic
machine learning literature.

This chapter addresses how to use benchmarks appropriately to draw valid
conclusions about model performance. Chapter~\ref{sec-benchmarks}
catalogs what benchmark tasks exist, how they are constructed, and what
capabilities they probe. Here we address the complementary question:
given a benchmark, how do we apply it to produce trustworthy results?
The difference between valid and misleading evaluation often lies not in
benchmark choice but in methodological details: data splitting
strategies, metric selection, baseline comparisons, ablation designs,
and statistical testing. These principles apply across all benchmark
categories, from chromatin state prediction to clinical variant
classification. By mastering evaluation methodology, practitioners can
distinguish genuine advances from artifacts that will not survive
deployment.

\section{Why Random Splits Fail}\label{why-random-splits-fail}

The standard machine learning recipe calls for randomly partitioning
data into training, validation, and test sets. For image classification
or sentiment analysis, this approach works well because individual
examples are approximately independent. A photograph of a cat shares no
special relationship with another photograph of a different cat beyond
their common label. Random assignment ensures that training and test
distributions match, and performance on the test set provides an
unbiased estimate of performance on new examples from the same
distribution.

Genomic data violates these assumptions at every level. Consider a
protein dataset where the goal is to predict stability from sequence.
Proteins in the same family share evolutionary history and often similar
structures. If a training set includes beta-lactamase variants from
\emph{E. coli} and the test set includes beta-lactamase variants from
\emph{Klebsiella}, the model may appear to generalize to ``new''
proteins while actually recognizing sequence patterns it saw during
training. The test performance reflects memorization of family-specific
features rather than general principles of protein stability.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-random-splits-fail}{[}Essential{]} Three-panel
comparison. Panel A (Image classification): Grid of images; random
assignment works; independent. Panel B (Protein classification): Family
tree; random split places related across train/test; 80\% identity
highlighted; ``memorization pathway.'' Panel C (Variant prediction):
Gene diagram with variants; same gene in train/test; family pedigree
spanning splits; population structure. Annotations: sequence identity,
relatedness, pseudo-replication.}

\end{figure}%

The problem compounds when sequence identity is high. Two proteins
sharing 80\% sequence identity will typically have similar structures
and functions. A model trained on one and tested on the other is not
really being tested on a novel example; it is being asked to interpolate
within a region of sequence space it has already explored. Even at 30\%
sequence identity, the so-called ``twilight zone'' of homology
detection, proteins often share structural and functional similarities
that can be exploited by sufficiently powerful models.

Variant-level data presents analogous challenges. Variants within the
same gene share genomic context, and variants affecting the same protein
domain share structural environment. Variants from the same individual
share haplotype background. Variants from the same population share
allele frequency distributions shaped by demographic history. Each of
these relationships creates opportunities for models to learn shortcuts
that generalize within the training distribution but fail on genuinely
novel examples.

The consequence is that random splits systematically overestimate
generalization. A model that achieves 0.90 AUROC with random splitting
might achieve only 0.75 AUROC when evaluated on truly held-out examples,
with the gap reflecting how much the model learned about biology versus
how much it learned about the structure of the training data.
Recognizing this problem is the first step toward solving it.

\section{Homology-Aware Splitting}\label{homology-aware-splitting}

The solution to homology-driven leakage is to explicitly account for
sequence similarity when constructing data splits. Rather than random
assignment, examples are clustered by sequence identity, and entire
clusters are assigned to training, validation, or test sets. This
ensures that no test example is ``too similar'' to any training example,
forcing the model to demonstrate genuine generalization.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-homology-splitting}{[}Essential{]} Step-by-step
workflow. Steps: (1) All sequences; (2) Clustering (CD-HIT/MMseqs2 at
threshold); (3) Cluster visualization; (4) Split assignment (entire
clusters to train/val/test); (5) Validation (no test \textgreater X\%
identity to training). Code snippets. Comparison table (strategy,
strictness, efficiency, use case). Key insight: threshold determines
hardness.}

\end{figure}%

\subsection{Clustering Tools and
Workflows}\label{clustering-tools-and-workflows}

Two tools dominate homology-aware splitting in practice. \textbf{CD-HIT}
clusters sequences by greedy incremental clustering, assigning each
sequence to an existing cluster if it exceeds a similarity threshold to
the cluster representative, or creating a new cluster otherwise. The
algorithm is fast and scales to millions of sequences. For proteins, a
typical workflow clusters at 40\% sequence identity for stringent
splitting or 70\% for moderate splitting. For nucleotide sequences,
thresholds are typically higher (80-95\%) due to different evolutionary
rates.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cluster proteins at 40\% identity}
\ExtensionTok{cd{-}hit} \AttributeTok{{-}i}\NormalTok{ proteins.fasta }\AttributeTok{{-}o}\NormalTok{ proteins\_clustered }\AttributeTok{{-}c}\NormalTok{ 0.4 }\AttributeTok{{-}n}\NormalTok{ 2}

\CommentTok{\# Cluster nucleotides at 90\% identity  }
\ExtensionTok{cd{-}hit{-}est} \AttributeTok{{-}i}\NormalTok{ sequences.fasta }\AttributeTok{{-}o}\NormalTok{ sequences\_clustered }\AttributeTok{{-}c}\NormalTok{ 0.9 }\AttributeTok{{-}n}\NormalTok{ 8}
\end{Highlighting}
\end{Shaded}

\textbf{MMseqs2} offers faster clustering with similar sensitivity,
becoming essential for large-scale analyses. The tool supports multiple
clustering modes and can handle databases with hundreds of millions of
sequences. For foundation model pretraining where deduplication affects
billions of sequences, MMseqs2 is often the only practical option.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create database and cluster at 30\% identity}
\ExtensionTok{mmseqs}\NormalTok{ createdb proteins.fasta DB}
\ExtensionTok{mmseqs}\NormalTok{ cluster DB DB\_clu tmp }\AttributeTok{{-}{-}min{-}seq{-}id}\NormalTok{ 0.3}
\ExtensionTok{mmseqs}\NormalTok{ createtsv DB DB DB\_clu clusters.tsv}
\end{Highlighting}
\end{Shaded}

The choice of identity threshold involves trade-offs. Stricter
thresholds (lower identity for proteins, higher for nucleotides) create
harder generalization tests but may leave insufficient data for training
if clusters are small. Permissive thresholds retain more training data
but allow more leakage through homologous sequences. For protein
function prediction, 30-40\% identity thresholds are common; for variant
effect prediction within genes, even stricter gene-family-level splits
may be necessary.

\subsection{Practical Considerations}\label{practical-considerations-1}

Several subtleties affect the quality of homology-aware splits.
\textbf{Cluster size distribution} matters: if one cluster contains half
the data and is assigned to training, the remaining clusters may be too
small or too biased to serve as representative test sets. Stratified
sampling within clusters or careful balancing across splits can mitigate
this issue.

\textbf{Transitive homology} creates hidden relationships that pairwise
clustering can miss. Protein A may share 35\% identity with protein B,
and protein B may share 35\% identity with protein C, yet A and C share
only 20\% identity. If A is in training and C is in testing, B serves as
an indirect bridge. Connected component analysis or multi-step
clustering can address these transitive relationships, though at
increased computational cost.

\textbf{Domain-level homology} complicates whole-protein clustering. A
multi-domain protein may share one domain with training proteins and
another domain with test proteins. Whether this represents leakage
depends on the prediction task: if predicting whole-protein function,
shared domains matter; if predicting domain-specific properties, they
matter more acutely. Domain-aware splitting assigns domains rather than
whole proteins to clusters, though this requires domain annotation that
may not always be available.

For genomic (non-protein) sequences, repeat elements and transposable
elements create analogous challenges. A model trained to predict
chromatin state may learn features of LINE elements that recur
throughout the genome. Excluding repetitive regions from evaluation or
explicitly accounting for repeat content can clarify what the model has
actually learned about regulatory sequences versus repetitive element
patterns.

\section{Splitting by Biological
Axis}\label{splitting-by-biological-axis}

Beyond sequence homology, genomic data admits multiple axes along which
splits can be constructed. The choice of axis determines what kind of
generalization is being tested.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-splitting-strategies}{[}High{]} Matrix: Splitting
strategies vs.~what they test. Rows: Random, individual-aware,
family-aware, chromosome, gene/protein family, cohort/site, temporal,
ancestry. Columns: Sequence generalization, individual, population,
temporal robustness, cross-site. Cells: âœ“, âœ—, \textasciitilde.
Schematics for each strategy; performance drop annotations; combinable
strategies note.}

\end{figure}%

\subsection{Splitting by Individual}\label{splitting-by-individual}

For tasks involving human genetic variation, ensuring that data from the
same individual (or related individuals) does not appear in both
training and test sets is essential. A variant effect predictor trained
on variants from person A and tested on other variants from person A may
learn individual-specific patterns, such as haplotype structure or
ancestry-correlated allele frequencies, that do not generalize to new
individuals.

Family structure creates subtler leakage. First-degree relatives share
approximately 50\% of their genomes identical by descent. Even distant
relatives share genomic segments that can be exploited by sufficiently
powerful models. Best practice involves computing kinship coefficients
across all individuals and either excluding one member of each related
pair or assigning entire family clusters to the same split. The UK
Biobank provides pre-computed relatedness estimates; other cohorts may
require explicit calculation using tools like KING or PLINK.

\subsection{Splitting by Genomic
Region}\label{splitting-by-genomic-region}

Chromosome-based splits assign entire chromosomes to training or
testing. This approach is common in regulatory genomics, where models
trained on chromosomes 1-16 are tested on chromosomes 17-22 (or similar
partitions). The advantage is simplicity and reproducibility; the
disadvantage is that chromosomes are not independent. Chromosome 6
contains the HLA region with its unusual patterns of variation and
selection; chromosome 21 is small and gene-poor; sex chromosomes have
distinct biology. Results may vary substantially depending on which
chromosomes are held out.

Region-based splits hold out contiguous segments (e.g., 1 Mb windows)
distributed across the genome. This provides more uniform coverage than
chromosome splits but requires careful attention to boundary effects. If
a regulatory element spans the boundary between training and test
regions, parts of its context may leak into training.

\subsection{Splitting by Gene or Protein
Family}\label{splitting-by-gene-or-protein-family}

For variant effect prediction, holding out entire genes or protein
families tests whether models learn general principles versus
gene-specific patterns. A model that achieves high accuracy by
memorizing that TP53 variants are often pathogenic has not demonstrated
understanding of mutational mechanisms. Gene-level splits force models
to generalize to genes they have never seen, providing stronger evidence
of biological insight.

Family-level splits extend this logic to groups of related genes.
Holding out all kinases or all GPCRs tests whether models can generalize
across evolutionary families. This is particularly stringent for protein
structure and function prediction, where family membership strongly
predicts properties.

\subsection{Splitting by Experimental
Context}\label{splitting-by-experimental-context}

Multi-task models that predict chromatin marks across cell types can be
split by cell type rather than genomic position. Training on liver,
lung, and brain while testing on heart and kidney assesses whether
learned regulatory logic transfers across tissues. This matters because
cell-type-specific factors drive much of regulatory variation; a model
that has simply learned which regions are accessible in the training
cell types may fail on novel cell types even when sequence features
should transfer.

Similarly, models can be split by assay type (e.g., training on
ATAC-seq, testing on DNase-seq), laboratory (to assess batch effects),
or time point (for longitudinal data). Each split tests a different axis
of generalization.

\subsection{Splitting by Ancestry}\label{splitting-by-ancestry}

For human genomic applications, ancestry-stratified evaluation has
become essential. Models trained predominantly on European ancestry
cohorts often show degraded performance in African, East Asian, South
Asian, and admixed populations. This degradation reflects both
differences in allele frequency spectra and differences in linkage
disequilibrium patterns that affect which variants are informative.

Best practice reports performance separately for each major ancestry
group represented in the data. When held-out ancestry groups are
available (e.g., training on Europeans and testing on Africans), this
provides the strongest test of cross-population generalization. When
only European data are available, this limitation should be explicitly
acknowledged, and claims about generalization should be appropriately
modest. The confounding effects of ancestry on genomic predictions are
detailed in Chapter~\ref{sec-confounding}.

\subsection{Splitting by Time}\label{splitting-by-time}

Temporal splits assign data to training and test sets based on when
observations were collected, annotations were created, or variants were
classified. This strategy tests whether models generalize forward in
time, the actual deployment scenario for any predictive system.

For variant pathogenicity prediction, temporal splits are particularly
revealing. Training on ClinVar annotations from 2018 and testing on
variants first classified in 2022 asks whether the model can predict
labels that did not yet exist during training. This avoids the
circularity that arises when training and test labels were assigned by
similar processes at similar times. Variants classified more recently
may reflect updated curation standards, new functional evidence, or
reclassifications of previously uncertain variants; a model that
performs well on these genuinely new classifications demonstrates
predictive validity rather than recapitulation of historical curation
patterns.

Implementing temporal splits requires metadata that many datasets lack.
ClinVar provides submission dates, enabling clean temporal partitioning.
UniProt tracks annotation dates for functional assignments. Clinical
cohorts with longitudinal follow-up naturally admit temporal splits
based on diagnosis dates. When temporal metadata is unavailable,
publication dates of source literature can serve as proxies, though
these may not perfectly reflect when information became available to
model developers.

The key limitation of temporal splits is non-stationarity. The
distribution of variants classified in 2022 may differ systematically
from those classified in 2018, not because biology changed but because
research priorities, sequencing technologies, and ascertainment patterns
evolved. Performance degradation on temporally held-out data may reflect
distribution shift rather than genuine failure to generalize. Combining
temporal splits with stratified analysis (performance by variant type,
gene category, or evidence strength) helps disentangle these factors.

\section{Leakage Taxonomy and
Detection}\label{leakage-taxonomy-and-detection}

Even with careful splitting, leakage can enter evaluations through
multiple pathways. Understanding common leakage patterns helps
practitioners design cleaner evaluations and critically assess published
results.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-leakage-taxonomy}{[}High{]} Taxonomy tree. Top: Data
Leakage. Branches: Feature Leakage (features encode target;
conservationâ†’pathogenicity; detection via ablation), Label Leakage
(labels derived from model-accessible info; ClinVar circularity;
temporal validation), Benchmark Leakage (pretraining overlaps
evaluation; ENCODE everywhere; document and check overlap). For each:
red warning, genomic example, detection strategy, mitigation.}

\end{figure}%

\subsection{Feature Leakage}\label{feature-leakage}

Feature leakage occurs when input features encode information about the
target that would not be available at prediction time. In genomics,
conservation scores are a common source. If a model uses PhyloP scores
as features and the target is pathogenicity, the model may learn that
``conserved positions are more likely pathogenic'' without learning
anything about variant biology. This would be fine if conservation
scores are intended to be part of the prediction pipeline, but
problematic if the goal is to develop a model that can predict
pathogenicity from sequence alone.

Similarly, population allele frequency encodes selection pressure. A
model that learns ``rare variants are more likely pathogenic'' has
discovered a useful heuristic but not necessarily a mechanistic
understanding. Whether this counts as leakage depends on the intended
use case. For clinical variant interpretation where allele frequency is
always available, exploiting this feature is appropriate. For
understanding variant biology, it may mask whether the model has learned
anything beyond allele frequency.

\subsection{Label Leakage}\label{label-leakage}

Label leakage occurs when target labels are derived from information
that the model can access through its features. The classic example is
training pathogenicity predictors on ClinVar annotations while using
sequence features that were used to construct ClinVar annotations. If
ClinVar curators used SIFT and PolyPhen scores when classifying
variants, and the model uses similar sequence features, high performance
may reflect recapitulation of curation criteria rather than independent
predictive power.

Temporal label leakage is subtler. A model trained on ClinVar
annotations from 2020 and tested on annotations from 2023 may perform
well because new annotations were informed by model-like predictions.
The apparent validation is circular: the model predicts labels that were
partially derived from model-like reasoning.

\subsection{Benchmark Leakage}\label{benchmark-leakage}

Benchmark leakage occurs when test set construction was influenced by
methods similar to those being evaluated. If a protein function
benchmark was created by selecting proteins with high-confidence
annotations, and those annotations were partly derived from sequence
similarity searches, sequence-based models may perform well by
exploiting the same similarity that guided benchmark construction.

Foundation models face particular challenges with benchmark leakage. If
a DNA language model is pretrained on all publicly available genomic
sequence including ENCODE data, and then evaluated on ENCODE-derived
benchmarks, the pretraining has exposed the model to information about
the test distribution even if specific test examples were held out. The
model may have learned statistical patterns in ENCODE data that transfer
to ENCODE benchmarks without reflecting genuine biological
understanding.

\subsection{Detection Strategies}\label{detection-strategies}

Several strategies help detect leakage. \textbf{Baseline analysis} asks
whether simple models that could not plausibly have learned biology
achieve suspiciously high performance. If a linear model using only
allele frequency achieves 0.80 AUROC on a pathogenicity benchmark, and a
sophisticated deep model achieves 0.82, the marginal improvement may not
justify claims of biological insight.

\textbf{Feature ablation} systematically removes potentially leaky
features and measures performance degradation. If removing conservation
scores causes a 20-point drop in AUROC, the model was heavily dependent
on conservation rather than learning independent predictors.

\textbf{Confounder analysis} explicitly models potential confounders and
tests whether model predictions remain informative after conditioning.
If a variant effect predictor's scores become non-predictive after
controlling for gene length and expression level, the model may have
learned gene-level confounders rather than variant-level effects.

\textbf{Temporal validation} evaluates models on data collected after
the training data was frozen. If performance degrades substantially on
newer data, the model may have been fitted to temporal artifacts in the
original dataset.

\section{Metrics for Genomic Tasks}\label{metrics-for-genomic-tasks}

Metrics quantify model performance but different metrics answer
different questions. Choosing appropriate metrics requires clarity about
what aspect of performance matters for the intended application.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-metric-selection}{[}High{]} Decision flowchart.
Decision points: Binary vs continuous? Balanced? Ranking vs probability?
Clinical decisions? Metric recommendations at each terminal. Metric
descriptions: AUROC, AUPRC, calibration, net benefit. Warning callouts:
``AUROC invariant to monotonic transforms''; ``High correlation â‰ 
clinically meaningful.''}

\end{figure}%

\subsection{Discrimination Metrics}\label{discrimination-metrics}

For binary outcomes (pathogenic versus benign, bound versus unbound,
accessible versus closed), discrimination metrics assess how well the
model separates classes. The \textbf{area under the receiver operating
characteristic curve (AUROC)} measures the probability that a randomly
selected positive example is ranked above a randomly selected negative
example. AUROC is threshold-independent and widely reported but can be
misleading when classes are highly imbalanced.

The \textbf{area under the precision-recall curve (AUPRC)} better
reflects performance when positives are rare. For variant pathogenicity
prediction, where perhaps 1\% of variants are truly pathogenic, a model
achieving 0.95 AUROC might still have poor precision at useful recall
levels. AUPRC directly captures the precision-recall trade-off that
matters for applications requiring both high sensitivity and manageable
false positive rates.

\textbf{Sensitivity}, \textbf{specificity}, \textbf{positive predictive
value}, and \textbf{negative predictive value} require specifying a
decision threshold. These metrics are more interpretable for specific
use cases (e.g., ``the model identifies 90\% of pathogenic variants
while flagging only 5\% of benign variants as false positives'') but
require choosing thresholds that may not generalize across settings.

\subsection{Regression and Correlation
Metrics}\label{regression-and-correlation-metrics}

For continuous predictions (expression levels, effect sizes, binding
affinities), correlation metrics assess agreement between predicted and
observed values. \textbf{Pearson correlation} measures linear
association; \textbf{Spearman correlation} measures rank association and
is robust to nonlinear relationships. The \textbf{coefficient of
determination (RÂ²)} measures variance explained, though interpretation
requires care when baseline performance is near zero.

For predictions at genomic scale (e.g., predicted versus observed
expression across thousands of genes), these metrics may obscure
important patterns. A model might achieve high genome-wide correlation
by correctly predicting which genes are highly expressed while failing
on the genes where predictions matter most. Task-specific
stratification, such as correlation within expression quantiles or among
disease-relevant genes, provides more actionable information.

\subsection{Ranking and Prioritization
Metrics}\label{ranking-and-prioritization-metrics}

Many genomic workflows care about ranking rather than absolute
prediction. Variant prioritization pipelines rank candidates for
follow-up; gene prioritization ranks targets for experimental
validation. \textbf{Top-k recall} measures the fraction of true
positives captured in the top k predictions. \textbf{Enrichment at k}
compares the true positive rate in the top k to the background rate.
\textbf{Normalized discounted cumulative gain (NDCG)} weights ranking
quality by position, penalizing relevant items placed lower in the list
more than those placed near the top.

These metrics align with how predictions are actually used. If
experimental capacity permits validating only 20 variants per locus,
top-20 recall matters more than global AUROC. Reporting both global
metrics and rank-aware metrics at relevant cutoffs provides a complete
picture.

\subsection{Calibration Metrics}\label{calibration-metrics}

Calibration assesses whether predicted probabilities match observed
frequencies. A well-calibrated model that predicts 0.8 probability of
pathogenicity should be correct about 80\% of the time across all
variants receiving that score. \textbf{Reliability diagrams} plot
predicted probabilities against observed frequencies within binned
intervals, with deviations from the diagonal indicating miscalibration.
\textbf{Expected calibration error (ECE)} summarizes miscalibration as
the weighted average absolute deviation across bins.

Calibration matters whenever predictions inform decisions. A
miscalibrated model that reports 0.95 probability when the true
probability is 0.60 will lead to inappropriate confidence in uncertain
predictions. Even models with excellent discrimination can be poorly
calibrated, requiring post-hoc calibration methods such as Platt scaling
or isotonic regression. Chapter~\ref{sec-uncertainty} addresses
calibration and uncertainty quantification in greater depth.

\subsection{Clinical Utility Metrics}\label{clinical-utility-metrics}

For clinical applications, discrimination and calibration are necessary
but not sufficient. \textbf{Decision curves} plot net benefit across
decision thresholds, where net benefit weighs the value of true
positives against the cost of false positives at each threshold. A model
may achieve high AUROC but offer no net benefit at clinically relevant
thresholds if it fails to discriminate in the region where decisions are
actually made.

\textbf{Net reclassification improvement (NRI)} measures how often
adding genomic features to a clinical model changes risk classifications
in the correct direction. This directly addresses whether genomics adds
clinical value beyond existing predictors.
Chapter~\ref{sec-clinical-risk} provides detailed treatment of clinical
evaluation frameworks.

\section{Baseline Selection}\label{baseline-selection}

Baseline comparisons determine the meaning of reported performance. A
model achieving 0.85 AUROC might represent a major advance if the best
prior method achieved 0.70, or a trivial improvement if simple
heuristics achieve 0.83. Choosing appropriate baselines is as important
as choosing appropriate metrics.

\subsection{Strong Baselines, Not Straw
Men}\label{strong-baselines-not-straw-men}

The temptation to compare against weak baselines inflates apparent
contributions. A deep learning model compared against a naive prior or a
deliberately crippled baseline will appear impressive regardless of
whether it offers genuine value. Strong baselines force honest
assessment of improvement.

For sequence-based predictions, position weight matrices (PWMs) and
k-mer logistic regression provide classical baselines that capture
sequence composition without deep learning. If a convolutional model
barely outperforms logistic regression on k-mer counts, the
convolutional architecture may not be contributing as much as claimed.

For variant effect prediction, simple features like allele frequency,
conservation scores, and amino acid properties provide baselines that
any sophisticated model should substantially exceed. CADD
(Chapter~\ref{sec-vep-classical}) serves as a well-calibrated baseline
that combines many hand-crafted features; outperforming CADD
demonstrates that learning provides value beyond feature engineering.

For foundation models, comparisons should include both randomly
initialized models of similar architecture (to isolate the value of
pretraining) and simpler pretrained models (to isolate the value of
scale or architectural innovations). Claiming that pretraining helps
requires demonstrating improvement over training from scratch on the
same downstream data.

\subsection{Historical Baselines and Progress
Tracking}\label{historical-baselines-and-progress-tracking}

Comparing to methods from five years ago may demonstrate progress but
overstates the contribution of any single method. Comparisons should
include the best currently available alternatives, not just historically
important ones. When prior work is not directly comparable (different
data, different splits, different metrics), reimplementing baselines on
common benchmarks provides fairer comparison.

Field-wide progress tracking benefits from persistent benchmarks with
frozen test sets. Once test set results for a benchmark are published,
that benchmark becomes less useful for future model development because
the test set is no longer truly held out. Periodic benchmark refresh
with new held-out data helps maintain evaluation integrity.

\subsection{Non-Deep-Learning
Baselines}\label{non-deep-learning-baselines}

Deep learning models should be compared against strong non-deep
alternatives. Gradient-boosted trees, random forests, and regularized
linear models often achieve competitive performance with far less
computation. If a 100-million-parameter transformer barely outperforms
XGBoost on tabular features, the complexity may not be justified.

This comparison is especially important for clinical deployment, where
simpler models may be preferred for interpretability, computational
efficiency, or regulatory approval. Demonstrating that deep learning
provides substantial gains over strong non-deep baselines strengthens
the case for adoption.

\section{Ablation Studies}\label{ablation-studies}

Ablation studies systematically remove or modify model components to
understand their contributions. Where baselines compare across methods,
ablations investigate within a method, revealing which design choices
actually matter.

\subsection{Component Isolation}\label{component-isolation}

Standard ablations remove individual components: attention layers, skip
connections, normalization schemes, specific input features. If removing
attention heads causes minimal performance degradation, the model may
not be exploiting long-range dependencies as claimed. If removing a
particular input modality has no effect, that modality may not be
contributing useful information.

Ablations should be designed to test specific hypotheses. If the claim
is that a foundation model learns biologically meaningful
representations, ablating pretraining (comparing to random
initialization) directly tests this claim. If the claim is that
cross-attention between modalities enables integration, ablating
cross-attention while retaining separate encoders tests whether
integration provides value.

\subsection{Hyperparameter
Sensitivity}\label{hyperparameter-sensitivity}

Reporting performance across hyperparameter ranges reveals robustness. A
model that achieves state-of-the-art performance only at a narrow
learning rate range with specific regularization may be overfit to the
evaluation setup. Consistent performance across reasonable
hyperparameter variations provides stronger evidence of genuine
capability.

\subsection{Architecture Search
Confounds}\label{architecture-search-confounds}

When model development involves extensive architecture search, reported
performance conflates the value of the final architecture with the value
of search on the validation set. The validation set is no longer truly
held out; it has been used to select among architectures. Final
evaluation on a completely untouched test set, with the architecture
fixed before test set examination, provides cleaner assessment.

\subsection{Reporting Standards}\label{reporting-standards}

Ablation tables should clearly indicate what was changed in each
condition, the number of random seeds or runs, and measures of variance.
Single-run ablations can produce misleading results due to training
stochasticity. Reporting means and standard deviations across multiple
runs reveals whether observed differences exceed random variation.

\section{Statistical Rigor}\label{statistical-rigor}

Performance differences between models may reflect genuine capability
differences or random variation in training and evaluation. Statistical
analysis distinguishes signal from noise.

\subsection{Significance Testing}\label{significance-testing}

For classification metrics, significance tests ask whether observed
differences exceed what would be expected from sampling variation.
\textbf{Bootstrap confidence intervals} resample the test set with
replacement, recompute metrics on each resample, and report the
distribution of metric values. Non-overlapping 95\% confidence intervals
suggest significant differences. \textbf{Permutation tests} shuffle
predictions between models and measure how often shuffled differences
exceed observed differences.

For comparing multiple models across multiple benchmarks, correction for
multiple testing becomes important. Without correction, 20 pairwise
comparisons will produce an expected one false positive at the 0.05
level even when all models perform equally. The \textbf{Bonferroni
correction} divides the significance threshold by the number of tests;
the \textbf{Benjamini-Hochberg procedure} controls false discovery rate
with more power than Bonferroni.

\subsection{Effect Sizes}\label{effect-sizes}

Statistical significance does not imply practical significance. A
difference of 0.001 AUROC might be statistically significant with
millions of test examples while being practically meaningless.
\textbf{Effect sizes} quantify the magnitude of differences independent
of sample size. Cohen's d for continuous outcomes and odds ratios for
binary outcomes provide standardized measures of effect magnitude.

Reporting both significance tests and effect sizes provides complete
information. A result that is statistically significant with a tiny
effect size warrants different interpretation than one that is
significant with a large effect size.

\subsection{Confidence Intervals on
Metrics}\label{confidence-intervals-on-metrics}

Point estimates of AUROC or correlation should be accompanied by
confidence intervals. \textbf{DeLong's method} provides analytical
confidence intervals for AUROC; \textbf{bootstrap methods} provide
distribution-free intervals for any metric. Reporting ``AUROC = 0.85
(95\% CI: 0.82-0.88)'' is more informative than ``AUROC = 0.85'' alone.

\subsection{Variance Across Random
Seeds}\label{variance-across-random-seeds}

Deep learning models are sensitive to initialization and optimization
stochasticity. Training the same architecture with different random
seeds can produce substantially different results. Best practice trains
multiple runs and reports means and standard deviations. If the standard
deviation across runs exceeds the difference between methods, claimed
improvements may not be reproducible.

\section{Evaluating Foundation
Models}\label{evaluating-foundation-models}

Genomic foundation models (Chapter~\ref{sec-fm-principles}) admit
multiple evaluation paradigms, each testing different aspects of learned
representations.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-fm-evaluation-paradigms}{[}Enhancing{]} Three-column
comparison. Column 1 (Zero-shot): Frozen model â†’ direct prediction;
tests alignment; ESM-1v log-likelihood example; pros/cons. Column 2
(Linear Probing): Frozen â†’ embeddings â†’ linear classifier; tests linear
accessibility; isolates representation quality; pros/cons. Column 3
(Fine-tuning): Gradients â†’ adaptation; tests total potential; best
performance but conflates representation with adaptation. Bottom: Data
efficiency curve; ``Pretraining value = gap at low data.''}

\end{figure}%

\subsection{Zero-Shot Evaluation}\label{zero-shot-evaluation}

In zero-shot evaluation, the pretrained model is applied without any
task-specific training. For masked language models, this typically means
using token probabilities to score variants or classify sequences. A
variant that disrupts a position the model predicts with high confidence
may indicate functional importance.

Zero-shot performance tests whether pretraining captures task-relevant
structure without explicit supervision. Strong zero-shot performance
suggests the pretraining objective aligned with the evaluation task;
weak zero-shot performance suggests misalignment. Comparing zero-shot
performance to simple baselines (e.g., conservation scores for variant
effects) calibrates whether the foundation model provides value beyond
what simpler approaches achieve.

\subsection{Linear Probing}\label{linear-probing-1}

Linear probing freezes the foundation model and trains only a linear
classifier on extracted embeddings. This isolates representation quality
from fine-tuning capacity. If a linear probe on foundation model
embeddings substantially outperforms a linear probe on random
embeddings, the foundation model has learned useful features.

Layer-wise probing reveals where information is encoded. Early layers
may capture local sequence features while later layers capture more
abstract patterns. If the information needed for a task is extractable
from early layers, the model may not require the full depth of the
architecture for that application.

\subsection{Fine-Tuning Evaluation}\label{fine-tuning-evaluation}

Full fine-tuning adapts all model parameters to the downstream task.
This provides the best performance but conflates representation quality
with adaptation capacity. A foundation model might achieve high
fine-tuned performance through the capacity of its architecture rather
than the quality of its pretrained representations.

Comparing fine-tuned foundation models to equivalently architected
models trained from scratch isolates the value of pretraining. If both
approaches converge to similar performance given sufficient downstream
data, pretraining provides label efficiency (less data needed to reach a
given performance level) rather than improved final performance. Data
efficiency curves, plotting performance against downstream training set
size, reveal this trade-off.

\subsection{Transfer Across Tasks}\label{transfer-across-tasks}

Foundation models justify their ``foundation'' designation by
transferring to diverse downstream tasks. Evaluating on a single task,
however well-designed, cannot assess breadth of transfer. Multi-task
evaluation across regulatory prediction, variant effects, protein
properties, and other applications reveals whether foundation models
provide general-purpose representations or excel only on tasks similar
to their pretraining objective.

Transfer across species, tissues, and experimental modalities provides
additional evidence of generalization. A DNA language model that
transfers from human to mouse, or from blood cells to neurons,
demonstrates that its representations capture biological principles
rather than species-specific or tissue-specific patterns.

\section{Calibration Essentials}\label{calibration-essentials}

Even models with strong discrimination may produce probability estimates
that mislead decisions. Calibration assesses whether predicted
probabilities match observed frequencies, a property essential for
rational decision-making.

\subsection{Assessing Calibration}\label{assessing-calibration-1}

\textbf{Reliability diagrams} bin predictions by predicted probability
and plot the mean predicted probability against the observed frequency
within each bin. Perfect calibration produces points along the diagonal;
deviations reveal systematic over-confidence (points below the diagonal)
or under-confidence (points above).

\textbf{Expected calibration error} summarizes miscalibration as a
single number: the weighted average absolute difference between
predicted and observed probabilities across bins. Lower ECE indicates
better calibration. However, ECE is sensitive to binning choices and
should be reported alongside reliability diagrams for interpretability.

\textbf{Calibration by subgroup} reveals whether miscalibration varies
across populations or conditions. A model might be well-calibrated
overall but systematically overconfident for rare variant classes or
underrepresented ancestries. Stratified calibration analysis identifies
these disparities.

\subsection{Recalibration Methods}\label{recalibration-methods}

Post-hoc recalibration adjusts predicted probabilities to improve
calibration without retraining the model. \textbf{Temperature scaling}
divides logits by a learned temperature parameter before applying
softmax, compressing or expanding the probability distribution.
\textbf{Platt scaling} fits a logistic regression from model outputs to
true labels. \textbf{Isotonic regression} fits a monotonic function that
maps model outputs to calibrated probabilities.

These methods require held-out calibration data distinct from training
and test sets. Calibrating on test data and then evaluating calibration
on the same test data produces overoptimistic estimates.

For detailed treatment of calibration and uncertainty quantification,
including epistemic versus aleatoric uncertainty and selective
prediction, see Chapter~\ref{sec-uncertainty}.

\section{Putting It All Together}\label{putting-it-all-together}

When designing or evaluating a genomic model assessment, working through
a systematic checklist helps identify gaps and potential problems.

\textbf{Level of decision}: Is the model intended for molecular
prediction, variant prioritization, patient risk stratification, or
clinical action? Metrics should align with the actual decision context.
Enrichment metrics suit variant ranking; net benefit matters for
clinical decisions.

\textbf{Data splits}: Are individuals, genomic regions, gene families,
and ancestries appropriately separated? Has homology-aware clustering
been applied with appropriate identity thresholds? Is there any
plausible pathway for leakage or circularity?

\textbf{Baselines}: Are comparisons made against the best available
alternatives, not just historical or deliberately weak baselines? Do
non-deep-learning baselines establish floors? Does the improvement over
baselines justify the complexity?

\textbf{Metrics}: Are multiple metrics reported to capture
discrimination, calibration, and ranking quality? Are metrics computed
with confidence intervals? Are subgroup-stratified metrics reported for
fairness assessment?

\textbf{Ablations}: Have component contributions been isolated through
systematic ablation? Is performance robust across hyperparameter ranges
and random seeds?

\textbf{Statistical rigor}: Are significance tests applied with multiple
testing correction? Are effect sizes reported alongside p-values? Are
confidence intervals provided for key metrics?

\textbf{Foundation model specifics}: For foundation models, is
performance reported across zero-shot, probing, and fine-tuning regimes?
Do data efficiency curves reveal where pretraining value lies? Has
transfer been tested across diverse tasks?

\textbf{Robustness}: How does performance vary across cohorts,
platforms, and ancestries? How does the model behave under distribution
shift, missing data, or label noise?

This checklist is not exhaustive but covers the most common evaluation
pitfalls. Working through it systematically at the design stage can
prevent problems that are difficult to fix retrospectively. Reviewers
and readers can use the same checklist to critically assess published
work.

\section{The Question Behind the
Metric}\label{the-question-behind-the-metric}

The question is never simply ``what is the AUROC?'' but rather ``what
has been demonstrated, and how much should we trust it?'' A reported
metric summarizes one aspect of model behavior on one dataset under one
evaluation protocol. Whether that metric predicts performance in
deployment depends on details that standard reporting obscures: how data
were split, whether leakage occurred, which subgroups were evaluated,
what baselines were compared, and whether statistical conclusions
account for multiple comparisons and estimation uncertainty.

The shortcuts that accelerate research in other machine learning domains
produce misleading conclusions when applied to genomic data. Random
train-test splits ignore homology that creates pseudo-replication.
Single-metric comparisons miss failure modes in clinically relevant
subgroups. Significance tests without effect sizes conflate statistical
and practical importance. Benchmark evaluation without temporal
awareness allows indirect leakage through shared community resources.
Homology, population structure, batch effects, and label circularity
create countless opportunities for self-deception, and genomic data
exhibit all of these in abundance.

Rigorous evaluation requires sustained effort at every stage, from
experimental design through statistical analysis. The chapters that
follow address complementary aspects of this challenge. Confounding and
leakage (Chapter~\ref{sec-confounding}) examines how population
stratification, batch effects, and ascertainment bias produce results
that evaporate under deployment. Uncertainty quantification
(Chapter~\ref{sec-uncertainty}) extends calibration assessment to
epistemic versus aleatoric uncertainty and selective prediction.
Interpretability (Chapter~\ref{sec-interpretability}) addresses whether
models have learned genuine biology or exploited confounded patterns.
For clinical applications specifically, risk prediction frameworks
(Chapter~\ref{sec-clinical-risk}) develop evaluation approaches tailored
to decision-making, where net benefit and decision curves supplement
discrimination metrics. Together, these chapters provide the critical
apparatus for engaging with genomic foundation model claims.

\chapter{Confounders and Leakage}\label{sec-confounding}

A variant effect predictor trained on ClinVar achieves 0.92 AUC on
held-out variants from the same database, yet performance drops to 0.71
when evaluated on a prospectively collected clinical cohort. A polygenic
risk score for coronary artery disease stratifies European-ancestry
individuals with impressive discrimination, then fails almost completely
when applied to individuals of African ancestry. A gene expression model
trained on GTEx data predicts tissue-specific patterns with apparent
precision, until deployment reveals it learned to distinguish sequencing
centers rather than biological states. Each model worked brilliantly in
evaluation and failed quietly in practice.

These failures share a common cause: the models learned shortcuts rather
than biology. Genomic datasets encode hidden structure from ancestry and
family relatedness to sequencing center, capture kit, and label curation
protocol. These factors correlate with both features and labels. When
such confounders remain uncontrolled, models exploit them. The central
challenge is that confounded models can appear to work, sometimes
spectacularly well, until they encounter data where the shortcuts no
longer apply.

This problem is not unique to deep learning. Linear regression and
logistic models suffer from the same biases when fit on confounded data.
What makes confounding particularly dangerous in the foundation model
era is scale: larger datasets and more expressive architectures make it
easier to discover subtle shortcuts that remain invisible in standard
diagnostics but cause dramatic failures when distributions shift at
deployment. A shallow model might miss the correlation between
sequencing center and disease status; a transformer with hundreds of
millions of parameters will find it if that correlation helps optimize
the training objective. This chapter examines the major sources of
confounding in genomic datasets, methods for detecting and controlling
confounders, and strategies for building models robust to the
distribution shifts they will inevitably encounter.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-confounding-dag}{[}Essential{]} DAG with genomic
example. Main DAG: Confounder (Ancestry) with arrows to Exposure
(Genomic features) and Outcome (Disease status); spurious path X â† C â†’ Y
highlighted. Annotation boxes explaining how ancestry affects both.
Second DAG showing adjustment blocking spurious path. Concrete example:
Rare disease clinic â†’ ClinVar submissions â†’ ancestry proxy learned. Key
insight: shortcuts appear to work until deployment shifts.}

\end{figure}%

\section{Confounding, Bias, and
Leakage}\label{confounding-bias-and-leakage}

The terminology of confounding, bias, and leakage describes distinct
phenomena that often co-occur and reinforce each other. Precision in
language helps clarify what has gone wrong when a model fails.

A \textbf{confounder} is a variable that influences both the input
features and the label. Ancestry provides a canonical example: it
affects allele frequencies across the genome (the features) and disease
risk through environmental, socioeconomic, and healthcare pathways (the
labels). If ancestry is not explicitly modeled or controlled, a model
trained to predict disease may learn to identify ancestry rather than
disease biology. The prediction appears accurate because ancestry
correlates with outcome, but the model has captured correlation rather
than mechanism.

\textbf{Bias} refers to systematic deviation from the quantity we intend
to estimate or predict. Bias can result from confounding, but also
arises from measurement error, label definitions, sampling procedures,
or deployment differences. A case-control study with 50\% disease
prevalence will train models that systematically over-predict risk when
deployed in populations where true prevalence is 5\%. The model may be
perfectly calibrated for the training distribution yet dangerously
miscalibrated for clinical use.

\textbf{Leakage} occurs when information about the test set
inadvertently influences model training or selection. Leakage pathways
include overlapping individuals or variants between training and
evaluation, shared family members across splits, duplicated samples
under different identifiers, and indirect channels such as pretraining
on resources that later serve as benchmarks. The circularity between
computational predictors and ClinVar annotations discussed in
Chapter~\ref{sec-vep-classical} exemplifies this last category:
CADD-like scores influence which variants receive pathogenic
annotations, and those annotations then become training labels for the
next generation of predictors.

\textbf{Distribution shift} describes mismatch between training and
deployment data distributions. Shift can be driven by changes in
ancestry composition, sequencing technology, clinical coding practices,
or temporal trends in care. A model that learns hospital-specific coding
patterns will fail when deployed at a different institution, not because
the biology differs but because the label generation process does.

These phenomena interact. Confounders create biases in estimated
effects. Leakage hides those biases by making held-out performance
appear better than warranted. Distribution shift then reveals the
problem when deployment performance collapses. For foundation models,
three features magnify these risks. First, genomes encode ancestry,
relatedness, and assay conditions in thousands of subtle features, even
when those labels are never explicitly provided. Second, large
transformers find shortcuts that smaller models would miss if those
shortcuts improve the training objective. Third, complex training
regimes involving pretraining on biobank-scale data, fine-tuning on
curated labels, and evaluation on community benchmarks create many
opportunities for direct and indirect leakage.

\section{Sources of Confounding in Genomic
Data}\label{sources-of-confounding-in-genomic-data}

Confounders in genomic modeling cluster into several categories, though
the same underlying variable (such as recruitment site) may
simultaneously induce ancestry differences, batch effects, and label
bias.

\textbf{Population structure and relatedness} encompasses continental
and sub-continental ancestry, family relationships (siblings,
parent-offspring pairs, cryptic relatedness detectable only through
genotype similarity), and founder effects that create local haplotype
structure. Ancestry affects both features and many phenotypes of
interest, creating classic confounding. Relatedness creates a more
subtle problem: when close relatives appear in both training and test
sets, models can memorize shared haplotype segments rather than learning
generalizable patterns, producing inflated performance estimates that
collapse for unrelated individuals.

\textbf{Technical batch and platform effects} arise throughout the
sequencing and analysis pipeline. Different instruments produce distinct
error profiles. Library preparation protocols vary in GC bias, coverage
uniformity, and adapter content. Capture kits determine which genomic
regions receive adequate coverage. Alignment algorithms and variant
callers make different decisions at ambiguous positions. When samples
from a particular batch disproportionately represent a specific label
class (cases sequenced at one center, controls at another), models learn
to distinguish batches rather than biology.

\textbf{Cohort and institutional effects} reflect differences in patient
populations, clinical practices, and data collection procedures.
Hospital systems use distinct coding practices, diagnostic thresholds,
and follow-up schedules. Population-based biobanks differ from
referral-center cohorts in disease severity, comorbidity patterns, and
demographic composition. Individuals who receive genomic testing may be
more severely affected, more affluent, or preferentially drawn from
particular ancestry groups, introducing selection bias that distorts
apparent variant-phenotype relationships.

\textbf{Label and curation bias} stems from how ground truth annotations
are generated. Clinical labels derived from billing codes or problem
lists reflect documentation practices as much as underlying disease.
Variant pathogenicity databases exhibit the systematic biases detailed
in Chapter~\ref{sec-data}: ClinVar annotations over-represent European
ancestry, well-studied genes, and variants submitted by high-volume
clinical laboratories (Landrum et al. 2018). Expression, regulatory, or
splicing labels derived from specific tissues or cell lines may not
generalize to other biological contexts. The circularity problem
identified in Chapter~\ref{sec-vep-classical} persists into the
foundation model era: when model predictions influence which variants
receive expert review, and expert classifications become training
labels, feedback loops amplify historical biases.

\textbf{Temporal drift} encompasses changes in clinical practice,
diagnostic criteria, and coding conventions over time, evolving
sequencing technologies and quality control pipelines, and shifts in the
patient population served by a healthcare system. A model trained on
2015 data may fail on 2024 data not because biology changed but because
documentation practices, coding standards, and available treatments all
evolved.

\textbf{Knowledge-base and benchmark leakage} occurs when resources like
gnomAD or UK Biobank appear in both model training and evaluation. A
foundation model pretrained on gnomAD allele frequencies, then evaluated
on a benchmark that uses gnomAD for population filtering, faces indirect
leakage even if specific variants do not overlap. Community benchmarks
that reuse widely available variant sets across multiple publications
create additional leakage pathways that accumulate over time as the
field iterates.

\section{Population Structure as a
Shortcut}\label{population-structure-as-a-shortcut}

Population structure represents one of the most pervasive confounders in
genomic modeling. The core issue is that ancestry simultaneously affects
genomic features and many phenotypes through pathways that have nothing
to do with direct genetic causation.

Human genetic variation is structured by ancestry: allele frequencies,
haplotype blocks, and linkage disequilibrium patterns differ across
populations in ways that reflect demographic history. Principal
components computed from genome-wide genotypes provide a low-dimensional
summary of this structure and have become standard in GWAS to correct
for stratification (Patterson, Price, and Reich 2006; Price et al.
2006). Yet ancestry is not merely a statistical nuisance. It is
intertwined with geography, environment, socioeconomic status, and
access to healthcare, factors that directly impact disease risk,
likelihood of receiving genetic testing, and the quality of phenotyping
when testing occurs.

The statistical genetics community developed these corrections precisely
because early genome-wide association studies produced spurious signals
driven by ancestry differences between cases and controls rather than
causal variant effects (see Chapter~\ref{sec-gwas} for detailed
treatment of population stratification in association testing).
Foundation models face the same fundamental problem in a different
guise: ancestry structure that confounded linear regression in GWAS now
confounds neural network predictions, and the solutions require similar
conceptual foundations even when the technical implementations differ.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-population-structure-shortcut}{[}Essential{]}
Multi-panel figure. Panel A (PCA of genomic data): PC1 vs PC2; colored
by ancestry; clear clustering. Panel B (Ancestry in k-mer frequencies):
Heatmap across populations; even local composition differs. Panel C (The
shortcut pathway): Flow diagram (Ancestry â†’ Sequencing â†’ Labels;
Ancestry â†’ Allele frequencies â†’ Features; model learns via ancestry).
Panel D (Cross-population performance): Bar chart showing 40-75\% PGS
reduction; ``Shortcuts fail when relationship changes.''}

\end{figure}%

Consider a rare disease clinic serving primarily individuals of European
ancestry. This clinic contributes most pathogenic variant submissions to
ClinVar, while variants observed predominantly in other ancestries
remain classified as variants of uncertain significance (Landrum et al.
2018). A model trained on ClinVar may learn that European-enriched
variants tend to have pathogenic labels and non-European-enriched
variants tend to have uncertain or benign labels, not because of any
biological difference in pathogenicity but because of differential
clinical characterization. The model appears to predict pathogenicity
while actually predicting ancestry-correlated ascertainment.

Foundation models trained on nucleotide sequences see ancestry
information directly: the distribution of k-mers and haplotypes differs
by population. When such models are fine-tuned to predict disease risk
or variant effects, they may leverage ancestry as a shortcut. Increasing
model capacity does not solve this problem; it often makes it worse by
enabling detection of increasingly subtle ancestry-linked features. The
polygenic score portability literature provides stark evidence: risk
scores derived from European ancestry cohorts show 40-75\% reductions in
prediction accuracy when applied to African ancestry individuals (Duncan
et al. 2019). Similar patterns emerge for variant effect predictors and
regulatory models, though they are often less thoroughly documented due
to limited cross-ancestry evaluation.

\textbf{This mismatch between the populations used for model development
and the populations that would benefit from genomic medicine creates a
fundamental tension between current practice and equitable healthcare.}
Models that work primarily for European ancestry individuals perpetuate
existing health disparities, regardless of their benchmark performance.

\section{Technical Artifacts as Biological
Signal}\label{technical-artifacts-as-biological-signal}

Technical pipelines are complex, and each step from sample collection
through final variant calls can introduce systematic differences that
models may learn.

Sequencing centers differ in instruments, reagents, and quality control
thresholds. Library preparation protocols produce distinct coverage
profiles and GC bias patterns. Capture kits determine which genomic
regions are well-covered and which have systematic dropout. Read length
affects the ability to span repetitive regions and call structural
variants. Alignment and variant calling algorithms make different
decisions at ambiguous genomic positions.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-batch-effects}{[}High{]} Three-panel figure. Panel A
(Batch structure in embeddings): UMAP colored by sequencing center;
samples cluster by batch not phenotype. Panel B (Coverage patterns by
batch): Genome browser tracks; different centers show systematic
differences. Panel C (Batch predicts phenotype): Contingency table
(batch Ã— case/control); imbalanced distribution. Warning: ``Model
predicting disease may actually predict sequencing center.''}

\end{figure}%

When samples from a particular batch or platform are disproportionately
drawn from a specific phenotype class, models learn to distinguish
batches. In high-dimensional feature spaces, even subtle batch-specific
artifacts (coverage dips at particular loci, variant density patterns
reflecting caller behavior, residual adapter sequences) can become
predictive. Foundation models that process raw reads, coverage tracks,
or variant streams are particularly vulnerable because batch signatures
may be encoded in features that preprocessing would typically remove.

Common patterns suggesting batch confounding include embedding spaces
where samples cluster by sequencing center rather than phenotype, strong
predictive performance that collapses when evaluated on data from a new
platform, and models that can accurately predict batch identity
(sequencing center, capture kit, processing date) from inputs that
should be batch-independent. When a model designed to predict disease
can also predict which laboratory processed the sample, something has
gone wrong.

\section{Label Bias and Circularity}\label{label-bias-and-circularity}

Labels in genomic applications rarely represent ground truth in any
absolute sense. They represent the outputs of complex processes
involving clinical documentation, expert review, computational
prediction, and database curation. These processes introduce biases that
models absorb and may amplify.

Clinical phenotypes derived from electronic health records inherit the
limitations of medical documentation. Billing codes capture what was
reimbursable, not necessarily what was present. Problem lists reflect
what clinicians chose to document, which varies by specialty,
institution, and individual practice patterns. Diagnostic criteria
change over time, creating apparent temporal trends in disease
prevalence that reflect evolving definitions rather than changing
biology.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-label-circularity}{[}High{]} Circular flow diagram.
Steps: (1) Clinical lab submits to ClinVar using CADD/REVEL as evidence;
(2) ClinVar aggregates (computational evidence influences labels); (3)
New model trains on ClinVar (learns to replicate patterns); (4) New
model used by labs (influences next submissions); (5) Return to step 1.
Annotations: circularity, apparent validation reflects agreement not
insight. Side panel: Breaking cycle (prospective, temporal, independent
functional).}

\end{figure}%

Variant pathogenicity labels illustrate the problem of circularity.
ClinVar aggregates submissions from clinical laboratories, research
groups, and expert panels (Landrum et al. 2018). The evidence underlying
these submissions often includes computational predictions: a laboratory
may cite CADD, REVEL, or other predictors as supporting evidence for a
pathogenic classification. When the next generation of predictors trains
on ClinVar, it learns to replicate the computational predictions that
contributed to those labels. Performance on ClinVar-derived benchmarks
thus reflects, in part, agreement with previous predictors rather than
independent biological insight.

This circularity extends across the ecosystem of genomic resources.
gnomAD allele frequencies inform variant filtering in clinical
pipelines. UK Biobank genotype-phenotype associations shape which
variants receive functional follow-up. Structural annotations from
ENCODE and Roadmap Epigenomics influence which regulatory regions are
considered biologically important. Foundation models pretrained on these
resources, then evaluated against benchmarks derived from the same
resources, may achieve impressive scores while learning to reproduce the
assumptions and biases of existing annotations rather than discovering
new biology.

\section{Data Splitting}\label{data-splitting}

Data splitting is among the primary tools for assessing generalization,
yet naive splits can silently permit leakage that inflates apparent
performance.

Random individual-level splits assign samples randomly to training,
validation, and test sets. This approach fails when samples are not
independent: family members may appear on both sides of a split,
allowing models to memorize shared haplotypes. Rare variant analysis is
particularly vulnerable because disease-causing variants may be private
to specific families, and memorizing which families have which variants
is far easier than learning generalizable sequence-function
relationships.

Family-aware splits address relatedness by ensuring that all members of
a family appear in the same split. This prevents direct memorization of
family-specific variants but does not address population structure
(ancestry groups may remain imbalanced across splits) or other
confounders.

Locus-level splits hold out entire genomic positions, ensuring that no
variant at a test position appears during training. This stringent
approach prevents models from memorizing site-specific patterns and is
essential for variant effect prediction where the goal is to score novel
variants at positions the model has never seen. Many published
benchmarks fail to implement locus-level splitting, allowing models to
achieve high scores by recognizing familiar positions rather than
learning generalizable effects.

Region or chromosome splits hold out entire genomic regions, testing
whether models learn biology that transfers across the genome rather
than region-specific patterns. This is particularly relevant for
regulatory prediction, where local chromatin context may differ between
regions.

Cohort or site splits hold out entire institutions, sequencing centers,
or biobanks, directly testing robustness to the batch and cohort effects
discussed above. Models that perform well only within their training
cohort but fail on held-out cohorts have learned institution-specific
patterns.

Time-based splits use temporal ordering, training on earlier data and
evaluating on later data. This approach simulates prospective deployment
and tests robustness to temporal drift. A model trained on 2018 data and
evaluated on 2023 data faces realistic distribution shift that random
splits would obscure.

Beyond explicit split design, indirect leakage remains a concern. A
variant that appears in ClinVar may also appear in gnomAD (with
population frequency information), in functional assay datasets (with
splicing or expression effects), and in literature-derived databases
(with disease associations). Pretraining on any of these resources while
evaluating on another creates indirect information flow that standard
deduplication would miss.

\section{Data Leakage}\label{data-leakage}

Data leakage takes three principal forms in genomic modeling, each
creating distinct pathways for inflated performance.

\textbf{Label leakage} occurs when information about the target variable
inadvertently enters the feature set. A variant effect predictor that
includes conservation scores computed using alignments that incorporated
known pathogenic variants has access to label-correlated information
that would not exist for truly novel variants. Similarly, expression
models trained on features derived from the same samples used to define
expression labels face circular information flow. The ClinVar
circularity problem represents a particularly insidious form: when
computational predictions contributed to the pathogenicity
classifications that later become training labels, the new model learns
to replicate its predecessors rather than discover independent signal.

\textbf{Feature leakage} arises when features encode information about
data partitions or batch structure rather than biology. Coverage
patterns that differ systematically between cases and controls, quality
metrics that correlate with sequencing center, or variant density
profiles that reflect caller-specific behavior all constitute feature
leakage. Unlike label leakage, feature leakage may persist even when
labels are rigorously separated, because the features themselves carry
information about which samples belong together.

\textbf{Temporal leakage} violates the causal structure of prediction by
using future information to predict past events. Training on variants
classified in 2023 to predict classifications that were uncertain in
2020 allows models to learn from reclassification patterns rather than
intrinsic variant properties. Clinical outcome prediction faces similar
risks when laboratory values, imaging results, or clinical notes
recorded after the prediction timepoint enter the feature set. Proper
temporal splits must respect not only when samples were collected but
when each feature became available. These leakage types interact and
compound. A model suffering from all three may achieve extraordinary
benchmark performance while learning nothing transferable to prospective
clinical use.

\section{Detecting Confounding}\label{detecting-confounding}

Confounding is often subtle, requiring systematic diagnostics rather
than reliance on aggregate performance metrics.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-confounding-detection}{[}High{]} Diagnostic
checklist with visualizations. Diagnostic 1 (Confounder-only baseline):
Bar chart comparing full model vs ancestry PCs only vs batch only; if
simple baseline approaches full â†’ confounding. Diagnostic 2 (Subgroup
stratification): Multiple reliability diagrams by ancestry. Diagnostic 3
(Prediction-confounder association): Scatter of predictions vs PC1;
residual after controlling for label. Diagnostic 4 (Split sensitivity):
Table showing performance across split strategies; large drop =
memorization. Diagnostic 5 (Negative controls): Accuracy on outcomes
unrelated to genetics; should be chance.}

\end{figure}%

\textbf{Confounder-only baselines} provide the most direct test. Train
simple models (logistic regression, gradient boosting) using only
potential confounders: ancestry principal components, batch indicators,
sequencing center identifiers, recruitment site. If these baselines
approach the performance of complex genomic models, confounding likely
drives a substantial portion of the signal. Reporting confounder-only
baselines alongside genomic model results makes hidden shortcuts
visible.

\textbf{Subgroup stratification} reveals whether aggregate performance
masks heterogeneity. Report metrics stratified by ancestry group,
sequencing platform, institution, and time period. Include both
discrimination (AUROC, AUPRC) and calibration diagnostics for each
subgroup. Models may achieve high overall AUC while being poorly
calibrated or nearly useless for specific subpopulations. Performance
that varies dramatically across subgroups suggests confounding or
distribution shift even when overall metrics appear strong.

\textbf{Prediction-confounder association} tests whether model outputs
encode confounders beyond what the label requires. Plot predictions
against ancestry principal components, adjusting for true label status.
Compare mean predicted risk across batches or time periods within the
same true label class. Conduct formal association tests (regression,
mutual information) between predictions and confounders. Strong residual
associations indicate the model has learned confounder-related features
that go beyond predicting the label itself.

\textbf{Split sensitivity analysis} varies the splitting strategy to
probe for leakage. Re-evaluate performance under locus-level splits,
cohort holdouts, or temporal splits. Large drops in performance under
stricter splitting indicate that initial results were inflated. A model
that achieves 0.90 AUC with random splits but only 0.75 AUC with
locus-level splits has likely memorized site-specific patterns.

\textbf{Negative controls} use outcomes known to be unrelated to
genomics. If a model trained to predict disease from genotypes can also
predict administrative outcomes (insurance type, documentation
completeness) with similar accuracy, it has learned confounders.
Shuffling labels within batch or ancestry strata should eliminate
predictive signal; if it does not, the model exploits structure that
transcends any specific outcome.

\section{Mitigation Strategies}\label{mitigation-strategies}

No mitigation strategy eliminates confounding entirely, and each
involves trade-offs between bias, variance, and coverage. Nonetheless,
several practical approaches substantially reduce reliance on
confounders.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-mitigation-strategies}{[}Enhancing{]} Strategy
comparison table. Strategies: Study design (match cases/controls, before
collection, reduces sample), Covariate adjustment (include
ancestry/batch as inputs, during training, may remove real signal),
Domain adaptation (train invariant to confounders, complex), Robust
optimization (minimize worst-group error, requires group labels),
Benchmark design (locus-level splits, during evaluation, harder scores
by design). Annotation: ``Approaches complementary---use multiple.''}

\end{figure}%

\textbf{Study design and cohort construction} provide the most robust
protection. Matching cases and controls on age, sex, ancestry, and
recruitment site removes these variables as potential confounders.
Balanced sampling (down-sampling majority groups or up-sampling minority
groups within mini-batches) prevents models from optimizing primarily
for the majority pattern. Prospective collection with diversity targets
ensures that training data represent the populations where models will
be deployed. These design-based approaches constrain confounding before
modeling begins, avoiding the need for post-hoc correction.

\textbf{Covariate adjustment} explicitly models confounders rather than
ignoring them. Including ancestry principal components, batch
indicators, and site variables as covariates in regression or
classification models allows estimation of outcome effects that are
adjusted for these factors. Residualizing features or phenotypes with
respect to known confounders before training genomic models removes
confounder-associated variance, though this risks removing genuine
biological signal when confounders correlate with causal variants. Mixed
models or hierarchical structures model institution or batch as random
effects, allowing estimation of genomic effects while accounting for
clustering.

\textbf{Domain adaptation and invariance learning} aim to learn
representations that do not encode confounders. Adversarial training
adds a discriminator that attempts to predict batch or ancestry from
learned representations; the feature extractor is trained to maximize
prediction accuracy while minimizing the discriminator's ability to
recover confounder labels, promoting invariance. Domain adversarial
networks and importance weighting align distributions across batches or
cohorts. Group-robust optimization targets worst-group performance
(minimizing maximum error across subgroups) rather than average
performance, encouraging models that work for all groups rather than
only the majority.

\textbf{Data curation and benchmark design} determine what signals are
available to learn. Deduplicating individuals, families, and variants
across training and evaluation prevents direct memorization. Locus-level
or region-level splits prevent site-specific pattern learning.
Benchmarks that explicitly include diverse ancestries, institutions, and
platforms test generalization rather than fitting to a single
distribution. Documentation of overlaps between training resources and
benchmarks enables readers to assess potential leakage.

These approaches complement each other. Good design reduces the need for
modeling corrections. Adjustment handles residual confounding that
design did not eliminate. Invariance learning provides additional
protection when explicit confounder measurement is incomplete. Rigorous
benchmark construction ensures that evaluation reflects generalization
rather than shortcut learning.

\subsection{Causal Inference
Approaches}\label{causal-inference-approaches}

When observational confounding cannot be eliminated through design or
statistical adjustment, causal inference frameworks offer principled
alternatives that leverage the structure of genetic inheritance itself.

\textbf{Mendelian randomization} exploits the random assortment of
alleles at meiosis to create natural experiments
(\textbf{davey\_smith\_mendelian\_2003?}). Because genotypes are
assigned before birth and cannot be influenced by most environmental
confounders, genetic variants that affect an exposure (such as a
biomarker level or gene expression) can serve as instrumental variables
for estimating causal effects on downstream outcomes. A foundation model
trained to predict expression levels can be evaluated for causal
relevance by testing whether its predictions, instrumented through
genetic variants, associate with disease outcomes in ways that survive
Mendelian randomization assumptions. This approach has revealed that
many observational biomarker-disease associations reflect confounding
rather than causation, and similar logic applies to model-derived
predictions.

\textbf{Directed acyclic graphs (DAGs)} formalize assumptions about
causal structure and clarify which variables should be adjusted, which
should be left unadjusted, and which adjustments would introduce bias
rather than remove it (\textbf{pearl\_causality\_2009?}). Conditioning
on a collider (a variable caused by both exposure and outcome) induces
spurious associations; conditioning on a mediator blocks causal pathways
of interest. Explicit DAG construction forces researchers to articulate
their causal assumptions, making hidden confounding visible and enabling
principled variable selection. For genomic models, DAGs clarify the
relationships among ancestry, technical factors, biological mechanisms,
and phenotypic outcomes, revealing which adjustment strategies address
confounding versus which inadvertently condition on consequences of the
outcome.

\textbf{Negative control outcomes and exposures} provide empirical tests
of residual confounding without requiring complete causal knowledge
(\textbf{lipsitch\_negative\_2010?}). A negative control outcome is one
that should not be causally affected by the exposure of interest; if the
model predicts it anyway, confounding is present. A negative control
exposure is one that should not causally affect the outcome; association
with the outcome again indicates confounding. For a variant effect
predictor, administrative outcomes (insurance status, documentation
completeness) serve as negative control outcomes that genotypes should
not predict. Synonymous variants in non-conserved regions can serve as
negative control exposures that should not affect protein function.
Strong predictions for negative controls reveal that the model has
learned confounders rather than biology.

These causal approaches do not replace careful study design and rigorous
splitting, but they provide additional tools for distinguishing genuine
biological signal from confounded associations, particularly when the
same observational data must serve both training and evaluation
purposes.

\section{Fairness and External
Validity}\label{fairness-and-external-validity}

Confounding connects directly to fairness and health equity. Models that
achieve high average performance while failing for specific populations
may appear successful while exacerbating existing disparities.

Polygenic risk scores illustrate this tension. European ancestry-derived
scores predict cardiovascular disease, diabetes, and breast cancer risk
reasonably well within European ancestry populations. Applied to African
or Asian ancestry individuals, the same scores show substantially worse
discrimination and calibration (Duncan et al. 2019). Healthcare systems
that deploy these scores without ancestry-specific validation risk
providing inferior risk stratification to already underserved
populations.

Variant interpretation exhibits similar patterns. ClinVar contains many
more pathogenic variant classifications for European ancestry
individuals than for other populations (Landrum et al. 2018). Predictors
trained on ClinVar inherit this imbalance, performing better for
variants common in European populations and worse for variants enriched
in other ancestries. Clinical deployment of such predictors may reduce
diagnostic yield for non-European patients.

The uncertainty quantification approaches discussed in
Chapter~\ref{sec-uncertainty} provide partial mitigation: models that
report high uncertainty for under-represented populations at least flag
predictions that should not be trusted. The interpretability methods in
Chapter~\ref{sec-interpretability} can reveal when models rely on
ancestry-correlated features. Yet technical solutions alone are
insufficient. Addressing fairness requires intentional data collection
that prioritizes under-represented populations, evaluation protocols
that mandate subgroup analysis, and deployment decisions that consider
equity alongside aggregate accuracy.

External validity asks whether a model's performance in one setting
predicts its performance in another. Confounding and distribution shift
often cause dramatic external validity failures. A model that achieves
excellent metrics in the development cohort may fail when deployed at a
different institution, in a different healthcare system, or in a
different country. The clinical risk prediction frameworks in
Chapter~\ref{sec-clinical-risk} emphasize multi-site validation
precisely because single-site performance frequently fails to
generalize.

The fairness implications of confounding extend beyond technical model
performance into questions of justice in healthcare resource allocation,
diagnostic equity, and the distribution of benefits from genomic
medicine. Chapter Chapter~\ref{sec-confounding} examines these issues
systematically, connecting the technical confounding problems discussed
here to frameworks for algorithmic fairness, regulatory requirements for
equitable AI deployment, and practical strategies for ensuring that
genomic foundation models reduce rather than amplify existing health
disparities.

\section{A Practical Checklist}\label{a-practical-checklist}

The following checklist synthesizes the diagnostics and mitigations
discussed above. Systematic application during model development and
evaluation surfaces confounding that would otherwise remain hidden.

\textbf{Population structure and relatedness}: Quantify ancestry via
principal components and relatedness via kinship coefficients. Decide
explicitly whether to match, stratify, or adjust for these factors, and
document the justification. Report performance stratified by ancestry
group. When family structure exists in the data, verify that relatives
do not appear across train-test boundaries.

\textbf{Data splits and leakage}: Ensure individuals, families, and
genomic loci do not cross the train-validation-test boundaries for
target tasks. Implement stricter splits (locus-level, chromosome-level,
cohort-based, time-based) and report the performance differences. Check
for overlap with external databases or benchmarks used in evaluation and
document any shared resources.

\textbf{Batch, platform, and cohort effects}: Catalog technical
variables (sequencing center, instrument, protocol, assay) and cohort
identifiers. Test whether these variables predict labels or align with
subgroups of interest. Use embedding visualizations, principal
components, or simple classifiers to detect batch signatures. Apply
mitigation (design matching, covariate adjustment, domain adaptation)
when batch effects are detected.

\textbf{Label quality and curation bias}: Document how labels were
defined and what processes (billing codes, expert review, computational
prediction, registry inclusion) produced them. Quantify label noise
where possible. Consider robust training strategies when labels are
noisy. Assess how curated resources like ClinVar reflect historical
biases and whether those biases affect evaluation validity.

\textbf{Cross-group performance and fairness}: Report metrics for each
major subgroup (ancestry, sex, age, cohort, platform) rather than only
aggregate performance. Examine calibration across groups, not just
discrimination. Discuss clinical implications of residual performance
gaps and whether deployment might worsen existing disparities.

\textbf{Reproducibility and transparency}: Document dataset
construction, inclusion criteria, and splitting strategies completely.
Release preprocessing, training, and evaluation code when feasible.
Describe which confounders were measured, how they were handled, and
what limitations remain.

Models that pass this checklist provide more reliable evidence of
genuine biological learning. Models that fail at multiple points may
achieve benchmark success while learning shortcuts that will not
transfer to new settings.

\section{Rigor as Response}\label{rigor-as-response}

The confounding and bias problems examined in this chapter are not
reasons for despair. They are reasons for rigor. The same expressive
capacity that enables foundation models to discover subtle shortcuts
also enables them to learn complex biological patterns when training
data and evaluation protocols are designed appropriately. The goal is
not to abandon powerful models but to create conditions under which
their power serves biological discovery rather than benchmark gaming.

Several trends support progress. Multi-ancestry biobanks and
international collaborations expand the diversity of available training
data. Benchmark developers implement stricter splitting protocols and
require subgroup analyses. Pretraining strategies that explicitly
promote invariance to technical factors are emerging. Uncertainty
quantification methods (Chapter~\ref{sec-uncertainty}) provide
mechanisms for models to express appropriate caution when inputs fall
outside their training distribution. The problem of confounding is
tractable with sustained attention to data provenance, evaluation
design, and deployment monitoring.

Yet vigilance remains essential. New datasets bring new confounders.
Novel architectures create new opportunities for shortcut learning.
Community benchmarks accumulate indirect leakage as resources are reused
across studies. Treating confounding as a first-order concern throughout
model development, rather than an afterthought addressed only when
reviewers complain, distinguishes models that actually work from models
that merely perform well on convenient benchmarks. The interpretability
methods in Chapter~\ref{sec-interpretability} provide tools for
distinguishing genuine regulatory insight from sophisticated pattern
matching, while the uncertainty quantification approaches in
Chapter~\ref{sec-uncertainty} enable models to communicate when their
predictions should not be trusted. Together with rigorous evaluation,
these capabilities move the field toward models that reveal genuine
biology and behave reliably across the diverse clinical and scientific
settings where they will be deployed.

\chapter{Uncertainty Quantification}\label{sec-uncertainty}

A pathogenicity score of 0.73 means nothing unless we know what 0.73
means. If the model is well-calibrated, approximately 73\% of variants
receiving this score are truly pathogenic, and a clinician can weigh
this probability against the costs of further testing. If the model is
miscalibrated, the true pathogenicity rate among variants scored at 0.73
could be 40\% or 95\%, and the nominal probability provides no reliable
basis for decision-making. The distinction is not between accurate and
inaccurate models but between models that know what they know and models
that do not. A miscalibrated model with high average accuracy can be
more dangerous than a calibrated model with lower accuracy, because the
miscalibrated model provides false confidence that leads to
systematically wrong decisions.

Foundation models produce continuous scores, but clinical decisions
require categorical actions: test or do not test, treat or do not treat,
report to the family or continue monitoring. This translation from
probability to action only works when probabilities are trustworthy. A
model that systematically overstates confidence will trigger unnecessary
interventions. A model that understates confidence will miss actionable
findings. A model that reports high confidence on inputs it has never
seen before fails at a fundamental level regardless of its average
performance on familiar data. Uncertainty quantification provides the
tools to assess when model predictions deserve trust.

This chapter develops the concepts and methods for quantifying,
calibrating, and communicating prediction uncertainty. We distinguish
aleatoric uncertainty (irreducible randomness in the data) from
epistemic uncertainty (reducible uncertainty from limited training data
or model capacity). We examine calibration methods that align predicted
probabilities with empirical frequencies, including temperature scaling,
isotonic regression, and Bayesian approaches. We address
out-of-distribution detection, identifying when inputs differ
sufficiently from training data that predictions should not be trusted.
Throughout, we connect to the clinical workflows examined in
Chapter~\ref{sec-clinical-risk} and Chapter~\ref{sec-rare-disease},
where uncertainty estimates determine which predictions are actionable
and which require human review.

\section{Types of Uncertainty in Genomic
Prediction}\label{sec-uncertainty-types}

\subsection{Why Uncertainty Matters}\label{why-uncertainty-matters}

Clinical genetics operates under fundamental uncertainty. When a
laboratory reports a variant of uncertain significance (VUS), they
acknowledge that current evidence cannot confidently classify the
variant as pathogenic or benign. ClinVar contains approximately two
million VUS compared to roughly 250,000 variants classified as
pathogenic (\textbf{landrum\_clinvar\_2024?}), reflecting the reality
that most genetic variation remains incompletely understood. Foundation
models inherit and sometimes amplify this uncertainty: they may produce
confident-seeming scores for variants where the underlying biology
remains genuinely unknown.

The consequences of ignoring uncertainty extend beyond statistical
abstraction. An overconfident pathogenic prediction may trigger
unnecessary interventions, from prophylactic surgeries to reproductive
decisions that alter family planning. An overconfident benign prediction
may provide false reassurance, delaying diagnosis while a treatable
condition progresses. In both cases, the harm stems not from prediction
error per se but from the mismatch between stated confidence and actual
reliability. A model that accurately conveys its uncertainty enables
appropriate clinical reasoning even when the prediction itself is
imperfect.

Decision theory formalizes this intuition. The expected value of a
clinical action depends on the probability of each outcome weighted by
its utility. When a model reports 0.73 probability of pathogenicity,
downstream decision-making implicitly assumes this probability is
accurate. If the true probability is 0.50, actions optimized for 0.73
will systematically err. Uncertainty quantification ensures that the
probabilities entering clinical decisions reflect genuine knowledge
rather than artifacts of model architecture or training procedure.

\subsection{Epistemic Uncertainty}\label{epistemic-uncertainty}

A model trained exclusively on European-ancestry data encounters its
first genome from an individual of African ancestry. The model's
predictions may be statistically valid within the distribution it has
seen, yet unreliable for this new input due to limited exposure to
ancestry-specific patterns of variation, linkage disequilibrium, and
regulatory architecture. This uncertainty about what the model has
learned, as distinct from noise inherent in the prediction task itself,
constitutes epistemic uncertainty.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-uncertainty-types}{[}Essential{]} Two-panel
conceptual diagram. Panel A (Epistemic, reducible): Novel protein fold
not in training; embedding space with dense clusters and isolated test
point; could reduce with more data; ensemble members disagree; examples
(under-represented ancestry, rare gene, novel pathogen). Panel B
(Aleatoric, irreducible): Incomplete penetrance in BRCA1; same variant,
different outcomes; inherent randomness; examples (penetrance variation,
DMS noise, context-dependent regulation). Bottom: Decomposition formula;
practical implication (high epistemic â†’ more data; high aleatoric â†’
accept limits).}

\end{figure}%

Epistemic uncertainty arises from limitations in training data that
could, in principle, be reduced by gathering more examples. In genomic
foundation models, epistemic uncertainty concentrates in predictable
regions of biological space. Proteins from poorly characterized
families, where training data contained few homologs, exhibit high
epistemic uncertainty because the model has limited basis for inference.
Genes with few characterized variants in ClinVar or gnomAD provide
sparse supervision, leaving the model uncertain about which sequence
features distinguish pathogenic from benign variation. Rare variant
classes, such as in-frame deletions in specific protein domains, appear
infrequently in training data and consequently generate uncertain
predictions. Populations under-represented in biobanks contribute fewer
training examples, creating systematic epistemic uncertainty for
individuals from these backgrounds.

Mathematically, epistemic uncertainty reflects uncertainty over model
parameters or learned representations. A Bayesian perspective treats the
trained model as one sample from a posterior distribution over possible
models consistent with the training data. Different plausible models may
disagree on predictions for inputs far from training examples while
agreeing on well-represented inputs. This disagreement manifests as high
variance in predictions across model variants, sensitivity to random
initialization, or instability under small perturbations to training
data.

Foundation models exhibit epistemic uncertainty through several
observable signatures. Embeddings for unfamiliar sequences cluster in
sparse regions of representation space, distant from the dense clusters
formed by well-represented sequence families. Ensemble members trained
with different random seeds produce divergent predictions for novel
inputs while converging for familiar ones. Fine-tuning on the same
downstream task with different random seeds yields inconsistent results
for edge cases. These signatures provide practical diagnostics for
identifying when epistemic uncertainty is high.

\subsection{Aleatoric Uncertainty}\label{aleatoric-uncertainty}

Some variants are genuinely ambiguous regardless of how much data we
collect. The same pathogenic variant in \emph{BRCA1} causes breast
cancer in one carrier but not another due to modifier genes, hormonal
exposures, or stochastic developmental processes. Incomplete penetrance,
the phenomenon where disease-associated variants do not always produce
disease, creates irreducible uncertainty that no amount of training data
can eliminate. This inherent randomness in the mapping from genotype to
phenotype constitutes aleatoric uncertainty.

Aleatoric uncertainty reflects noise or stochasticity intrinsic to the
prediction problem rather than limitations of the model. Variable
expressivity means that even when a variant causes disease, the severity
and specific manifestations vary across individuals. Measurement noise
in functional assays introduces uncertainty into the labels used for
training: deep mutational scanning experiments typically exhibit 10 to
20 percent technical variation between replicates
(\textbf{fowler\_deep\_2014?}; \textbf{rubin\_statistical\_2017?}),
creating a floor below which prediction error cannot decrease regardless
of model sophistication. Stochastic gene expression means that two
genetically identical cells may express a gene at different levels due
to random fluctuations in transcription and translation. These sources
of randomness set fundamental limits on predictive accuracy.

Aleatoric uncertainty often varies with the input, a property termed
heteroscedasticity. Coding variants in essential genes may have
relatively low aleatoric uncertainty because strong selection pressure
produces consistent phenotypic effects. Regulatory variants exhibit
higher aleatoric uncertainty because their effects depend on cellular
context, developmental timing, and interactions with other genetic and
environmental factors. A model that captures this heteroscedasticity can
provide more informative uncertainty estimates by conveying that some
predictions are inherently more reliable than others.

\subsection{Decomposing Total
Uncertainty}\label{decomposing-total-uncertainty}

Total predictive uncertainty combines epistemic and aleatoric
components, and distinguishing between them has practical implications
for decision-making. High epistemic uncertainty suggests that gathering
more data, either through additional training examples or further
investigation of the specific case, could reduce uncertainty and improve
the prediction. High aleatoric uncertainty indicates that the prediction
is as good as it can get given inherent noise in the problem; additional
data will not help because the underlying biology is stochastic.

The law of total variance provides a mathematical framework for
decomposition. Total variance in predictions equals the sum of variance
due to model uncertainty (epistemic) and variance inherent in the
data-generating process (aleatoric). In practice, ensemble methods
approximate epistemic uncertainty through disagreement between members:
if five independently trained models produce predictions of 0.65, 0.68,
0.70, 0.72, and 0.75, the spread reflects epistemic uncertainty, while
the residual variance within each model's predictions reflects aleatoric
uncertainty. Heteroscedastic neural networks, which output both a
predicted mean and a predicted variance, can estimate aleatoric
uncertainty by learning input-dependent noise levels.

These decompositions depend on modeling assumptions and provide
approximations rather than exact separations. Ensemble disagreement may
underestimate epistemic uncertainty if all members share similar biases
from common training data. Heteroscedastic models may confound aleatoric
and epistemic uncertainty if the training data is too sparse to reliably
estimate noise levels. Despite these limitations, approximate
decomposition provides actionable information: variants flagged for high
epistemic uncertainty warrant additional data collection or expert
review, while variants with high aleatoric uncertainty may require
acceptance of irreducible limits on predictive confidence.

\section{Calibration: Do Confidence Scores Mean What They
Say?}\label{sec-calibration}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-calibration-diagrams}{[}Essential{]} Four-panel
figure. Panel A (Perfect calibration): Reliability diagram on diagonal;
``Predictions match reality.'' Panel B (Overconfident): Points below
diagonal; model predicts 0.9, true frequency 0.6; common for DNNs;
``Dangerous for clinical use.'' Panel C (Underconfident): Points above
diagonal; ``May miss actionable variants.'' Panel D (Miscalibration by
subgroup): Two curves (European, African); one calibrated, one poor;
``Aggregate can mask disparities.'' Annotations: ECE, prediction
histograms, ``Calibration â‰  discrimination.''}

\end{figure}%

\subsection{The Calibration Problem}\label{the-calibration-problem}

AlphaMissense outputs a continuous score between 0 and 1 for each
possible missense variant in the human proteome. When it reports 0.85
for a particular variant, what does this number mean? If the model is
calibrated, collecting all variants scored near 0.85 and checking their
true clinical status should reveal that approximately 85\% are
pathogenic. Perfect calibration means that predicted probabilities match
observed frequencies across the entire range of model outputs: among
variants scored at 0.30, roughly 30\% should be pathogenic; among
variants scored at 0.95, roughly 95\% should be pathogenic. This
alignment between stated confidence and empirical accuracy is
calibration, and most foundation models fail to achieve it.

Formally, a model \(f\) mapping inputs \(X\) to probability estimates
\(p = f(X)\) is calibrated if \(P(Y=1 \mid f(X)=p) = p\) for all \(p\)
in the interval from 0 to 1. The calibration condition requires that the
model's stated confidence equals the true probability of the positive
class conditional on that stated confidence. Miscalibration occurs when
this equality fails: overconfident models produce predicted
probabilities that exceed true frequencies (a variant scored at 0.85 is
pathogenic only 60\% of the time), while underconfident models produce
predicted probabilities below true frequencies.

Modern deep neural networks are systematically miscalibrated despite
achieving high accuracy. Guo and colleagues demonstrated that
contemporary architectures exhibit worse calibration than older, less
accurate models (\textbf{guo\_calibration\_2017?}). The phenomenon
arises because standard training objectives like cross-entropy loss
optimize for discrimination (separating positive from negative examples)
rather than calibration (matching predicted probabilities to
frequencies). Over-parameterized models with capacity exceeding what the
data requires can achieve near-perfect training loss while producing
overconfident predictions on held-out data. The softmax temperature in
transformer architectures affects the sharpness of probability
distributions, and default settings often produce excessively peaked
outputs.

Calibration and discrimination are distinct properties. A model can
achieve perfect AUROC, correctly ranking all positive examples above all
negative examples, while being arbitrarily miscalibrated. If a
classifier assigns probability 0.99 to all positive examples and 0.98 to
all negative examples, it ranks perfectly but provides useless
probability estimates. Conversely, a calibrated model that assigns 0.51
to positives and 0.49 to negatives would be calibrated but nearly
useless for discrimination. Clinical applications typically require
both: accurate ranking to identify high-risk variants and accurate
probabilities to inform decision-making.

\subsection{Measuring Calibration}\label{measuring-calibration}

The evaluation metrics introduced in Chapter~\ref{sec-evaluation} assess
discrimination; here we focus on metrics that assess calibration.
\textbf{Reliability diagrams} provide visual assessment of calibration
by plotting predicted probabilities against observed frequencies.
Construction involves binning predictions into intervals (commonly ten
bins spanning 0 to 0.1, 0.1 to 0.2, and so forth), computing the mean
predicted probability within each bin, computing the fraction of
positive examples within each bin, and plotting these two quantities
against each other. A perfectly calibrated model produces points along
the diagonal where predicted probability equals observed frequency.
Systematic deviations reveal calibration patterns: points below the
diagonal indicate overconfidence (predictions exceed reality), points
above indicate underconfidence, and S-shaped curves suggest nonlinear
miscalibration requiring more flexible correction.

\textbf{Expected Calibration Error (ECE)} provides a scalar summary of
calibration quality. ECE computes the weighted average absolute
difference between predicted probabilities and observed frequencies
across bins:

\[\text{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{n} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|\]

where \(B_m\) denotes the set of examples in bin \(m\), \(|B_m|\) is the
number of examples in that bin, \(n\) is the total number of examples,
\(\text{acc}(B_m)\) is the accuracy (fraction of positives) in bin
\(m\), and \(\text{conf}(B_m)\) is the mean predicted probability in bin
\(m\). Lower ECE indicates better calibration, with zero representing
perfect calibration. ECE depends on binning strategy; equal-width bins
may place most examples in a few bins for models with concentrated
predictions, while equal-mass bins ensure each bin contains the same
number of examples but may span wide probability ranges.

\textbf{Maximum Calibration Error (MCE)} captures worst-case
miscalibration by reporting the largest absolute gap between predicted
and observed frequencies across all bins. MCE is appropriate when any
severe miscalibration is unacceptable, as in high-stakes clinical
applications where even rare catastrophic errors carry significant
consequences.

\textbf{Brier score} decomposes into components measuring calibration
and discrimination (refinement), providing a single proper scoring rule
that rewards both properties. The Brier score equals the mean squared
difference between predicted probabilities and binary outcomes, and its
decomposition reveals whether poor scores stem from miscalibration, poor
discrimination, or both.

\subsection{Why Foundation Models Are Often
Miscalibrated}\label{why-foundation-models-are-often-miscalibrated}

Foundation models face calibration challenges beyond those affecting
standard neural networks. Pretraining objectives like masked language
modeling optimize for predicting held-out tokens, not for producing
calibrated probability distributions over downstream tasks. The
representations learned during pretraining may encode useful information
about sequence biology while providing no guarantee that fine-tuned
classifiers will be well-calibrated.

Distribution shift between pretraining and evaluation compounds
miscalibration. A protein language model pretrained on UniRef sequences
encounters a fine-tuning task using ClinVar variants. The pretraining
distribution emphasizes common proteins with many homologs, while
clinical variants concentrate in disease-associated genes with different
sequence characteristics. Models may be well-calibrated on held-out
pretraining data while miscalibrated on clinically relevant evaluation
sets.

Label noise in training data propagates to calibration errors. ClinVar
annotations reflect the state of knowledge at submission time and may
contain errors, particularly for older entries or variants from
less-studied genes. Deep mutational scanning experiments provide
functional labels but with measurement noise that varies across assays.
Models trained on noisy labels may learn the noise distribution,
producing predictions that match training labels but not underlying
truth.

Zero-shot approaches present particular calibration challenges. ESM-1v
log-likelihood ratios measure how surprising a mutation is to the
language model, but these ratios are not probabilities and have no
inherent calibration. Converting log-likelihood ratios to pathogenicity
probabilities requires explicit calibration against external labels, and
the resulting calibration depends on the reference dataset used for this
conversion.

\subsection{Calibration Across
Subgroups}\label{calibration-across-subgroups}

Aggregate calibration metrics can mask severe miscalibration in
clinically important subgroups. A model might achieve low ECE overall
while being dramatically overconfident for variants in African-ancestry
individuals and underconfident for European-ancestry individuals, with
opposite errors canceling in aggregate statistics. Subgroup-stratified
calibration assessment is essential for any model intended for diverse
populations.

Ancestry-stratified calibration reveals systematic patterns in current
foundation models. Training data for protein language models and variant
effect predictors derive predominantly from European-ancestry cohorts,
creating differential epistemic uncertainty across populations.
Calibration curves stratified by ancestry often show that models are
better calibrated for populations well-represented in training data and
overconfident or underconfident for under-represented populations. This
differential calibration has direct fairness implications: clinical
decisions based on miscalibrated predictions will be systematically
worse for patients from under-represented backgrounds.

Calibration may also vary by variant class, gene constraint level,
protein family, or disease category. Missense variants in highly
constrained genes may show different calibration patterns than those in
tolerant genes. Variants in well-studied protein families with abundant
training examples may be better calibrated than variants in orphan
proteins. Stratified reliability diagrams across these categories reveal
whether a single calibration correction suffices or whether
subgroup-specific approaches are necessary.

\section{Post-Hoc Calibration Methods}\label{sec-post-hoc-calibration}

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-calibration-methods}{[}High{]} Three-column
comparison with before/after. Column 1 (Temperature scaling): Formula p
= softmax(z/T); single parameter T; before/after reliability; preserves
ranking; can't fix complex patterns. Column 2 (Platt scaling): Formula p
= Ïƒ(az + b); two parameters; handles bias and confidence; assumes
logistic. Column 3 (Isotonic regression): Non-parametric monotonic fit;
flexible, no assumptions; overfits with limited data. Bottom: Decision
guide for method selection.}

\end{figure}%

\subsection{Temperature Scaling}\label{temperature-scaling}

The simplest calibration fix is often the most effective. Temperature
scaling applies a single learned parameter to adjust model confidence,
dramatically improving calibration with minimal computational overhead
and no change to model predictions' ranking.

The method modifies the softmax function by dividing logits by a
temperature parameter \(T\) before applying softmax:

\[\hat{p}_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}\]

where \(z_i\) are the logits (pre-softmax outputs) and \(\hat{p}_i\) are
the calibrated probabilities. When \(T > 1\), the distribution becomes
softer (more uniform), reducing overconfidence. When \(T < 1\), the
distribution becomes sharper, increasing confidence. The optimal
temperature is learned by minimizing negative log-likelihood on a
held-out calibration set, typically yielding \(T\) between 1.5 and 3 for
overconfident deep networks.

Temperature scaling preserves the model's ranking because dividing all
logits by the same constant does not change their relative ordering. A
variant ranked as more likely pathogenic than another remains more
likely after temperature scaling; only the magnitudes of probability
estimates change. This preservation of discrimination while improving
calibration makes temperature scaling particularly attractive:
calibration improves without sacrificing the model's hard-won ability to
distinguish pathogenic from benign variants.

The method's simplicity (one parameter) is both strength and limitation.
A single global temperature cannot fix heterogeneous miscalibration
where the model is overconfident in some regions of input space and
underconfident in others. When reliability diagrams show complex
nonlinear patterns, more flexible calibration methods are necessary.

\subsection{Platt Scaling}\label{platt-scaling}

Platt scaling fits a logistic regression model on the original model's
outputs, learning both a slope and intercept to transform scores into
calibrated probabilities. For binary classification:

\[\hat{p} = \sigma(a \cdot f(x) + b)\]

where \(f(x)\) is the original model's output, \(\sigma\) is the sigmoid
function, and parameters \(a\) and \(b\) are learned on calibration
data. The two parameters provide more flexibility than temperature
scaling's single parameter, allowing correction of both the sharpness
and the location of the probability distribution.

Platt scaling is appropriate when miscalibration involves systematic
bias (predictions consistently too high or too low) in addition to over-
or underconfidence. The method assumes that a monotonic logistic
transformation suffices to correct miscalibration, which may not hold
for models with complex, non-monotonic calibration curves.

\subsection{Isotonic Regression}\label{isotonic-regression}

Isotonic regression provides a non-parametric approach that fits a
monotonically increasing function mapping raw scores to calibrated
probabilities. Unlike temperature or Platt scaling, isotonic regression
makes no assumptions about the functional form of miscalibration,
allowing it to correct arbitrary monotonic patterns.

The method works by pooling adjacent bins whose empirical frequencies
violate monotonicity, then assigning each bin its pooled frequency. The
resulting calibration function is a step function that increases with
the original score. This flexibility comes at a cost: with limited
calibration data, isotonic regression may overfit to noise in the
calibration set, and the step-function output can appear discontinuous.
Additionally, isotonic regression provides no uncertainty estimate on
the calibration itself; we learn a point estimate of the calibration
function without knowing how reliable that estimate is.

\subsection{Calibrating Foundation Model
Outputs}\label{calibrating-foundation-model-outputs}

Genomic foundation models present specific calibration considerations
beyond standard classification settings. The choice of calibration
approach depends on whether the model produces logits, log-likelihood
ratios, or continuous regression outputs, and on whether calibration
targets are available for the deployment distribution.

For zero-shot variant effect scores like ESM-1v log-likelihood ratios,
raw outputs have no inherent probabilistic interpretation. Calibration
requires mapping these continuous scores to pathogenicity probabilities
using external labels, typically from ClinVar or population frequency
data. This mapping should occur on held-out genes or variants not used
for any model development, and the resulting calibration reflects the
specific label set used; calibration against ClinVar pathogenic/benign
labels may not transfer to other clinical contexts.

Multi-output models that predict across many tasks (multiple cell types,
tissues, or assays) may require separate calibration for each output. A
regulatory model predicting expression across 200 cell types is unlikely
to be uniformly calibrated across all outputs; cell types with more
training data may show better calibration than rare cell types.

Temporal stability of calibration deserves consideration. As ClinVar
annotations evolve with new evidence, the ground truth against which
models were calibrated changes. A model calibrated against 2020 ClinVar
labels may become miscalibrated relative to 2025 labels as variant
classifications are updated. Periodic recalibration against current
labels helps maintain clinical relevance.

\section{Uncertainty Quantification Methods for Foundation
Models}\label{sec-uq-methods}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-uq-methods}{[}High{]} Method comparison. Deep
ensembles: 5 models independently trained; variance/entropy across
predictions; 5Ã— training/inference; gold standard for epistemic. MC
Dropout: Single model, multiple stochastic passes; variance across
dropout samples; 1Ã— training, NÃ— inference; approximates ensemble.
Last-layer ensembles: Frozen backbone, ensemble of heads; head
disagreement; 1Ã— pretraining, 5Ã— heads (cheap); fine-tuning uncertainty
only. Heteroscedastic networks: Single model outputs mean + variance;
learned per input; 1Ã— training/inference; aleatoric not epistemic.
Comparison table: Epistemic, Aleatoric, Cost, FM-friendly.}

\end{figure}%

\subsection{Deep Ensembles}\label{deep-ensembles}

If one model expresses uncertainty about a prediction, querying multiple
models reveals whether that uncertainty reflects genuine ambiguity in
the data or an artifact of a particular training run. When five
independently trained models agree on a prediction, confidence is
warranted; when they disagree, the disagreement itself signals
uncertainty. Ensemble disagreement provides one of the most reliable
uncertainty estimates available in deep learning, at the cost of
training and maintaining multiple models.

Deep ensembles train \(M\) models (typically 5 to 10) with different
random initializations, data orderings, or minor architectural
variations. At inference time, all members produce predictions, and
uncertainty is estimated from the variance or entropy of the ensemble
distribution. For classification, epistemic uncertainty appears as
disagreement in predicted class probabilities across members. For
regression, epistemic uncertainty appears as variance in predicted
values.

The theoretical basis for ensemble uncertainty estimation rests on the
observation that disagreement between models reflects regions of input
space where the training data provides insufficient constraint. Where
training examples are dense, gradient descent from different
initializations converges to similar solutions, producing agreement.
Where training examples are sparse or conflicting, different
initializations find different local optima, producing disagreement.
This interpretation connects ensembles to Bayesian model averaging,
where predictions are averaged over the posterior distribution of model
parameters.

For foundation models with billions of parameters, training full
ensembles becomes prohibitively expensive. Training five copies of ESM-2
requires approximately five times the compute of a single model,
potentially millions of dollars in cloud computing costs. Several
practical alternatives reduce this burden. Last-layer ensembles freeze
the pretrained backbone and train only an ensemble of prediction heads,
reducing cost by orders of magnitude while still capturing uncertainty
from the fine-tuning process. Snapshot ensembles save model checkpoints
at various points during optimization and use these snapshots as
ensemble members, requiring only single-model training time. Multi-seed
fine-tuning trains the same architecture from multiple random seeds on
the fine-tuning task, which is far cheaper than multi-seed pretraining.

\subsection{Monte Carlo Dropout}\label{monte-carlo-dropout}

Monte Carlo (MC) dropout provides uncertainty estimates from a single
trained model by treating dropout regularization as approximate Bayesian
inference. During standard training with dropout, random subsets of
neurons are zeroed at each forward pass. MC dropout keeps dropout active
at test time and performs multiple stochastic forward passes, treating
the variation across passes as a measure of model uncertainty.

Gal and Ghahramani showed that this procedure approximates variational
inference over the model's weights (\textbf{gal\_dropout\_2016?}). Each
forward pass with dropout samples a different subnetwork, and the
distribution of predictions across samples approximates the predictive
distribution under a particular prior over weights. High variance across
MC samples indicates epistemic uncertainty about the model's parameters
for that input.

MC dropout offers the significant advantage of requiring only a single
trained model, avoiding the computational overhead of ensembles.
Implementation is straightforward: enable dropout during inference and
average predictions over 10 to 50 stochastic forward passes. The
variance or entropy of these predictions serves as the uncertainty
estimate.

Limitations temper the method's appeal. Modern transformer architectures
often do not use dropout in their standard configurations, or use
dropout only in specific locations (attention dropout, residual dropout)
where the approximation may be less accurate. The quality of uncertainty
estimates depends on the dropout rate and architecture, with higher
dropout rates providing better uncertainty estimates but potentially
degrading mean predictions. Empirical comparisons often find that MC
dropout underestimates uncertainty relative to deep ensembles,
particularly in low-data regimes where epistemic uncertainty should be
high.

\subsection{Heteroscedastic Models}\label{heteroscedastic-models}

Standard regression models predict a single output value, implicitly
assuming constant noise variance across all inputs. Heteroscedastic
models instead predict both a mean and a variance for each input,
capturing the intuition that prediction uncertainty varies depending on
the input. For genomic applications, this approach naturally handles the
observation that some prediction tasks are inherently noisier than
others: coding variant effects may be more predictable than regulatory
variant effects, constrained genes more predictable than tolerant genes.

Architecture modifications are minimal. Instead of outputting a single
value, the model outputs two values interpreted as the mean \(\mu(x)\)
and variance \(\sigma^2(x)\) of a Gaussian distribution over outputs.
Training uses negative log-likelihood loss under this Gaussian, which
penalizes both prediction errors and miscalibrated variance estimates:

\[\mathcal{L} = \frac{1}{2\sigma^2(x)}(y - \mu(x))^2 + \frac{1}{2}\log \sigma^2(x)\]

The first term penalizes prediction errors, weighted by inverse variance
so that high-variance predictions are penalized less for the same
absolute error. The second term prevents the model from simply
predicting infinite variance to avoid all penalties. The result is a
model that learns to predict larger variance for inputs where training
labels are noisy or inconsistent, capturing aleatoric uncertainty in an
input-dependent manner.

\subsection{Evidential Deep Learning}\label{evidential-deep-learning}

Evidential deep learning places a prior distribution over the class
probabilities themselves rather than directly predicting probabilities.
For classification, the model outputs parameters of a Dirichlet
distribution, which serves as a prior over the simplex of class
probabilities. The concentration parameters of this Dirichlet encode
both the predicted class probabilities (via their relative magnitudes)
and the model's uncertainty (via their absolute magnitudes).

Low total concentration indicates high uncertainty: the model is unsure
which class is correct. High total concentration with one dominant class
indicates confident prediction. This framework provides a principled way
to separate epistemic uncertainty (low concentration) from confident
predictions (high concentration), all from a single forward pass without
ensembling or MC sampling.

Critics have noted that evidential deep learning can produce unreliable
uncertainty estimates when the distributional assumptions are violated
or when training data is limited. Practical experience suggests that
ensembles and MC dropout often provide more robust uncertainty
estimates, though evidential methods continue to be refined.

\section{Conformal Prediction: Distribution-Free
Guarantees}\label{sec-conformal}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-conformal-prediction}{[}High{]} Workflow with
examples. Steps: (1) Calibration: Score variants on held-out set; (2)
Threshold: Find threshold for 90\% coverage; (3) Prediction sets:
Include classes above threshold. Example outputs: High confidence
\{Pathogenic\}, moderate \{Pathogenic, VUS\}, low \{P, VUS, B\},
abstention \{\}. Key properties: Coverage guarantee (â‰¥90\% marginal);
set size conveys uncertainty; no probability interpretation needed.
Visualization: Score distributions; threshold line; set sizes.
Limitation: Marginal not conditional coverage.}

\end{figure}%

\subsection{The Conformal Prediction
Framework}\label{the-conformal-prediction-framework}

Most uncertainty quantification methods make assumptions about model
behavior or data distributions that may not hold in practice.
Temperature scaling assumes miscalibration follows a particular
functional form. Ensembles assume that disagreement reflects epistemic
uncertainty rather than artifacts of training. Bayesian methods assume
specific priors over model parameters. When these assumptions fail,
uncertainty estimates may be unreliable precisely when reliability
matters most.

Conformal prediction offers something stronger: finite-sample coverage
guarantees that hold under minimal assumptions. Instead of outputting a
point prediction, conformal methods produce a prediction set guaranteed
to contain the true label with probability at least \(1 - \alpha\),
where \(\alpha\) is a user-specified error rate. If we request 90\%
coverage (\(\alpha = 0.10\)), the prediction set will contain the true
label at least 90\% of the time, regardless of the model's accuracy or
calibration. This guarantee requires only that calibration and test
examples are exchangeable (a condition weaker than independent and
identically distributed), making conformal prediction robust to model
misspecification.

The coverage guarantee is finite-sample: it holds exactly for any sample
size, not just asymptotically. For clinical genomics applications where
individual predictions carry significant consequences, this
finite-sample property provides assurance that cannot be obtained from
asymptotic calibration arguments.

\subsection{Split Conformal
Prediction}\label{split-conformal-prediction}

The most practical conformal method, split conformal prediction,
proceeds in five steps. First, split available labeled data into a
proper training set and a calibration set. Second, train the model on
the training set only. Third, compute non-conformity scores on the
calibration set, where higher scores indicate poorer fit between the
model's prediction and the true label. Fourth, find the threshold \(q\)
at the \((1-\alpha)(1+1/n)\) quantile of calibration scores, where \(n\)
is the calibration set size. Fifth, at test time, include in the
prediction set all labels whose non-conformity score falls below \(q\).

Non-conformity scores measure how ``strange'' a candidate label is given
the model's output. For classification, a common choice is
\(1 - \hat{p}_y\), where \(\hat{p}_y\) is the predicted probability of
the true class. High predicted probability means low non-conformity (the
label conforms to the model's expectations); low predicted probability
means high non-conformity. For regression, absolute residuals
\(|y - \hat{y}|\) serve as non-conformity scores.

The construction ensures coverage because calibration scores are
exchangeable with test scores under the exchangeability assumption. The
quantile threshold is set so that a random calibration score exceeds the
threshold with probability at most \(\alpha\); by exchangeability, the
same holds for test scores. This elegant argument yields exact coverage
guarantees without requiring the model to be accurate or
well-calibrated.

\subsection{Conformal Prediction for Variant Effect
Prediction}\label{conformal-prediction-for-variant-effect-prediction}

Variant effect prediction, examined in detail in
Chapter~\ref{sec-vep-fm}, concentrates the challenges of uncertainty
quantification. Instead of reporting a single pathogenicity score, a
conformalized variant classifier outputs a prediction set from the
possibilities: \{pathogenic\}, \{benign\}, \{pathogenic, benign\}, or
the empty set. The set is guaranteed to contain the true label at the
specified coverage rate.

Adaptive set sizes convey uncertainty naturally. Confident predictions
yield small sets (\{pathogenic\} alone), while uncertain predictions
yield larger sets (\{pathogenic, benign\}). The set size itself
communicates the model's confidence without requiring users to interpret
numerical probabilities. A clinician seeing \{pathogenic, benign\} knows
immediately that the model cannot distinguish between these
possibilities, whereas a score of 0.55 might be misinterpreted as mild
confidence in pathogenicity.

Calibration set construction requires careful thought. Holding out
variants at random may not prevent information leakage if related
variants (same gene, same protein domain) appear in both calibration and
test sets. Holding out entire genes or protein families provides more
stringent evaluation but may reduce calibration set size for rare gene
families. For applications intended to work across populations,
calibration sets should include diverse ancestries to ensure coverage
guarantees hold across patient populations.

Conformal prediction intervals for regression tasks (expression
prediction, quantitative trait prediction) provide bounds rather than
sets. Conformalized quantile regression produces intervals guaranteed to
contain the true value with specified probability, directly applicable
to predicting gene expression changes or polygenic score uncertainty.

\subsection{Limitations and Practical
Considerations}\label{limitations-and-practical-considerations}

Conformal guarantees are marginal rather than conditional. The coverage
guarantee holds on average across all test examples, not for each
individual example. A model might achieve exact 90\% coverage overall
while dramatically undercovering some subgroups and overcovering others.
Subgroup-conditional coverage requires additional assumptions or methods
like stratified conformal prediction.

The exchangeability assumption can fail in practice. If the calibration
set derives from one population and the test set from another, coverage
guarantees may not hold. Temporal shifts (calibration on historical
data, testing on future data) similarly violate exchangeability. Methods
for conformal prediction under distribution shift exist but require
additional assumptions about the nature of the shift.

Prediction set size trades off against informativeness. Larger sets
provide more reliable coverage but less useful predictions. A model that
produces \{pathogenic, benign\} for every variant achieves perfect
coverage but provides no discrimination. Careful model development to
improve underlying accuracy reduces average set size while maintaining
coverage guarantees.

\section{Out-of-Distribution Detection}\label{sec-ood-detection}

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-ood-detection}{[}High{]} Embedding space
visualization. Main panel: UMAP/t-SNE; dense training clusters (blue);
isolated OOD points (red); examples (novel archaeal sequence, synthetic
protein, variant in poorly characterized gene). Detection methods (side
panels): Mahalanobis distance (histogram ID vs OOD, threshold), nearest
neighbor distance, ensemble disagreement. Key insight: Flag for manual
review rather than automate.}

\end{figure}%

\subsection{The Out-of-Distribution
Problem}\label{the-out-of-distribution-problem}

A DNA language model trained on mammalian genomes encounters a novel
archaeal sequence. The model's embedding places this sequence in an
unfamiliar region of representation space, far from the clusters formed
by training examples. Yet the model still produces a prediction,
potentially with high confidence, because standard neural networks are
not designed to recognize when inputs lie outside their training
distribution. Detecting out-of-distribution (OOD) inputs is essential
for safe deployment of foundation models in settings where novel
sequences are inevitable.

OOD detection identifies inputs that differ meaningfully from training
data, allowing systems to flag uncertain predictions before they cause
harm. Novel pathogens may share little sequence similarity with
characterized viruses in training data. Synthetic proteins designed for
therapeutic purposes may occupy regions of sequence space unsampled by
evolution. Variants in poorly characterized genes may lack the
contextual information that models rely on for accurate prediction. In
each case, recognizing that the input is unusual enables appropriate
caution.

The confidence problem compounds OOD challenges. Neural networks often
produce high-confidence predictions on OOD inputs because nothing in
standard training penalizes confidence on unfamiliar examples. A
classifier trained to distinguish pathogenic from benign variants may
confidently predict ``pathogenic'' for a completely random sequence, not
because it has evidence for pathogenicity but because it lacks the
capacity to say ``I don't know.'' This failure mode makes OOD detection
essential rather than optional.

\subsection{Likelihood-Based Detection and Its
Failures}\label{likelihood-based-detection-and-its-failures}

The intuitive approach to OOD detection uses model likelihood: inputs
the model finds improbable should be flagged as OOD. Language models
assign likelihoods to sequences; surely OOD sequences should receive low
likelihood?

This intuition fails for deep generative models. Complex models can
assign high likelihood to OOD data for reasons unrelated to semantic
similarity to training examples. In high-dimensional spaces, typical
sets (regions where most probability mass concentrates) do not coincide
with high-density regions. A sequence might land in a high-density
region of the model's distribution while being semantically distant from
any training example.

Empirically, language models assign high likelihood to repetitive
sequences, sequences with unusual but consistent patterns, and sequences
from different domains that happen to share statistical properties with
training data. For genomic models, this means likelihood alone cannot
reliably distinguish novel biological sequences from sequences within
the training distribution.

\subsection{Embedding-Based Detection}\label{embedding-based-detection}

Learned representations provide more reliable OOD detection than raw
likelihood. The key insight is that embeddings encode semantic
structure: similar sequences cluster together in embedding space, and
OOD sequences land in sparse regions distant from training clusters.

Mahalanobis distance measures how far a test embedding lies from
training data, accounting for the covariance structure of the embedding
space. For each class, compute the mean embedding and covariance matrix
from training examples. For a test input, compute its distance to each
class centroid in units of standard deviations, accounting for
correlations between embedding dimensions. Large Mahalanobis distance
indicates OOD inputs.

Nearest-neighbor methods provide a non-parametric alternative. For a
test embedding, find the \(k\) nearest neighbors among training
embeddings and compute the average distance. Large average distance to
neighbors indicates the test input lies in a sparse region of embedding
space, suggesting it is OOD. This approach makes no distributional
assumptions and scales well with modern approximate nearest-neighbor
algorithms.

For genomic foundation models, embedding-based OOD detection enables
practical deployment safeguards. ESM embeddings place novel protein
folds in regions distant from characterized folds, allowing detection of
sequences outside the model's training experience. DNABERT embeddings
reveal unusual sequence composition or repeat structures that may
confound predictions. Flagging these cases for expert review prevents
confident but unreliable predictions from reaching clinical decisions.

\subsection{Practical OOD Detection for Genomic
Applications}\label{practical-ood-detection-for-genomic-applications}

Defining what counts as OOD requires domain knowledge. Novel species or
clades may share evolutionary history with training examples yet differ
enough to warrant caution. Extreme GC content can indicate
contamination, unusual biology, or simply under-represented genomic
regions. Engineered sequences (designed proteins, synthetic regulatory
elements) intentionally explore regions of sequence space not
represented in natural sequences.

Combining multiple OOD signals improves reliability. Embedding distance,
likelihood, and prediction confidence each capture different aspects of
distributional difference. An input flagged by multiple methods is more
reliably OOD than one flagged by a single method. Threshold selection
involves trade-offs between false positives (flagging in-distribution
examples unnecessarily) and false negatives (missing true OOD examples).

The operational response to OOD detection depends on the application.
For variant interpretation, OOD inputs might trigger automatic flagging
for expert review rather than automated classification. For
high-throughput screening, OOD inputs might receive tentative
predictions with explicit uncertainty warnings. For safety-critical
applications, OOD inputs might trigger rejection with a request for
additional information.

\section{Selective Prediction and
Abstention}\label{sec-selective-prediction}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-selective-prediction}{[}Enhancing{]}
Accuracy-coverage tradeoff. Main plot: Coverage (x) vs Accuracy (y);
curve rising as coverage decreases (more selective = more accurate).
Operating point selection: Clinical applications need different
tradeoffs. Reject option: High uncertainty â†’ abstain. Example: 90\%
coverage with 85\% accuracy vs 70\% coverage with 95\% accuracy.
Connection to clinical workflow: Rejected variants â†’ expert review.}

\end{figure}%

\subsection{When to Abstain}\label{when-to-abstain}

A variant effect predictor achieving 95\% accuracy overall provides more
clinical value if it can identify which predictions are reliable.
Selective prediction allows models to abstain on difficult cases,
concentrating predictions on inputs where confidence is warranted. The
trade-off between coverage (fraction of inputs receiving predictions)
and accuracy (correctness among predictions made) defines the selective
prediction problem.

The coverage-accuracy trade-off reflects a fundamental tension. At 100\%
coverage, the model predicts on all inputs and achieves its baseline
accuracy. As coverage decreases (more abstention), accuracy among
predictions made typically increases because the model abstains on its
most uncertain cases. The shape of this trade-off curve characterizes
the model's ability to identify reliable predictions.

Abstention is appropriate when the cost of errors exceeds the cost of
deferral. In clinical variant interpretation, a confident but incorrect
pathogenic prediction may trigger unnecessary medical intervention,
while abstention merely defers the decision to expert review. If expert
review is available and affordable relative to error costs, abstaining
on uncertain cases improves overall decision quality. Conversely, in
high-throughput screening where expert review is infeasible, abstention
may provide little benefit because all predictions eventually require
automated handling.

\subsection{Selective Prediction
Methods}\label{selective-prediction-methods}

Confidence-based selection abstains when the model's maximum predicted
probability falls below a threshold. For a classifier producing
probabilities over classes, if \(\max_c \hat{p}_c < \tau\), the model
abstains. This simple approach works well when model confidence
correlates with correctness, but fails when models are confidently
wrong.

Ensemble-based selection abstains when ensemble members disagree beyond
a threshold. High disagreement indicates epistemic uncertainty about the
correct prediction, warranting abstention even if individual members
express confidence. This approach captures uncertainty that
confidence-based selection misses when models are overconfident.

Conformal selection abstains when prediction sets exceed a size
threshold. If the conformal prediction set contains more than one class,
the model lacks confidence to make a unique prediction. This approach
connects selective prediction to the coverage guarantees of conformal
methods: the model makes predictions with guaranteed coverage on the
non-abstained cases.

Learned selection trains a separate model to predict whether the primary
model will be correct on each input. This ``rejection model'' learns to
identify failure modes that simple confidence thresholds miss,
potentially achieving better coverage-accuracy trade-offs than heuristic
methods.

\subsection{Evaluating Selective
Prediction}\label{evaluating-selective-prediction}

Risk-coverage curves plot accuracy (or its complement, risk) as a
function of coverage, revealing how performance improves as the model
becomes more selective. The area under the risk-coverage curve
summarizes overall selective prediction quality. Models with better
uncertainty estimates produce steeper curves, achieving high accuracy at
lower coverage.

Selective accuracy at fixed coverage specifies a coverage level (e.g.,
80\%) and reports accuracy among predictions made at that coverage. This
metric directly answers practical questions: ``If we let the model
predict on its 80\% most confident cases, how accurate will it be?''

Comparison across methods requires matched coverage levels. A method
that achieves 99\% accuracy at 50\% coverage and 95\% accuracy at 90\%
coverage may be preferable to a method achieving 97\% accuracy at both
levels, depending on operational requirements. Reporting full
risk-coverage curves enables stakeholders to select operating points
appropriate to their cost structures.

\section{Uncertainty for Specific Genomic Tasks}\label{sec-genomic-uq}

\subsection{Variant Effect Prediction
Uncertainty}\label{variant-effect-prediction-uncertainty}

Variant effect prediction concentrates the challenges of uncertainty
quantification. Epistemic uncertainty arises from poorly characterized
genes, novel protein folds, and under-represented populations in
training data. Aleatoric uncertainty stems from incomplete penetrance,
variable expressivity, and noise in functional assay labels. Both types
of uncertainty must be estimated and communicated for variant
predictions to inform clinical decisions appropriately.

Calibration challenges for VEP include the evolving nature of ground
truth labels. ClinVar annotations change as new evidence emerges;
variants classified as VUS may be reclassified as pathogenic or benign,
and even confident classifications occasionally reverse. A model
calibrated against a historical version of ClinVar may appear
miscalibrated against current annotations, not because the model changed
but because the labels did. Periodic recalibration against current
databases maintains alignment between model outputs and contemporary
clinical understanding.

Population-specific calibration addresses the reality that training data
predominantly derive from European-ancestry cohorts. For patients from
other ancestral backgrounds, both epistemic uncertainty (fewer training
examples) and calibration (different baseline pathogenicity rates,
different patterns of variation) may differ from the aggregate.
Stratified reliability diagrams by ancestry reveal these differences;
ancestry-conditional calibration may be necessary for equitable
performance across populations.

\subsection{Regulatory Variant
Uncertainty}\label{regulatory-variant-uncertainty}

Regulatory variants present distinct uncertainty challenges. Unlike
coding variants where effects can be localized to specific amino acid
changes, regulatory variants act through complex mechanisms involving
transcription factor binding, chromatin accessibility, and
three-dimensional genome organization. This mechanistic complexity
translates to higher aleatoric uncertainty: even perfectly characterized
regulatory variants may have context-dependent effects that vary across
cell types, developmental stages, and genetic backgrounds. A variant
that disrupts a transcription factor binding site may have dramatic
effects in tissues where that factor is active and negligible effects
elsewhere, yet the model must predict across all contexts
simultaneously.

The context-dependence of regulatory effects creates a calibration
challenge distinct from coding variants. A model may be well-calibrated
for predicting expression changes in cell types abundant in training
data (lymphoblastoid cell lines, common cancer lines) while poorly
calibrated for clinically relevant primary tissues rarely profiled at
scale. Stratified calibration assessment across tissue types reveals
these disparities, but the sparsity of ground truth labels for many
tissues limits the precision of tissue-specific calibration estimates.

Expression prediction models like Enformer and Borzoi provide
uncertainty estimates for predicted expression changes through several
approaches. Ensemble methods quantify disagreement across model variants
trained with different random seeds. Heteroscedastic architectures
predict tissue-specific confidence alongside tissue-specific expression,
learning that predictions for well-characterized tissues deserve higher
confidence than those for rarely profiled contexts. These uncertainties
propagate to downstream interpretations: a variant predicted to alter
expression with high uncertainty warrants different treatment than one
with narrow confidence bounds, and the tissue-specificity of uncertainty
may itself be informative about which experimental follow-up would most
reduce ambiguity.

\subsection{Uncertainty Across
Populations}\label{uncertainty-across-populations}

Differential uncertainty across populations has direct implications for
health equity. Models trained predominantly on European-ancestry data
exhibit higher epistemic uncertainty for other populations, manifesting
in several observable ways: larger prediction sets from conformal
methods, higher abstention rates from selective prediction, greater
ensemble disagreement, and less reliable confidence estimates from
calibration. These differences arise from multiple sources. Linkage
disequilibrium patterns differ across populations, meaning that variant
correlations learned from European data may not transfer.
Population-specific variants absent from training data generate pure
epistemic uncertainty. Even shared variants may have different effect
sizes across populations due to gene-environment interactions or
epistatic backgrounds that vary by ancestry.

Quantifying population-specific uncertainty requires appropriate
calibration and evaluation datasets. A model calibrated exclusively on
European-ancestry ClinVar submissions may appear well-calibrated on
aggregate metrics while being systematically miscalibrated for other
populations. The scarcity of diverse calibration data creates a
challenging circularity: we cannot assess population-specific
calibration without diverse labeled datasets, yet diverse labeled
datasets are precisely what current genomic databases lack. Initiatives
like the All of Us Research Program and population-specific biobanks
(Uganda Genome Resource, Taiwan Biobank, BioBank Japan) are beginning to
address this gap, enabling population-stratified uncertainty assessment
that was previously impossible.

Transparent reporting of population-stratified uncertainty metrics
enables informed decisions about model deployment. If a model abstains
on 30\% of variants in one population but only 10\% in another, users
can make informed choices about supplementary analyses for the
higher-abstention population. Clinical laboratories might establish
ancestry-specific thresholds for automated reporting versus expert
review. Research applications might weight predictions by
ancestry-specific confidence when aggregating across diverse cohorts.
Ignoring these differences risks providing lower-quality predictions to
already under-served populations while presenting a false appearance of
uniform reliability, compounding existing disparities in genomic
medicine.

\section{Communicating Uncertainty to End
Users}\label{sec-uncertainty-communication}

\subsection{The Communication
Challenge}\label{the-communication-challenge}

A pathogenicity score of \(0.73 \pm 0.15\) may be statistically accurate
but nearly useless to a clinician deciding whether to order confirmatory
testing. The gap between statistical uncertainty and decision-relevant
communication presents a persistent challenge for genomic AI deployment.
Different users reason differently about probability and risk; effective
communication requires understanding these differences.

Cognitive biases complicate probability interpretation. Humans tend
toward overconfidence in point estimates, treating 0.73 as more certain
than warranted. Prediction intervals are frequently misunderstood: a
90\% confidence interval does not mean the true value has a 90\% chance
of being in that specific interval (a Bayesian interpretation) but
rather that 90\% of such intervals would contain the true value (a
frequentist interpretation). Base rate neglect leads users to interpret
variant-level pathogenicity predictions without accounting for prior
probability based on clinical presentation, family history, and
phenotypic specificity.

Different stakeholders have different needs. Clinicians require
actionable categories that map to clinical decision points, not
continuous scores requiring interpretation. Researchers may prefer full
probability distributions enabling flexible downstream analysis.
Patients and families need understandable risk communication that
supports informed decision-making without inducing inappropriate anxiety
or false reassurance.

\subsection{Categorical Reporting}\label{categorical-reporting}

Clinical genetics has established categorical frameworks for variant
interpretation. The ACMG-AMP guidelines define five categories:
pathogenic, likely pathogenic, variant of uncertain significance, likely
benign, and benign. Mapping continuous model outputs to these categories
requires threshold selection that balances sensitivity and specificity
at clinically meaningful operating points.

Uncertainty within categories can be conveyed through confidence
qualifiers or numerical confidence scores attached to categorical calls.
A ``likely pathogenic'' call with 95\% confidence differs meaningfully
from one with 60\% confidence, even though both receive the same
categorical label. Two-dimensional reporting combining category and
confidence enables more nuanced interpretation without abandoning the
categorical framework that clinicians expect.

Threshold selection involves value judgments beyond pure statistics. The
consequences of false positive and false negative pathogenic calls
differ by clinical context. For a severe, treatable condition, false
negatives carry higher cost, warranting lower thresholds for pathogenic
classification. For untreatable conditions where pathogenic
classification affects reproductive decisions, the calculus differs.
Uncertainty quantification enables informed threshold selection by
revealing the trade-offs at different operating points.

\subsection{Visual Communication}\label{visual-communication}

Probability bars and confidence intervals provide visual representation
of uncertainty, though their interpretation depends on user familiarity
with statistical graphics. Icon arrays, which represent probabilities as
proportions of colored icons in a grid (e.g., 73 red icons and 27 blue
icons out of 100), improve comprehension for users without statistical
training. The visual representation of proportion is more intuitive than
numerical probability for many audiences.

Risk ladders place the prediction in context by showing where it falls
relative to other risks of varying magnitude. A variant with 0.73
probability of pathogenicity can be placed alongside risks from other
genetic conditions, environmental exposures, or common medical
procedures, enabling intuitive comparison.

Interactive visualizations allow users to explore uncertainty in detail,
examining how predictions change under different assumptions or how
uncertainty varies across related variants. These approaches suit
sophisticated users engaged in research or detailed clinical analysis
but may overwhelm users seeking simple answers.

\subsection{Decision-Theoretic
Framing}\label{decision-theoretic-framing}

Rather than communicating probability alone, decision-theoretic framing
presents expected outcomes under different actions. Instead of ``this
variant has 73\% probability of being pathogenic,'' the report might
state ``if we assume this variant is pathogenic and proceed with
surveillance, the expected outcomes are X; if we assume it is benign and
decline surveillance, the expected outcomes are Y.''

This framing integrates uncertainty with action, helping users
understand how uncertainty affects what they should do rather than
treating probability as an end in itself. The approach requires modeling
clinical outcomes, which introduces additional assumptions, but makes
explicit the decision-relevant implications of uncertainty rather than
leaving users to integrate probability with consequences on their own.

\section{Necessary but Insufficient}\label{necessary-but-insufficient}

Uncertainty quantification transforms foundation model outputs from
opaque scores into components of rational decision processes. A
well-calibrated pathogenicity prediction that honestly communicates its
limitations enables appropriate clinical reasoning: high confidence
warrants action, low confidence warrants additional testing or expert
review. An overconfident score that claims false precision causes harm
through both false positives (unnecessary interventions) and false
negatives (missed diagnoses). The methods developed in this chapter,
from temperature scaling through conformal prediction to
out-of-distribution detection, provide the technical foundation for
trustworthy genomic AI.

The path from uncertainty quantification to clinical impact requires
integrating these methods into operational workflows. Selective
prediction enables triage between automated handling and expert review
based on model confidence. Conformal prediction sets provide coverage
guarantees that support risk-aware decision-making. Out-of-distribution
detection prevents confident predictions on inputs that fall outside the
training distribution. Calibration ensures that numerical probabilities
mean what they claim to mean. Together, these tools enable foundation
models to participate in clinical decisions without overstating their
reliability.

Yet uncertainty quantification alone is insufficient. A perfectly
calibrated black box remains a black box. The clinician who receives an
uncertain prediction wants to understand why the model is uncertain: Is
it because the variant falls in a poorly characterized gene? Because the
model has never encountered this protein fold? Because the underlying
biology is genuinely ambiguous? Interpretability, examined in
Chapter~\ref{sec-interpretability}, complements uncertainty by revealing
the basis for predictions and their associated confidence. The
conjunction of calibrated uncertainty and mechanistic understanding
approaches what trustworthy clinical AI requires. Neither alone
suffices; together they provide the foundation for models that
clinicians can reason with rather than merely defer to.

\chapter{Interpretability}\label{sec-interpretability}

An attribution method highlights a GATA motif when explaining why a
model predicts enhancer activity. The explanation is biologically
plausible: GATA transcription factors bind this motif and drive
tissue-specific expression. But plausibility is not faithfulness. The
model may have learned a completely different pattern (perhaps GC
content correlating with enhancer labels in the training data) and the
attribution method may be highlighting the GATA motif because
human-interpretable explanations tend to find human-interpretable
patterns. The explanation matches biological intuition without
accurately reflecting model computation. This distinction between
plausible and faithful interpretation structures the entire field of
model interpretability, and failing to respect it produces explanations
that provide false comfort rather than genuine insight.

The stakes extend beyond scientific curiosity. Variant interpretation
guidelines from the American College of Medical Genetics require that
computational evidence be weighed alongside functional assays,
segregation data, and population frequency. A pathogenicity score alone
satisfies only weak evidence criteria; knowing that a variant disrupts a
specific CTCF binding site in a cardiac enhancer provides interpretable
mechanistic evidence that can be combined with clinical presentation and
family history. When models cannot explain their predictions faithfully,
clinicians cannot integrate computational evidence with biological
reasoning. The same limitation affects research: a model that predicts
enhancer activity cannot generate testable hypotheses about regulatory
grammar unless its internal computations can be translated into
statements about motifs, spacing constraints, and combinatorial logic
that can be experimentally validated.

This chapter examines methods for understanding what genomic foundation
models learn and how they make predictions. Attribution methods identify
important input positions. Motif discovery algorithms translate
attributions into regulatory vocabularies. Probing classifiers diagnose
what representations encode. Mechanistic interpretability traces
computational circuits within transformer architectures. Throughout, the
plausible-versus-faithful distinction guides interpretation. We examine
how to validate interpretability claims experimentally, distinguishing
explanations that accurately reflect model computation from those that
merely satisfy human intuition. Understanding when these diverge
determines whether model explanations accelerate discovery or mislead
researchers pursuing patterns the model never actually learned.

\section{Attribution Methods and Input
Importance}\label{attribution-methods-and-input-importance}

When a model predicts that a 200-kilobase genomic region will show high
chromatin accessibility in hepatocytes, a natural question arises: which
bases within that region drive the prediction? Attribution methods
answer this question by assigning importance scores to input positions,
identifying where the model focuses its computational attention. These
scores can reveal candidate regulatory elements, highlight the sequence
features underlying variant effects, and provide the raw material for
downstream motif discovery.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-attribution-comparison}{[}Essential{]} Attribution
methods on same input sequence. Methods: Gradient Ã— Input (fast, noisy),
Integrated Gradients (principled, slower), DeepLIFT (reference-based),
Attention weights (inspect what model attends), In silico mutagenesis
(exhaustive, expensive). Visualization: Heatmaps on same sequence;
correlation between methods; areas of agreement/disagreement.
Annotations: Compute cost, principled basis, what each reveals.}

\end{figure}%

\subsection{In Silico Mutagenesis}\label{in-silico-mutagenesis}

The most direct approach to measuring input importance is simply to
change each base and observe what happens to the prediction. In silico
mutagenesis (ISM) systematically introduces mutations at every position,
computing the difference between mutant and reference predictions. For a
sequence of length \(L\), ISM creates three mutant sequences at each
position (substituting each non-reference nucleotide), yielding \(3L\)
forward passes through the model. The resulting mutation effect matrix
captures how sensitive the prediction is to changes at each position and
to each alternative base.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-in-silico-mutagenesis}{[}High{]} Four-panel figure.
Panel A (ISM procedure): For each position, systematically mutate to all
alternatives; compute prediction change; produces position Ã— mutation
matrix. Panel B (ISM profile): Heatmap aligned to sequence; functional
regions show large effects; silent regions near zero. Panel C
(Saturation mutagenesis comparison): ISM predictions vs experimental DMS
data; correlation validation. Panel D (Mechanistic insights): ISM
reveals: binding site boundaries, position-specific tolerance,
allele-specific effects. Note: Computational cost \textasciitilde LÃ—4
forward passes.}

\end{figure}%

ISM provides true counterfactual information rather than approximations.
When ISM shows that mutating position 47 from A to G reduces the
predicted accessibility by 0.3 log-fold, that is a direct observation
about model behavior, not an estimate derived from gradients or
attention weights. This directness makes ISM the gold standard for
faithfulness: if ISM identifies a position as important, perturbing that
position genuinely changes the output.

The limitation is computational cost. Scoring all single-nucleotide
substitutions in a 200-kilobase input requires 600,000 forward passes,
which becomes prohibitive for large models or genome-wide analysis.
Practical applications often restrict ISM to targeted windows around
variants of interest, using faster methods to identify candidate regions
for detailed analysis. For variant effect prediction specifically, ISM
reduces to comparing reference and alternative allele predictions,
requiring only two forward passes per variant.

\subsection{Gradient-Based
Attribution}\label{gradient-based-attribution}

Gradient-based methods approximate the counterfactual information from
ISM using backpropagation. The gradient of the output with respect to
each input position measures how much an infinitesimal change at that
position would affect the prediction. With one-hot encoded sequence, the
gradient at each base indicates the sensitivity to substituting that
nucleotide.

The simplest approach, often called saliency mapping, computes raw
gradients and visualizes their magnitudes across the sequence. A common
variant multiplies gradients by inputs (gradient Ã— input), focusing on
positions where the current nucleotide is both important and present.
These methods require only a single backward pass, making them orders of
magnitude faster than ISM.

Gradient-based methods suffer from saturation in regions where the model
is already confident. If a strong motif drives the prediction into a
saturated region of the output nonlinearity, small perturbations produce
near-zero gradients even though the motif is functionally critical.
DeepLIFT addresses this limitation by comparing activations between an
input sequence and a reference, propagating differences through the
network using custom rules that avoid gradient saturation. The resulting
attributions satisfy a completeness property: contributions sum to the
difference between input and reference predictions.

Integrated gradients provide theoretical grounding through the path
integral of gradients along a linear interpolation from reference to
input:

\[\text{IG}_i(x) = (x_i - x'_i) \int_{\alpha=0}^{1} \frac{\partial f(x' + \alpha(x - x'))}{\partial x_i} \, d\alpha\]

This integral, approximated by summing gradients at discrete
interpolation steps, satisfies sensitivity (any input that affects the
output receives nonzero attribution) and implementation invariance
(functionally equivalent networks produce identical attributions).
Integrated gradients have become a standard choice for genomic models,
balancing computational efficiency with theoretical guarantees.

All gradient-based methods require choosing a reference sequence, which
substantially affects the resulting attributions. Common choices include
dinucleotide-shuffled versions of the input (preserving local
composition while disrupting motifs), average non-functional sequence,
or simply zeros. The reference defines what counts as informative:
attributions highlight features that differ from the reference and
contribute to the prediction difference. A shuffled reference emphasizes
motif content; a zero reference treats any sequence information as
potentially important.

\subsection{Reconciling Attribution
Methods}\label{reconciling-attribution-methods}

Different attribution methods can produce strikingly different
importance maps for the same sequence and prediction. A position might
show high importance under ISM but near-zero gradients due to
saturation, or high gradient magnitude but minimal effect when actually
mutated due to redundancy with nearby positions. This disagreement
reflects genuine differences in what each method measures: gradients
capture local sensitivity, ISM captures counterfactual effects, and
DeepLIFT captures contribution relative to a reference.

Practical workflows often combine multiple methods. Gradient-based
approaches efficiently scan long sequences to identify candidate
regions, ISM validates importance in targeted windows, and agreement
across methods increases confidence that identified features genuinely
drive predictions. Disagreement flags positions for closer
investigation, potentially revealing saturation effects, redundancy, or
artifacts in individual methods.

\section{Interpreting Convolutional
Filters}\label{interpreting-convolutional-filters}

Convolutional neural networks remain central to genomic sequence
modeling, as discussed in Chapter~\ref{sec-cnn}, and their first-layer
filters offer a particularly tractable interpretability target. Each
filter slides along the sequence computing dot products with local
windows, and high activation indicates that the local sequence matches
the filter's learned pattern. This architecture creates a natural
correspondence between filters and sequence motifs.

\subsection{From Filters to Position Weight
Matrices}\label{from-filters-to-position-weight-matrices}

Converting learned filters to interpretable motifs follows a standard
workflow. The trained model processes a large sequence set, typically
training data or genome-wide tiles, recording positions where each
filter's activation exceeds a threshold. The fixed-length windows around
high-activation positions are extracted and aligned, and nucleotide
frequencies at each position are computed to build a position weight
matrix (PWM). This PWM can be visualized as a sequence logo and compared
to databases like JASPAR or HOCOMOCO.

When this procedure is applied to models trained on chromatin
accessibility or transcription factor binding, first-layer filters
frequently match known transcription factor motifs. DeepSEA filters
include recognizable matches to CTCF, AP-1, and cell-type-specific
factors. This correspondence validates that models discover biologically
meaningful patterns rather than arbitrary correlations, and it provides
a direct link between model weights and decades of experimental
characterization of transcription factor binding preferences.

Several complications affect filter interpretation. DNA is
double-stranded, and models may learn forward and reverse-complement
versions of the same motif as separate filters. Some filters capture
general sequence composition (GC-rich regions, homopolymer runs) rather
than specific binding sites. These patterns can be biologically
meaningful in contexts like nucleosome positioning or purely artifactual
depending on the training task. Distinguishing informative filters from
compositional shortcuts requires cross-referencing with known biology
and testing whether filter-derived motifs predict binding in held-out
data.

\subsection{Deeper Layers and Combinatorial
Patterns}\label{deeper-layers-and-combinatorial-patterns}

Beyond the first layer, convolutional filters combine lower-level
patterns into complex representations. Deeper layers can encode motif
pairs that co-occur at characteristic spacing, orientation preferences
between binding sites, and contextual dependencies where a motif's
importance varies with surrounding sequence. These combinatorial
patterns capture aspects of regulatory grammar that individual motifs
cannot represent.

Direct interpretation of deeper filters becomes increasingly difficult
as receptive fields expand and nonlinearities accumulate. The activation
of a layer-5 filter depends on intricate combinations of earlier
patterns, resisting simple biological annotation. Indirect approaches
prove more tractable: analyzing which input regions drive high
activation at deeper layers, clustering high-activation sequences to
find common themes, or probing whether deeper representations encode
specific biological properties.

\section{Motif Discovery from
Attributions}\label{motif-discovery-from-attributions}

Attribution maps highlight important positions but do not directly
reveal motifs. A DeepLIFT track might show scattered high-importance
bases throughout a sequence without indicating that those bases
collectively form instances of the same transcription factor binding
site. TF-MoDISco (Transcription Factor Motif Discovery from Importance
Scores) bridges this gap by discovering motifs from attribution scores
rather than raw sequences.

The insight underlying TF-MoDISco is that importance-weighted sequences
focus motif discovery on positions the model actually uses. Traditional
motif finders must contend with the fact that most positions in
regulatory sequences do not participate in functional motifs. By
extracting seqlets (short windows where total importance exceeds a
threshold) and clustering them based on both sequence content and
importance profiles, TF-MoDISco identifies patterns that drive model
predictions.

The workflow proceeds through several stages. Base-level importance
scores are computed for many sequences using DeepLIFT, ISM, or
integrated gradients. Windows where total importance exceeds a threshold
are extracted as seqlets, each representing a candidate motif instance.
These seqlets are compared using metrics that consider both sequence
content and importance profiles, then clustered into groups
corresponding to putative motifs. Within each cluster, seqlets are
aligned and consolidated into PWMs and importance-weighted logos. The
resulting motifs can be matched to known transcription factors or
flagged as novel patterns.

Beyond individual motifs, TF-MoDISco enables grammar inference by
analyzing motif co-occurrence. Mapping discovered motif instances back
to genomic coordinates reveals characteristic spacing between motif
pairs, orientation preferences, and cell-type-specific usage patterns.
These grammatical rules can be validated through in silico experiments:
inserting or removing motifs in synthetic sequences and checking whether
predictions change as expected.

Applications to models like BPNet trained on ChIP-seq data have
recovered known transcription factor motifs, discovered novel sequence
variants, and revealed spacing constraints validated through synthetic
reporter assays. The same workflow applies to foundation model analysis:
use the model to produce base-level attributions for a downstream task,
run TF-MoDISco to extract a task-specific motif vocabulary, and analyze
how motif usage varies across conditions.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-tfmodisco}{[}High{]} Motif discovery pipeline.
Steps: (1) Attribution scores across many sequences; (2) Cluster
high-attribution regions; (3) Align and aggregate into motifs; (4) Match
to known databases (JASPAR). Examples: Discovered motif aligned to known
TF (CTCF, GATA); novel motifs with unknown biology; composite motifs (TF
combinations). Key insight: Bridge between black-box attributions and
interpretable biology.}

\end{figure}%

\section{Probing Learned
Representations}\label{probing-learned-representations}

Attribution methods ask which input positions matter; probing asks what
information the model's internal representations encode. A probing
classifier is a simple supervised model (typically linear) trained to
predict some property of interest from the hidden representations of a
pretrained model. If a linear probe can accurately predict a property,
that property is encoded in an accessible form within the
representation.

\subsection{Probing Methodology}\label{probing-methodology}

The standard probing workflow extracts hidden states from a pretrained
model for a set of inputs where the property of interest is known. These
hidden states, without further transformation, serve as features for
training a simple classifier to predict the property. The classifier's
accuracy indicates how well the representation encodes the probed
property, while its simplicity (linearity, minimal parameters) ensures
that the probe identifies information present in the representation
rather than information the probe itself computes.

For protein language models like ESM-2, probing has revealed that
representations encode secondary structure, solvent accessibility,
contact maps, and even 3D coordinates to a surprising degree, as
discussed in Chapter~\ref{sec-protein-lm}. These properties emerge
despite training on sequence alone, demonstrating that masked language
modeling on evolutionary sequences induces representations that capture
structural information. For DNA language models, probing can assess
whether representations encode chromatin state, gene boundaries,
promoter versus enhancer identity, or species-specific regulatory
signatures.

Probing provides diagnostic information distinct from downstream task
performance. A model might achieve high accuracy on a regulatory
prediction task by learning shortcuts (correlations with GC content,
distance to annotated genes) rather than encoding genuine regulatory
grammar. Probing can detect such shortcuts: if representations strongly
encode GC content but weakly encode transcription factor binding site
presence, the model may be exploiting composition rather than sequence
logic. This diagnostic function complements the confounder analysis
discussed in Chapter~\ref{sec-confounding}.

\subsection{Limitations of Probing}\label{limitations-of-probing}

Probing results require careful interpretation. A probe's failure to
predict some property might indicate that the representation does not
encode it, or might reflect limitations of the probe architecture,
insufficient training data, or mismatch between the probe's capacity and
the complexity of the encoding. Linear probes may miss nonlinearly
encoded information; more complex probes risk learning the property
themselves rather than reading it from the representation.

The selectivity-accessibility tradeoff complicates interpretation. A
representation might encode a property accessibly (recoverable by a
linear probe) or selectively (encoded but requiring nonlinear decoding).
Properties encoded selectively might be present but not easily
extracted, while properties encoded accessibly might be incidentally
correlated with the training objective rather than causally important.
Combining probing with causal interventions (ablating representation
components and measuring effects on downstream predictions) provides
stronger evidence about which encoded properties actually matter.

\section{Attention Patterns in Transformer
Models}\label{attention-patterns-in-transformer-models}

Transformer-based genomic models use self-attention to aggregate
information across long sequence contexts, potentially capturing distal
regulatory interactions invisible to models with narrow receptive
fields. Attention weights indicate which positions each position attends
to, creating natural candidates for interpretability: perhaps high
attention weights identify functionally related sequence elements.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-attention-visualization}{[}High{]} Three-panel
figure. Panel A (Attention heatmap): Position Ã— position; patterns
revealing (promoter-enhancer contacts, local structure). Panel B
(Biological overlay): Attention on genome browser; peaks align with
known regulatory elements. Panel C (Multi-head specialization):
Different heads capturing different patterns; local context head,
long-range head, motif-specific head. Caveats: Attention â‰  causation;
sanity checks needed.}

\end{figure}%

\subsection{What Attention Patterns
Reveal}\label{what-attention-patterns-reveal}

When attention weights are analyzed in genomic language models, certain
heads exhibit strikingly structured patterns. Some heads preferentially
connect positions within the same predicted gene or operon, suggesting
the model has learned gene boundaries from sequence alone. Other heads
show long-range connections that align with known enhancer-promoter
relationships or chromatin loop anchors. Still others cluster positions
by functional annotation, connecting genes with similar Gene Ontology
terms despite lacking explicit functional labels during training.

In models like Enformer that predict regulatory outputs from long
genomic windows, attention can reveal which distal regions influence
predictions at a target gene. Contribution scores aggregated across
attention heads often peak at known enhancers, insulators, and chromatin
domain boundaries. These patterns suggest that the model has learned
aspects of regulatory architecture from the correlation between sequence
and chromatin output labels.

\subsection{Why Attention Weights
Mislead}\label{why-attention-weights-mislead}

Raw attention weights require skeptical interpretation. High attention
between two positions indicates information flow in the model's
computation but does not necessarily indicate causal influence on
predictions. Attention serves multiple computational roles beyond
identifying important features: routing information for intermediate
computations, implementing positional reasoning, and satisfying
architectural constraints. A position receiving high attention might be
used for bookkeeping rather than contributing to the final output.

Several specific issues undermine naive attention interpretation.
Attention weights describe information movement before value vectors are
applied; positions with high attention but small value vector magnitudes
contribute little to the output. Multi-head attention averages across
heads with different functions; examining average attention obscures
specialized head behavior. Cross-layer effects mean that the importance
of early-layer attention depends on what later layers do with the routed
information.

More robust approaches combine attention analysis with perturbation
experiments. If deleting a position that receives high attention changes
the prediction substantially, the attention is functionally meaningful.
If deletion has minimal effect, the attention may serve computational
purposes unrelated to the target output. Attention rollout and attention
flow methods propagate attention through layers to better capture
information movement across the full network, though these too provide
correlational rather than causal evidence.

\section{Regulatory Vocabularies and Global
Interpretability}\label{regulatory-vocabularies-and-global-interpretability}

Local interpretability methods explain individual predictions; global
interpretability characterizes what a model has learned across its
entire training distribution. For genomic models trained to predict
thousands of chromatin features, global interpretability asks whether
the model has learned a coherent vocabulary of regulatory sequence
classes and how those classes map to biological programs.

\subsection{Sequence Classes from Sei}\label{sequence-classes-from-sei}

Sei exemplifies the global interpretability approach by learning a
vocabulary of regulatory sequence classes that summarize chromatin
profile diversity across the genome. The model predicts tens of
thousands of chromatin outputs (transcription factor binding, histone
modifications, accessibility across cell types), then compresses this
high-dimensional prediction space into approximately 40 sequence classes
through dimensionality reduction and clustering.

Each sequence class corresponds to a characteristic regulatory activity
pattern. Some classes show promoter-like signatures (H3K4me3, TSS
proximity, broad expression). Others exhibit enhancer patterns (H3K27ac,
H3K4me1, cell-type-restricted activity). Repressive classes display
H3K27me3 or H3K9me3 enrichment. Cell-type-specific classes capture
lineage-restricted regulatory programs (neuronal, immune, hepatic). This
vocabulary transforms thousands of raw chromatin predictions into a
compact, interpretable representation.

Variants can be characterized by their effects on sequence class scores,
yielding functional descriptions more informative than raw pathogenicity
predictions. A variant that shifts a region from enhancer-like to
promoter-like class, or from active to repressive, provides mechanistic
hypotheses about its functional consequences. GWAS enrichment analysis
can identify which sequence classes are overrepresented among
disease-associated variants, revealing the regulatory programs most
relevant to specific phenotypes.

\subsection{Embedding Geometry and Regulatory
Programs}\label{embedding-geometry-and-regulatory-programs}

Beyond discrete sequence classes, the continuous geometry of learned
representations encodes regulatory relationships. Sequences with similar
regulatory functions cluster in embedding space; directions in this
space correspond to biological axes of variation. Dimensionality
reduction techniques (UMAP, t-SNE, PCA) visualize these relationships,
revealing how the model organizes regulatory diversity.

For foundation models trained on diverse genomic tasks, embedding
geometry can capture cross-task relationships. Sequences that function
as enhancers in one cell type might cluster near sequences with enhancer
function in related cell types, even if trained independently. Variants
that disrupt shared regulatory logic should produce similar embedding
perturbations. These geometric properties enable transfer of
interpretability insights across tasks and provide compact summaries of
model knowledge.

\section{Mechanistic
Interpretability}\label{mechanistic-interpretability-1}

Classical interpretability methods treat models as input-output
functions, probing what they compute without examining how they compute
it. Mechanistic interpretability takes a different approach, attempting
to reverse-engineer the algorithms implemented by neural network
weights. This emerging field, most developed for language models, offers
tools increasingly applicable to genomic foundation models.

\subsection{Circuits and Features}\label{circuits-and-features}

The central hypothesis of mechanistic interpretability is that neural
networks implement interpretable computations through identifiable
circuits: connected subnetworks that perform specific functions. A
circuit might detect whether a motif is present, compute the distance
between two motifs, or integrate evidence across regulatory elements.
Identifying circuits requires tracing information flow through the
network and characterizing what each component contributes.

Features are the atomic units of this analysis: directions in activation
space that correspond to interpretable concepts. In language models,
features have been found that activate for specific topics, syntactic
structures, or semantic properties. Analogous features in genomic models
might activate for transcription factor binding sites, coding versus
non-coding sequence, or regulatory element types. Sparse autoencoders
trained on model activations can extract interpretable features by
encouraging representations where most features are inactive for any
given input.

Superposition complicates feature identification. Neural networks can
represent more features than they have dimensions by using overlapping,
nearly orthogonal directions. Features active for different inputs can
share parameters, enabling high-capacity representations but
complicating interpretation. Techniques from compressed sensing and
dictionary learning help decompose superposed representations into
constituent features.

\subsection{Applications to Genomic
Models}\label{applications-to-genomic-models}

Mechanistic interpretability remains nascent for genomic foundation
models, but initial applications show promise. Attention head analysis
in DNA language models has identified heads specialized for different
genomic functions: some attend within genes, others across regulatory
regions, still others implement positional computations. Probing
activations at different layers reveals hierarchical feature
construction, from local sequence patterns in early layers to long-range
regulatory relationships in later layers.

Circuit analysis can explain specific model behaviors. If a model
predicts that a variant disrupts regulation, mechanistic analysis can
trace which features activate differently for reference versus variant
sequence, which attention heads route information about the variant to
the prediction, and which intermediate computations change. This
mechanistic trace provides far richer explanation than attribution
scores alone, potentially identifying the regulatory logic the model has
learned.

The challenge is scalability. Current mechanistic interpretability
techniques require substantial manual analysis and work best for small
models or specific behaviors. Foundation models with billions of
parameters resist exhaustive circuit enumeration. Developing automated
tools for circuit discovery and scaling mechanistic analysis to large
genomic models represents an active research frontier.

\section{Validation: From Explanations to
Experiments}\label{validation-from-explanations-to-experiments}

Interpretability methods produce explanations, but explanations are only
valuable if they accurately reflect model behavior and connect to
biological reality. Validation closes the loop by testing whether
interpretability-derived hypotheses hold when subjected to experimental
scrutiny.

\subsection{Faithfulness Testing}\label{faithfulness-testing}

An interpretation is faithful if it accurately describes what the model
does. Testing faithfulness requires interventions: changing the features
identified as important and verifying that predictions change
accordingly. If an attribution method highlights certain positions as
driving a prediction, deleting or scrambling those positions should
reduce the prediction. If discovered motifs are claimed to be necessary
for regulatory activity, removing them from sequences should impair
predicted and measured function.

Sanity checks provide baseline validation. When model weights are
randomized, attributions should degrade to uninformative noise. When
training labels are scrambled, discovered motifs should disappear or
lose predictive power. These checks identify methods that produce
plausible-looking outputs regardless of model content, revealing
explanations that reflect method biases rather than genuine model
features.

Counterfactual experiments go further by testing whether identified
features are sufficient as well as necessary. Inserting discovered
motifs into neutral sequences should increase predicted regulatory
activity if the motifs genuinely encode functional elements.
Constructing synthetic sequences that combine motifs according to
discovered grammatical rules should produce predictions consistent with
those rules. Discrepancies between expected and observed effects
indicate gaps in the interpretation.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-plausible-vs-faithful}{[}Essential{]} Two-path
diagram. Scenario: Model predicts high enhancer activity. Path A
(Plausible but unfaithful): Attribution highlights GATA motif
(biologically reasonable); but model learned GC content correlate;
validation fails (mutating GATA doesn't change prediction; inserting
GATA doesn't increase); explanation matches intuition not computation.
Path B (Faithful): Attribution highlights GATA; validation succeeds
(mutating reduces prediction; inserting increases). Validation tests:
Necessity (removing reduces?), Sufficiency (adding increases?), Sanity
checks (random weights different?). Key distinction: Plausible matches
intuition; faithful reflects computation; unfaithful provides false
comfort.}

\end{figure}%

\subsection{Experimental Validation}\label{experimental-validation}

The ultimate test of interpretability connects model-derived hypotheses
to biological experiments. Motifs discovered through TF-MoDISco can be
tested through electrophoretic mobility shift assays, ChIP-qPCR, or
reporter constructs. Predicted spacing constraints can be validated by
varying distances between motifs in synthetic constructs and measuring
activity. Hypothesized enhancer-promoter connections can be tested
through CRISPR deletion of predicted enhancers and measurement of target
gene expression.

This experimental validation distinguishes genuine mechanistic discovery
from pattern matching that happens to produce plausible-looking results.
A model might learn that certain k-mers correlate with regulatory
activity for confounded reasons (batch effects, mappability artifacts)
yet produce motif logos resembling real transcription factors. Only
experimental testing can determine whether model-derived hypotheses
reflect causal regulatory logic.

High-throughput functional assays enable systematic validation at scale.
Massively parallel reporter assays (MPRAs) can test thousands of
model-predicted regulatory elements simultaneously. Perturb-seq combines
CRISPR perturbations with single-cell RNA-seq to measure effects of
knocking out predicted regulatory factors. These technologies create
opportunities for iterative model improvement: interpretability
generates hypotheses, experiments test them, and results refine both
model architecture and training.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-validation-pipeline}{[}High{]} Circular workflow.
Steps: (1) Model prediction; (2) Interpretability analysis (attribution,
TF-MoDISco, attention patterns); (3) Hypothesis generation (``GATA motif
drives activity''); (4) Experimental validation (EMSA for binding,
reporter for activity, CRISPR for necessity, MPRA for systematic
testing); (5) Model refinement (validated â†’ improved training; failed â†’
identify limitations; return to step 1). Example pathways: TF-MoDISco â†’
EMSA âœ“; attention enhancer â†’ CRISPR âœ“; GC attribution â†’ MPRA no effect âœ—
â†’ confounder identified. Key insight: Interpretability advances biology
only when closed with validation.}

\end{figure}%

\section{Interpretability in Clinical Variant
Assessment}\label{interpretability-in-clinical-variant-assessment}

Variant interpretation guidelines require that computational predictions
be weighed alongside experimental and clinical evidence, as discussed
further in Chapter~\ref{sec-rare-disease}. Interpretability determines
whether model predictions can contribute meaningful evidence beyond raw
pathogenicity scores.

Current ACMG-AMP criteria allow computational evidence as supporting
(PP3) or opposing (BP4) pathogenicity, but the evidence strength depends
on understanding what the prediction reflects. A splice site disruption
score from SpliceAI provides interpretable mechanistic evidence: the
variant is predicted to alter splicing because it changes the consensus
splice site sequence. This prediction can be evaluated against splice
site models, tested with minigene assays, and combined with observations
of aberrant transcripts in patient samples. The interpretation enables
evidence integration.

Foundation model predictions are less immediately interpretable but
potentially more informative. A pathogenicity score from ESM-1v reflects
evolutionary constraint inferred from protein language modeling, but the
specific sequence features driving the prediction require attribution
analysis to identify. An expression effect predicted by Enformer might
result from disrupted transcription factor binding, altered chromatin
accessibility, or changed 3D regulatory contacts; interpretability
analysis distinguishes these mechanisms and guides experimental
validation.

For clinical utility, interpretability must be communicated effectively.
Genome browsers displaying attribution tracks alongside variant calls
help clinicians identify mechanistic hypotheses. Reports that accompany
pathogenicity scores with regulatory vocabulary classifications (this
variant shifts an enhancer toward a repressive state) provide actionable
context. These communication challenges extend interpretability beyond
algorithm development to user interface design and clinical workflow
integration.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-clinical-interpretability}{[}Enhancing{]} Clinical
workflow integration. ACMG evidence framework: PP3 (computational
supports pathogenicity), BP4 (supports benign). Evidence strength
depends on interpretability: Weak (score only, 0.85, no mechanism,
limited ACMG weight); Moderate (score + attribution: disrupts splice
site; SpliceAI supports; can evaluate against transcript data); Strong
(score + validated mechanism: disrupts CTCF binding; ChIP confirms; 3D
genome shows contact; minigene assay confirms). Clinical report
elements: Annotation, score with uncertainty, mechanistic hypothesis,
supporting/conflicting evidence, recommended follow-up.}

\end{figure}%

\section{Practical Approaches for Foundation Model
Analysis}\label{practical-approaches-for-foundation-model-analysis}

Working with genomic foundation models requires matching
interpretability methods to specific questions. Several complementary
strategies address different aspects of model behavior.

For understanding variant effects, the primary goal is explaining why a
specific variant receives a particular prediction. Attribution methods
(ISM for validation, integrated gradients for efficiency) identify which
input positions drive the difference between reference and alternative
predictions. If the variant falls within a discovered motif, the
interpretation is straightforward. If attributions spread across the
sequence, the effect may operate through long-range regulatory changes
requiring attention analysis or contribution scores from models like
Enformer.

For characterizing model representations, probing classifiers diagnose
what information is encoded and at which layers. Probing for known
regulatory features (promoter versus enhancer, tissue specificity,
evolutionary conservation) establishes which biological properties the
model captures. Probing for potential confounders (GC content, distance
to annotated genes, technical artifacts) identifies shortcuts that might
inflate benchmark performance without reflecting genuine regulatory
understanding.

For discovering regulatory logic, TF-MoDISco applied to high-confidence
predictions extracts motif vocabularies specific to prediction tasks or
cell types. Grammar analysis of motif co-occurrence reveals
combinatorial rules. Sei-style sequence class analysis situates local
motifs within global regulatory programs. Comparing discovered
vocabularies across models or training conditions reveals shared versus
idiosyncratic features.

For debugging and auditing, interpretability methods identify what
features drive predictions in held-out distributions. If a model fails
on a new cell type, attribution analysis can reveal whether it relies on
cell-type-specific versus generalizable features. If performance
degrades on specific genomic regions, local interpretability can
identify confounding patterns or training data gaps.

For generating experimental hypotheses, interpretability produces
testable predictions. Discovered motifs can be synthesized and tested.
Predicted regulatory elements can be perturbed. Hypothesized TF binding
can be validated by ChIP. Model-derived predictions that survive
experimental testing represent genuine mechanistic insights; predictions
that fail point toward model limitations or confounding.

\section{Plausibility Is Not
Faithfulness}\label{plausibility-is-not-faithfulness}

The distinction between plausibility and faithfulness remains central to
interpretability for genomic foundation models. Models can produce
compelling motifs, structured attention patterns, and interpretable
probing results while operating through mechanisms that do not
correspond to biological reality. A model that correctly predicts splice
site strength may do so by recognizing confounded sequence features
rather than learning splice site grammar. A model that attributes
importance to a transcription factor binding site may be exploiting
correlation with GC content rather than modeling regulatory mechanism.
Plausible explanations that match biological intuition are not the same
as faithful explanations that accurately reflect model computation.

Only interventional experiments can distinguish genuine regulatory
insight from sophisticated pattern matching. Computational interventions
(deletion tests, counterfactual sequence generation, circuit analysis)
probe whether identified features are necessary and sufficient for model
predictions. Biological interventions (reporter assays, CRISPR
perturbations, massively parallel experiments) test whether
model-derived hypotheses hold in living systems. The conjunction of
computational and experimental validation transforms interpretability
from rationalization into discovery, generating testable hypotheses that
advance biological understanding rather than merely explaining model
behavior.

As foundation models grow in scale and capability, interpretability
becomes simultaneously more important and more challenging. Larger
models implement more complex computations, potentially capturing
subtler regulatory logic but resisting simple interpretation.
Mechanistic interpretability offers a path forward by characterizing
model internals directly, though scaling these techniques to
billion-parameter genomic models remains an open problem. The
integration of interpretability with model development points toward a
future where understanding and prediction advance together: motifs
discovered through interpretation inform architecture design,
experimentally validated hypotheses become supervision signals, and
interpretability failures that reveal confounding drive improvements in
training data and evaluation. In this vision, interpretability is not
merely a tool for explaining existing models but a methodology for
building models whose predictions we trust because we understand the
mechanisms they have learned.

\part{Part VI: Clinical Translation}

The question shifts from how these models work to how they are used, and
from what they can predict to what they enable us to do. This transition
is not merely practical but conceptual: deploying a model in a clinical
or industrial setting exposes assumptions that benchmarks leave implicit
and reveals failure modes that curated evaluations obscure. A model
achieving impressive metrics on held-out test sets may falter when
deployed on populations underrepresented in training data, when
integrated into workflows designed around different assumptions, or when
its outputs must be communicated to clinicians and patients who lack the
technical background to interpret confidence intervals. The gap between
benchmark performance and real-world utility represents one of the most
consequential challenges in genomic AI.

The preceding parts of this book traced genomic foundation models from
their architectural foundations through the challenges of reliable
evaluation. Part VI turns to practice, examining how these models are
deployed in clinical risk prediction, rare disease diagnosis, drug
discovery, and biological design. The discussion throughout emphasizes
what changes when models leave the research setting: the calibration
requirements become stricter, the fairness considerations become urgent,
the interpretability demands become concrete, and the consequences of
failure become measured in patient outcomes rather than leaderboard
rankings.

Five chapters span the major application domains.
Chapter~\ref{sec-clinical-risk} examines clinical risk prediction, where
foundation model features combine with electronic health records to
stratify patients for disease, progression, and treatment response.
Chapter~\ref{sec-rare-disease} focuses on variant interpretation in rare
disease, where models enter diagnostic pipelines alongside clinical
geneticists and laboratory scientists. Chapter~\ref{sec-drug-discovery}
explores drug discovery, where genomic foundation models contribute to
target identification, genetic validation, and biomarker development.
Chapter~\ref{sec-design} reverses the direction of inference from
prediction to generation, examining how foundation models guide protein
engineering, regulatory element design, and programmable biology.
Finally, Chapter~\ref{sec-future} addresses the regulatory, ethical, and
frontier challenges that will shape how genomic AI moves from research
to practice. The goal is not definitive protocols for each domain but a
framework for reasoning about deployment: what questions to ask, what
pitfalls to anticipate, and what principles should guide responsible
development.

\chapter{Clinical Risk Prediction}\label{sec-clinical-risk}

A risk prediction has clinical value only if it changes what happens
next. A cardiologist who receives a polygenic risk score for coronary
artery disease faces a simple question: does this information alter the
treatment recommendation? If a patient with a high score receives the
same statin prescription, lifestyle counseling, and follow-up schedule
as a patient without genetic testing, the score added nothing to care
regardless of its statistical validity. The fundamental challenge is not
generating genomic predictions but translating them into actions that
improve outcomes. This translation requires more than discrimination
between who will and will not develop disease; it requires that the
prediction reach clinicians in a usable form, at a decision point where
alternatives exist, for a patient population where the prediction
performs equitably.

Traditional polygenic scores, despite their scientific validity, often
fail this translation test. They reduce entire genomes to single numbers
that provide little mechanistic insight. They transfer poorly across
ancestries because training data overrepresent European populations.
They exist outside the electronic health records where clinical
decisions actually happen, requiring manual lookup that busy clinicians
rarely perform. Most fundamentally, the clinical actions available in
response to a PRS (lifestyle modification, earlier screening, preventive
medication) are often the same actions recommended for patients with
conventional risk factors, leaving unclear what the genetic information
specifically enables.

Genomic foundation models offer capabilities that may address some of
these limitations. Rather than collapsing genetic information into
scalar risk scores, foundation models produce embeddings that capture
sequence context, regulatory grammar, and functional consequences. These
representations can integrate with clinical data through fusion
architectures (Chapter~\ref{sec-multi-omics}), adapt to diverse
prediction tasks through transfer learning (Chapter~\ref{sec-transfer}),
and provide feature attributions that connect predictions to biological
mechanisms (Chapter~\ref{sec-interpretability}). Whether these
capabilities translate into tools that change practice remains the open
question. This chapter examines that translation challenge: how
foundation model features combine with electronic health records, what
evidence standards clinical deployment requires, how fairness
considerations determine whether genomic AI reduces or amplifies health
disparities, and what practical realities govern integration into care
delivery.

\section{From Polygenic Scores to Foundation Model
Features}\label{from-polygenic-scores-to-foundation-model-features}

The limitations of classical polygenic risk scores define the
opportunity for foundation model approaches. As discussed in
Chapter~\ref{sec-gwas}, polygenic scores aggregate the effects of common
variants into weighted sums, with weights derived from genome-wide
association study effect sizes. This framework has demonstrated that
common genetic variation contributes substantially to risk for
conditions including coronary artery disease, type 2 diabetes, and
breast cancer. A patient in the top percentile of polygenic risk for
coronary disease faces roughly threefold higher lifetime risk than one
in the bottom percentile, a gradient comparable to traditional risk
factors like smoking or hyperlipidemia.

Three limitations constrain the clinical impact of this approach. First,
the linear additive model cannot capture epistatic interactions where
the effect of one variant depends on the presence of others, nor can it
represent the complex nonlinear relationships between genetic variation
and disease that emerge from regulatory networks and cellular pathways.
Second, polygenic scores derived from European-ancestry genome-wide
association studies substantially underperform in other populations,
with effect sizes often attenuating by half or more in African or East
Asian ancestries due to differences in linkage disequilibrium structure
and allele frequencies (Chapter~\ref{sec-confounding}). Third, a single
scalar provides no mechanistic insight: a high polygenic score for
diabetes does not indicate whether risk stems from impaired insulin
secretion, insulin resistance, or altered satiety signaling, information
that might guide intervention selection.

Foundation models address these limitations through richer
representations. Instead of treating variants as independent weighted
features, models like Delphi and G2PT learn genome-wide embeddings that
encode sequence context, regulatory annotations, and cross-variant
interactions (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025).
These approaches can capture nonlinear structure in genetic risk,
leverage functional priors that transfer across ancestries, and provide
attention-based attributions that highlight which genomic regions
contribute most to predictions. Fine-mapping models like MIFM estimate
posterior probabilities for causal variants within association loci,
allowing risk models to weight variants by evidence for causality rather
than treating all correlated variants equally (Rakowski and Lippert
2025).

The practical architecture of a foundation model-enabled risk system
typically involves three components: pretrained encoders that transform
genomic data into embeddings, aggregation modules that summarize
variant-level or region-level representations into patient-level
features, and prediction heads that map these features (combined with
clinical covariates) to risk estimates. This modular design separates
the computationally expensive foundation model inference from the
task-specific prediction layer, enabling updates to either component
while maintaining clear interfaces for validation.

\section{Defining Clinical Risk
Prediction}\label{defining-clinical-risk-prediction}

A risk prediction model is only as useful as the decision it informs.
Effective clinical risk prediction requires precise specification of
four elements: the outcome being predicted, the time horizon over which
prediction applies, the target population for whom the model is
intended, and the clinical action the prediction will trigger.

Consider a 55-year-old woman with moderately elevated cholesterol and a
family history of early coronary disease. Her cardiologist must decide
whether to initiate statin therapy, a decision traditionally guided by
10-year cardiovascular risk estimates from tools like the Pooled Cohort
Equations. A genomic foundation model could augment this decision in
several ways. It might refine her absolute risk estimate by
incorporating polygenic information that the traditional calculator
ignores. It might identify whether her genetic risk concentrates in
pathways amenable to specific interventions (LDL metabolism favoring
statins versus inflammatory pathways suggesting alternative approaches).
It might flag pharmacogenomic variants affecting statin metabolism that
influence dose selection or drug choice.

Each of these applications represents a different prediction task with
distinct requirements. The 10-year risk estimate for major adverse
cardiovascular events is an individual-level incident risk problem where
discrimination and calibration matter most. The pathway-level
attribution is an interpretability challenge requiring mechanistic
grounding. The pharmacogenomic prediction is a treatment selection
problem where the relevant outcome is adverse drug reaction risk
conditional on therapy initiation.

Clinical risk prediction tasks cluster into several archetypes. Incident
risk concerns whether a currently disease-free individual will develop
disease within a specified window, such as 10-year diabetes risk for
prediabetic patients. Progression risk asks which patients with existing
disease will develop complications, for instance nephropathy in diabetes
or heart failure after myocardial infarction. Survival and prognosis
involve time-from-diagnosis to events like death, recurrence, or
transplant, often requiring survival models that handle censoring and
competing risks. Treatment response and toxicity concerns whether a
patient will benefit from one therapy versus another and their
probability of experiencing serious adverse effects.

Foundation models enter these problems as feature generators. They
transform raw sequence data into structured representations that
downstream prediction models combine with clinical covariates. The
architectural choices for this combination, and the evidence required to
trust the resulting predictions, constitute the core methodological
challenges of clinical translation.

\section{Feature Integration
Architectures}\label{feature-integration-architectures}

The features available for clinical risk models draw on multiple
foundation model families, each capturing different aspects of genetic
and molecular risk.

DNA-level foundation models provide variant effect predictions without
requiring trait-specific training. Systems like Nucleotide Transformer,
HyenaDNA, and GPN compute sequence-based deleteriousness scores that
reflect how mutations disrupt regulatory grammar, splice sites, or
protein-coding sequences (Dalla-Torre et al. 2023; Nguyen et al. 2023;
Benegas, Batra, and Song 2023). These zero-shot predictions transfer
across traits and ancestries because they derive from sequence
properties rather than population-specific association statistics.
Fine-mapping models like MIFM integrate such functional priors with
association evidence to estimate which variants within a locus are
likely causal, providing principled weights for aggregation (Rakowski
and Lippert 2025).

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-feature-integration}{[}Essential{]} Three-column
comparison. Column 1 (Early Fusion): All features concatenated â†’ single
model â†’ risk. Column 2 (Intermediate Fusion): Separate encoders (FM as
genomic encoder) â†’ embeddings â†’ fusion â†’ risk. Column 3 (Late Fusion):
Independent models â†’ scores â†’ ensemble â†’ risk. For each: pros/cons,
missing data handling, compute. Comparison table: Cross-modal
interactions, missing data, modularity, best for. Key insight:
Intermediate balances modularity with interaction learning.}

\end{figure}%

Protein language models add coding variant interpretation. AlphaMissense
and related systems predict pathogenicity for missense mutations based
on evolutionary conservation patterns learned from millions of protein
sequences, as discussed in Chapter~\ref{sec-protein-lm}. For conditions
with strong coding variant contributions (Mendelian cardiomyopathies,
cancer predisposition syndromes), these predictions provide crucial
signal beyond what noncoding regulatory models capture.

Multi-omics foundation models extend beyond germline sequence.
Cell-type-resolved representations from GLUE, scGLUE, and CpGPT capture
regulatory state across chromatin accessibility, methylation, and
expression (Chapter~\ref{sec-single-cell}) (Cao and Gao 2022; Camillo et
al. 2024). Rare variant burden scores from DeepRVAT aggregate predicted
effects across genes into pathway-level impairment measures (Clarke et
al. 2024). For oncology applications, tumor embedding models like
SetQuence and graph neural network-based subtypers encode complex
somatic mutation landscapes into patient-level representations
(Jurenaite et al. 2024; X. Li et al. 2022).

Electronic health record features provide the clinical context without
which genomic predictions lack meaning. Demographics, vital signs,
laboratory values, medication lists, problem codes, and procedure
histories characterize the patient's current state and trajectory.
Time-varying biomarker trajectories (estimated glomerular filtration
rate trends, hemoglobin A1c patterns, tumor marker dynamics) capture
disease evolution that static snapshots miss.

The architectural question is how to combine these heterogeneous inputs.
Three fusion strategies offer different tradeoffs.

Early fusion concatenates all features into a single input vector and
trains a unified model (neural network, gradient boosting, survival
regression) on the combined representation. This approach allows the
model to learn arbitrary interactions between genomic and clinical
features but requires all inputs to be present for every patient,
handles scale differences between modalities poorly, and can be
dominated by whichever input provides the most features or strongest
signal.

Intermediate fusion trains separate encoders for each modality,
producing genomic embeddings, clinical embeddings, and multi-omic
embeddings that a fusion module then combines. The fusion module might
use attention mechanisms to weight modality contributions dynamically,
cross-modal transformers that allow features from one modality to attend
to features from another, or simpler concatenation with learned
combination weights. This approach offers modularity (foundation model
encoders can be swapped as new versions become available) while still
enabling learned cross-modal interactions.

Late fusion trains independent models for each modality and combines
their predictions through ensemble methods or meta-learning. A polygenic
score model, an electronic health record model, and a multi-omic model
each produce risk estimates that a final layer integrates. This approach
handles missing modalities gracefully (each submodel operates
independently) and allows modality-specific architectures but may
underutilize cross-modal structure since interactions can only be
captured at the final combination stage.

For clinical deployment, intermediate fusion often provides the best
balance. It enables modular updates as foundation models improve, allows
graceful degradation when modalities are missing, and captures
cross-modal interactions that late fusion misses. The specific fusion
mechanism (attention, concatenation, cross-modal transformer) matters
less than ensuring the architecture supports the operational
requirements of clinical deployment: batch computation, uncertainty
quantification, and interpretable feature attribution.

\section{Temporal Modeling
Architectures}\label{temporal-modeling-architectures}

Clinical risk prediction spans diverse temporal structures, and the
choice of modeling framework must match the prediction task. A screening
tool estimating whether a patient will develop diabetes within ten years
faces different statistical challenges than a monitoring system tracking
whether a patient's kidney function trajectory signals imminent decline.
Foundation model features can integrate into each framework, but the
integration patterns differ.

Survival models address time-to-event outcomes where patients are
followed until an event occurs or observation ends. The Cox proportional
hazards model remains the workhorse of clinical risk prediction,
estimating hazard ratios for features while making minimal assumptions
about baseline hazard shape. Foundation model embeddings enter as
covariates alongside clinical variables, with the proportional hazards
assumption requiring that genomic risk effects remain constant over
time. When this assumption fails (as when genetic effects on cancer
recurrence differ between early and late periods), stratified or
time-varying coefficient extensions accommodate the violation.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-temporal-modeling}{[}High{]} Three-panel comparison.
Panel A (Survival Models): Timeline with events (X) or censoring (O);
Cox PH diagram; FM embeddings as static covariates; ``10-year CV risk at
baseline.'' Panel B (Longitudinal Models): Repeated measurements over
time; time-varying features; FM re-encodes at each timepoint; ``Updated
prognosis as disease evolves.'' Panel C (Hybrid/Joint): Baseline genomic
risk + trajectory updating; static genomic embedding + dynamic clinical
transformer; ``Genetic predisposition modified by treatment response.''}

\end{figure}%

Deep survival models extend this framework through neural network
architectures that learn nonlinear feature interactions. DeepSurv
replaces the linear Cox predictor with a multilayer network while
preserving the partial likelihood objective
(\textbf{katzman\_deepsurv\_2018?}). Deep Survival Machines model the
survival distribution as a mixture of parametric components, enabling
richer distributional assumptions than the semiparametric Cox approach
(\textbf{nagpal\_deep\_2021?}). These architectures naturally
accommodate the high-dimensional embeddings that foundation models
produce, though the risk of overfitting increases and careful
regularization becomes essential.

Longitudinal models address a different challenge: patients observed
repeatedly over time, with measurements that evolve and interact. A
patient's hemoglobin A1c trajectory over five years contains information
that a single baseline measurement cannot capture. Whether values are
stable, rising, or fluctuating conveys prognostic signal beyond their
current level. Joint longitudinal-survival models connect these repeated
measurements to event outcomes, modeling how biomarker trajectories
associate with hazard while accounting for informative dropout when
sicker patients are measured more frequently or die before later
observations.

Foundation model features integrate into longitudinal frameworks at
multiple levels. Static genomic embeddings (computed once from germline
sequence) serve as time-invariant covariates influencing both trajectory
shape and event hazard. Time-varying molecular features (expression
profiles, methylation states, circulating tumor DNA levels) can be
encoded through foundation models at each measurement occasion,
producing sequences of embeddings that recurrent or attention-based
architectures process into trajectory representations. The computational
cost of re-encoding molecular data at each timepoint is substantial,
making efficient inference strategies essential for deployment.

Transformer architectures designed for irregularly sampled time series
offer a natural framework for clinical trajectories. Models like STraTS
and similar clinical transformers handle the variable timing and missing
measurements characteristic of real-world healthcare data
(\textbf{tipirneni\_self\_2022?}). Position encodings based on actual
timestamps rather than sequence position accommodate irregular sampling.
Attention mechanisms identify which historical measurements most inform
current predictions. Foundation model embeddings at each timepoint
provide richer input representations than raw laboratory values alone.

The choice between survival and longitudinal frameworks depends on the
clinical question and available data. When the goal is baseline risk
stratification (identifying high-risk patients at a single decision
point), survival models with static genomic features often suffice. When
the goal is dynamic monitoring (detecting deterioration as it develops),
longitudinal models that update predictions as new measurements arrive
become necessary. Hybrid approaches that initialize with genomic risk
and update based on clinical trajectory combine the strengths of both
paradigms.

\section{Evaluation for Clinical
Deployment}\label{evaluation-for-clinical-deployment}

High performance on held-out test sets is necessary but far from
sufficient for clinical deployment. Risk models must satisfy multiple
evidence standards that typical machine learning papers do not address,
and teams planning translation must understand these requirements from
the outset rather than discovering them after development is complete.

\subsection{Discrimination}\label{discrimination}

Discrimination measures how well a model ranks patients by risk,
distinguishing those who will experience outcomes from those who will
not. For binary endpoints like disease occurrence within a fixed time
window, the area under the receiver operating characteristic curve
(AUROC) summarizes discrimination across all classification thresholds.
When outcomes are rare (severe adverse drug reactions, specific disease
subtypes), the area under the precision-recall curve (AUPRC) better
reflects how well the model identifies true positives among many
negatives. For survival tasks with censoring, the concordance index and
time-dependent AUC generalize these metrics to the time-to-event
setting.

Strong discrimination is necessary but not sufficient. A model that
correctly ranks patients but systematically overestimates or
underestimates absolute risk magnitudes will lead to inappropriate
clinical decisions. If a model predicts 5\% risk for patients who
actually experience 15\% event rates, physicians using those predictions
will undertreat. Conversely, systematically inflated predictions lead to
overtreatment with attendant harms and costs.

\subsection{Calibration}\label{calibration}

Calibration asks whether predicted probabilities match observed
frequencies. If a model assigns 20\% risk to a group of patients,
approximately 20\% should experience the outcome. Well-calibrated
predictions can be interpreted at face value and used directly for
clinical decision-making; miscalibrated predictions mislead regardless
of discrimination quality.

Assessment involves calibration plots comparing predicted risk deciles
to observed event rates, statistical tests like the Hosmer-Lemeshow
test, and proper scoring rules like the Brier score that combine
calibration and discrimination. These assessments must be stratified by
clinically relevant subgroups (ancestry, sex, age, comorbidity burden)
because a model well-calibrated overall may be systematically
miscalibrated for specific populations.

For polygenic score-informed models, calibration requires particular
attention. Raw polygenic scores are typically centered and scaled rather
than calibrated to absolute risk. Mapping a score to an absolute event
probability requires post-hoc models incorporating baseline incidence
and clinical covariates. Foundation models can shift score distributions
as architectures evolve, meaning recalibration may be necessary when
updating encoders. The connection to Chapter~\ref{sec-uncertainty} is
direct: calibration is one form of uncertainty quantification, assessing
whether model confidence aligns with actual outcome frequencies.

\subsection{Clinical Utility}\label{clinical-utility}

Beyond discrimination and calibration, clinical utility asks whether
using the model will change decisions beneficially. Net reclassification
improvement quantifies how many patients are appropriately moved across
risk thresholds compared to a baseline model. Decision curve analysis
estimates net benefit across threshold probabilities, accounting for the
relative costs of false positives and false negatives in specific
clinical contexts.

For foundation model-based tools, these analyses must demonstrate
incremental value over existing alternatives. If a complex genomic
foundation model provides only marginal improvement over a traditional
polygenic score plus standard clinical calculator, the additional
complexity, cost, and implementation burden may not be justified. The
relevant comparison is not ``better than nothing'' but ``better than
what clinicians can already access.''

\subsection{The Validation Hierarchy}\label{the-validation-hierarchy}

Evidence strength depends critically on validation design. Internal
validation through cross-validation or temporal splits within
development data is useful but insufficient due to potential overfitting
and subtle data leakage issues discussed in
Chapter~\ref{sec-confounding}. External validation across institutions
and ancestries tests the same locked model in independent health systems
and diverse populations. This step is essential for assessing whether
performance reflects genuine biological signal versus idiosyncratic
features of the development dataset.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-validation-hierarchy}{[}Essential{]}
Pyramid/staircase with evidence levels. Levels bottom to top: (1)
Internal validation (CV, temporal splits; low evidence; proof of
concept; overfitting risk); (2) External validation (same locked model
in independent systems; moderate; transportability; multiple
institutions/ancestries); (3) Prospective observational (model runs
silently; moderate-high; real-time performance, drift detection;
months-years); (4) Prospective interventional
(randomized/quasi-experimental; high; clinical outcome improvement;
trial registration, IRB). Annotations: Cost/time increasing; regulatory
requirements; ``Most FM tools stop here'' at external validation.}

\end{figure}%

Prospective observational validation runs the model silently alongside
clinical care without influencing decisions, measuring real-time
performance and drift in deployment conditions. Prospective
interventional trials use randomized or quasi-experimental designs to
assess whether model-guided care actually improves outcomes, equity, and
cost-effectiveness compared to usual care.

For most foundation model-based tools, regulators and health systems
expect robust external validation at minimum. High-stakes applications
(cancer prognosis affecting treatment intensity, pharmacogenomic
predictions affecting drug choice) may require prospective
interventional evidence. The investment required increases at each level
of the hierarchy, but so does the confidence that deployment will
produce benefit rather than harm.

\section{Uncertainty Quantification}\label{uncertainty-quantification-1}

In clinical settings, models must know when they do not know. A risk
prediction offered with false confidence is more dangerous than one
accompanied by appropriate uncertainty bounds, because the former
invites unwarranted action while the latter prompts appropriate caution
or additional evaluation.

Two sources of uncertainty require distinction. Aleatoric uncertainty
reflects irreducible noise in the outcome: even with perfect input
features, some patients with identical measured characteristics will
experience different outcomes due to unmeasured variables, stochastic
biology, or measurement error. Epistemic uncertainty reflects model
limitations: insufficient training data, architectural constraints, or
distributional shift between training and deployment conditions.
Aleatoric uncertainty cannot be reduced by collecting more data or
improving models; epistemic uncertainty can.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-clinical-uncertainty}{[}High{]} Uncertainty
decomposition. Components: (1) Genomic Uncertainty (under-represented
ancestry, rare variants, novel genes; detection: embedding distance;
response: flag for review); (2) Clinical Uncertainty (extrapolation to
new settings; detection: feature distribution monitoring; response:
local calibration); (3) Outcome Uncertainty/Aleatoric (biological
variability, incomplete penetrance; detection: heteroscedastic models;
response: communicate irreducible). Visualization: Venn or stacked bar.
Example patient profiles with different compositions. Decision support:
When total exceeds threshold â†’ abstain/flag.}

\end{figure}%

Practical uncertainty quantification methods include ensemble
approaches, where multiple models trained with different random seeds
provide prediction intervals based on their disagreement. Monte Carlo
dropout approximates Bayesian uncertainty by averaging predictions
across stochastic forward passes. Conformal prediction provides
principled prediction intervals with guaranteed coverage under
exchangeability assumptions, avoiding the distributional assumptions
required by parametric methods. Temperature scaling post-hoc adjusts
model outputs to improve calibration without retraining.

For foundation model-based systems, uncertainty decomposes into genomic
and clinical components. Genomic uncertainty reflects confidence in
variant effect predictions, fine-mapping probabilities, or embedding
reliability; it increases for variants from underrepresented
populations, rare variants with limited training examples, or sequences
falling outside the distribution seen during pretraining. Clinical
uncertainty reflects extrapolation to new care settings, practice
patterns, or patient populations not represented in development data.

Selective prediction allows models to abstain when uncertainty exceeds
thresholds, flagging cases for human review rather than providing
potentially misleading predictions. This is particularly important for
patients from rare ancestries underrepresented in training data or with
unusual clinical presentations. The tension between coverage (providing
predictions for all patients) and reliability (ensuring predictions are
trustworthy) must be navigated thoughtfully, ideally with input from the
clinicians who will use the system.

\section{Fairness and Health Equity}\label{fairness-and-health-equity-1}

Many genomic and electronic health record datasets encode historical
inequities in who gets genotyped, which populations are recruited into
biobanks, and how healthcare is documented and delivered. Risk models
trained on such data can amplify disparities if not carefully evaluated
and designed.

The ancestry bias in genome-wide association studies persists in
foundation model applications. As discussed in Chapter~\ref{sec-gwas},
polygenic scores derived from European-ancestry data substantially
underperform in other populations. Foundation models have the
opportunity but not the guarantee to improve portability by leveraging
functional priors that transfer across ancestries (sequence-based
deleteriousness does not depend on population-specific linkage
disequilibrium) and by incorporating multi-ancestry training data.
Whether they succeed depends on training data composition, evaluation
practices, and explicit attention to cross-ancestry performance
throughout development.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
\newline
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1234.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER D}}
\end{minipage}%

\caption{\label{fig-fairness-assessment}{[}High{]} Equity dashboard.
Panel A (Performance Disparity): Bar chart AUROC by ancestry; confidence
intervals; reference line; highlight underperformance. Panel B
(Calibration Disparity): Multiple reliability diagrams overlaid; ECE per
group. Panel C (Clinical Utility Disparity): Decision curves by
subgroup; net benefit at thresholds; model improves over
treat-all/treat-none? Panel D (Access and Outcome Metrics): Who receives
testing? Who benefits? Model reduces or amplifies disparities?
Mitigation callout: Reweighting, group-wise calibration, expanding
sequencing access.}

\end{figure}%

Electronic health record features introduce additional bias sources.
Which patients receive genetic testing, which laboratory tests are
ordered, how diagnoses are coded, and how thoroughly clinical notes are
documented all differ systematically across patient populations, care
settings, and health systems. A model trained on one institution's data
may encode those institutional patterns rather than underlying biology.

Health equity evaluation requires disparity metrics measuring
performance differences in discrimination, calibration, and clinical
utility across subgroups defined by ancestry, sex, socioeconomic
proxies, and care site. Access metrics assess whether financial,
geographic, or systemic barriers limit which patients can benefit from
genomic risk tools. Outcome metrics evaluate whether clinical actions
triggered by predictions differ across groups and whether benefits
accrue equitably or concentrate among already-advantaged populations.

Technical mitigation strategies include reweighting training data to
reduce representation disparities, group-wise calibration ensuring
equitable performance across subgroups, and localized fine-tuning using
deployment-site data. However, technical interventions alone cannot
overcome structural inequities. Non-technical approaches including
expanding sequencing access, subsidizing testing for underserved
populations, and designing workflows that accommodate diverse care
settings are equally essential.

The core principle is that equity cannot be an afterthought addressed
during final evaluation. It must inform pretraining data selection,
benchmark choice, validation study design, and deployment planning from
the outset. A model that appears well-calibrated overall but is
miscalibrated for specific populations will exacerbate rather than
reduce health disparities.

\section{Clinical Integration}\label{clinical-integration}

Even a comprehensively validated model can fail in practice if it does
not integrate into clinical workflows. Genomic risk predictions must
reach clinicians at decision points, in formats that support rather than
disrupt care delivery, with appropriate interpretability and uncertainty
communication.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-clinical-workflow}{[}High{]} Workflow diagram.
Clinical workflow: Patient presents â†’ Assessment â†’ Decision point â†’
Action. Integration patterns: (1) Laboratory Interpretation Augmentation
(FM scores in variant report; human geneticist reviews; variant
classification); (2) Risk Embedded in EHR (precomputed scores in
structured fields; dashboard at point of care; clinical decision
support); (3) Pharmacogenomic Alert (synchronous alert at prescription
entry; drug-gene interaction flagged; medication ordering). For each:
When computation happens, who sees output, what action triggered. System
architecture: Model serving, adapters, logging, version control.}

\end{figure}%

\subsection{Workflow Integration
Patterns}\label{workflow-integration-patterns}

Clinical genomics has established pathways for returning results through
CLIA-certified laboratories, structured reports, and genetic counseling.
Foundation model-based risk tools can augment these pathways in two
primary ways. Laboratory interpretation augmentation uses foundation
model predictions to prioritize variants for manual review, provide
richer functional annotations, and suggest likely disease mechanisms
supporting differential diagnosis. Direct risk embedding in electronic
health records precomputes risk scores for patients with genomic data,
surfaces them in structured fields or clinical dashboards, and triggers
alerts when thresholds are crossed.

Design choices include batch versus on-demand computation (batch
overnight processing is often preferable given foundation model
computational costs and the relative stability of genomic data),
synchronous alerts at order entry versus asynchronous reports in
clinical inboxes, and whether high-impact predictions require
human-in-the-loop review before reaching front-line clinicians.

The specifics vary by clinical context. Pharmacogenomic alerts might
appear synchronously at prescription order entry, providing immediate
guidance on drug selection or dosing. Cardiometabolic risk scores might
appear in primary care dashboards updated weekly, informing prevention
discussions at annual visits. Oncology prognosis estimates might be
generated at diagnosis and reviewed in tumor board settings where
multidisciplinary teams make treatment decisions.

\subsection{System Architecture}\label{system-architecture}

From an engineering perspective, foundation model-based clinical tools
typically require a secure model-serving endpoint handling inference
requests, input adapters transforming laboratory and electronic health
record data into model-ready formats, output adapters mapping
predictions to structured clinical concepts or user-facing text, and
logging infrastructure providing audit trails and enabling drift
detection.

Regulated settings impose additional requirements: versioning of models,
data pipelines, and reference genomes with complete reproducibility;
access controls and network segmentation protecting genomic data; and
validation environments separated from production for safe testing of
updates. Practical guidance on hardware requirements, deployment
patterns, and cost estimation appears in Appendix~\ref{sec-apx-compute}.

\subsection{Post-Deployment
Monitoring}\label{post-deployment-monitoring}

Clinical deployment begins rather than ends the model lifecycle.
Practice patterns evolve as new treatments and guidelines emerge.
Patient populations shift as screening programs expand or contract.
Laboratory assays and sequencing pipelines change, introducing
distributional shifts in input features.

Monitoring systems should track input distributions (genotype
frequencies, electronic health record feature patterns) to detect when
current patients differ from training populations. Output distributions
(risk score histograms, threshold-crossing rates) reveal whether model
behavior is changing. Performance metrics computed via rolling windows
or periodic audits detect calibration or discrimination degradation
before clinical consequences accumulate.

When drift is detected, responses range from recalibration (adjusting
the score-to-probability mapping while preserving ranking behavior)
through partial retraining (updating prediction heads while keeping
foundation model weights fixed) to full model updates (retraining
encoders, requiring renewed validation). The modular separation between
foundation model backbones and clinical prediction heads facilitates
this maintenance: encoders can be versioned and swapped with
compatibility testing while prediction heads adapt to local deployment
conditions.

Incident response processes allow clinicians to report surprising or
harmful predictions, triggering root-cause analysis and potential
remediation. Governance structures including AI oversight committees
review models periodically and establish clear criteria for deprecation
when performance degrades below acceptable thresholds.

\section{Regulatory and Quality
Frameworks}\label{regulatory-and-quality-frameworks}

Foundation model-based clinical tools exist on a spectrum from
research-only applications supporting hypothesis generation through
clinical decision support tools informing diagnosis or management to
regulated medical devices subject to formal oversight. The regulatory
classification depends on intended use, risk level, and the claims made
for the tool.

Jurisdictions differ in specifics, but common expectations include
transparent descriptions of training data and known limitations,
quantitative performance evidence across relevant subgroups, plans for
post-market surveillance and incident reporting, and change management
procedures for model updates. Beyond formal regulation, health systems
typically require standard operating procedures for deployment and
decommissioning, model cards describing training data and limitations,
validation reports documenting evaluation evidence, and governance
structures reviewing and approving new tools.

Foundation models introduce additional documentation requirements.
Descriptions of pretraining corpora must specify which genomes, assays,
and populations were included. Fine-tuning datasets and label
definitions require detailed documentation. Procedures for updating to
new genome builds, reference panels, or assay types must be established
and tested. The modular separation between pretrained encoders and
clinical prediction heads can ease regulatory management by allowing
independent updates to each component, but this requires careful version
control and compatibility testing to ensure that updating one component
does not degrade performance of the combined system.

\section{Case Studies}\label{case-studies}

Three stylized case studies illustrate how foundation model features
integrate into clinical risk prediction across different disease
contexts, time horizons, and decision types.

\subsection{Cardiometabolic Risk
Stratification}\label{cardiometabolic-risk-stratification}

A 52-year-old man presents to his primary care physician for an annual
wellness visit. His LDL cholesterol is 145 mg/dL, blood pressure is
138/88 mmHg, and hemoglobin A1c is 5.9\%, placing him in the prediabetic
range. His father had a myocardial infarction at age 58. The standard
Pooled Cohort Equations estimate his 10-year atherosclerotic
cardiovascular disease risk at 8.2\%, just below the threshold where
guidelines recommend statin therapy.

A foundation model-augmented risk system could refine this assessment.
Variant effect scores from DNA foundation models annotate variants in
cardiometabolic risk loci with predicted regulatory and coding impacts.
A polygenic embedding model like Delphi or G2PT produces a genome-wide
representation capturing nonlinear risk structure beyond simple effect
size sums (Georgantas, Kutalik, and Richiardi 2024; Lee et al. 2025).
This genomic embedding combines with electronic health record features
through an intermediate fusion architecture, producing an updated
10-year risk estimate of 11.4\%, above the treatment threshold.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-cardio-case-study}{[}Enhancing{]} Patient journey
diagram. Patient profile: 52yo man, LDL 145, BP 138/88, HbA1c 5.9\%;
family history (father MI at 58); traditional risk 8.2\% (below statin
threshold). FM integration: DNA FMs annotate variants; polygenic
embedding (Delphi/G2PT) captures nonlinear risk; fusion with EHR;
updated estimate 11.4\% (above threshold). Clinical decision impact:
Threshold crossed â†’ statin indicated; pathway attribution â†’ LDL
metabolism (supports mechanism); attention attribution â†’ regions for
counseling. Validation requirements: External validation, equity
analysis. Key insight: Value depends on whether refined estimate enables
different action.}

\end{figure}%

The clinical value depends on what this refined estimate enables. If
genomic foundation model features merely replicate traditional polygenic
score information with higher computational cost, the benefit is
marginal. But if the embedding captures pathway-level structure that
identifies this patient's risk as concentrating in LDL metabolism
pathways rather than inflammatory or thrombotic mechanisms, that
information might strengthen the indication for statin therapy
specifically. Attention-based attributions highlighting which genomic
regions contribute most to the elevated risk could inform counseling
about heritability and family screening.

External validation across multiple health systems and ancestries would
need to demonstrate that the foundation model approach provides
calibrated predictions and meaningful reclassification improvement over
traditional tools. Equity analysis would verify that performance holds
across the diverse populations the health system serves rather than
degrading for non-European ancestries underrepresented in training data.

\subsection{Oncology Prognosis}\label{oncology-prognosis}

A 64-year-old woman has undergone surgical resection for stage II
colorectal cancer with microsatellite stable tumor characteristics. Her
oncology team must decide whether adjuvant chemotherapy is warranted
given the balance between recurrence risk reduction and treatment
toxicity. Traditional staging provides prognostic information, but
substantial heterogeneity exists within stage categories.

Foundation models can enrich prognostic assessment through multiple
channels. Tumor mutation profiles encoded through models like SetQuence
or SetOmic produce embeddings capturing the specific constellation of
somatic alterations beyond simple mutation counts (Jurenaite et al.
2024). Transcriptomic profiling integrated through GLUE-style latent
spaces adds expression context reflecting tumor microenvironment and
pathway activity (Cao and Gao 2022). Graph neural network-based
subtyping assigns the tumor to a molecular subtype with characteristic
prognosis and treatment response patterns (X. Li et al. 2022).

These tumor-level representations combine with germline pharmacogenomic
features (variants affecting fluoropyrimidine metabolism that influence
toxicity risk) and clinical features (performance status, comorbidities,
patient preferences) in a survival model predicting two-year recurrence
hazard. A high-risk prediction might favor more intensive adjuvant
therapy, while low-risk predictions might support observation with close
surveillance.

The validation requirements are stringent. Retrospective analysis of
institutional cohorts establishes proof of concept, but prospective
validation in cohorts receiving contemporary treatment regimens is
necessary given the rapid evolution of oncology care. Interpretability
connecting predictions to specific mutations, pathways, or molecular
subtypes supports clinical adoption by providing rationale beyond a
black-box hazard estimate.

\subsection{Pharmacogenomic Adverse Event
Prediction}\label{pharmacogenomic-adverse-event-prediction}

A 45-year-old man with newly diagnosed epilepsy requires anticonvulsant
therapy. Carbamazepine is a common first-line choice, but it carries
risk of severe cutaneous adverse reactions including Stevens-Johnson
syndrome and toxic epidermal necrolysis. The HLA-B*15:02 allele is
strongly associated with carbamazepine hypersensitivity in patients of
Asian ancestry, and FDA labeling recommends genetic testing before
initiating therapy in at-risk populations.

This established pharmacogenomic association illustrates both the
potential and limitations of current approaches. Single-variant
associations with high effect sizes enable straightforward clinical
implementation, but they cover a small fraction of drug-gene
interactions. Many patients who do not carry HLA-B*15:02 still
experience adverse reactions, suggesting additional genetic (and
non-genetic) risk factors that single-variant testing misses.

Foundation models could extend pharmacogenomic prediction beyond
established single-gene associations. Variant effect scores across HLA
genes, drug metabolism enzymes, and immune-related loci provide features
reflecting the patient's overall pharmacogenetic landscape. These
features aggregate into a polygenic adverse event risk score that
captures contributions from many variants rather than relying on
individual high-effect alleles. Combined with clinical features (renal
function affecting drug clearance, concomitant medications with
interaction potential, prior adverse reaction history), the model
predicts adverse event probability specific to the proposed drug.

The validation challenge is severe. Serious adverse drug reactions are
rare, making endpoint ascertainment difficult and underpowered.
Case-control designs enriched for adverse events may overestimate model
performance compared to prospective deployment. Multi-site validation
across healthcare systems with different prescribing patterns and
population ancestry compositions is essential.

Clinical implementation requires integration at the point of
prescribing, providing actionable information when drug selection
decisions are being made. This argues for pre-computed pharmacogenomic
profiles that alert at order entry rather than reactive testing after a
prescription is written. The interpretability requirement is
particularly acute: clinicians must understand why a model flags a
patient as high-risk for a specific drug to make informed risk-benefit
decisions.

\section{Translation Checklist}\label{translation-checklist}

Teams translating foundation model-based risk tools into clinical
practice navigate a complex landscape of technical, clinical,
regulatory, and operational requirements. The following considerations
distill this chapter's themes into actionable guidance.

Problem definition comes first. The outcome, time horizon, target
population, and intended clinical action must be precisely specified
before model development begins. A model designed to screen broadly
differs fundamentally from one designed to guide treatment selection in
high-risk patients. The clinical action the prediction will trigger
should be concrete: initiate therapy, intensify surveillance, refer for
specialist evaluation, or adjust dosing.

Evidence generation spans the validation hierarchy. Internal validation
establishes proof of concept but cannot support clinical deployment.
External validation across institutions and ancestries assesses
transportability and robustness. Prospective validation in deployment
conditions (ideally with interventional designs measuring outcome
impact) provides the strongest evidence for clinical benefit.

Equity evaluation is not optional. Performance and calibration must be
assessed across subgroups defined by ancestry, sex, age, and
socioeconomic factors. Disparities identified during development should
be addressed before deployment rather than documented and ignored.
Access to the tool and downstream benefits should be monitored for
equitable distribution.

Regulatory and governance requirements depend on intended use. Clinical
decision support tools may require FDA clearance or institutional
governance review. Documentation including model cards, validation
reports, and standard operating procedures should anticipate these
requirements from project inception.

Clinical workflow integration determines real-world impact. Co-design
with clinicians ensures the tool fits decision points and information
needs. Alert fatigue from excessive notifications undermines adoption.
Interpretability and uncertainty communication support appropriate trust
calibration.

Post-deployment monitoring detects drift and maintains performance.
Input and output distributions, performance metrics, and incident
reports should be tracked continuously. Clear triggers for investigation
and potential retraining should be established. Governance structures
should review models periodically and authorize updates or deprecation
as needed.

If genomic foundation models are to realize their promise in clinical
medicine, success will depend less on model scale and more on rigorous
translation: careful problem selection, comprehensive evidence
generation, stakeholder engagement, and vigilant post-deployment
stewardship. The representational advances that foundation models
provide become valuable only when they flow through validated,
equitable, well-integrated clinical tools into decisions that improve
patient outcomes. The next chapter examines a specific high-stakes
application of this principle: using foundation models to interpret
individual genetic variants and guide the diagnostic odyssey for
patients with suspected genetic disease.

\section{Translation as the Test}\label{translation-as-the-test}

If genomic foundation models are to realize their promise in clinical
medicine, success will depend less on model scale and more on rigorous
translation. The checklist above condenses requirements spanning problem
definition, evidence generation, equity evaluation, regulatory
compliance, workflow integration, and post-deployment monitoring. Each
stage introduces opportunities for failure; models that clear all
hurdles are rare, and models that skip stages fail in deployment
regardless of their technical sophistication.

The representational advances that foundation models provide become
valuable only when they flow through validated, equitable,
well-integrated clinical tools into decisions that improve patient
outcomes. A pathogenicity score with state-of-the-art discrimination
adds nothing to care if it reaches clinicians at the wrong moment, in
the wrong format, without appropriate uncertainty communication. A risk
prediction that performs excellently on average but fails systematically
for underrepresented populations may widen health disparities rather
than narrow them. The technical capabilities examined throughout this
book are necessary but not sufficient for clinical impact.

The next chapter examines a specific high-stakes application of these
translation principles: using foundation models to interpret individual
genetic variants and guide the diagnostic odyssey for patients with
suspected rare genetic disease. Where risk prediction addresses
population-level stratification, rare disease diagnosis addresses
individual-level variant interpretation, with different evidence
requirements, clinical workflows, and definitions of success.

\chapter{Rare Disease Diagnosis}\label{sec-rare-disease}

A four-year-old presents with developmental delay, hypotonia, and
seizures that began at eighteen months. Standard metabolic testing
reveals nothing. A gene panel for epilepsy returns negative. The
neurologist orders whole-exome sequencing, which identifies 23,847
single nucleotide variants and 1,203 small insertions or deletions
compared to the reference genome. Somewhere in this list of
approximately 25,000 variants may lie the molecular explanation for this
child's condition. The clinical team must reduce this number to a
handful of candidates for expert review, ideally to a single variant or
gene that explains the phenotype and guides management. This is the
diagnostic odyssey: the gap between sequencing a genome and
understanding what it means for a patient.

This scenario plays out thousands of times daily across clinical
laboratories worldwide. Rare diseases collectively affect approximately
300 million people globally, yet each individual condition may have only
a handful of known cases. Over 7,000 rare diseases have been
characterized, the majority following Mendelian inheritance patterns
where single genes exert large effects. For these patients, identifying
the causal variant can end years of uncertainty, enable accurate genetic
counseling for families, and increasingly guide targeted therapies. The
technical capacity to sequence genomes has advanced enormously; the
interpretive bottleneck has not kept pace. Variant interpretation
remains largely manual, relying on clinical geneticists and laboratory
directors who cannot scale to meet demand.

Foundation models offer new tools for this interpretive challenge. As
detailed in Chapter~\ref{sec-vep-fm}, models like AlphaMissense provide
proteome-wide estimates of missense pathogenicity, while regulatory
models like Enformer predict variant effects on gene expression across
tissues. These computational predictions become one line of evidence
within structured interpretation frameworks. This chapter examines how
foundation model outputs integrate into clinical variant interpretation
workflows: from initial prioritization that reduces 25,000 variants to
dozens, through ACMG-AMP evidence classification that structures expert
review, to family-based analysis that leverages inheritance patterns,
and laboratory validation that confirms computational predictions. The
goal is not prediction for its own sake but actionable clinical insight:
which variant explains this patient's disease, and what should we do
about it?

\section{The Variant Prioritization
Funnel}\label{the-variant-prioritization-funnel}

Clinical variant interpretation operates through progressive filtering,
narrowing tens of thousands of candidates to a manageable set for expert
review. Each filtering step applies different types of evidence, and
foundation models contribute at multiple stages.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-rare-disease-funnel}{[}Essential{]} Inverted funnel
with filtering stages. Stages with numbers: (1) Raw variants
(\textasciitilde25,000); (2) Quality filtered (\textasciitilde22,000);
(3) Population frequency filtered (\textasciitilde500-1,000); (4)
Consequence filtered (\textasciitilde100-200); (5) Foundation model
scored (\textasciitilde20-50); (6) Expert review candidates
(\textasciitilde5-10). Annotations: Percentage removed, time/compute
cost, where FMs contribute most. Key insight: FMs contribute at scoring
stage, after basic filtering, before expert review.}

\end{figure}%

\subsection{Quality and Technical
Filters}\label{quality-and-technical-filters}

The first filter removes variants that are likely technical artifacts
rather than true biological variation. Sequencing depth below 20x,
strand bias exceeding established thresholds, and clustering of variants
in repetitive regions all raise suspicion of false positives. Variant
calling pipelines like GATK and DeepVariant (Chapter~\ref{sec-ngs})
produce quality scores that guide this initial triage. As discussed in
Section~\ref{sec-calibration}, these confidence estimates require
careful calibration; systematic miscalibration in specific genomic
contexts propagates directly into interpretation, creating blind spots
where uncertain calls masquerade as confident ones or vice versa.
Variants failing quality thresholds are removed before any biological
interpretation begins.

For trio analysis (proband plus both parents), Mendelian inheritance
consistency provides an additional quality check. A variant called
heterozygous in the child should appear in at least one parent unless it
arose de novo. Widespread Mendelian inconsistencies indicate sample
swaps, contamination, or systematic calling errors that must be resolved
before interpretation proceeds.

\subsection{Population Frequency
Filters}\label{population-frequency-filters}

Variants common in the general population are unlikely to cause rare,
severe disease. If a variant appears in 1\% of gnomAD individuals, it
cannot plausibly explain a condition affecting one in 100,000 people
under a dominant model. Frequency thresholds depend on inheritance mode
and disease prevalence: dominant conditions with complete penetrance
require extremely rare variants (often absent from population
databases), while recessive conditions can tolerate higher carrier
frequencies.

The gnomAD database provides allele frequencies across over 800,000
individuals from diverse ancestries. Applying a frequency threshold of
0.01\% for dominant conditions and 1\% for recessive carriers typically
removes 95\% or more of variants from consideration. Ancestry-matched
frequencies matter: a variant rare in European populations may be common
in African or East Asian populations, and global frequency alone can be
misleading.

\subsection{Consequence and Gene
Filters}\label{consequence-and-gene-filters}

Predicted functional consequence shapes prioritization. Loss-of-function
variants (frameshift, nonsense, canonical splice site) in genes
intolerant to haploinsufficiency receive immediate attention. Missense
variants require additional assessment, as most are benign. Intronic and
intergenic variants have historically been deprioritized, though
foundation models are beginning to identify functional noncoding
variants with greater precision.

Gene-level filters incorporate prior knowledge. Curated gene panels for
specific phenotypes (such as the PanelApp epilepsy panel or
cardiomyopathy panel) restrict analysis to genes with established
disease associations. For undiagnosed cases without clear phenotype
match, broader approaches may include all OMIM disease genes or genes
with high constraint (low observed/expected loss-of-function ratios in
gnomAD).

\subsection{Foundation Model Scoring}\label{foundation-model-scoring}

After quality, frequency, and consequence filters, foundation model
predictions provide quantitative effect estimates for remaining
candidates. For missense variants, AlphaMissense scores offer
genome-wide pathogenicity estimates derived from protein structure and
evolutionary conservation. For splice-region variants, SpliceAI
predictions quantify the probability and magnitude of splicing
disruption. For regulatory variants, Enformer and related models
estimate effects on chromatin accessibility and gene expression in
relevant tissues.

These scores do not directly translate to pathogenicity classifications.
A high AlphaMissense score indicates that the protein change is likely
functionally disruptive, not that it causes a specific disease. The
clinical relevance of any functional disruption depends on the gene's
role in the patient's phenotype, the inheritance pattern, and whether
disruption of that gene produces the observed clinical features.
Foundation model scores become one input to a structured evidence
framework, not a standalone answer.

\section{ACMG-AMP Criteria and Computational
Evidence}\label{acmg-amp-criteria-and-computational-evidence}

The American College of Medical Genetics and Genomics and Association
for Molecular Pathology (ACMG-AMP) framework provides the dominant
structure for clinical variant classification. Published in 2015 and
subsequently refined through ClinGen expert panels, this framework
assigns variants to five categories: pathogenic, likely pathogenic,
variant of uncertain significance (VUS), likely benign, and benign.
Classification emerges from combining multiple evidence types, each
assigned a strength level (very strong, strong, moderate, supporting)
and direction (pathogenic or benign).

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-acmg-amp}{[}Essential{]} Evidence combination
diagram. Columns (evidence categories): Population data, Computational
predictions, Functional data, Segregation, De novo, Clinical. Rows
(evidence strength): Very Strong (PVS1), Strong, Moderate, Supporting.
Combination rules: Pathogenic criteria, Likely Pathogenic, VUS, Likely
Benign, Benign. FM contribution highlighted: PP3/BP4 box emphasized;
``Traditionally supporting strength''; arrow showing potential upgrade
for calibrated predictors. Classification distribution showing VUS
dominates.}

\end{figure}%

\subsection{Evidence Categories}\label{evidence-categories}

ACMG-AMP evidence spans several domains. Population data includes allele
frequency in controls (BA1, BS1, BS2 for benign; PM2 for pathogenic
support when absent). Computational predictions include in silico tools
predicting deleterious effects (PP3 for pathogenic support) or benign
effects (BP4 for benign support). Functional data includes
well-established functional assays demonstrating deleterious (PS3) or no
(BS3) effect. Segregation data addresses co-segregation with disease in
multiple affected family members (PP1) or lack of segregation (BS4). De
novo status assigns strong (PS2) or moderate (PM6) evidence when
parental samples are available and the variant is absent in both
parents. Clinical information incorporates specific phenotype match
(PP4) and prevalence considerations.

The framework combines these evidence types through defined rules.
Pathogenic classification requires either one very strong criterion plus
one strong, or two strong criteria, with additional supporting evidence.
Likely pathogenic requires somewhat less evidence. Most variants end up
as VUS because available evidence is insufficient for confident
classification in either direction.

\subsection{PP3 and BP4: Computational
Evidence}\label{pp3-and-bp4-computational-evidence}

Computational predictions enter the ACMG-AMP framework primarily through
PP3 (pathogenic supporting evidence from computational predictions) and
BP4 (benign supporting evidence). These criteria apply when multiple in
silico tools agree that a variant is deleterious (PP3) or benign (BP4).

The original 2015 guidelines assigned these criteria only ``supporting''
strength, reflecting appropriate caution about computational predictions
available at the time. Tools like SIFT, PolyPhen-2, and CADD had limited
accuracy and concerning circularity issues
(Chapter~\ref{sec-vep-classical}). ClinGen sequence variant
interpretation working groups have subsequently refined how
computational evidence is weighted, in some cases upgrading to moderate
strength for well-calibrated predictors in specific genes.

Foundation models raise new questions about computational evidence
strength. AlphaMissense achieves substantially higher accuracy than
traditional tools on held-out ClinVar variants and deep mutational
scanning data. Should predictions from these models receive greater
evidentiary weight? The answer is not straightforward. Higher accuracy
on aggregate benchmarks does not guarantee reliability for any
individual prediction. Gene-specific calibration matters: a model may
perform well across all genes but poorly for genes with unusual
structure or function. And the fundamental limitation remains that
computational predictions estimate functional impact, not clinical
pathogenicity.

Responsible application of foundation model predictions in ACMG-AMP
classification requires gene-specific and variant-type-specific
calibration whenever possible, explicit acknowledgment that PP3/BP4
evidence is supporting unless upgraded by expert panel guidance, use of
multiple orthogonal predictors rather than reliance on any single model,
and clear documentation of which tools were applied and how predictions
were interpreted.

\subsection{Calibrating Predictions to Evidence
Strength}\label{calibrating-predictions-to-evidence-strength}

The calibration problem is central to using foundation model predictions
clinically. A model outputs a continuous score; clinical classification
requires discrete evidence categories. How should thresholds be set, and
what strength should be assigned?

The ClinGen Sequence Variant Interpretation Recommendations address this
through the concept of odds of pathogenicity. Supporting evidence
corresponds to an odds ratio of approximately 2 (twice as likely
pathogenic as benign given this evidence). Moderate evidence corresponds
to odds of approximately 4, and strong evidence to odds of approximately
18. For a computational predictor to warrant upgrading from supporting
to moderate strength, its predictions should demonstrably achieve odds
ratios meeting these thresholds in relevant validation datasets.

For AlphaMissense and similar foundation models, published validation
shows that the highest-scoring variants (above 0.9) achieve odds ratios
exceeding the strong evidence threshold in some gene contexts. ClinGen
expert panels have begun incorporating these calibrations for specific
genes, allowing upgraded evidence strength when predictions meet defined
criteria. Clinicians should follow gene-specific expert panel
recommendations when available rather than applying uniform thresholds
across all genes.

\section{Family-Based Analysis}\label{family-based-analysis}

Rare disease interpretation rarely relies on proband sequence alone.
Family structure provides powerful additional information through
inheritance pattern constraints, de novo status determination, and
segregation analysis.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-family-analysis}{[}High{]} Three-panel scenarios.
Panel A (De Novo Detection): Trio pedigree; variant present in proband,
absent in parents; strong evidence (PS2); FM prioritizes among multiple
de novos. Panel B (Compound Het Trans): Two variants in same gene from
different parents; biallelic disruption; FM assesses severity of each.
Panel C (Compound Het Cis): Both from same parent; one functional copy
remains; recessive doesn't fit; FM identifies if either alone
sufficient. Phasing methods sidebar: Long-read, trio, statistical.}

\end{figure}%

\subsection{De Novo Variants}\label{de-novo-variants}

De novo variants arise newly in the proband and are absent in both
parents. For severe, early-onset dominant conditions, de novo mutations
are expected: affected individuals rarely reproduce, so the
disease-causing allele must arise fresh each generation. Observing a
damaging variant as de novo provides strong evidence for pathogenicity
under ACMG-AMP (PS2), often sufficient to push a candidate toward likely
pathogenic or pathogenic classification.

The informativeness of de novo status depends on the mutation rate at
that site and the expected de novo rate for the variant class. The human
germline mutation rate is approximately 1 to 1.5 new mutations per 100
million base pairs per generation. For protein-coding exons
(approximately 30 million base pairs), each individual carries roughly
one new coding variant on average. Finding a damaging de novo variant in
a candidate gene is therefore much more suspicious than finding an
inherited variant of similar predicted effect.

Foundation models assist de novo interpretation by providing effect
estimates that help prioritize among multiple de novo variants (typical
trio sequencing identifies one to three de novo coding variants) and by
identifying de novo variants in noncoding regions that might disrupt
critical regulatory elements. A de novo variant in a brain-specific
enhancer upstream of a known epilepsy gene, predicted by Enformer to
substantially reduce gene expression, warrants investigation even though
traditional pipelines might overlook noncoding de novo events.

\subsection{Compound Heterozygosity and
Phasing}\label{compound-heterozygosity-and-phasing}

Recessive diseases require biallelic disruption: both copies of the gene
must be affected for disease to manifest. When a proband carries two
different heterozygous variants in the same gene, the critical question
is whether these variants are in trans (on opposite chromosomes, leading
to biallelic disruption) or in cis (on the same chromosome, leaving one
copy functional).

Phasing determines which configuration applies. Several approaches are
available. Physical phasing through long-read sequencing directly
observes which variants occur on the same DNA molecule, providing
definitive phase information when reads span both variant positions.
Trio phasing infers phase from parental genotypes: if one variant is
inherited from the mother and one from the father, they must be in
trans. Statistical phasing uses population haplotype patterns to
estimate phase, though accuracy decreases for rare variants not
well-represented in reference panels.

For clinical interpretation, trio phasing is often the most practical
approach. If both variants are confirmed in trans and both are predicted
damaging, this supports pathogenicity under a recessive model. If both
variants were inherited from a single parent (in cis), the gene cannot
explain a recessive phenotype unless a third variant exists.

Foundation models contribute by estimating the functional severity of
each variant. A missense variant with marginal AlphaMissense score might
not warrant attention alone, but paired in trans with a clear
loss-of-function variant, the compound heterozygous combination could
produce sufficient functional disruption to cause disease.

\subsection{Segregation Analysis}\label{segregation-analysis}

In larger families with multiple affected and unaffected individuals,
segregation analysis examines whether candidate variants track with
disease status. Under a dominant model, all affected individuals should
carry the variant, and penetrance assumptions constrain how many
unaffected carriers are expected. Under a recessive model, affected
individuals should be homozygous or compound heterozygous, carriers
should be heterozygous, and unaffected non-carriers should lack the
variant entirely.

Strong segregation evidence (PP1, upgradable to strong evidence with
sufficient meioses) can substantially support pathogenicity
classification. Equally important, failure to segregate provides benign
evidence (BS4): a variant present in unaffected family members at rates
inconsistent with the proposed inheritance model is unlikely to be
causal.

Segregation analysis requires accurate pedigree information, confirmed
sample identities, and careful consideration of age-dependent penetrance
and phenocopies. A variant might be present in an unaffected young
relative who will develop disease later, or an affected relative might
have a different etiology (phenocopy). These complexities require
clinical judgment that no computational model can replace.

\section{Somatic Variant Interpretation in
Cancer}\label{somatic-variant-interpretation-in-cancer}

Cancer genomics presents distinct interpretive challenges. Tumor genomes
accumulate mutations throughout malignant evolution, creating a mix of
driver mutations (those conferring selective advantage and contributing
to cancer development) and passenger mutations (bystanders with no
functional consequence). The interpretive task shifts from identifying
variants causing inherited disease to identifying variants driving tumor
biology and predicting therapeutic response.

\subsection{Germline versus Somatic
Distinction}\label{germline-versus-somatic-distinction}

Cancer sequencing must distinguish germline variants (present in all
cells, inherited or de novo) from somatic variants (acquired in the
tumor lineage). Tumor-only sequencing cannot make this distinction
reliably, as rare germline variants may be mistaken for somatic events.
Paired tumor-normal sequencing, comparing tumor to a non-malignant
sample from the same patient, enables confident somatic variant
identification.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-somatic-germline}{[}High{]} Two-column comparison.
Column 1 (Germline): Present in all cells; question: explains inherited
disease?; framework: ACMG-AMP pathogenicity; FM role: functional impact;
clinical: genetic counseling, family screening, prevention. Column 2
(Somatic): Acquired in tumor; question: driver? therapy response?;
framework: recurrence, functional impact, biomarkers; FM role: driver vs
passenger; clinical: treatment selection, prognosis. Key distinctions:
Same variant, different implications; BRCA1 example. Practical
confusion: Tumor-only sequencing cannot distinguish.}

\end{figure}%

This distinction has direct clinical implications. A germline pathogenic
variant in \emph{BRCA1} indicates hereditary cancer predisposition
affecting the patient and potentially their family members, warranting
genetic counseling and possibly risk-reducing interventions. A somatic
\emph{BRCA1} mutation arose in the tumor and has no implications for
inherited risk, though it may still predict response to PARP inhibitors.

\subsection{Driver Classification}\label{driver-classification}

Among somatic mutations, identifying drivers requires different evidence
than germline pathogenicity assessment. Recurrence across independent
tumors suggests selective advantage: if \emph{BRAF} V600E appears in
50\% of melanomas, this frequency far exceeds what chance would predict,
implying functional importance. Databases like COSMIC catalog somatic
mutation frequencies across cancer types, enabling recurrence-based
prioritization.

Functional impact predictions from foundation models apply somewhat
differently in the somatic context. A missense variant predicted highly
damaging by AlphaMissense in a tumor suppressor gene suggests loss of
function consistent with a driver role. The same prediction in an
oncogene might indicate loss of normal regulation, potentially
activating rather than inactivating the protein. Interpretation must
consider the gene's role (oncogene versus tumor suppressor) and the
specific functional consequence of the variant.

Tumor mutational burden provides context for individual variant
interpretation. Hypermutated tumors (from mismatch repair deficiency or
POLE mutations) may carry thousands of coding mutations, making it
difficult to identify drivers against this noisy background. In such
cases, restricting attention to known hotspots, truncating mutations in
tumor suppressors, and variants with strong functional predictions helps
prioritize the likely relevant events.

\subsection{Therapeutic Biomarkers}\label{therapeutic-biomarkers}

Somatic variant interpretation increasingly focuses on therapeutic
implications. Specific variants predict response to targeted therapies:
\emph{EGFR} exon 19 deletions and L858R mutations predict erlotinib
response in lung cancer; \emph{BRAF} V600E predicts vemurafenib response
in melanoma; \emph{PIK3CA} mutations indicate alpelisib benefit in
breast cancer. These associations derive from clinical trials
demonstrating differential response by mutation status.

Foundation models do not directly predict therapeutic response, as they
lack the clinical outcome data that would be required. Their
contribution is in characterizing novel variants in known therapeutic
target genes. A patient whose tumor carries an unusual \emph{EGFR}
mutation not previously characterized might be evaluated using
structural models and effect predictions to estimate whether the
mutation likely preserves the drug-binding site and confers similar
dependency as canonical sensitizing mutations. Such analyses are
hypothesis-generating rather than definitive but can inform clinical
decision-making when direct trial evidence is unavailable.

\section{Laboratory Validation}\label{laboratory-validation-1}

Computational predictions, however accurate, remain predictions.
Functional assays provide direct experimental evidence of variant
effects, and ACMG-AMP appropriately weights functional data (PS3 for
damaging functional effect, BS3 for no functional effect) as strong
evidence when assays are well-validated.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-functional-validation}{[}High{]} Circular feedback
loop. Steps: (1) Computational prediction (FM scores variants); (2)
Prioritization for testing (select VUS where functional data resolves);
(3) Functional assay (protein function, splicing, regulatory); (4)
Result integration (PS3/BS3 evidence; classification update); (5) Model
refinement (functional data improves training). Example pathway:
AlphaMissense high impact â†’ enzymatic assay â†’ loss confirmed â†’ PS3 â†’
Likely Pathogenic â†’ training data. Key insight: FM predictions are
hypotheses; validation definitive but can only test limited variants.}

\end{figure}%

\subsection{Types of Functional
Assays}\label{types-of-functional-assays}

Different variant types require different assay approaches. For missense
variants, protein function assays measure specific biochemical
activities of the mutant protein: enzyme activity, DNA binding,
protein-protein interactions, or cellular phenotypes in model systems.
Deep mutational scanning systematically characterizes all possible amino
acid substitutions at each position in a protein, creating comprehensive
functional maps. These maps enable immediate lookup of functional
effects for any observed missense variant, though coverage remains
incomplete across the proteome.

For splicing variants, minigene assays clone genomic regions containing
the variant into expression vectors and measure splicing patterns in
cultured cells. RNA sequencing from patient tissue (when accessible)
directly observes whether aberrant splicing occurs in vivo. SpliceAI
predictions can be validated by these direct measurements, establishing
whether computational predictions match experimental reality for
specific variants.

For regulatory variants, reporter assays measure whether
variant-containing regulatory elements drive appropriate expression
patterns. Massively parallel reporter assays (MPRAs) enable testing
thousands of variants simultaneously, generating the training data that
informs foundation model development while also providing direct
validation for specific variants of clinical interest. CRISPR-based
approaches can introduce variants into endogenous genomic contexts
rather than artificial reporter constructs, providing more
physiologically relevant readouts.

\subsection{Integrating Functional
Evidence}\label{integrating-functional-evidence}

Functional data enters ACMG-AMP classification through PS3 (strong
pathogenic evidence from functional studies showing deleterious effect)
and BS3 (strong benign evidence from functional studies showing no
effect). The strength assignment depends on assay validation:
well-established assays measuring physiologically relevant endpoints
warrant strong evidence, while novel or less-validated assays may
warrant only moderate or supporting strength.

ClinGen has developed detailed recommendations for functional evidence
evaluation. The specific gene and disease mechanism should guide assay
selection. Controls (known pathogenic and known benign variants) should
be included to validate assay performance. The biological relevance of
the assay endpoint to the disease mechanism must be justified. These
requirements reflect appropriate caution: not all functional assays are
equally informative, and inappropriate assays can mislead
classification.

Foundation model predictions can prioritize which variants most warrant
functional follow-up. When resources limit testing to a subset of VUS,
selecting those with discordant computational predictions (high
predicted impact but uncertain clinical classification) maximizes the
information gained. Variants where functional testing might resolve
classification provide greater value than variants where classification
is already clear or unlikely to change regardless of functional results.

\subsection{Closing the VUS Loop}\label{closing-the-vus-loop}

The accumulation of variants of uncertain significance represents a
major challenge in clinical genetics. Patients receive results that
cannot be interpreted, creating anxiety and uncertainty. As more
individuals undergo sequencing, VUS prevalence grows. Systematic efforts
to resolve VUS through functional characterization could dramatically
improve the clinical utility of genetic testing.

High-throughput functional approaches offer a path forward. Saturation
genome editing applies CRISPR to introduce every possible
single-nucleotide variant at clinically important loci, then measures
functional consequences through cellular phenotypes or growth selection.
These experiments generate comprehensive functional maps that can
immediately classify any observed variant. The Brotman Baty Institute's
ongoing efforts for \emph{BRCA1}, mismatch repair genes, and other
clinically important loci exemplify this approach.

Foundation models trained on these functional datasets can generalize
beyond directly measured variants, predicting effects for positions or
genes not yet characterized experimentally. This creates a productive
cycle: functional data improves model training, improved models identify
high-priority variants for follow-up, and targeted experiments fill gaps
while further improving models.

\section{Practical Workflow
Integration}\label{practical-workflow-integration}

Translating foundation model capabilities into clinical practice
requires integration with existing laboratory and clinical workflows.
The technical and interpretive steps must fit within established
regulatory frameworks, electronic health record systems, and clinical
team structures.

\subsection{Laboratory Workflow}\label{laboratory-workflow}

Clinical sequencing laboratories operate under regulatory oversight
(CLIA certification, state licensure, and potentially CAP accreditation
in the United States). Validated pipelines must produce consistent,
reproducible results. Introducing new computational tools requires
formal validation demonstrating that the tool performs as expected on
representative sample types, that outputs are interpretable and
actionable by clinical staff, and that results are documented and
traceable.

For foundation model integration, validation studies should assess
performance on variants with known clinical classifications, compare
predictions to existing tools to understand concordance and discordance,
evaluate performance across variant types (missense, splice, regulatory)
and gene categories, and document threshold selection and evidence
strength assignment.

Laboratory information management systems must capture foundation model
predictions alongside other variant annotations. Reports to clinicians
should clearly indicate the role of computational evidence, the specific
tools applied, and the evidence strength assigned. Overreliance on
computational predictions, or failure to communicate their limitations,
risks inappropriate clinical decisions.

\subsection{Clinical Decision-Making}\label{clinical-decision-making}

Variant interpretation reports ultimately inform clinical decisions:
whether to pursue additional testing, what genetic counseling to
provide, whether to adjust medical management, and what surveillance or
prevention strategies to recommend. These decisions rest with clinicians
and genetic counselors working with patients, not with computational
algorithms.

Foundation model predictions support this process by improving the
efficiency and accuracy of variant prioritization, reducing the number
of VUS through more informative computational evidence, identifying
potentially actionable variants in previously overlooked genomic
regions, and enabling rapid assessment of novel variants not previously
observed.

The interpretive report should convey both what computational
predictions indicate and the uncertainty that remains. Clinicians must
understand that even highly accurate models make errors, that
predictions may be less reliable for underrepresented populations or
unusual variant types, and that computational evidence is one component
of a comprehensive assessment. Shared decision-making with patients
should acknowledge these limitations while conveying the best current
understanding.

\subsection{Regulatory and Ethical
Considerations}\label{regulatory-and-ethical-considerations}

Clinical use of foundation model predictions raises regulatory questions
addressed more fully in Chapter~\ref{sec-future}. In the United States,
laboratory-developed tests using computational predictions fall under
CLIA oversight, with additional FDA jurisdiction increasingly asserted
for software as a medical device. European regulations under IVDR impose
their own requirements. Laboratories must navigate this evolving
landscape while ensuring that clinical utility keeps pace with
regulatory compliance.

Equity concerns deserve particular attention. Foundation models trained
predominantly on data from individuals of European ancestry may perform
less well for other populations. If computational predictions
systematically provide less informative evidence for underrepresented
groups, this could widen existing disparities in diagnostic yield and
clinical care. Ongoing efforts to diversify training data and evaluate
performance across ancestries are essential for equitable clinical
deployment.

\section{The Interpretive
Partnership}\label{the-interpretive-partnership}

Foundation models transform variant interpretation by providing more
accurate, comprehensive, and fine-grained predictions than previous
computational approaches. Missense pathogenicity can be estimated
proteome-wide with unprecedented accuracy. Regulatory variant effects
can be predicted across tissues and cell types. Splicing disruption can
be quantified with clinical-grade precision. These capabilities
accelerate the diagnostic odyssey, enabling faster and more confident
resolution for patients who have often waited years for answers.

Yet foundation models do not replace human judgment in clinical
genetics. They do not understand phenotypes, family structures, or
therapeutic implications. They do not weigh the psychological impact of
uncertain results or navigate the ethical complexities of predictive
testing in unaffected relatives. They provide evidence that must be
integrated within clinical frameworks designed around human
decision-making, alongside family history, physical examination, prior
testing, and the accumulated wisdom of clinical experience.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-interpretive-partnership}{[}Enhancing{]} Partnership
diagram. FM Capabilities: Pattern recognition at scale, proteome-wide
prediction, cross-gene generalization, consistent scoring, speed. Human
Expert Capabilities: Clinical context, family reasoning, therapeutic
implications, ethical navigation, communication, judgment under
uncertainty. Partnership Zone: Efficient prioritization, evidence
integration, quality assurance, continuous improvement. What remains
human: Final classification, clinical communication, counseling, ethical
judgment. Key insight: FMs accelerate diagnostic odyssey; human judgment
essential for clinical decisions.}

\end{figure}%

The productive framing positions foundation models as partners in
interpretation: computational systems that handle pattern recognition at
scales beyond human capacity, freeing clinical experts to focus on
integration, communication, and the decisions where human judgment
remains essential. This partnership model, rather than replacement or
autonomy, defines the path forward for genomic foundation models in rare
disease diagnosis. The next chapter extends these interpretive
principles beyond diagnosis to therapeutic discovery, examining how
foundation models identify drug targets, predict biomarkers, and guide
precision medicine approaches that translate genetic understanding into
treatment.

\chapter{Drug Discovery}\label{sec-drug-discovery}

More than 90\% of drug candidates that enter clinical trials fail. They
fail because they targeted the wrong gene. They fail because the patient
population was too heterogeneous for a single mechanism to succeed. They
fail because preclinical models predicted efficacy that did not
translate to humans. They fail because safety signals emerged only at
scale. The pharmaceutical industry spends billions of dollars on
programs that will not produce approved therapies, and the cost of this
attrition propagates to the drugs that do succeed: higher prices, longer
development timelines, and reduced investment in diseases with smaller
markets. The fundamental bottleneck is not generating drug candidates
but identifying, early in the process, which targets and which patients
offer the highest probability of success.

Human genetics provides one of the strongest predictors of clinical
success. Targets with genetic support succeed in trials at roughly twice
the rate of targets without such support; human mutations that cause
phenotypes resembling the therapeutic goal provide direct evidence that
modulating the target affects the disease. Yet exploiting this signal is
harder than it sounds. Genome-wide association studies have identified
thousands of disease-associated loci, but most point to noncoding
regions where the causal gene is unclear. Even when a gene is
implicated, the direction of effect often remains ambiguous. Translating
a statistical association into a validated therapeutic target typically
requires a decade of work and hundreds of millions of dollars in
follow-up studies.

Genomic foundation models offer a path through this translational
bottleneck. Rather than treating each target identification program as a
de novo effort, foundation models encode biological knowledge learned
across millions of sequences into reusable representations. A variant
effect predictor can score any missense mutation's functional impact
without task-specific retraining. A regulatory model can predict
expression consequences of noncoding variants across tissues. Network
models can propagate genetic signals to identify pathway relationships
invisible in single-gene analyses. This chapter examines how these
capabilities connect to drug discovery: target prioritization and
genetic validation, network-aware approaches that identify modules and
repurposing opportunities, foundation model-guided functional genomics
screens, and biomarker development for patient stratification.

\section{The Genetic Foundation of Target
Selection}\label{the-genetic-foundation-of-target-selection}

Human genetics provides uniquely causal evidence for target selection.
Unlike expression correlations or pathway membership, genetic
associations reflect the consequences of lifelong modulation of gene
activity in human populations. Multiple analyses over the past decade
have demonstrated that genetically supported targets succeed in clinical
trials at roughly twice the rate of targets without genetic evidence.
Targets implicated by Mendelian disease genetics, GWAS hits, or
functional variants show higher probabilities of success in phase II and
III trials compared to targets selected through other means.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-target-prioritization}{[}Essential{]} Pipeline
diagram. Steps: (1) GWAS summary statistics (Manhattan plot); (2)
Fine-mapping (credible sets, posteriors); (3) FM scoring (coding:
AlphaMissense, GPN-MSA; regulatory: Enformer, Borzoi); (4)
Variant-to-gene mapping (coding direct; noncoding via Hi-C, eQTL,
enhancer-gene); (5) Target ranking (genetic evidence, druggability,
safety, mechanism clarity). Output: Ranked target list with evidence,
effect sizes, fine-mapping probabilities, predicted mechanism,
constraint/druggability. Key insight: FMs enrich genetic evidence with
mechanistic context.}

\end{figure}%

This empirical observation motivates building pipelines where genetic
architecture serves as a first-class input to target identification.
Genomic foundation models extend this logic in two directions. First,
they provide richer biological context: instead of simple ``variants
near gene X,'' foundation models encode regulatory architecture,
chromatin state, three-dimensional genome interactions, cell-type
specificity, and perturbation responses. Second, they enable transfer
across diseases and modalities: a single model trained on diverse
genomic and multi-omic data can be reused for multiple diseases and
therapeutic areas, analogous to how language models transfer across
domains.

\subsection{From Variant-Level Predictions to Gene-Level
Evidence}\label{from-variant-level-predictions-to-gene-level-evidence}

Drug discovery teams rarely care about individual variants per se; they
care about genes and pathways. The fundamental challenge in target
identification is therefore aggregating variant-level information into
gene-level evidence that can guide target selection.

Consider a typical workflow. Starting from GWAS summary statistics (see
Chapter~\ref{sec-gwas}), statistical fine-mapping methods identify
credible sets of potentially causal variants at each locus.
Sequence-based foundation models then score each candidate variant for
regulatory or coding impact. Protein-centric variant effect predictors
such as AlphaMissense, GPN-MSA, and the missense components of
AlphaGenome combine protein language models, structural information, and
evolutionary conservation to assess coding variants (J. Cheng et al.
2023; Benegas et al. 2024; Z. Avsec, Latysheva, and Cheng 2025; Brandes
et al. 2023). Regulatory foundation models including Enformer, Borzoi,
and long-context DNA language models predict the consequences of
noncoding variants on chromatin accessibility, transcription factor
binding, and gene expression (Å½. Avsec et al. 2021; Linder et al. 2025;
Dalla-Torre et al. 2023; Nguyen et al. 2023).

The critical step is connecting variants to genes. For coding variants,
this mapping is straightforward: the variant lies within a gene's coding
sequence, and protein-level scores directly inform that gene's
candidacy. For noncoding variants, the mapping requires integrating
chromatin conformation data (Hi-C, promoter-capture Hi-C), enhancer-gene
predictions from models like Enformer, and expression quantitative trait
locus data that empirically links variants to gene expression changes.
Fine-mapping approaches such as MIFM can help distinguish truly causal
regulatory variants from correlated passengers, tightening the map from
GWAS locus to variant to target gene (Wu et al. 2024; Rakowski and
Lippert 2025).

Gene-level aggregation proceeds by summarizing variant effects across
all variants linked to each gene. For a given gene, this summary might
include the burden of predicted loss-of-function variants in cases
versus controls, the strongest regulatory variant effect sizes predicted
by foundation models, constraint metrics indicating the gene's
intolerance to damaging variation, and pleiotropy scores reflecting
associations with other traits that might indicate safety liabilities or
broader biological importance. From a foundation model perspective, the
core idea is to treat gene-level evidence as an aggregation problem over
high-dimensional variant embeddings. Rather than manually defining a
handful of summary statistics, variant embeddings and predicted
functional profiles can feed into downstream models that learn which
patterns matter most for disease.

\subsection{Linking Genetics to Target Safety and
Efficacy}\label{linking-genetics-to-target-safety-and-efficacy}

Classical human genetics has established several heuristics for target
selection that foundation models can reinforce and extend. Human
knockout individuals, people carrying biallelic loss-of-function
variants, provide natural experiments on the consequences of gene
inactivation. Protective variants that reduce disease risk suggest the
directionality of therapeutic intervention: partial inhibition of a
protein may be beneficial rather than harmful. Pleiotropy, meaning
associations with many unrelated traits, may signal safety liabilities
if modulating a target affects multiple physiological systems.

Foundation models sharpen these assessments. Fine-mapping methods
combined with regulatory foundation models can distinguish causal
variants from those merely in linkage disequilibrium with causal
variants. Variant effect scores from protein and regulatory models
approximate effect sizes, helping differentiate subtle modulators from
catastrophic loss-of-function mutations. Multi-task predictions across
chromatin marks, transcription factor binding, expression, and splicing
provide mechanistic hypotheses for how risk loci affect biology, moving
beyond statistical association toward functional understanding.

The output of this workflow is a ranked list of candidate targets with
structured evidence that can be compared across diseases and programs.
Each target comes annotated with the strength of genetic evidence
(effect sizes, fine-mapping probabilities), predicted mechanisms (coding
versus regulatory, affected tissues), constraint information (tolerance
to loss-of-function, essentiality), and druggability features (protein
family, structural information, existing ligands).

\section{Network-Aware Target Discovery and
Repurposing}\label{network-aware-target-discovery-and-repurposing}

Individual genes do not operate in isolation. Proteins interact in
complexes, genes participate in pathways, and regulatory networks
coordinate cellular responses. Even with excellent variant-to-gene
mapping, the biological context of a target shapes its therapeutic
potential. Network-aware approaches propagate genetic signals through
these relational structures to identify modules, bottleneck nodes, and
repurposing opportunities.

\subsection{Propagating Genetic Signals Through
Networks}\label{propagating-genetic-signals-through-networks}

The basic intuition is that GWAS signals concentrated in a pathway or
protein interaction module provide stronger evidence than isolated hits.
A single gene with modest genetic support but tight functional
connections to several strongly implicated genes may be a more
attractive target than an isolated hit with stronger statistics but
unclear biology.

Network-based methods integrate noncoding GWAS loci, regulatory
annotations, and protein-protein interactomes to identify disease genes
and evaluate drug repurposing opportunities in complex diseases. Graph
neural network architectures (see Chapter~\ref{sec-networks}) can learn
to propagate genetic evidence through interaction networks, scoring each
gene not just by its direct genetic association but by its network
context. The key methodological insight is that genes can be embedded
jointly with their network neighbors, allowing the model to capture how
genetic perturbations in one gene might affect functionally related
genes.

Foundation model representations enhance these network approaches.
Instead of representing each gene by a sparse vector of annotations,
genes can be embedded using features derived from protein language
models, regulatory foundation models, and expression-based cell state
encoders. These rich representations capture functional similarity
beyond what interaction databases alone can provide. Two genes with
similar protein language model embeddings likely share functional
properties even if no direct interaction has been catalogued.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-network-target-discovery}{[}High{]} Network
visualization. Nodes: Genes/proteins; colors: GWAS signal strength;
sizes: FM embedding similarity to disease genes. Key concepts: (1)
Direct GWAS hits (strong statistical evidence); (2) Network neighborhood
(connected to hits, may be better targets); (3) Pathway enrichment
(multiple signals converge); (4) Bottleneck nodes (druggable connecting
disease modules). Repurposing application: Drug targets marked; drugs
near disease genes in embedding space â†’ repurposing candidates. Caution:
Proximity â‰  causation; MR needed.}

\end{figure}%

\subsection{Drug Repurposing Through Shared
Representations}\label{drug-repurposing-through-shared-representations}

The same framework enables systematic drug repurposing. By representing
drugs via their targets, gene expression signatures, and phenotypic
effects, and representing diseases via their genetic architecture and
molecular signatures, models can score drug-disease pairs based on
representation similarity. If a drug's target sits near genetically
implicated genes in representation space, or if the drug's expression
signature opposes the disease signature, that drug becomes a repurposing
candidate.

Network proximity provides one concrete operationalization: drugs whose
targets are enriched near disease-risk genes, as measured by network
diffusion or embedding similarity, may have therapeutic potential for
that disease. Several retrospective analyses have found that such
proximity predicts reduced disease incidence among users of particular
drugs, though prospective validation remains limited.

The caution here is fundamental: representation similarity is not
causation. A drug that appears near disease genes in embedding space
might act through that mechanism, or the association might reflect
confounding by indication, survivorship bias, or other artifacts of
observational data. Network-based repurposing generates hypotheses;
Mendelian randomization, natural experiments, and prospective trials
must test them.

\section{Drug-Target Interaction
Prediction}\label{drug-target-interaction-prediction-1}

Beyond identifying disease-relevant targets, foundation models can
predict which molecules might modulate those targets. Drug-target
interaction prediction sits at the interface between genomic and
chemical foundation models, using biological representations to inform
molecular design decisions.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-drug-target-prediction}{[}High{]} Dual-encoder
architecture. Components: (1) Target Encoder (protein sequence â†’ ESM-2 â†’
protein embedding capturing structure, function, binding); (2) Molecule
Encoder (SMILES/graph â†’ chemical FM â†’ molecule embedding); (3)
Interaction Predictor (combine embeddings â†’ predict binding affinity).
Applications: Novel target screening, off-target prediction, selectivity
profiling. Proteome-scale visualization: 2D protein embeddings; drug
binding profile heatmap overlay; intended target and off-targets. Safety
integration: Off-targets with CV expression flagged. Key insight: FM
embeddings enable binding prediction without experimental structures.}

\end{figure}%

\subsection{Representing Targets for Binding
Prediction}\label{representing-targets-for-binding-prediction}

Traditional drug-target interaction methods rely on sequence similarity,
structural docking, or chemical fingerprint matching. Foundation model
approaches replace these hand-crafted features with learned
representations. Protein language model embeddings from ESM-2 or similar
architectures capture evolutionary and structural information that
correlates with binding site properties
(\textbf{lin\_evolutionary-scale\_2023?}). Ligand representations from
chemical foundation models encode molecular properties relevant to
binding affinity and selectivity.

The prediction task becomes: given a protein embedding (derived from a
protein language model) and a molecule embedding (derived from a
chemical language model or graph neural network), predict binding
affinity or interaction probability. These models can be trained on
large databases of known drug-target interactions and binding
affinities, then applied to predict interactions for novel targets or
molecules.

For genomics-focused applications, the protein representation is the
critical contribution. A target identified through genetic validation
can be immediately embedded using protein foundation models, enabling
binding prediction without waiting for experimental structures or
extensive biochemical characterization. This acceleration is
particularly valuable for understudied targets where structural data is
sparse.

\subsection{Selectivity and Off-Target
Prediction}\label{selectivity-and-off-target-prediction}

The same framework extends to selectivity prediction. By comparing a
drug's predicted binding across all proteins in a proteome-scale
embedding space, models can flag potential off-target interactions. A
compound predicted to bind its intended target but also showing high
affinity for kinases with cardiovascular expression, for example, might
warrant additional safety characterization before advancement.

Foundation model representations capture protein family relationships
and binding site similarities that inform off-target predictions. Two
proteins with similar embeddings likely share structural features that
could bind similar molecules. This information, combined with tissue
expression data and phenome-wide association data (linking genes to
thousands of traits), enables preliminary safety profiling before
expensive preclinical experiments.

\section{Toxicity Prediction from Genomic
Context}\label{toxicity-prediction-from-genomic-context}

Safety failures represent a major cause of drug attrition, particularly
in late-stage development where failures are most expensive. Genomic
information provides several routes to earlier toxicity prediction.

\subsection{Genetic Evidence of Target
Liabilities}\label{genetic-evidence-of-target-liabilities}

Human genetic data offers direct evidence of target-related toxicity. If
loss-of-function variants in a target gene associate with adverse
phenotypes in biobank populations, those phenotypes may emerge as
on-target toxicities during therapeutic inhibition. Phenome-wide
association studies across biobanks link genes to thousands of traits,
from laboratory values to disease diagnoses to imaging features. A
target strongly associated with QT prolongation, hepatotoxicity markers,
or nephrotoxicity phenotypes warrants careful safety evaluation.

Foundation models enhance this analysis by providing more accurate
variant effect predictions (distinguishing true loss-of-function from
benign variants) and by integrating across evidence types. A gene might
show modest individual associations with several safety-relevant traits
that, when aggregated using foundation model representations, reveal a
concerning pattern.

\subsection{Expression-Based Toxicity
Prediction}\label{expression-based-toxicity-prediction}

Tissue expression patterns inform toxicity risk. A target expressed
highly in hepatocytes poses greater hepatotoxicity risk than one
expressed primarily in the target tissue. Single-cell foundation models
(see Chapter~\ref{sec-single-cell}) provide cell-type-resolved
expression information, enabling predictions about which cell types
might be affected by target modulation.

More sophisticated approaches use perturbation-response models trained
on CRISPR screens and drug treatment data. Given a target knockdown or
drug treatment, these models predict transcriptomic responses across
cell types. If the predicted response signature resembles known toxicity
signatures (mitochondrial stress, DNA damage response, inflammatory
activation), that prediction informs safety risk assessment.

\subsection{Integrating Genomic Context with Chemical
Properties}\label{integrating-genomic-context-with-chemical-properties}

Ultimate toxicity prediction requires integrating target information
with compound properties. The same molecule might be safe or toxic
depending on its selectivity profile, metabolism, and tissue
distribution. Foundation models provide the biological context (target
properties, off-target predictions, expression patterns) that
complements chemical property predictions (metabolism, reactivity,
distribution) in integrated toxicity models.

The field remains early: prospective validation of foundation model
toxicity predictions against clinical outcomes is limited. Current
utility lies in prioritizing compounds for experimental toxicity testing
and in generating hypotheses about liability mechanisms, rather than
replacing traditional safety pharmacology.

\section{Functional Genomics Screens and Perturbation
Models}\label{functional-genomics-screens-and-perturbation-models}

While human genetics offers observational evidence, drug discovery
relies heavily on perturbation experiments that directly test
hypotheses. CRISPR knockout and knockdown screens, saturation
mutagenesis of protein domains, massively parallel reporter assays
(MPRAs) of regulatory elements, and Perturb-seq experiments linking
genetic perturbations to single-cell transcriptomic responses all
generate data that both validates targets and improves models.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-perturb-seq}{[}High{]} Perturbation-response
workflow. Components: (1) Perturb-seq experiment (pooled CRISPR +
scRNA-seq; each cell has perturbation + transcriptome); (2) Perturbation
signatures (knockdown â†’ expression profile change; FM learns mapping);
(3) Disease signature (patient samples characteristic changes); (4)
Perturbation matching (find perturbations reversing disease signature;
knockdown X reverses disease â†’ X is candidate; drug Y similar to
knockdown X â†’ Y targets X mechanism). Visualization: Expression heatmap
(genes Ã— perturbations); disease signature overlay. Lab-in-the-loop:
Screen â†’ update model â†’ design next screen; active learning.}

\end{figure}%

\subsection{Designing Informative Perturbation
Libraries}\label{designing-informative-perturbation-libraries}

Traditional pooled screens use simple design rules: one guide RNA per
exon, or tiling a regulatory region at fixed spacing. Foundation models
enable smarter library design by providing priors over which
perturbations are likely to be informative.

Variant effect scores from protein foundation models can prioritize
which amino acid positions are most likely to reveal functional
differences when mutated. Positions predicted to be highly constrained
and structurally important warrant more thorough coverage than positions
predicted to be mutationally tolerant. Regulatory foundation models can
highlight which enhancer or promoter regions are predicted to have the
largest expression effects in the cell type of interest, focusing
screening effort on high-impact regions.

Beyond prioritization, foundation models can guide combinatorial design.
Model uncertainty, the degree to which a model is confident in its
predictions, identifies regions where experimental data would be most
informative. Positions where the model makes uncertain predictions are
precisely those where experimental measurement adds the most value.
Active learning strategies that select perturbations to maximize
expected information gain can dramatically improve the efficiency of
screening campaigns.

\subsection{Perturb-seq and Transcriptomic
Readouts}\label{perturb-seq-and-transcriptomic-readouts}

Perturb-seq experiments combine pooled genetic screens with single-cell
RNA sequencing, linking each perturbation to its transcriptomic
consequences. These data are exceptionally rich: rather than a single
phenotypic readout (viability, fluorescence), each cell provides a
high-dimensional expression profile reflecting how the perturbation
affected cellular state.

Foundation models trained on Perturb-seq data learn to predict
transcriptomic responses to genetic perturbations (see
Chapter~\ref{sec-single-cell} for architectural details). Given a gene
knockdown, these models predict which other genes will be up- or
down-regulated, providing a functional signature for each target.
Similar signatures suggest similar biology; divergent signatures suggest
distinct mechanisms even for targets in the same pathway.

The drug discovery application is perturbation matching. Given a disease
state characterized by a transcriptomic signature (perhaps derived from
patient samples or disease models), foundation models can identify
perturbations whose predicted response signature would move the system
toward a healthier state. If knocking down gene X reverses the disease
signature, X becomes a candidate therapeutic target. If treating with
drug Y produces a signature similar to knocking down gene X, Y becomes a
candidate molecule for that mechanism.

\subsection{Closing the Loop: Lab-in-the-Loop
Refinement}\label{closing-the-loop-lab-in-the-loop-refinement}

Perhaps the most powerful application of foundation models in functional
genomics is iterative refinement. Screen outcomes provide labeled
examples that can fine-tune sequence-to-function models for the specific
biological context of interest.

Consider an MPRA that assays thousands of enhancer variants for their
effects on reporter gene expression in a disease-relevant cell type.
These sequence-activity pairs directly supervise expression-prediction
foundation models, dramatically improving their accuracy for that locus
and tissue. The refined model then makes better predictions for the next
round of experiments, suggesting which additional variants would be most
informative to test.

This lab-in-the-loop cycle accelerates discovery while improving model
accuracy in disease-relevant regions of sequence space. Foundation
models provide the prior (general knowledge about sequence-function
relationships); experiments provide the likelihood (specific
measurements in the system of interest); and the posterior (updated
model) makes better predictions for subsequent experiments.

\section{Biomarker Development and Patient
Stratification}\label{biomarker-development-and-patient-stratification}

Even when a target is well validated, many programs fail in clinical
trials because the right patients were not enrolled, the right endpoints
were not measured, or the treatment effect was diluted across a
heterogeneous population. Foundation model representations provide new
tools for defining and validating biomarkers.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-biomarker-pipeline}{[}Enhancing{]} Pipeline from
features to trial. Stages: (1) FM features (variant embeddings,
regulatory predictions, pathway scores; multi-omic); (2) Signature
development (supervised learning on retrospective cohorts; predict
response, toxicity, progression); (3) Validation (external cohorts,
cross-ancestry, calibration); (4) Trial enrichment (select likely
responders; smaller trial, higher effect); (5) Companion diagnostic
(regulatory pathway; clinical deployment alongside therapy). Example:
Genomic + transcriptomic signature predicts PARP inhibitor response; 3Ã—
efficiency improvement.}

\end{figure}%

\subsection{From Polygenic Scores to Foundation Model
Features}\label{from-polygenic-scores-to-foundation-model-features-1}

Classical polygenic scores summarize additive effects of common variants
on disease risk. These scores have proven useful for patient enrichment
in cardiovascular and metabolic disease trials, selecting patients at
highest genetic risk who might benefit most from intervention. Deep
learning methods extend this approach by learning nonlinear
genotype-phenotype mappings that capture interactions and nonadditive
effects.

Foundation models enhance polygenic prediction in several ways. Instead
of using raw genotypes as inputs, models can use variant effect scores,
regulatory predictions, or gene-level embeddings derived from foundation
models. This captures biological context that simple genotypes miss.
Models trained on variant embeddings rather than binary genotype calls
can capture subtle differences between variants at the same position,
distinguishing a mildly damaging missense from a severely damaging one
even when both are heterozygous.

Transfer across populations represents a particular strength. Foundation
models trained on diverse genomes provide representations that may
generalize more robustly across ancestries than models trained on
individual cohorts. Fine-mapping-aware approaches that use foundation
model features can reduce dependence on linkage disequilibrium patterns
that vary across populations, potentially improving the portability of
genetic risk predictors.

\subsection{Multi-Omic Biomarker
Discovery}\label{multi-omic-biomarker-discovery}

Beyond germline genetics, drug development increasingly leverages
somatic genomics, transcriptomics, proteomics, and other molecular
readouts. Tumor sequencing combined with expression profiling
characterizes the molecular landscape of each patient's cancer.
Single-cell multiome data (RNA plus ATAC) reveal cell-state
heterogeneity that bulk assays miss.

Foundation models trained on these data types provide embeddings that
capture patient-level molecular profiles. Set-based architectures that
treat each patient's genomic features as a set (rather than assuming
fixed feature positions) can handle the heterogeneity of tumor genomes,
where different patients have different mutations. Gene regulatory
network inference models trained on atlas-scale single-cell data can
extract pathway activity scores that serve as mechanistically
interpretable biomarkers.

The key shift is that biomarkers are no longer limited to a handful of
hand-picked variants or expression markers. They become functions over
high-dimensional genomic and multi-omic embeddings, learned in a
data-driven way yet grounded in biological priors from foundation
models. A biomarker might be a region of embedding space corresponding
to patients with particular molecular subtypes, defined by the model
rather than by manual curation.

\subsection{Trial Design and Endpoint
Selection}\label{trial-design-and-endpoint-selection}

Foundation model predictions inform trial design at multiple stages.
Patient enrichment uses genetic risk scores or molecular subtypes to
select patients most likely to respond, increasing statistical power and
reducing required sample sizes. Adaptive designs use intermediate
biomarker responses to modify randomization or dosing during the trial.
Endpoint selection uses molecular signatures to define pharmacodynamic
biomarkers that indicate target engagement, supporting dose selection
and early efficacy signals.

Regulatory agencies increasingly accept genomic biomarkers for patient
selection in oncology and are developing frameworks for other
therapeutic areas. The challenge is validation: demonstrating that
foundation model predictions actually stratify patient outcomes requires
prospective trials or well-designed retrospective analyses with
appropriate controls for confounding.

\section{Industry Workflows and
Infrastructure}\label{industry-workflows-and-infrastructure}

For pharmaceutical and biotechnology organizations, the challenge is not
whether they can access a foundation model but how to integrate these
models into existing data platforms, governance structures, and
decision-making processes.

\subsection{Building Model
Infrastructure}\label{building-model-infrastructure}

In mature organizations, foundation models should be treated as shared
infrastructure rather than ad hoc scripts developed by individual
project teams. A well-organized model catalog contains DNA language
models (Nucleotide Transformer, HyenaDNA, GENA-LM), sequence-to-function
models (Enformer, Borzoi), and variant effect predictors (AlphaMissense,
GPN-MSA, CADD) with documented capabilities, limitations, and
appropriate use cases (Dalla-Torre et al. 2023; Nguyen et al. 2023;
Fishman et al. 2025; Å½. Avsec et al. 2021; Linder et al. 2025; J. Cheng
et al. 2023; Benegas et al. 2024; Schubach et al. 2024).

Feature services provide centralized APIs that accept variants, genomic
intervals, or genes as input and return embeddings, predicted functional
profiles, or risk features. Centralization enables consistency across
programs and avoids redundant computation. Logging and versioning ensure
that analyses can be reproduced even as models and data evolve.

Data governance maintains clear separation between models trained on
public data versus sensitive internal cohorts. Internal data, including
proprietary clinical trial data, patient samples, and collaborator
contributions, requires careful handling. Guardrails define where
internal data can be used for fine-tuning and how resulting models can
be shared or published.

\subsection{Strategic Choices: Build, Buy, or
Fine-Tune}\label{strategic-choices-build-buy-or-fine-tune}

Organizations face three strategic options when adopting foundation
models. Using external models as-is offers low upfront cost and benefits
from community benchmarking, but may not capture organization-specific
populations, assays, or therapeutic areas. A model trained primarily on
European ancestry populations may perform poorly on a company's
Asian-focused programs; a model trained on common cell lines may miss
biology relevant to rare disease indications.

Fine-tuning open-source foundation models on internal data retains
powerful general representations while adapting to local data
distributions. This approach requires computational investment and
careful privacy controls, but often provides the best balance of
generality and specificity. A company with large internal biobank data
can fine-tune a general variant effect predictor on that cohort,
improving predictions for its patient populations without sacrificing
the broad knowledge captured during pretraining.

Training bespoke internal models from scratch offers maximum control and
allows alignment of pretraining objectives with specific use cases. A
company focused on rare diseases might pretrain on sequences and
phenotypes particularly relevant to that space. The cost is substantial:
pretraining requires significant compute, data engineering, and machine
learning expertise. There is also risk of overfitting to narrow internal
datasets if the pretraining corpus is not sufficiently diverse.

In practice, most organizations adopt hybrid strategies. They start with
public foundation models for early exploration and non-sensitive
applications, gradually fine-tune on internal data as value becomes
clear, and reserve from-scratch training for cases where unique data
assets justify the investment. Lightweight model-serving infrastructure
handles latency-sensitive applications such as clinical decision
support, while heavier offline systems support large-scale research
workloads.

\subsection{Industry Context: Timelines and Decision
Gates}\label{industry-context-timelines-and-decision-gates}

Academic machine learning research optimizes benchmark performance; drug
discovery optimizes probability of clinical and commercial success under
time and resource constraints. Understanding industry context helps
foundation model practitioners contribute effectively.

Drug discovery programs progress through gates: target validation,
candidate selection, investigational new drug filing, and clinical trial
phases. Each gate requires specific evidence: biological rationale,
efficacy data, safety data, manufacturing feasibility. Foundation model
contributions must align with gate requirements. A beautiful embedding
space is valueless if it cannot be translated into evidence that
advances a program through its next gate.

Timelines matter. A prediction that takes six months to validate
experimentally may be worthless if the program decision must be made in
three months. Foundation models that enable faster experiments (through
better library design, prioritization, or interpretation) create more
value than models that provide incrementally better predictions but
require the same experimental timeline to validate.

Biotechnology companies and pharmaceutical companies operate
differently. Biotechs often focus on single programs with limited
resources, prioritizing speed and risk-taking. Pharma companies manage
portfolios across therapeutic areas, prioritizing consistency and
scalability. Foundation model infrastructure that serves one context may
not serve the other. A boutique biotech might prefer lightweight,
single-purpose models; a large pharma might invest in comprehensive
infrastructure serving dozens of programs.

\subsection{Intellectual Property and Data
Considerations}\label{intellectual-property-and-data-considerations}

Foundation models raise new questions around intellectual property, data
sharing, and regulatory expectations that organizations must navigate.

Models trained on proprietary data can be valuable assets but are
difficult to patent directly. The model architecture is typically based
on published methods; the weights reflect training data that may include
public and proprietary components. Downstream discoveries, including
specific targets, biomarkers, and therapeutic hypotheses derived from
foundation model analyses, are more clearly protectable but require
careful documentation of inventive contribution.

Collaborative model development across institutions may require
federated learning or model-to-data paradigms, especially for
patient-level data. Genomic data carries re-identification risk; sharing
raw data, even for model training, requires appropriate consent and data
use agreements. Federated approaches that train on local data without
centralizing raw information can enable multi-institutional
collaboration while respecting privacy constraints.

For regulatory submissions, foundation models used in drug development
create documentation requirements. If a model informed target selection,
patient stratification, or safety assessment, regulators may request
information about model training, validation, and performance across
subgroups. The confounding and interpretability challenges discussed in
Chapter~\ref{sec-confounding} and Chapter~\ref{sec-interpretability}
become acute when models inform pivotal decisions in drug development.
Clear documentation trails from model prediction to program decision
support regulatory review.

\section{Evaluation and Validation}\label{evaluation-and-validation}

Evaluating foundation model contributions to drug discovery requires
carefully separating model performance from scientific and clinical
validity. A model that achieves high benchmark scores may still fail to
improve drug discovery outcomes; a model with modest benchmarks may
provide actionable insights that advance programs.

\subsection{Benchmark Limitations}\label{benchmark-limitations-1}

Many published benchmarks draw targets and drugs from the same databases
used to pretrain models, creating risk of leakage that inflates
performance estimates. Repurposing success stories often rely on
retrospective data mining with limited prospective validation. The
ultimate test of a foundation model for drug discovery is whether it
identifies targets that succeed in clinical trials, a test that takes
years and confounds model contribution with countless other factors.

Confounding pervades drug discovery data. Models trained on
observational clinical and genomic data inherit confounders from those
data. Drug-disease associations learned by foundation models may reflect
treatment patterns rather than true causal relationships. Confounding by
indication (sicker patients receive different treatments), survivorship
bias (only patients who survived long enough enter certain analyses),
and healthcare access patterns all threaten validity. Genetic
instruments and careful epidemiologic designs remain essential for
causal claims that foundation model predictions cannot provide alone.

\subsection{From Prediction to
Validation}\label{from-prediction-to-validation}

Foundation model predictions are hypotheses, not conclusions. A target
ranked highly by genetic evidence and foundation model scoring still
requires experimental validation. The value of foundation models lies in
prioritizing which hypotheses to test, not in replacing experimental
testing.

Validation strategies depend on the application. Target predictions can
be validated through functional genomics screens that test whether
predicted targets affect disease-relevant phenotypes. Biomarker
predictions require retrospective validation on held-out cohorts or
prospective validation in clinical trials. Repurposing predictions
require real-world evidence analyses or prospective trials.

The timeline for validation matters. Some predictions can be tested in
weeks (cell-based assays for target validation); others require years
(clinical outcomes for biomarkers). Foundation model contributions
should be assessed on timescales relevant to drug discovery decisions,
not just immediate benchmark performance.

\section{Connections to Molecular
Design}\label{connections-to-molecular-design}

While this chapter focuses on target identification and indication
selection, foundation model representations connect downstream to
molecular design. The bridge between genomic and molecular foundation
models typically involves using target context as conditioning signals
for molecule generation. Gene-level embeddings from genomic foundation
models, reflecting genetic evidence, tissue specificity, and network
context, can condition chemistry models that propose molecules targeting
that gene.

Multi-modal foundation models jointly trained on DNA, RNA, proteins,
structures, small molecules, and phenotypic readouts learn
representations that span these modalities. Such models can predict not
just whether a molecule binds a target, but how target modulation in a
particular genetic context might affect cellular phenotypes. Closed-loop
optimization uses genomic foundation models to predict target relevance
and liability, chemistry and protein foundation models to propose
molecules, and experimental feedback to update both model types in
active learning cycles.

The detailed treatment of molecular design belongs to
Chapter~\ref{sec-design}. From a target identification perspective, the
key point is that genomic foundation models determine whether a target
is worth pursuing; downstream models then optimize how to hit it. The
investment in accurate target identification and validation pays
dividends throughout the drug discovery pipeline by ensuring that
optimization efforts focus on targets with the highest probability of
clinical success.

\section{Acceleration Through
Prioritization}\label{acceleration-through-prioritization}

Foundation models connect to drug discovery not as replacements for
experimental validation but as tools that reduce risk and focus
resources. Target discovery workflows aggregate variant-level
predictions into gene-level evidence, integrating fine-mapping, variant
effect prediction, and regulatory modeling to prioritize candidates with
strong genetic and mechanistic support. Network-aware approaches
propagate genetic signals through protein and regulatory networks to
identify druggable nodes and repurposing opportunities. Drug-target
interaction prediction uses foundation model representations to assess
binding, selectivity, and safety liabilities before synthesis.
Functional genomics screens leverage foundation models for library
design and iterative refinement. Biomarker development uses foundation
model features for patient stratification and trial enrichment.

Throughout these applications, the value proposition is acceleration and
prioritization, not automation of discovery. Foundation models help
identify the most promising targets, design the most informative
experiments, and select the patients most likely to benefit. Programs
that would have required years of hypothesis testing can reach the right
target faster. Screens that would have required exhaustive enumeration
can focus on high-priority candidates. The fundamental uncertainties of
drug development remain: targets validated genetically may still fail in
trials, predictions of binding may not translate to efficacy, and
patients selected by biomarkers may still not respond. Foundation models
reduce risk without eliminating it.

The subsequent chapter on sequence design (Chapter~\ref{sec-design})
extends these ideas from analysis to generation, where foundation models
not only evaluate existing sequences but propose new ones optimized for
therapeutic function. The transition from interpretation to design
represents the frontier where foundation models become engines for
creating biology, not just understanding it.

\chapter{Sequence Design}\label{sec-design}

The foundation models examined throughout this book predict the
consequences of sequence variation with increasing accuracy. A protein
language model estimates whether a missense variant disrupts function. A
regulatory model forecasts how a promoter mutation alters expression
across cell types. These predictive capabilities represent genuine
advances. Yet prediction alone cannot create a therapeutic protein that
nature never evolved, design a promoter that drives expression only in
diseased tissue, or engineer an mRNA vaccine against a novel pathogen.
The gap between reading genomes and writing them defines one of the
central challenges in translational biology: we can characterize
biological sequences with unprecedented resolution, but translating that
understanding into designed molecules remains largely empirical,
expensive, and slow.

This asymmetry reflects a fundamental mismatch between what evolution
produced and what therapeutics require. Evolution optimizes for
reproductive fitness over geological timescales, producing sequences
that satisfied survival constraints under ancestral conditions.
Therapeutic applications demand sequences optimized for entirely
different objectives: binding a specific epitope with high affinity,
expressing at therapeutic levels in a particular tissue, or evading
immune recognition while retaining function. The sequences we need often
lie far from natural evolutionary trajectories, in regions of sequence
space that foundation models have never observed during training.
Navigating this terra incognita requires not just accurate oracles that
score candidate sequences, but principled strategies for proposing,
testing, and refining designs where model reliability is uncertain.

Foundation models have begun to address this challenge by providing both
generative priors over plausible sequences and differentiable oracles
that guide optimization. Protein language models sample novel sequences
respecting the statistical patterns of natural proteins. Structure-aware
diffusion models generate backbones and sequences simultaneously,
enabling design of proteins with specified geometries. Regulatory
sequence models predict expression outcomes across thousands of
candidate promoters, enabling gradient-based optimization toward desired
activity profiles. When coupled with high-throughput experimental assays
in closed-loop design cycles, these capabilities are transforming
biological engineering. This chapter examines the methods, applications,
and limitations of foundation model-guided sequence design across
proteins, regulatory elements, and mRNA therapeutics.

\section{The design formalism}\label{the-design-formalism}

Sequence design inverts the standard prediction problem. Where
prediction maps from sequence to function (given sequence \(x\),
estimate property \(f(x)\)), design maps from desired function to
sequence (given target property \(y^\star\), find sequence \(x^\star\)
such that \(f(x^\star) \approx y^\star\)). This inversion is
computationally challenging because biological sequence spaces are
astronomically large. A 200-residue protein admits \(20^{200}\) possible
sequences, vastly exceeding the number of atoms in the observable
universe. Even a modest 500-base-pair regulatory element spans
\(4^{500}\) possibilities. Exhaustive enumeration is impossible;
intelligent search strategies are essential.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-design-formalism}{[}Essential{]} Two-direction
diagram. Top (Prediction Problem): Input sequence x â†’ model f\_Î¸ â†’
property f(x); arrow left to right; ``Given sequence, estimate
function.'' Bottom (Design Problem): Desired property y* â†’ search for x*
such that f(x\emph{) â‰ˆ y} â†’ optimized sequence x*; arrow right to left;
``Given function, find sequence.'' Design formulations: Optimization
argmax f\_Î¸(x), conditional generation x \textasciitilde{}
p\_Î¸(x\textbar y), constrained optimization. Scale callout: 20\^{}200
protein possibilities, 4\^{}500 regulatory; ``Exhaustive
impossible---intelligent search required.'' FM roles: Generative prior,
differentiable oracle, embedding function.}

\end{figure}%

The design objective can take several mathematical forms depending on
the application. Optimization problems seek sequences that maximize (or
minimize) a scalar objective, such as finding
\(x^\star = \arg\max_x f_\theta(x)\) where \(f_\theta\) might represent
predicted binding affinity, expression level, or stability. Conditional
generation problems sample sequences from a distribution conditioned on
desired properties, drawing \(x \sim p_\theta(x \mid y)\) where \(y\)
specifies structural constraints, functional requirements, or context.
Constrained optimization problems combine objective maximization with
explicit constraints, seeking \(x^\star = \arg\max_x f_\theta(x)\)
subject to \(c(x) \leq 0\), where constraints \(c\) might enforce GC
content limits, avoid restriction sites, or maintain similarity to
natural sequences.

Foundation models contribute to design through multiple mechanisms. As
generative priors, they assign higher probability to sequences
resembling natural biology, regularizing optimization toward plausible
regions of sequence space. As differentiable oracles, they enable
gradient-based optimization where sequence modifications are guided by
gradients of predicted properties. As embedding functions, they map
discrete sequences into continuous spaces where interpolation and
optimization become tractable. The challenge lies in searching enormous
combinatorial spaces while remaining within regimes where these
model-based estimates remain reliable.

\section{Protein design with language
models}\label{protein-design-with-language-models}

Protein language models (PLMs) trained on evolutionary sequence
databases (see Chapter~\ref{sec-protein-lm}) have emerged as powerful
tools for protein design, providing both generative sampling
capabilities and fitness estimation for candidate sequences. The success
of these approaches stems from a key insight: evolution has conducted
billions of years of experiments on protein sequence space, and PLMs
trained on the surviving sequences implicitly encode constraints on what
works.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%

\caption{\label{fig-protein-design}{[}Essential{]} Two-pathway
comparison. Pathway A (Sequence-Based): Protein LM (ProGen, ESM-2) â†’
generate by sampling/infilling â†’ screen for properties â†’ iterate with
fitness-guided selection. Pathway B (Structure-Aware): RFdiffusion
generates backbone from noise (conditioned on interface, topology,
symmetry) â†’ ProteinMPNN/ESM-IF inverse folds â†’ multiple sequences,
select by expression/stability. Comparison table: Prior knowledge, novel
structures, compute, examples. Multi-objective: Binding, stability,
expression, immunogenicity; Pareto frontier; FM oracles for each.}

\end{figure}%

\subsection{Sequence generation from language model
priors}\label{sequence-generation-from-language-model-priors}

Autoregressive protein language models such as ProGen and ProtGPT2
generate novel protein sequences by sampling tokens sequentially from
learned distributions (Madani et al. 2023; Ferruz, Schmidt, and HÃ¶cker
2022). Given a partial sequence, the model predicts probability
distributions over the next amino acid, enabling iterative extension
until a complete protein emerges. This generation process can be
unconditional (sampling from the full learned distribution) or
conditional on control signals such as protein family annotations,
organism of origin, or functional keywords.

The quality of generated sequences depends critically on how closely the
sampling distribution matches functional proteins. Sequences sampled at
low temperature (more deterministic) tend to resemble common protein
families but may lack novelty. Sequences sampled at high temperature
(more stochastic) exhibit greater diversity but risk straying into
nonfunctional regions. Practical design workflows often generate large
libraries of candidates across temperature ranges, then filter using
downstream oracles for structure, stability, or function.

Masked language models like ESM-2 support design through a different
mechanism. Rather than generating sequences de novo, these models
estimate the probability of each amino acid at each position given the
surrounding context. Design proceeds by iterative refinement: starting
from an initial sequence, positions are masked and resampled according
to model predictions, gradually shifting the sequence toward
higher-likelihood regions. This Gibbs-sampling-like procedure can be
biased toward specific objectives by combining PLM likelihoods with
scores from downstream predictors.

The key advantage of PLM-based design lies in data efficiency. Because
models are pretrained on millions of natural sequences, they generalize
to design tasks with minimal task-specific data. A PLM fine-tuned on a
few hundred functional variants can propose candidates across sequence
space, extrapolating far beyond the training examples. This contrasts
with traditional directed evolution approaches that require extensive
experimental screening to navigate sequence space.

\subsection{Structure-aware design with diffusion
models}\label{structure-aware-design-with-diffusion-models}

Structure-aware design addresses a fundamental limitation of
sequence-only approaches: proteins function through three-dimensional
structures, and sequence optimization without structural guidance may
produce sequences that fail to fold correctly. The advent of accurate
structure prediction (AlphaFold2, ESMFold) enables new design paradigms
that jointly consider sequence and structure.

RFdiffusion exemplifies this approach by generating protein backbones
through a diffusion process in three-dimensional coordinate space
(Watson et al. 2023). Starting from random noise, the model iteratively
denoises toward plausible backbone geometries, conditioned on design
specifications such as target binding interfaces, desired topology, or
symmetric assembly requirements. The resulting backbones represent novel
structures not observed in nature but predicted to be physically
realizable.

Converting designed backbones to sequences requires inverse folding
models that predict amino acid sequences likely to adopt a given
structure. ProteinMPNN and ESM-IF operate on this principle, taking
backbone coordinates as input and outputting probability distributions
over sequences predicted to fold onto that backbone (Dauparas et al.
2022; \textbf{hsu\_learning\_2022?}). ESM-IF leverages the
representations learned by ESM-2 to condition sequence generation on
structural constraints, connecting the inverse folding task directly to
the protein language model paradigm. The model can generate thousands of
candidate sequences for a single backbone, enabling selection based on
additional criteria such as expression likelihood or immunogenicity.

This two-stage pipeline (structure diffusion followed by inverse
folding) has demonstrated remarkable success in creating novel proteins.
Designed binders targeting challenging therapeutic targets, de novo
enzymes with specified active site geometries, and symmetric protein
assemblies with precise nanoscale dimensions have all been realized
experimentally. The key insight is that structure provides a powerful
intermediate representation: rather than searching directly in the vast
space of sequences, design proceeds through the more constrained space
of physically realizable structures.

\subsection{Functional conditioning and multi-objective
optimization}\label{functional-conditioning-and-multi-objective-optimization}

Real therapeutic or industrial applications rarely optimize a single
objective. A designed enzyme must not only be catalytically active but
also stable at process temperatures, expressible in the production host,
and resistant to proteolytic degradation. A therapeutic antibody must
bind its target with high affinity while avoiding off-target
interactions, maintaining solubility, and minimizing immunogenicity.
These competing demands create multi-objective optimization problems
where no single sequence optimizes all criteria simultaneously.

Multi-objective design produces Pareto frontiers of solutions
representing different trade-offs among objectives. A sequence might
achieve exceptional binding affinity at the cost of reduced stability,
while another balances moderate affinity with excellent developability
properties. Practitioners must select among Pareto-optimal solutions
based on application-specific priorities, and foundation models
increasingly support this selection by providing diverse oracles across
multiple property dimensions.

Foundation models contribute to multi-objective design in three ways.
Generative priors propose candidate sequences that satisfy basic
plausibility constraints (foldability, expressibility) before
optimization begins. Multiple differentiable oracles (for binding,
stability, immunogenicity) enable gradient-based optimization toward
Pareto frontiers. Embedding spaces support interpolation between
sequences with different property profiles, enabling exploration of
intermediate trade-offs. The combination of these capabilities makes
foundation models central to modern protein design pipelines.

\section{Regulatory sequence design}\label{regulatory-sequence-design}

Genomic foundation models trained on chromatin accessibility,
transcription factor binding, and gene expression data enable design of
synthetic regulatory elements with specified activity profiles. Unlike
protein design where the sequence-to-function mapping operates through
three-dimensional structure, regulatory design must account for the
genomic and cellular context in which elements function.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-regulatory-design}{[}High{]} Optimization workflow.
Steps: (1) Initial sequence (natural promoter or random); (2) FM
prediction (Enformer â†’ activity across cell types); (3) Gradient
computation (âˆ‚expression/âˆ‚position); (4) Sequence update (relaxed
representation, gradient step, project back); (5) Multi-objective
balancing (maximize in target tissue, minimize off-target, respect
constraints). Output examples: Cell-type-specific enhancer, inducible
promoter, compact element for gene therapy. Generative alternative
sidebar: Diffusion or autoregressive; condition on cell type labels;
diverse library for screening. Key insight: Same saliency for
interpretation runs in reverse for design.}

\end{figure}%

\subsection{Promoter and enhancer
engineering}\label{promoter-and-enhancer-engineering}

Massively parallel reporter assays (MPRAs) have generated training data
for models that predict expression levels from promoter and enhancer
sequences (Boer et al. 2020). These models learn sequence determinants
of regulatory activity, including transcription factor binding sites,
spacing constraints between elements, and context-dependent
interactions. Once trained, the same models serve as oracles for design:
by evaluating expression predictions across millions of candidate
sequences, optimization algorithms can identify synthetic regulatory
elements with desired properties.

Gradient-based design treats the sequence-to-expression model as a
differentiable function. Starting from an initial sequence, gradients of
predicted expression with respect to input positions indicate which
mutations would increase (or decrease) activity. Because sequences are
discrete while gradients are continuous, optimization requires
relaxation strategies that operate on ``soft'' sequence representations
before projecting back to discrete nucleotides. These approaches
leverage the same saliency map computations used for model
interpretation (see Chapter~\ref{sec-interpretability}), running the
analysis in reverse to guide design rather than explain predictions.

Design objectives for regulatory elements extend beyond maximizing
expression in a target context. Cell-type-specific enhancers should
drive high expression in desired tissues while remaining inactive
elsewhere. Inducible promoters should respond to specific signals while
maintaining low basal activity. Compact regulatory elements are
preferred for gene therapy applications where vector capacity is
limited. These constraints transform simple optimization into
multi-objective problems requiring careful balancing of competing
requirements.

Generative models trained directly on regulatory sequences offer an
alternative to optimization-based approaches. Autoregressive or
diffusion models learn to sample novel enhancers and promoters that
match the statistical properties of natural regulatory elements.
Conditioning on cell type labels, chromatin state annotations, or other
metadata enables generation of elements with targeted activity profiles.
The advantage of generative approaches lies in their ability to produce
diverse candidate libraries for experimental screening, rather than
converging on a single optimized sequence that may exploit model
artifacts rather than genuine biology.

\subsection{Splicing and RNA processing
elements}\label{splicing-and-rna-processing-elements}

Models trained on splicing outcomes (SpliceAI and related architectures
described in Chapter~\ref{sec-cnn}; see also Chapter~\ref{sec-rna} for
RNA-specific foundation models) enable design of sequences that modulate
RNA processing. Therapeutic applications include correcting pathogenic
splice site mutations by strengthening weak splice sites or weakening
aberrant ones, designing antisense oligonucleotides that redirect
splicing to skip exons containing disease-causing mutations, and
engineering alternative splicing outcomes to produce desired protein
isoforms.

The design space for splicing elements encompasses splice site sequences
themselves (the canonical GT-AG dinucleotides and surrounding intronic
and exonic enhancers and silencers), branch point sequences, and
auxiliary sequences that recruit splicing regulatory proteins.
Foundation models that predict splicing patterns from local sequence
context serve as oracles for evaluating candidate modifications, while
gradient-based optimization identifies changes predicted to shift
splicing toward therapeutic outcomes.

Design of splicing modulators requires particular attention to
off-target effects. The splicing code is highly context-dependent, and
sequence modifications intended to affect one splice site may
inadvertently alter recognition of others. Genome-wide splicing models
that predict effects across all splice sites provide essential
off-target assessment, flagging candidate designs that would disrupt
normal splicing at unintended locations.

\section{mRNA design and
optimization}\label{mrna-design-and-optimization-1}

The clinical success of mRNA vaccines has intensified interest in
systematic approaches to mRNA sequence design. Unlike protein or
regulatory element design where the primary challenge is achieving
desired function, mRNA design must simultaneously optimize translation
efficiency, molecular stability, immune evasion, and manufacturing
tractability. Foundation models increasingly contribute to each of these
objectives.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-mrna-optimization}{[}High{]} Multi-objective radar
chart. Objectives (axes): Translation efficiency, mRNA stability,
immunogenicity (minimize), manufacturing tractability, codon optimality.
Design elements per objective: Translation (ribosome profiling models,
avoid rare codons); Stability (UTR engineering, secondary structure);
Immunogenicity (avoid TLR motifs, modified nucleosides); Manufacturing
(GC content, complexity, yield). Trade-off visualization: Two candidate
designs on same radar; Design A (high translation, moderate stability);
Design B (balanced). UTR design callout: 5' affects ribosome
recruitment, 3' affects stability/localization. Key insight: Codon
synonymy creates vast space; FMs navigate tradeoffs.}

\end{figure}%

\subsection{Codon optimization
principles}\label{codon-optimization-principles}

The genetic code is degenerate: sixty-one sense codons encode twenty
amino acids, meaning that any protein sequence can be encoded by many
different mRNA sequences. These synonymous sequences differ in
translation efficiency, mRNA stability, and immunogenicity despite
producing identical proteins. Codon optimization exploits this
redundancy to improve therapeutic mRNA performance.

Traditional codon optimization relied on codon adaptation indices
derived from highly expressed genes in target organisms. Codons
frequently used in abundant proteins were assumed to be efficiently
translated, leading to optimization strategies that maximize use of
preferred codons. However, this approach oversimplifies the complex
relationship between codon choice and expression. Translation elongation
rate varies with codon-anticodon interactions, tRNA abundance, mRNA
secondary structure, and ribosome queuing effects. Local codon context
matters: rare codons following abundant ones may be translated
efficiently, while runs of preferred codons can cause ribosome
collisions.

Machine learning models trained on ribosome profiling data and reporter
assays have begun to capture these context-dependent effects. These
models predict translation efficiency from sequence features including
codon frequencies, local secondary structure, and amino acid properties.
Using such models as oracles, optimization algorithms can search for
mRNA sequences that maximize predicted translation while avoiding
problematic sequence features. The resulting designs often differ
substantially from simple codon-frequency optimization, incorporating
rare codons at specific positions to optimize local translation
dynamics.

\subsection{Stability engineering and UTR
design}\label{stability-engineering-and-utr-design}

mRNA stability in the cytoplasm determines the duration of protein
production and thus the dose required for therapeutic effect. Stability
is governed by multiple sequence features: the 5' and 3' untranslated
regions (UTRs) that flank the coding sequence, the presence of
destabilizing sequence motifs recognized by RNA-binding proteins, and
secondary structures that protect against or expose the molecule to
nucleases.

UTR engineering represents a particularly active area of foundation
model application. Natural UTRs contain binding sites for regulatory
proteins and microRNAs, sequences that affect ribosome recruitment, and
structures that influence mRNA localization and stability. Foundation
models trained on expression data across diverse UTR sequences learn
which features promote stability and efficient translation. Design
algorithms then search for synthetic UTRs that maximize these properties
while avoiding sequences that trigger immune recognition or rapid
degradation.

Chemical modifications of mRNA (pseudouridine, N1-methylpseudouridine,
and other nucleoside analogs) dramatically improve stability and reduce
immunogenicity. These modifications alter the sequence-function
relationship in ways that current foundation models, trained primarily
on natural RNA, may not fully capture. Emerging models that incorporate
modification information promise to enable joint optimization of
sequence and modification patterns.

\subsection{Immunogenicity
considerations}\label{immunogenicity-considerations}

Exogenous mRNA triggers innate immune responses through pattern
recognition receptors including Toll-like receptors (TLR3, TLR7, TLR8)
and cytosolic sensors (RIG-I, MDA5). While some immune activation may be
beneficial for vaccine applications, excessive inflammation limits
dosing and causes adverse effects. For protein replacement therapies
where repeated dosing is required, minimizing immunogenicity is
essential.

The immunostimulatory potential of mRNA depends on sequence features
including GC content, specific sequence motifs recognized by pattern
receptors, and secondary structures that resemble viral replication
intermediates. Foundation models that predict immunogenicity from
sequence enable design of mRNAs that evade innate immune detection.
These predictions must be balanced against other objectives:
modifications that reduce immunogenicity may also reduce translation
efficiency, creating multi-objective trade-offs that characterize mRNA
design more broadly.

\section{Antibody and vaccine design}\label{antibody-and-vaccine-design}

Antibody engineering represents one of the most commercially significant
applications of computational protein design. The modular architecture
of antibodies (framework regions that maintain structural integrity
surrounding hypervariable complementarity-determining regions that
mediate antigen recognition) creates a well-defined design problem:
optimize CDR sequences to achieve desired binding properties while
maintaining framework stability and developability.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-antibody-design}{[}High{]} Antibody-specific
pipeline. Starting point: Initial antibody from immunization/display;
known antigen target. Steps: (1) Initial characterization (affinity,
specificity, stability, developability); (2) CDR optimization (FM-guided
mutations; sample CDR variants; maintain binding, improve
developability); (3) Framework engineering (humanization, stability,
expression); (4) Lead optimization (multi-objective: affinity,
specificity, half-life, immunogenicity, manufacturability). FM
contributions: Embedding space identifies similar therapeutic
antibodies; structure prediction without experimental structure;
developability prediction (aggregation, viscosity); liability scanning
(post-translational modification, deamidation).}

\end{figure}%

\subsection{CDR optimization and
humanization}\label{cdr-optimization-and-humanization}

Antibodies discovered through animal immunization or phage display often
require optimization before therapeutic use. Non-human framework
sequences may trigger immune responses in patients, necessitating
humanization that replaces framework residues with human equivalents
while preserving antigen binding. CDR sequences may require affinity
maturation to achieve therapeutic potency or specificity optimization to
reduce off-target binding.

Foundation models support antibody optimization through multiple
mechanisms. Antibody-specific language models trained on paired heavy
and light chain sequences learn the structural and functional
constraints on CDR sequences. These models predict which mutations are
compatible with the antibody fold and which are likely to disrupt
structure. Given a parental antibody sequence, the models can propose
libraries of variants enriched for functional candidates, reducing the
experimental screening burden required to identify improved variants.

Structure-aware approaches enable more targeted design. Given a
structure of the antibody-antigen complex (determined experimentally or
predicted computationally), optimization focuses on residues at the
binding interface. Computational saturation mutagenesis predicts the
effect of every possible amino acid substitution at each interface
position, identifying combinations expected to improve affinity. These
predictions guide the construction of focused libraries that explore the
most promising region of sequence space.

\subsection{Vaccine antigen design}\label{vaccine-antigen-design}

Vaccine development increasingly employs computational design to create
immunogens that elicit protective immune responses. The challenge
differs from therapeutic protein design: rather than optimizing for
direct biological activity, vaccine antigens must be recognized by the
immune system and induce antibodies or T cells that protect against
pathogen challenge.

Foundation models contribute to vaccine design in several ways. Epitope
prediction models identify regions of pathogen proteins most likely to
be recognized by antibodies or T cells, guiding selection of vaccine
targets. Structural models predict how mutations affect epitope
conformation, enabling design of stabilized antigens that maintain
native epitope structure during manufacturing and storage. Glycan
shielding analysis predicts which epitopes will be accessible on the
pathogen surface versus hidden by glycosylation, focusing vaccine design
on exposed regions.

The rapid development of mRNA vaccines against SARS-CoV-2 demonstrated
the potential of computational approaches to accelerate vaccine design.
Structure-guided stabilization of the prefusion spike conformation,
optimization of mRNA sequences for expression and stability, and
prediction of variant effects on vaccine efficacy all benefited from
computational modeling. Future vaccine development will increasingly
integrate foundation model predictions throughout the design process.

\section{Closed-loop design--build--test--learn
cycles}\label{closed-loop-designbuildtestlearn-cycles}

Foundation models achieve their full potential when integrated into
iterative experimental workflows. The design--build--test--learn (DBTL)
paradigm treats computational predictions as hypotheses to be tested
experimentally, with results feeding back to improve both the designed
molecules and the models that guide design.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-dbtl-cycle}{[}Essential{]} Circular workflow.
Stages: (1) DESIGN (FM proposes candidates; generative sampling or
optimization; active learning selects informative candidates); (2) BUILD
(automated synthesis, library construction, QC); (3) TEST
(high-throughput assays: binding, activity, stability, expression;
multiplexed: DMS, MPRA, Perturb-seq); (4) LEARN (results update model;
improve predictions; identify failure modes). Active learning
integration: Acquisition function balances exploitation/exploration;
maximize expected information gain. Safety and oversight callouts:
Safety filters at design; human review before build; stopping criteria.
Key insight: FMs achieve full potential in iterative cycles; each round
improves molecules and predictions.}

\end{figure}%

\subsection{Active learning for efficient
exploration}\label{active-learning-for-efficient-exploration}

Experimental validation remains the bottleneck in biological design.
Even high-throughput assays can test at most thousands to millions of
variants, a tiny fraction of possible sequences. Active learning
strategies select which experiments to perform by balancing two
competing objectives: exploiting current model predictions to test
sequences likely to succeed, and exploring regions of uncertainty to
gather data that will improve the model.

Bayesian optimization provides a principled framework for this
trade-off. A surrogate model (often a Gaussian process or neural network
with uncertainty estimates) approximates the sequence-to-function
mapping. Acquisition functions such as expected improvement or upper
confidence bound combine predicted function values with uncertainty
estimates to select informative test sequences. After each experimental
round, the surrogate model is updated with new data, and the process
repeats.

Foundation models enhance active learning by providing informative
priors and features. Rather than learning sequence-to-function mappings
from scratch, surrogate models can operate on PLM embeddings that
capture evolutionary relationships and structural constraints. These
embeddings provide a meaningful notion of sequence similarity even
before any task-specific data is available, accelerating the early
rounds of optimization when labeled data is scarce.

\subsection{Integration with high-throughput
experimentation}\label{integration-with-high-throughput-experimentation}

Modern experimental platforms generate data at scales well-matched to
foundation model training. Deep mutational scanning (DMS) systematically
characterizes thousands of single-mutant variants of a protein, mapping
the functional landscape around a parental sequence. Massively parallel
reporter assays test tens of thousands of regulatory element variants in
a single experiment. CRISPR screens introduce perturbations across the
genome and measure phenotypic consequences.

These assays generate dense local maps of sequence-function
relationships that complement the global patterns captured by foundation
models. The integration is bidirectional: model predictions prioritize
which variants to include in experimental libraries, and experimental
results fine-tune models for improved accuracy in relevant sequence
neighborhoods. After several DBTL cycles, the combined system
(fine-tuned model plus accumulated experimental data) can often design
sequences that substantially outperform the parental molecule.

The design of experiments themselves benefits from computational
guidance. Rather than testing all possible single mutants, active
learning identifies the most informative subset. Rather than random
library construction, computational analysis identifies epistatic
interactions that should be explored through combinatorial variants. The
cost of DNA synthesis and high-throughput assays makes efficient
experimental design increasingly important as design ambitions grow.

\section{Validation requirements and failure
modes}\label{validation-requirements-and-failure-modes}

Computational design generates hypotheses; experimental validation
determines whether those hypotheses are correct. The gap between
predicted and observed performance represents the ultimate test of
design methods, and understanding where predictions fail is essential
for improving both models and design strategies.

\subsection{The validation hierarchy}\label{the-validation-hierarchy-1}

Designed sequences must pass through multiple validation stages before
achieving real-world impact. Computational validation confirms that
designs satisfy specified constraints and achieve predicted scores,
filtering obvious failures before synthesis. In vitro validation tests
whether designed proteins express, fold, and exhibit predicted
activities in simplified experimental systems. In vivo validation
assesses function in cellular or animal contexts where additional
complexity may reveal unanticipated problems. Clinical validation, for
therapeutic applications, determines whether designs are safe and
effective in human patients.

Success rates decline at each stage of this hierarchy. Computationally
promising designs often fail to express or fold correctly. Designs that
succeed in vitro may lose activity in cellular contexts due to incorrect
localization, unexpected degradation, or off-target interactions.
Molecules that perform well in model organisms may fail in human
clinical trials due to immunogenicity, toxicity, or pharmacokinetic
limitations. The attrition from computational design to clinical success
remains substantial, motivating continued improvement in predictive
accuracy and earlier identification of failure modes.

\subsection{Characteristic failure
patterns}\label{characteristic-failure-patterns}

Foundation model-guided design exhibits systematic failure modes that
practitioners must recognize and mitigate. Distribution shift occurs
when optimization pushes sequences into regions where model predictions
are unreliable. A model trained on natural proteins may produce
confident but incorrect predictions for designed sequences that lie far
from training data. Regularization toward natural sequence statistics
and uncertainty quantification help identify when designs have strayed
beyond reliable prediction regimes.

Mode collapse in generative models produces designs that are variants of
training sequences rather than genuinely novel molecules. When generated
sequences can be matched to close homologs in training data, the design
process has failed to create anything new. Novelty filters and diversity
requirements during generation help ensure that computational design
adds value beyond database retrieval.

Reward hacking occurs when optimization exploits model artifacts rather
than genuine sequence-function relationships. A model might predict high
expression for sequences containing spurious features that happen to
correlate with expression in training data but have no causal effect.
Ensemble methods, where designs must score highly across multiple
independently trained models, provide some protection against hacking
individual model weaknesses.

The most insidious failures involve properties that models cannot
predict because they were absent from training data. A designed protein
might aggregate under manufacturing conditions never encountered during
model development. A regulatory element might be silenced by chromatin
modifications specific to the therapeutic context. These failures can
only be identified through experimental validation in relevant
conditions, motivating the closed-loop DBTL approach that continuously
tests designs in application-relevant settings.

\section{Practical constraints on
design}\label{practical-constraints-on-design}

Beyond achieving desired function, practical design must satisfy
numerous constraints arising from manufacturing, safety, and deployment
requirements.

\subsection{Manufacturing and
developability}\label{manufacturing-and-developability}

Designed proteins must be producible at scale in expression systems such
as bacteria, yeast, or mammalian cells. Expression levels, solubility,
and purification behavior determine manufacturing feasibility and cost.
Foundation models trained on expression data can predict which sequences
are likely to express well, enabling design pipelines that optimize not
only for function but for manufacturability.

For therapeutic proteins, developability encompasses additional
properties including stability during storage, compatibility with
formulation requirements, and behavior during analytical
characterization. Aggregation propensity, chemical degradation sites
(oxidation, deamidation), and glycosylation patterns all affect
developability. Computational tools increasingly predict these
properties from sequence, enabling their incorporation as design
constraints.

\subsection{Safety and biosecurity
considerations}\label{safety-and-biosecurity-considerations}

The same capabilities that enable beneficial design applications also
raise biosecurity concerns. Generative models trained on pathogen
sequences might in principle be used to design enhanced pathogens or
reconstruct dangerous organisms. The dual-use potential of biological
design technology requires ongoing attention to safety practices and
governance frameworks.

Current foundation models do not provide straightforward paths to
bioweapon development; designing a functional pathogen requires
capabilities far beyond predicting sequence properties. However, as
models improve and integrate with automated synthesis and testing
platforms, the barrier to misuse may decrease. Responsible development
practices, including careful consideration of training data, model
access policies, and monitoring for concerning use patterns, are
essential components of the foundation model ecosystem. These
considerations connect to the broader discussion of safety and ethics in
Chapter~\ref{sec-future}.

\section{Algorithmic approaches to search and
optimization}\label{algorithmic-approaches-to-search-and-optimization}

Design algorithms must navigate vast sequence spaces to identify
candidates with desired properties. Several algorithmic paradigms have
proven effective, each with characteristic strengths and limitations.

Gradient-based optimization treats foundation models as differentiable
functions and computes gradients of objectives with respect to input
sequence representations. Because sequences are discrete while gradients
are continuous, optimization operates on relaxed representations
(probability distributions over nucleotides or amino acids) that are
projected back to discrete sequences for evaluation. This approach
efficiently navigates high-dimensional spaces but can produce
adversarial sequences that exploit model weaknesses rather than
achieving genuine biological function.

Evolutionary algorithms maintain populations of candidate sequences that
undergo mutation, recombination, and selection based on fitness scores
from foundation model oracles or experimental assays. This approach
naturally handles discrete sequence spaces and can maintain diversity to
avoid local optima. Multi-objective evolutionary algorithms explicitly
construct Pareto frontiers of solutions trading off competing
objectives.

Bayesian optimization models the sequence-to-fitness mapping with a
probabilistic surrogate (typically a Gaussian process or ensemble neural
network) and uses acquisition functions to balance exploration of
uncertain regions with exploitation of predicted optima. This approach
is particularly effective when experimental evaluations are expensive
and each design round must be carefully chosen.

Monte Carlo methods sample sequences from distributions defined by
foundation model likelihoods, optionally biased toward high-scoring
regions through importance weighting or Markov chain Monte Carlo. These
approaches naturally integrate foundation model priors with
task-specific objectives and can generate diverse candidate sets for
experimental screening.

The choice among algorithmic approaches depends on the specific design
problem, available computational resources, and experimental
constraints. Many practical pipelines combine multiple approaches:
generative sampling to produce initial candidate pools, gradient-based
refinement to optimize specific objectives, and active learning to
select informative experimental tests.

\section{From Understanding to
Creating}\label{from-understanding-to-creating}

Sequence design represents the frontier where foundation models
transition from tools for understanding biology to engines for creating
it. The field has advanced from designing individual stable proteins to
engineering complex molecular machines, from optimizing isolated
regulatory elements to programming cellular behavior, from incremental
improvement of existing sequences to de novo creation of functions not
found in nature. The constraints of natural evolution no longer bound
the sequences we can consider; the statistical patterns of existing
biology provide priors that guide exploration of novel territory.

The validation bottleneck persists as perhaps the most fundamental
limitation. Computational design can propose candidates faster than
experiments can test them, creating pressure to improve both predictive
accuracy (reducing false positives that waste experimental resources)
and experimental throughput (enabling more designs to be evaluated).
Automated laboratories, standardized assay platforms, and improved
experimental design methods all contribute to accelerating the
design-build-test-learn cycle, but the gap between computational
proposal and experimental validation remains substantial.

The transition from prediction to design amplifies both the potential
benefits and the risks of these technologies. A model that predicts
protein function enables analysis; a model that designs protein function
enables creation. Ensuring that designed biology serves human
flourishing while minimizing potential harms requires not just technical
advances but thoughtful governance, inclusive deliberation about
applications, and ongoing attention to safety. These broader
considerations connect sequence design to the regulatory, ethical, and
societal themes examined in the final chapter, where the technical
capabilities developed throughout this book meet the human systems that
will determine how they are used.

\chapter{Ethical and Frontiers}\label{sec-future}

The first genomic foundation model to receive FDA clearance as a medical
device will face a peculiar regulatory challenge: demonstrating that a
system trained on millions of sequences from research biobanks, academic
databases, and public repositories can safely inform clinical decisions
for individual patients who never consented to such use. Clinical-grade
variant interpretation tools already incorporate deep learning
predictions, yet the regulatory frameworks governing their deployment
were designed for deterministic software with traceable decision logic,
not for neural networks whose internal representations resist simple
explanation. As of 2024, more than 500 AI-enabled medical devices have
received FDA authorization, but fewer than a dozen involve genomic
interpretation, and none yet deploys a foundation model at the scale
described in this book. The gap between technical capability and
regulatory readiness defines one of the central tensions facing the
field.

This asymmetry between what models can do in silico and what they may do
in clinical practice shapes every translational decision. A variant
effect predictor achieving 0.95 AUROC on a curated benchmark may fail
unpredictably on the rare variants that matter most for diagnosis. A
regulatory sequence model that accurately predicts chromatin
accessibility in well-characterized cell lines may produce unreliable
predictions in patient-derived tissues never seen during training. The
technical achievements documented throughout this book represent
necessary but insufficient conditions for clinical impact. Realizing the
benefits of genomic foundation models while managing their risks
requires navigating regulatory pathways designed for different
technologies, building governance structures for data that spans
generations and continents, and confronting ethical questions that
genomics and artificial intelligence raise independently but compound
when combined.

This chapter examines the regulatory, ethical, and frontier challenges
that will shape how genomic foundation models move from research tools
to clinical practice. We address the regulatory landscape for AI medical
devices, the data governance frameworks needed for responsible model
development, the equity considerations that determine whether genomic AI
reduces or amplifies health disparities, and the emerging capabilities
(from protein design to cellular reprogramming) that will define the
field's trajectory. The goal is not comprehensive coverage of rapidly
evolving policy, but a framework for reasoning about the sociotechnical
challenges that accompany powerful predictive and generative
capabilities.

\section{Regulatory Frameworks for Genomic
AI}\label{regulatory-frameworks-for-genomic-ai}

\begin{figure}

\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-1.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER A}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-12.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER B}}
\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-123.pdf}}

\subcaption{\label{}\textbf{FIGURE PLACEHOLDER C}}
\end{minipage}%

\caption{\label{fig-regulatory-pathways}{[}Essential{]} Jurisdiction
comparison. Columns: FDA (US), CE-IVD (EU), emerging (China, Japan,
others). For each: Pathway (510(k), De Novo, PMA; CE marking; national
approval); Evidence requirements; Update policies; Timeline; Key
considerations. Complexity factors: Software as a Medical Device (SaMD);
foundation models as ``substantially equivalent'' or novel; continuously
learning systems; multi-site deployment. Annotation: Regulatory
landscape evolving; early engagement recommended.}

\end{figure}%

\subsection{The Software as Medical Device
Paradigm}\label{the-software-as-medical-device-paradigm}

Regulatory agencies worldwide classify AI-based clinical tools as
\textbf{software as a medical device (SaMD)}, a category that applies
when software itself constitutes the medical device rather than merely
controlling hardware. The International Medical Device Regulators Forum
defines SaMD risk tiers based on the seriousness of the health condition
and the role software plays in clinical decision-making: software that
provides information to drive clinical management of a serious condition
receives higher scrutiny than software that merely informs decisions
about non-serious conditions.

Genomic foundation models typically fall into higher-risk categories. A
tool that classifies variants as pathogenic or benign directly
influences diagnostic conclusions for conditions ranging from hereditary
cancer syndromes to rare developmental disorders. The consequences of
misclassification can be severe: a false negative may delay life-saving
interventions, while a false positive may trigger unnecessary
prophylactic surgery or cascade into family-wide psychological harm.
Regulators accordingly require substantial evidence of analytical
validity (does the model measure what it claims to measure?), clinical
validity (does measurement correlate with the clinical outcome?), and in
some cases clinical utility (does using the model improve patient
outcomes?).

The FDA's approach to AI-enabled devices has evolved considerably since
the agency cleared the first autonomous diagnostic AI in 2018. The
agency now distinguishes between ``locked'' algorithms whose behavior is
fixed at approval and ``adaptive'' algorithms that continue learning
from new data after deployment. Most foundation models fall into neither
category cleanly: their weights are frozen after pretraining, but their
outputs depend on prompts, fine-tuning, or downstream heads that may
change across applications. This architectural ambiguity creates
regulatory uncertainty. A foundation model serving as the backbone for
multiple clinical applications might require separate submissions for
each use case, or a single submission might cover the shared backbone
while individual fine-tuned heads receive separate clearances.

\subsection{European and Global Regulatory
Landscapes}\label{european-and-global-regulatory-landscapes}

The European Union's approach differs from the FDA's in several respects
relevant to genomic AI. The EU Medical Device Regulation (MDR), which
fully replaced prior directives in 2021, classifies standalone software
according to similar risk principles but places greater emphasis on
conformity assessment by notified bodies rather than centralized agency
review. For high-risk software, manufacturers must demonstrate
compliance with essential safety and performance requirements through
technical documentation, quality management systems, and post-market
surveillance plans. The AI Act, which entered force in 2024, adds
another regulatory layer: high-risk AI systems (including those used in
medical diagnosis) must meet transparency, robustness, and human
oversight requirements that go beyond device-specific regulations.

Regulatory divergence across jurisdictions creates practical challenges
for global deployment. A genomic foundation model cleared by the FDA may
require separate CE marking for European markets, TGA approval in
Australia, and PMDA review in Japan, each with distinct evidentiary
standards and submission formats. Harmonization efforts through the
International Medical Device Regulators Forum provide common frameworks
for definitions and risk classification, but substantive requirements
continue to differ. Companies developing clinical-grade genomic AI must
either design validation programs that satisfy the most stringent
jurisdiction or pursue market-by-market strategies that delay access in
some regions.

The regulatory landscape for laboratory-developed tests (LDTs) further
complicates matters in the United States. Clinical laboratories have
historically been able to develop and offer tests under their own
validation without FDA premarket review, relying instead on CLIA
certification and state licensure. Many clinical genomics laboratories
use in-house bioinformatics pipelines, variant callers, and annotation
tools that incorporate machine learning components without seeking FDA
clearance. Recent FDA guidance signals intent to assert greater
oversight over LDTs, particularly those using complex algorithms, but
the boundary between regulated devices and unregulated laboratory
procedures remains contested.

\subsection{Validation Requirements for Clinical Genomic
AI}\label{validation-requirements-for-clinical-genomic-ai}

Regulatory submissions for genomic AI devices require validation
evidence spanning multiple dimensions. Analytical validation typically
involves demonstrating that the model performs consistently across
different sequencing platforms, library preparation methods, and sample
types. For a variant effect predictor, this might include showing that
scores remain calibrated when inputs come from whole-genome sequencing
versus targeted panels, from fresh blood versus archived FFPE tissue, or
from healthy individuals versus cancer patients with complex somatic
variation.

Clinical validation connects model outputs to clinical outcomes. For a
variant classifier, clinical validation might assess concordance with
expert panel classifications, correlation with functional assay results,
or agreement with segregation patterns in affected families. The choice
of reference standard is itself contentious: ClinVar classifications,
which many models use as training labels, reflect historical expert
consensus that may lag behind accumulating evidence, and circular
validation using the same database for training and evaluation produces
misleadingly optimistic results (see Chapter~\ref{sec-confounding}). The
deployment realities discussed in Chapter~\ref{sec-clinical-risk} and
Appendix~\ref{sec-apx-compute} illustrate how these validation
requirements interact with institutional workflows, reimbursement
constraints, and clinician trust; regulatory clearance represents only
one barrier among many.

Some regulators also require evidence of clinical utility, demonstrating
that model use improves patient outcomes compared to standard practice.
This higher bar is difficult to meet for genomic AI tools that operate
as components within larger clinical workflows. A variant effect
predictor may improve prioritization efficiency without changing
ultimate diagnoses, or may enable earlier diagnosis that only translates
to better outcomes when appropriate treatments exist. Designing trials
that isolate the model's contribution from confounding workflow factors
requires careful attention to study design and endpoint selection.

\section{Data Governance and Consent}\label{data-governance-and-consent}

\subsection{The Consent Problem at
Scale}\label{the-consent-problem-at-scale}

Foundation model training requires data at scales that challenge
traditional consent paradigms. A protein language model trained on
UniRef encompasses sequences from millions of organisms, including many
species for which consent concepts do not apply and human sequences
contributed under varied research protocols. A model trained on human
genomic data from multiple biobanks aggregates information collected
under different consent frameworks, some permitting broad secondary
research use and others restricting use to specific studies.

The legal and ethical status of such aggregated training depends on how
consent documents were written, how thoroughly participants understood
the scope of future use, and how jurisdictions interpret secondary use
provisions. European GDPR provisions treat genetic data as a special
category requiring explicit consent, but may permit research use under
legitimate interest or public interest provisions with appropriate
safeguards. United States regulations under the Common Rule permit
secondary research on properly deidentified data, but genomic data
resist complete deidentification given the uniqueness of individual
genomes.

Even when consent technically permits model training, broader ethical
questions remain. Participants who consented to genomic research in 2005
could not have anticipated that their data might train AI systems
capable of generating novel sequences or predicting sensitive traits.
The temporal gap between data collection and model development strains
the fiction of informed consent. Dynamic consent systems that allow
ongoing engagement and preference updates address some concerns but are
difficult to retrofit onto legacy collections and impose burdens on
participants and institutions alike.

\subsection{Biobank Governance Models}\label{biobank-governance-models}

Large biobanks have developed varied governance approaches that shape
how their data can be used for foundation model development. UK Biobank,
which combines genomic data with extensive phenotypic information on
approximately 500,000 participants, permits registered researchers to
use data for health-related research under terms that explicitly
anticipate computational and AI applications. Access requires
application review, data security commitments, and agreement to publish
results. The model has enabled substantial foundation model research
while maintaining participant trust through transparent policies and
active communication.

Other biobanks operate under more restrictive frameworks. Some
disease-specific registries limit use to research on particular
conditions. Some indigenous and community biobanks require tribal or
community approval for research access, reflecting concerns about
historical exploitation and the importance of data sovereignty. The
tension between open science norms that favor broad data sharing and
community governance norms that prioritize local control creates
friction for foundation model developers seeking diverse training data.

Federated learning and other privacy-preserving techniques offer partial
solutions by enabling model training without centralizing raw data.
Under federated approaches, each data custodian trains local models that
share only gradients or model updates with a central coordinator. The
approach protects against centralization risks but introduces technical
complexity, may reduce model quality compared to centralized training,
and does not eliminate all privacy risks (gradient updates can sometimes
reveal individual-level information). Practical federated training for
genomic foundation models remains an active research area with limited
deployment experience.

\subsection{Secondary Use and Data
Futures}\label{secondary-use-and-data-futures}

The genomic data collected today may be used for applications not yet
imagined. A variant database assembled for pharmacogenomic research
might later inform ancestry inference tools with implications for
immigration enforcement. Chromatin accessibility data generated for
cancer biology might reveal aging signatures relevant to insurance
underwriting. Foundation models trained on diverse genomic data acquire
emergent capabilities that their creators did not anticipate and may not
recognize.

Governance structures must therefore address not just present uses but
future possibilities. Some institutions adopt broad consent models that
authorize essentially unlimited research use, relying on institutional
review and public benefit assessments rather than individual
authorization for each application. Others implement tiered consent
allowing participants to authorize some uses while restricting others.
Still others propose data trusts or cooperatives that hold data on
participants' behalf and negotiate access terms collectively.

No consensus has emerged on optimal governance structures for genomic
foundation model development. The field operates within a patchwork of
institutional policies, national regulations, and community norms that
permit some training configurations while prohibiting others.
Researchers building foundation models must navigate this landscape
carefully, documenting data provenance, respecting access restrictions,
and anticipating how governance norms may evolve as AI capabilities
advance.

\section{Privacy and Genomic Data}\label{privacy-and-genomic-data}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-data-governance}{[}High{]} Multi-panel governance
challenges. Panel A (Privacy vs.~Utility): Anonymized data (limited
utility) vs identified data (utility but privacy risk); differential
privacy tradeoff curve. Panel B (Consent Complexity): Broad consent,
specific consent, dynamic consent, tiered consent; each with
implications. Panel C (Federated Learning): Data stays local; model
travels; gradient aggregation; privacy-preserving computation. Panel D
(Cross-Border Issues): Different jurisdictions with different rules;
GDPR, HIPAA, emerging regulations; data localization requirements. Key
insight: No perfect solution; governance requires ongoing negotiation
between stakeholder interests.}

\end{figure}%

\subsection{The Re-identification
Challenge}\label{the-re-identification-challenge}

Genomic data pose fundamental privacy challenges because genomes are
simultaneously unique identifiers and richly informative biological
records. A person's genome can be matched against public genealogy
databases, research repositories, or forensic databases to establish
identity with high confidence. Once identified, the genomic record
reveals information about disease predisposition, ancestry, family
relationships, and other sensitive attributes that the person may not
wish to disclose.

Conventional anonymization techniques that remove names and obvious
identifiers provide limited protection. Research has demonstrated
re-identification of individuals from genomic data alone, from genomic
data combined with demographic information, and even from aggregate
genomic statistics under certain conditions. Foundation models compound
these concerns by potentially extracting and recombining information in
ways that defeat simple deidentification. A model trained on sequences
from many individuals might, under adversarial prompting, generate
outputs that reveal information about specific training examples.

Technical safeguards include differential privacy (which adds calibrated
noise to training procedures to bound individual-level information
leakage), secure multi-party computation (which enables joint
computation over distributed data without revealing inputs), and
synthetic data generation (which produces training data that preserves
statistical properties without corresponding to real individuals). Each
approach involves tradeoffs between privacy protection and model
utility. Differential privacy with strong guarantees may degrade model
performance substantially. Secure computation adds computational
overhead and complexity. Synthetic data may fail to capture rare
variants or unusual correlations essential for clinical applications.

\subsection{Family and Relational
Privacy}\label{family-and-relational-privacy}

Genomic privacy extends beyond individuals to families and communities.
A person's genome reveals information about biological relatives who may
not have consented to any data collection. Identifying a carrier of a
hereditary cancer mutation implies elevated risk for parents, siblings,
and children. Revealing ancestry information for one family member
constrains inferences about relatives. These relational dimensions mean
that individual consent cannot fully protect the interests of those
affected by genomic disclosure.

Foundation models trained on family data, or capable of inferring family
relationships from population-level patterns, create new relational
privacy risks. A model that accurately predicts recessive disease
carrier status from sequence alone could identify at-risk couples
without explicit testing. A model that infers extended pedigree
structure from population genetics signals could reveal family secrets
or create legal complications. Governance frameworks must consider not
just the rights of data subjects but the interests of biological
relatives who cannot meaningfully consent.

Some jurisdictions have begun addressing relational genomic privacy
through legislation. The Genetic Information Nondiscrimination Act
(GINA) in the United States prohibits health insurers and employers from
using genetic information discriminatorily, providing partial protection
for individuals whose relatives have been tested. European GDPR
provisions on special category data extend some protections to inferred
genetic information. But legal frameworks lag behind technical
capabilities, and enforcement mechanisms remain limited.

\section{Intellectual Property and
Ownership}\label{intellectual-property-and-ownership}

\subsection{Who Owns Genomic Sequence
Data?}\label{who-owns-genomic-sequence-data}

Legal frameworks for sequence data ownership vary across jurisdictions
and remain contested. In the United States, the Supreme Court's 2013
\emph{Myriad} decision held that naturally occurring DNA sequences
cannot be patented, eliminating one barrier to data sharing but leaving
property rights in datasets unclear. Databases may receive limited
copyright protection for their selection and arrangement, but individual
sequences typically cannot be copyrighted as facts or natural phenomena.
Contractual restrictions, such as data use agreements attached to
biobank access, provide the primary mechanism for controlling sequence
data use.

The situation differs for synthetic or engineered sequences, which may
qualify for patent protection if they meet novelty, utility, and
non-obviousness requirements. Foundation models that generate novel
sequences thus operate in complex IP territory: sequences generated by
the model may be patentable if sufficiently innovative, but determining
inventorship (human researcher versus AI system) raises unresolved legal
questions. Courts and patent offices are only beginning to address
AI-generated inventions, with varying approaches across jurisdictions.

For foundation model developers, the key practical questions concern
what restrictions apply to training data and what rights attach to model
outputs. Training on publicly available sequences may be permissible
under database terms of use, research exemptions, or fair use principles
depending on jurisdiction and use context. Commercial deployment of
models trained on restricted-access data may require additional
authorization. Outputs generated by models may be freely usable by the
model operator, or may carry through restrictions from training data,
depending on legal interpretation and contractual provisions.

\subsection{Model Weights as Assets}\label{model-weights-as-assets}

Foundation model weights represent substantial investments of compute,
data, and expertise, creating obvious commercial value. Companies
training large genomic models face decisions about whether to release
weights openly, provide API access without weight release, or restrict
access entirely. Each approach carries different implications for
scientific progress, commercial competition, and safety management.

Open release of weights enables independent research, reproduction, and
adaptation but forfeits commercial control and complicates
responsibility for misuse. API access maintains control while enabling
broad use but creates dependencies and may restrict scientific scrutiny.
Restricted access protects competitive advantage and may enhance safety
oversight but limits beneficial applications and concentrates power.

The genomics community has historically favored open data sharing, with
major databases and biobanks making data freely available under
permissive terms. Whether this norm extends to foundation model weights
is contested. Arguments for openness emphasize scientific
reproducibility, broad access benefits, and the difficulty of
maintaining meaningful restrictions given technical capabilities for
weight reconstruction or distillation. Arguments for restriction
emphasize dual-use risks from highly capable generative models,
commercial incentives necessary to sustain development investment, and
the potential for open models to be fine-tuned for harmful purposes.

\subsection{Prediction Ownership and
Liability}\label{prediction-ownership-and-liability}

When a foundation model generates a clinically relevant prediction (this
variant is likely pathogenic, this regulatory sequence will increase
expression), questions arise about who owns that prediction and who
bears responsibility if it proves wrong. The model developer, the
clinical laboratory using the model, the health system employing the
laboratory, and the clinician acting on results all have potential roles
and potential liability.

Current legal frameworks generally hold clinicians responsible for
clinical decisions, with laboratories liable for test quality and
medical device manufacturers liable for product defects. How these
responsibilities apply when decisions incorporate foundation model
outputs remains uncertain. If a model developer provides a variant
classifier as SaMD, the developer likely bears some responsibility for
the classifier's performance. If a laboratory integrates foundation
model embeddings into a proprietary pipeline, the laboratory may assume
primary responsibility for overall system performance. If a clinician
overrides a model recommendation based on clinical judgment, liability
may shift toward the clinician's decision-making.

These liability questions have practical implications for foundation
model deployment. Developers may structure their offerings to minimize
liability exposure, for instance by providing research-use-only tools
that shift responsibility to users, or by limiting outputs to
information that falls short of clinical recommendations. Such
structuring may impede beneficial clinical applications if it creates
uncertainty about appropriate use or fragments responsibility in ways
that leave harms uncompensated.

\section{Responsible Development
Practices}\label{responsible-development-practices}

\subsection{Transparency and
Documentation}\label{transparency-and-documentation}

Responsible foundation model development requires transparency about
training data, model capabilities, limitations, and intended use. Model
cards, datasheets, and similar documentation frameworks provide
structured approaches to capturing this information. For genomic
foundation models, relevant documentation includes:

Training data composition encompasses which species are represented,
what genomic regions are covered, which populations contribute human
data, what functional annotations are included, and how data were
filtered or preprocessed. Data provenance documentation traces sources,
access conditions, and any restrictions on use or redistribution.
Evaluation results cover performance across relevant benchmarks,
disaggregated by ancestry, variant type, gene family, and other relevant
strata. Limitation disclosure identifies known failure modes,
out-of-distribution detection capabilities, and contexts where model
outputs should not be trusted.

The challenge is ensuring that documentation reaches users who need it
and influences their decisions. A detailed model card published
alongside model weights may be ignored by users seeking quick results.
Clinical deployments may strip away documentation as models are
integrated into larger systems. Effective transparency requires not just
producing documentation but designing workflows that surface relevant
information at decision points and verifying that users understand
limitations.

\subsection{Fairness and Performance
Equity}\label{fairness-and-performance-equity}

Genomic foundation models inherit biases from their training data. If
training corpora over-represent European ancestry populations, models
may perform worse on variants common in other populations, on regulatory
elements active in non-European tissues, or on genes under different
selective pressures across populations. If functional annotations derive
primarily from well-funded research programs focused on common diseases,
models may underperform on rare diseases or conditions affecting
underserved populations.

Fairness assessment requires disaggregated evaluation across relevant
population strata, not just aggregate performance metrics. A variant
effect predictor achieving 0.92 AUROC overall might achieve 0.95 in
European populations and 0.82 in African populations, a disparity masked
by aggregate reporting. A regulatory model might perform well on cell
types common in training data (lymphocytes, hepatocytes) while failing
on less-studied cell types (specialized neurons, rare immune subsets)
that matter for particular diseases.

Mitigation approaches include diversifying training data, applying
reweighting or resampling strategies during training, and developing
adaptation techniques that improve performance on underrepresented
groups. But data diversification has limits when underlying resources
remain skewed, and post-hoc corrections may trade off overall
performance for equity gains. The deeper solution involves changing
incentive structures to prioritize diverse data collection and equitable
performance from the outset.

\subsection{Human Oversight and Decision
Support}\label{human-oversight-and-decision-support}

Even highly capable foundation models should operate as decision support
tools rather than autonomous decision-makers in clinical contexts. Human
oversight serves multiple functions: catching model errors that fall
outside training distribution, integrating clinical context that models
cannot access, navigating value trade-offs where technical optimization
is insufficient, and maintaining accountability structures that enable
error correction and redress.

Effective oversight requires that model outputs be interpretable enough
for humans to exercise meaningful judgment. If a variant classifier
provides only a pathogenic/benign label without supporting evidence, the
overseeing clinician has no basis for assessing whether the model's
reasoning applies to the case at hand. If a regulatory effect predictor
reports a large effect without indicating uncertainty, the user may not
know when skepticism is warranted. Interpretability tools discussed in
Chapter~\ref{sec-interpretability} support oversight by revealing
internal model reasoning, but interpreting such explanations requires
expertise and time that may not be available in clinical workflows.

System design must also prevent automation bias, the tendency for human
operators to defer to automated recommendations even when independent
judgment would lead to different conclusions. Training clinicians to use
AI tools effectively, designing interfaces that prompt critical
evaluation rather than passive acceptance, and monitoring for
over-reliance patterns are all components of responsible oversight
architecture.

\section{Dual Use and Biosecurity}\label{dual-use-and-biosecurity}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-dual-use-governance}{[}High{]} Risk assessment
matrix. Axes: Capability (what the model can do) Ã— Access (who can use
it). Quadrants: Low capability/restricted (current academic); high
capability/restricted (industrial deployment with oversight); low
capability/open (open-source educational); high capability/open (highest
concern). Risk factors: Pathogen design, antibiotic resistance,
agricultural biosecurity, emergent capabilities. Governance mechanisms:
Pre-release evaluation, staged release, monitoring, audit trails,
benefit-risk assessment. Key insight: Balancing open science benefits
against misuse risks.}

\end{figure}%

\subsection{Generative Models and Pathogen
Enhancement}\label{generative-models-and-pathogen-enhancement}

Foundation models capable of generating functional biological sequences
raise biosecurity concerns distinct from those posed by predictive
models. A protein language model trained to generate functional enzymes
might, in principle, be prompted to design proteins with enhanced
pathogenic properties. A regulatory sequence model might generate
promoters optimized for expression in human tissues of concern. A
generative DNA model might propose sequences that evade detection by
standard diagnostics.

The severity of these risks depends on technical factors that remain
uncertain. Current generative models often produce sequences that are
theoretically functional but fail in experimental validation; the gap
between computational generation and biological realization provides a
natural barrier. Specialized knowledge required to translate generated
sequences into actual biological threats remains substantial, though it
may decrease as wetlab automation advances. Many dangerous sequences are
already documented in public databases, making novel generation less
marginal than it might appear. The generative architectures examined in
Chapter~\ref{sec-design}, which demonstrate increasing capability for
producing functional sequences, make these concerns more than
hypothetical; the same capabilities that enable therapeutic protein
design also lower barriers to misuse.

Nonetheless, responsible development requires attention to dual-use
potential. Strategies include capability evaluation (probing models for
ability to generate concerning sequences before release), staged
deployment (limiting access to highly capable generative models while
monitoring for misuse indicators), and output filtering (screening
generated sequences against known hazard databases). The optimal balance
between open scientific exchange and biosecurity restriction remains
contested, with reasonable experts holding divergent views on where
lines should be drawn.

\subsection{Access Controls and Responsible
Release}\label{access-controls-and-responsible-release}

Foundation model developers must decide how to release models in ways
that enable beneficial use while limiting potential for harm. Complete
openness maximizes beneficial applications but foregoes control over
misuse. Complete restriction limits misuse but also limits beneficial
applications and may prove impossible to maintain as model capabilities
become reproducible. Graduated access models attempt to balance these
considerations by providing broader access to less capable models while
restricting access to more capable systems.

Access controls can operate at multiple levels: restricting weight
access while providing API availability, limiting API capabilities
through output filtering, requiring applications and use agreements for
access, or monitoring usage patterns for indicators of concerning
applications. Each control imposes costs on legitimate users and may
prove circumventable by determined malicious actors. The effectiveness
of controls depends on the specific model, the capability of concern,
and the technical sophistication of potential misusers.

For genomic foundation models specifically, the biosecurity risks are
generally lower than for models capable of synthesizing pathogen
sequences from scratch, but concerns about privacy violations,
discriminatory applications, and scientific misconduct remain. A model
capable of inferring sensitive traits from genomic data might be misused
for unauthorized health prediction. A model capable of generating
realistic synthetic genomic data might be used to fabricate research
results. Responsible release strategies must consider these diverse risk
profiles.

\section{Open Technical Problems}\label{open-technical-problems}

\subsection{Scaling and Efficiency}\label{scaling-and-efficiency}

The largest foundation models in natural language processing now exceed
a trillion parameters and were trained on trillions of tokens. Genomic
foundation models remain substantially smaller, with typical models
ranging from hundreds of millions to low billions of parameters. Whether
genomic applications require comparable scale remains uncertain. The
human genome spans 3 billion base pairs and encompasses perhaps 20,000
protein-coding genes, a smaller and more constrained space than natural
language. But capturing the full complexity of gene regulation, protein
structure, and cellular context may require parameter counts that
approach or exceed language model scale.

Scaling genomic foundation models faces several bottlenecks. Training
data availability constrains scale when models exhaust unique sequences
and must rely on data augmentation or repetition. Compute costs remain
prohibitive for most academic groups and limit experimentation with
truly large architectures. Long sequence lengths required for genomic
context (regulatory elements can span hundreds of kilobases) create
quadratic attention costs that limit practical context windows despite
architectural innovations.

Efficiency improvements that reduce compute requirements without
sacrificing capability are thus particularly valuable for genomic
applications. Approaches include sparse attention patterns that avoid
full quadratic costs, state space models that process sequences in
linear time, knowledge distillation that transfers capability from large
models to smaller ones, and quantization that reduces precision
requirements for inference. Each approach involves trade-offs between
efficiency gains and capability preservation that must be evaluated
empirically on genomic tasks.

\subsection{Context and Multi-Scale
Integration}\label{context-and-multi-scale-integration}

Biological phenomena span scales from nucleotides to ecosystems.
Foundation models must integrate information across these scales to
capture biological reality: local sequence motifs, regulatory element
architecture, chromosome-level organization, cellular context, tissue
environment, organism-level physiology, and population-level variation
all contribute to genotype-phenotype relationships.

Current approaches typically focus on single scales or model multi-scale
relationships implicitly through large training datasets rather than
explicitly through architectural design. A DNA language model processes
sequence tokens without explicit representation of chromatin structure.
A single-cell model embeds cells without explicit representation of
tissue organization. A regulatory model predicts expression without
explicit representation of 3D genome contacts.

Architectures that explicitly integrate across scales remain a frontier.
Hierarchical models that compose representations at different
resolutions, graph neural networks that encode biological relationships
across scales, and hybrid systems that combine modality-specific
encoders with cross-modal attention layers all represent active research
directions. Success will require not just architectural innovation but
appropriate training data that captures multi-scale relationships and
evaluation protocols that probe multi-scale reasoning.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-multiscale-integration}{[}High{]} Scale hierarchy
visualization. Levels: (1) Nucleotide (base pairs, modifications); (2)
Sequence element (motifs, domains, genes); (3) Molecular complex
(chromatin, ribonucleoprotein); (4) Cell (transcriptome, proteome,
state); (5) Cell population (tissue, heterogeneity); (6) Tissue/Organism
(intercellular communication, development). Current model coverage:
DNA-LM (nucleotide â†’ gene); Protein models (residue â†’ protein);
Single-cell models (cell state). Scale boundary challenges: Arrow
indicating information loss at each boundary; hierarchical models
emerging; graph networks for relational structure. Key insight: Biology
operates across scales; future models must integrate.}

\end{figure}%

\subsection{Causality and Mechanism}\label{causality-and-mechanism-1}

The distinction between correlation and causation pervades genomic
analysis. A variant associated with disease in GWAS may be causal, in
linkage disequilibrium with a causal variant, or confounded by
population structure or other factors. A regulatory element predicted to
affect expression may directly drive transcription or may merely
co-occur with other causal elements. Foundation models, like other
statistical learners, capture patterns in training data without
distinguishing causal from correlational relationships.

Progress toward causal and mechanistic reasoning in genomic AI likely
requires integrating diverse evidence types. Perturbation experiments
(CRISPR knockouts, drug treatments, environmental exposures) provide
interventional data that can distinguish causal effects from
correlations. Mendelian randomization approaches leverage genetic
instruments to estimate causal effects from observational data.
Structural causal models provide formal frameworks for encoding and
reasoning about causal relationships.

Incorporating causal structure into foundation models is technically
challenging. Causal relationships are often unknown, contested, or
context-dependent. Training objectives that encourage causal reasoning
must balance causal accuracy against predictive performance on tasks
where correlation suffices. Evaluation of causal reasoning requires
benchmarks with known causal ground truth, which are scarce for complex
biological systems.

\section{Emerging Directions}\label{emerging-directions-1}

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-agentic-systems}{[}High{]} Autonomous design cycle
with oversight. Cycle components: (1) Generative model (proposes
candidates, optimizes toward objective); (2) Safety filter (screen
against hazard databases, reject concerning, log for audit); (3)
Automated synthesis (DNA/protein production, QC, physical realization);
(4) High-throughput assay (functional measurement, multiplexed readouts,
data generation); (5) Model update (results improve predictions, refine
objective, guide next iteration). Human oversight points: Objective
specification (before cycle); periodic review (during); stopping
criteria (when to halt); anomaly investigation (if unexpected). Risk
management: Containment, audit trails, escalation, kill switches. Key
insight: Agentic systems require careful objective specification and
monitoring; human oversight essential.}

\end{figure}%

\subsection{Multimodal Integration}\label{multimodal-integration}

The foundation models discussed throughout this book largely operate on
single modalities: DNA sequence, protein sequence, gene expression
counts, chromatin accessibility signals. Biological reality is
irreducibly multimodal, with information flowing across modalities
through transcription, translation, signaling, and metabolism. The next
generation of genomic foundation models will need to integrate across
modalities more deeply.

Early multimodal genomic models combine encoders trained separately on
different modalities, using cross-attention or shared embedding spaces
to enable cross-modal reasoning. More ambitious architectures train
end-to-end on multimodal data, learning unified representations that
capture relationships between sequence and structure, expression and
chromatin state, genotype and phenotype. The data requirements for such
training are substantial, requiring aligned measurements across
modalities at scale.

Clinical applications particularly benefit from multimodal integration.
A diagnostic model that combines genomic variants with electronic health
record data, imaging findings, and laboratory values can capture
patterns invisible to any single modality. A prognostic model that
integrates germline genetics with tumor transcriptomics and treatment
history can personalize predictions in ways that purely genetic models
cannot. Building such systems requires not just technical capability but
also data governance frameworks that permit multimodal combination while
protecting privacy.

\subsection{Agentic and Closed-Loop
Systems}\label{agentic-and-closed-loop-systems}

Foundation models have traditionally operated as passive tools: given an
input, they produce an output, and humans decide what to do with it.
Emerging agentic architectures allow models to take actions, observe
outcomes, and adapt behavior based on feedback. In genomic contexts,
agentic systems might design experiments, interpret results, revise
hypotheses, and iterate toward biological goals with minimal human
intervention.

Closed-loop systems couple computational prediction with experimental
validation in automated cycles. A design model proposes sequences
optimized for a target function. An automated synthesis and screening
platform tests proposed sequences. Results feed back to update the model
or guide subsequent proposals. Such systems can explore sequence space
far more efficiently than sequential human-directed experimentation.

The promise of agentic and closed-loop approaches is accelerated
discovery: identifying functional sequences, characterizing biological
mechanisms, and optimizing therapeutic candidates faster than
traditional workflows. The risks include models pursuing objectives that
diverge from human intent, experimental systems generating safety
hazards, and accountability gaps when autonomous systems make
consequential errors. Realizing benefits while managing risks requires
careful attention to objective specification, monitoring and oversight
mechanisms, and safety boundaries that constrain autonomous action.

\subsection{Clinical Integration and Learning Health
Systems}\label{clinical-integration-and-learning-health-systems}

The ultimate test of genomic foundation models is whether they improve
health outcomes. Moving from research demonstrations to clinical impact
requires integration into care workflows, evidence of benefit from
prospective studies, regulatory clearance, and sustainable business
models that support ongoing development and maintenance.

Learning health systems provide a framework for continuous improvement:
clinical use generates data that feeds back into model refinement,
creating virtuous cycles where models improve as they serve more
patients. Such systems raise governance questions about who controls the
learning process, how improvements are validated before deployment, and
how benefits and risks are distributed across patients, providers, and
technology developers.

The foundation model paradigm offers particular advantages for learning
health systems. Pretrained models can be adapted to local populations
and practices through fine-tuning on institutional data. Improvements
demonstrated at one institution can potentially transfer to others
through shared model updates. Common architectures enable comparison
across sites and accumulation of evidence across diverse populations.

\begin{figure}

\centering{

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/300x200-text=FIGURE-.pdf}}
\caption{\textbf{FIGURE PLACEHOLDER}}
\end{figure}

}

\caption{\label{fig-learning-health-system}{[}Enhancing{]} Circular
learning system. Components: (1) Clinical deployment (FM informs
decisions, predictions reach patients); (2) Outcome observation (what
happened? correct/incorrect?); (3) Data aggregation (outcomes linked to
predictions, privacy-preserving); (4) Model refinement (updated training
data, improved predictions, federated learning); (5) Validation and
approval (updated model validated, regulatory approval, return to
deployment). Governance at each stage: Who controls learning? How are
improvements validated? How are benefits distributed? How are
underrepresented populations protected? Multi-site collaboration: Common
architecture enables comparison; improvements transfer; accumulate
evidence across diverse populations. Key insight: Learning systems
create virtuous cycles; governance must ensure benefits reach all
equitably.}

\end{figure}%

Realizing this vision requires infrastructure for secure data sharing,
governance frameworks that enable learning while protecting privacy,
regulatory pathways that accommodate evolving systems, and clinical
workflows that support appropriate use and oversight. The technical
capabilities described in this book are necessary but not sufficient.
Genomic foundation models will achieve their potential only through
sustained collaboration among technologists, clinicians, patients,
policymakers, and communities working together to build systems that are
both capable and trustworthy.

\section{The Work Ahead}\label{the-work-ahead}

The ultimate test of genomic foundation models is whether they improve
health outcomes. The technical capabilities surveyed throughout this
book, from sequence representations through foundation model
architectures to clinical applications, are necessary but not sufficient
for that goal. Between a model that predicts well on benchmarks and a
patient whose diagnosis comes faster or whose treatment works better
lies the full complexity of clinical translation: validation across
populations, integration into workflows, regulatory approval, equitable
access, and ongoing monitoring for drift and harm.

Learning health systems provide a framework for bridging this gap:
clinical use generates data that feeds back into model refinement,
creating virtuous cycles where models improve as they serve more
patients. Such systems raise governance questions as important as the
technical ones. Who controls the learning process? How are improvements
validated before deployment? How are benefits and risks distributed
across patients, providers, and technology developers? How do we ensure
that populations underrepresented in training data are not further
disadvantaged by systems that learn primarily from others?

Genomic foundation models will achieve their potential only through
sustained collaboration among technologists, clinicians, patients,
policymakers, and communities working together to build systems that are
both capable and trustworthy. Capability without trustworthiness is
dangerous: models that predict accurately but fail silently for certain
populations cause harm even as they help others. Trustworthiness without
capability is insufficient: systems that are transparent and fair but do
not improve on existing practice offer nothing worth adopting. The
technical achievements described in this book enable new capabilities;
the human systems that govern their development and deployment will
determine whether those capabilities translate into genuine benefit for
the patients and populations that genomic medicine aims to serve.

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-abramson_alphafold3_2024}
Abramson, Josh, Jonas Adler, Jack Dunger, Richard Evans, Tim Green,
Alexander Pritzel, Olaf Ronneberger, et al. 2024. {``{[}{AlphaFold3}{]}
{Accurate} Structure Prediction of Biomolecular Interactions with
{AlphaFold} 3.''} \emph{Nature} 630 (8016): 493--500.
\url{https://doi.org/10.1038/s41586-024-07487-w}.

\bibitem[\citeproctext]{ref-adzhubei_polyphen_2010}
Adzhubei, Ivan A., Steffen Schmidt, Leonid Peshkin, Vasily E. Ramensky,
Anna Gerasimova, Peer Bork, Alexey S. Kondrashov, and Shamil R. Sunyaev.
2010. {``A Method and Server for Predicting Damaging Missense
Mutations.''} \emph{Nature Methods} 7 (4): 248--49.
\url{https://doi.org/10.1038/nmeth0410-248}.

\bibitem[\citeproctext]{ref-null_all-of-us_2019}
All of Us Research Program Investigators, All of Us; 2019. {``The
{`{All} of {Us}'} {Research} {Program}.''} \emph{New England Journal of
Medicine} 381 (7): 668--76. \url{https://doi.org/10.1056/NEJMsr1809937}.

\bibitem[\citeproctext]{ref-amberger_omim_2015}
Amberger, Joanna S., Carol A. Bocchini, FranÃ§ois Schiettecatte, Alan F.
Scott, and Ada Hamosh. 2015. {``{OMIM}.org: {Online} {Mendelian}
{Inheritance} in {Man} ({OMIM}Â®), an Online Catalog of Human Genes and
Genetic Disorders.''} \emph{Nucleic Acids Research} 43 (D1): D789--98.
\url{https://doi.org/10.1093/nar/gku1205}.

\bibitem[\citeproctext]{ref-auton_1kgp_2015}
Auton, Adam, GonÃ§alo R. Abecasis, David M. Altshuler, Richard M. Durbin,
GonÃ§alo R. Abecasis, David R. Bentley, Aravinda Chakravarti, et al.
2015. {``A Global Reference for Human Genetic Variation.''}
\emph{Nature} 526 (7571): 68--74.
\url{https://doi.org/10.1038/nature15393}.

\bibitem[\citeproctext]{ref-avsec_enformer_2021}
Avsec, Å½iga, Vikram Agarwal, D. Visentin, J. Ledsam, A.
Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, J. Jumper, Pushmeet
Kohli, and David R. Kelley. 2021. {``{[}{Enformer}{]} {Effective} Gene
Expression Prediction from Sequence by Integrating Long-Range
Interactions.''} \emph{Nature Methods} 18 (October): 1196--1203.
\url{https://doi.org/10.1038/s41592-021-01252-x}.

\bibitem[\citeproctext]{ref-avsec_alphagenome_2025}
Avsec, Ziga, Natasha Latysheva, and Jun Cheng. 2025. {``{AlphaGenome}:
{AI} for Better Understanding the Genome.''} \emph{Google DeepMind}.
\url{https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/}.

\bibitem[\citeproctext]{ref-benegas_gpn-msa_2024}
Benegas, Gonzalo, Carlos Albors, Alan J. Aw, Chengzhong Ye, and Yun S.
Song. 2024. {``{GPN}-{MSA}: An Alignment-Based {DNA} Language Model for
Genome-Wide Variant Effect Prediction.''} \emph{bioRxiv}, April,
2023.10.10.561776. \url{https://doi.org/10.1101/2023.10.10.561776}.

\bibitem[\citeproctext]{ref-benegas_gpn_2023}
Benegas, Gonzalo, Sanjit Singh Batra, and Yun S. Song. 2023.
{``{[}{GPN}{]} {DNA} Language Models Are Powerful Predictors of
Genome-Wide Variant Effects.''} \emph{Proceedings of the National
Academy of Sciences} 120 (44): e2311219120.
\url{https://doi.org/10.1073/pnas.2311219120}.

\bibitem[\citeproctext]{ref-benegas_traitgym_2025}
Benegas, Gonzalo, GÃ¶kcen Eraslan, and Yun S. Song. 2025.
{``{[}{TraitGym}{]} {Benchmarking} {DNA} {Sequence} {Models} for
{Causal} {Regulatory} {Variant} {Prediction} in {Human} {Genetics}.''}
bioRxiv. \url{https://doi.org/10.1101/2025.02.11.637758}.

\bibitem[\citeproctext]{ref-deboer_deciphering_2020}
Boer, Carl G. de, Eeshit Dhaval Vaishnav, Ronen Sadeh, Esteban Luis
Abeyta, Nir Friedman, and Aviv Regev. 2020. {``Deciphering Eukaryotic
Gene-Regulatory Logic with 100 Million Random Promoters.''} \emph{Nature
Biotechnology} 38 (1): 56--65.
\url{https://doi.org/10.1038/s41587-019-0315-8}.

\bibitem[\citeproctext]{ref-brandes_genome-wide_2023}
Brandes, Nadav, Grant Goldman, Charlotte H. Wang, Chun Jimmie Ye, and
Vasilis Ntranos. 2023. {``Genome-Wide Prediction of Disease Variant
Effects with a Deep Protein Language Model.''} \emph{Nature Genetics} 55
(9): 1512--22. \url{https://doi.org/10.1038/s41588-023-01465-0}.

\bibitem[\citeproctext]{ref-brixi_evo_2025}
Brixi, Garyk, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg
Brockman, Daniel Chang, Gabriel A. Gonzalez, et al. 2025. {``{[}{Evo}
2{]} {Genome} Modeling and Design Across All Domains of Life with {Evo}
2.''} bioRxiv. \url{https://doi.org/10.1101/2025.02.18.638918}.

\bibitem[\citeproctext]{ref-browning_beagle_2021}
Browning, Brian L., Xiaowen Tian, Ying Zhou, and Sharon R. Browning.
2021. {``Fast Two-Stage Phasing of Large-Scale Sequence Data.''}
\emph{American Journal of Human Genetics} 108 (10): 1880--90.
\url{https://doi.org/10.1016/j.ajhg.2021.08.005}.

\bibitem[\citeproctext]{ref-bycroft_ukbiobank_2018}
Bycroft, Clare, Colin Freeman, Desislava Petkova, Gavin Band, Lloyd T.
Elliott, Kevin Sharp, Allan Motyer, et al. 2018. {``The {UK} {Biobank}
Resource with Deep Phenotyping and Genomic Data.''} \emph{Nature} 562
(7726): 203--9. \url{https://doi.org/10.1038/s41586-018-0579-z}.

\bibitem[\citeproctext]{ref-camillo_cpgpt_2024}
Camillo, Lucas Paulo de Lima, Raghav Sehgal, Jenel Armstrong, Albert T.
Higgins-Chen, Steve Horvath, and Bo Wang. 2024. {``{CpGPT}: A
{Foundation} {Model} for {DNA} {Methylation}.''} bioRxiv.
\url{https://doi.org/10.1101/2024.10.24.619766}.

\bibitem[\citeproctext]{ref-cao_glue_2022}
Cao, Zhi-Jie, and Ge Gao. 2022. {``{[}{GLUE}{]} {Multi}-Omics
Single-Cell Data Integration and Regulatory Inference with Graph-Linked
Embedding.''} \emph{Nature Biotechnology} 40 (10): 1458--66.
\url{https://doi.org/10.1038/s41587-022-01284-4}.

\bibitem[\citeproctext]{ref-chen_rna-fm_2022}
Chen, Jiayang, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze
Yu, Licheng Zong, et al. 2022. {``{[}{RNA}-{FM}{]} {Interpretable} {RNA}
{Foundation} {Model} from {Unannotated} {Data} for {Highly} {Accurate}
{RNA} {Structure} and {Function} {Predictions}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2204.00300}.

\bibitem[\citeproctext]{ref-chen_deepsea_2022}
Chen, Kathleen M., Aaron K. Wong, Olga G. Troyanskaya, and Jian Zhou.
2022. {``{[}{DeepSEA} {Sei}{]} {A} Sequence-Based Global Map of
Regulatory Activity for Deciphering Human Genetics.''} \emph{Nature
Genetics} 54 (7): 940--49.
\url{https://doi.org/10.1038/s41588-022-01102-2}.

\bibitem[\citeproctext]{ref-cheng_alphamissense_2023}
Cheng, Jun, Guido Novati, Joshua Pan, Clare Bycroft, AkvilÄ— Å½emgulytÄ—,
Taylor Applebaum, Alexander Pritzel, et al. 2023.
{``{[}{AlphaMissense}{]} {Accurate} Proteome-Wide Missense Variant
Effect Prediction with {AlphaMissense}.''} \emph{Science} 381 (6664):
eadg7492. \url{https://doi.org/10.1126/science.adg7492}.

\bibitem[\citeproctext]{ref-cheng_dnalongbench_2024}
Cheng, Wenduo, Zhenqiao Song, Yang Zhang, Shike Wang, Danqing Wang, Muyu
Yang, Lei Li, and Jian Ma. 2024. {``{DNALONGBENCH}: {A} {Benchmark}
{Suite} {For} {Long}-{Range} {DNA} {Prediction} {Tasks},''} October.
\url{https://openreview.net/forum?id=opv67PpqLS}.

\bibitem[\citeproctext]{ref-choi_prs_2020}
Choi, Shing Wan, Timothy Shin-Heng Mak, and Paul F. O'Reilly. 2020.
{``{[}{PRS}{]} {Tutorial}: A Guide to Performing Polygenic Risk Score
Analyses.''} \emph{Nature Protocols} 15 (9): 2759--72.
\url{https://doi.org/10.1038/s41596-020-0353-1}.

\bibitem[\citeproctext]{ref-clarke_deeprvat_2024}
Clarke, Brian, Eva Holtkamp, Hakime Ã–ztÃ¼rk, Marcel MÃ¼ck, Magnus
Wahlberg, Kayla Meyer, Felix Munzlinger, et al. 2024.
{``{[}{DeepRVAT}{]} {Integration} of Variant Annotations Using Deep Set
Networks Boosts Rare Variant Association Testing.''} \emph{Nature
Genetics} 56 (10): 2271--80.
\url{https://doi.org/10.1038/s41588-024-01919-z}.

\bibitem[\citeproctext]{ref-cui_scgpt_2024}
Cui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan
Duan, and Bo Wang. 2024. {``{scGPT}: Toward Building a Foundation Model
for Single-Cell Multi-Omics Using Generative {AI}.''} \emph{Nature
Methods} 21 (8): 1470--80.
\url{https://doi.org/10.1038/s41592-024-02201-0}.

\bibitem[\citeproctext]{ref-dabernig_ont_2024}
Dabernig-Heinz, Johanna, Mara Lohde, Martin HÃ¶lzer, Adriana Cabal, Rick
Conzemius, Christian Brandt, Matthias Kohl, et al. 2024. {``A
Multicenter Study on Accuracy and Reproducibility of Nanopore
Sequencing-Based Genotyping of Bacterial Pathogens.''} \emph{Journal of
Clinical Microbiology} 62 (9): e00628--24.
\url{https://doi.org/10.1128/jcm.00628-24}.

\bibitem[\citeproctext]{ref-dalla-torre_nucleotide_2023}
Dalla-Torre, Hugo, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez
Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago,
et al. 2023. {``Nucleotide {Transformer}: Building and Evaluating Robust
Foundation Models for Human Genomics.''} \emph{Nature Methods} 22 (2):
287--97. \url{https://doi.org/10.1038/s41592-024-02523-z}.

\bibitem[\citeproctext]{ref-dauparas_proteinmpnn_2022}
Dauparas, J., I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F.
Milles, B. I. M. Wicky, et al. 2022. {``Robust Deep Learning--Based
Protein Sequence Design Using {ProteinMPNN}.''} \emph{Science} 378
(6615): 49--56. \url{https://doi.org/10.1126/science.add2187}.

\bibitem[\citeproctext]{ref-davydov_gerp_2010}
Davydov, Eugene V., David L. Goode, Marina Sirota, Gregory M. Cooper,
Arend Sidow, and Serafim Batzoglou. 2010. {``Identifying a {High}
{Fraction} of the {Human} {Genome} to Be Under {Selective} {Constraint}
{Using} {GERP}++.''} \emph{PLOS Computational Biology} 6 (12): e1001025.
\url{https://doi.org/10.1371/journal.pcbi.1001025}.

\bibitem[\citeproctext]{ref-depristo_gatk_2011}
DePristo, Mark A., Eric Banks, Ryan Poplin, Kiran V. Garimella, Jared R.
Maguire, Christopher Hartl, Anthony A. Philippakis, et al. 2011. {``A
Framework for Variation Discovery and Genotyping Using Next-Generation
{DNA} Sequencing Data.''} \emph{Nature Genetics} 43 (5): 491--98.
\url{https://doi.org/10.1038/ng.806}.

\bibitem[\citeproctext]{ref-devlin_bert_2019}
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
{``{BERT}: {Pre}-Training of {Deep} {Bidirectional} {Transformers} for
{Language} {Understanding}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.1810.04805}.

\bibitem[\citeproctext]{ref-duncan_analysis_2019}
Duncan, L., H. Shen, B. Gelaye, J. Meijsen, K. Ressler, M. Feldman, R.
Peterson, and B. Domingue. 2019. {``Analysis of Polygenic Risk Score
Usage and Performance in Diverse Human Populations.''} \emph{Nature
Communications} 10 (1): 3328.
\url{https://doi.org/10.1038/s41467-019-11112-0}.

\bibitem[\citeproctext]{ref-edgar_geo_2002}
Edgar, Ron, Michael Domrachev, and Alex E. Lash. 2002. {``Gene
{Expression} {Omnibus}: {NCBI} Gene Expression and Hybridization Array
Data Repository.''} \emph{Nucleic Acids Research} 30 (1): 207--10.
\url{https://doi.org/10.1093/nar/30.1.207}.

\bibitem[\citeproctext]{ref-elnaggar_prottrans_2021}
Elnaggar, Ahmed, Michael Heinzinger, Christian Dallago, Ghalia Rihawi,
Yu Wang, Llion Jones, Tom Gibbs, et al. 2021. {``{ProtTrans}: {Towards}
{Cracking} the {Language} of {Life}'s {Code} {Through}
{Self}-{Supervised} {Deep} {Learning} and {High} {Performance}
{Computing}.''} arXiv. \url{https://doi.org/10.48550/arXiv.2007.06225}.

\bibitem[\citeproctext]{ref-ferruz_protgpt2_2022}
Ferruz, Noelia, Steffen Schmidt, and Birte HÃ¶cker. 2022. {``{ProtGPT2}
Is a Deep Unsupervised Language Model for Protein Design.''}
\emph{Nature Communications} 13 (1): 4348.
\url{https://doi.org/10.1038/s41467-022-32007-7}.

\bibitem[\citeproctext]{ref-fishman_gena-lm_2025}
Fishman, Veniamin, Yuri Kuratov, Aleksei Shmelev, Maxim Petrov, Dmitry
Penzar, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, and Mikhail
Burtsev. 2025. {``{GENA}-{LM}: A Family of Open-Source Foundational
{DNA} Language Models for Long Sequences.''} \emph{Nucleic Acids
Research} 53 (2): gkae1310. \url{https://doi.org/10.1093/nar/gkae1310}.

\bibitem[\citeproctext]{ref-frankish_gencode_2019}
Frankish, Adam, Mark Diekhans, Anne-Maud Ferreira, Rory Johnson, Irwin
Jungreis, Jane Loveland, Jonathan M Mudge, et al. 2019. {``{GENCODE}
Reference Annotation for the Human and Mouse Genomes.''} \emph{Nucleic
Acids Research} 47 (D1): D766--73.
\url{https://doi.org/10.1093/nar/gky955}.

\bibitem[\citeproctext]{ref-frazer_eve_2021}
Frazer, Jonathan, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K.
Min, Kelly Brock, Yarin Gal, and Debora S. Marks. 2021. {``{[}{EVE}{]}
{Disease} Variant Prediction with Deep Generative Models of Evolutionary
Data.''} \emph{Nature} 599 (7883): 91--95.
\url{https://doi.org/10.1038/s41586-021-04043-8}.

\bibitem[\citeproctext]{ref-gamazon_predixcan_2015}
Gamazon, Eric R., Heather E. Wheeler, Kaanan P. Shah, Sahar V.
Mozaffari, Keston Aquino-Michaels, Robert J. Carroll, Anne E. Eyler, et
al. 2015. {``A Gene-Based Association Method for Mapping Traits Using
Reference Transcriptome Data.''} \emph{Nature Genetics} 47 (9):
1091--98. \url{https://doi.org/10.1038/ng.3367}.

\bibitem[\citeproctext]{ref-garrison_vgtool_2018}
Garrison, Erik, Jouni SirÃ©n, Adam M. Novak, Glenn Hickey, Jordan M.
Eizenga, Eric T. Dawson, William Jones, et al. 2018. {``Variation Graph
Toolkit Improves Read Mapping by Representing Genetic Variation in the
Reference.''} \emph{Nature Biotechnology} 36 (9): 875--79.
\url{https://doi.org/10.1038/nbt.4227}.

\bibitem[\citeproctext]{ref-georgantas_delphi_2024}
Georgantas, Costa, ZoltÃ¡n Kutalik, and Jonas Richiardi. 2024. {``Delphi:
{A} {Deep}-Learning {Method} for {Polygenic} {Risk} {Prediction}.''}
medRxiv. \url{https://doi.org/10.1101/2024.04.19.24306079}.

\bibitem[\citeproctext]{ref-goodwin_10year_2016}
Goodwin, Sara, John D. McPherson, and W. Richard McCombie. 2016.
{``Coming of Age: Ten Years of Next-Generation Sequencing
Technologies.''} \emph{Nature Reviews Genetics} 17 (6): 333--51.
\url{https://doi.org/10.1038/nrg.2016.49}.

\bibitem[\citeproctext]{ref-gresova_genomic_2023}
GreÅ¡ovÃ¡, KatarÃ­na, Vlastimil Martinek, David ÄŒechÃ¡k, Petr Å imeÄek, and
Panagiotis Alexiou. 2023. {``Genomic Benchmarks: A Collection of
Datasets for Genomic Sequence Classification.''} \emph{BMC Genomic Data}
24 (1): 25. \url{https://doi.org/10.1186/s12863-023-01123-8}.

\bibitem[\citeproctext]{ref-gusev_twas_2016}
Gusev, Alexander, Arthur Ko, Huwenbo Shi, Gaurav Bhatia, Wonil Chung,
Brenda W. J. H. Penninx, Rick Jansen, et al. 2016. {``Integrative
Approaches for Large-Scale Transcriptome-Wide Association Studies.''}
\emph{Nature Genetics} 48 (3): 245--52.
\url{https://doi.org/10.1038/ng.3506}.

\bibitem[\citeproctext]{ref-ioannidis_revel_2016}
Ioannidis, Nilah M., Joseph H. Rothstein, Vikas Pejaver, Sumit Middha,
Shannon K. McDonnell, Saurabh Baheti, Anthony Musolf, et al. 2016.
{``{REVEL}: {An} {Ensemble} {Method} for {Predicting} the
{Pathogenicity} of {Rare} {Missense} {Variants}.''} \emph{The American
Journal of Human Genetics} 99 (4): 877--85.
\url{https://doi.org/10.1016/j.ajhg.2016.08.016}.

\bibitem[\citeproctext]{ref-jaganathan_spliceai_2019}
Jaganathan, Kishore, Sofia Kyriazopoulou Panagiotopoulou, Jeremy F.
McRae, Siavash Fazel Darbandi, David Knowles, Yang I. Li, Jack A.
Kosmicki, et al. 2019. {``{[}{SpliceAI}{]} {Predicting} {Splicing} from
{Primary} {Sequence} with {Deep} {Learning}.''} \emph{Cell} 176 (3):
535--548.e24. \url{https://doi.org/10.1016/j.cell.2018.12.015}.

\bibitem[\citeproctext]{ref-ji_dnabert_2021}
Ji, Yanrong, Zhihan Zhou, Han Liu, and Ramana V Davuluri. 2021.
{``{DNABERT}: Pre-Trained {Bidirectional} {Encoder} {Representations}
from {Transformers} Model for {DNA}-Language in Genome.''}
\emph{Bioinformatics} 37 (15): 2112--20.
\url{https://doi.org/10.1093/bioinformatics/btab083}.

\bibitem[\citeproctext]{ref-jiang_cutesv_2020}
Jiang, Tao, Yongzhuang Liu, Yue Jiang, Junyi Li, Yan Gao, Zhe Cui,
Yadong Liu, Bo Liu, and Yadong Wang. 2020. {``Long-Read-Based Human
Genomic Structural Variation Detection with {cuteSV}.''} \emph{Genome
Biology} 21 (1): 189. \url{https://doi.org/10.1186/s13059-020-02107-y}.

\bibitem[\citeproctext]{ref-jumper_alphafold2_2021}
Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael
Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021.
{``{[}{AlphaFold2}{]} {Highly} Accurate Protein Structure Prediction
with {AlphaFold}.''} \emph{Nature} 596 (7873): 583--89.
\url{https://doi.org/10.1038/s41586-021-03819-2}.

\bibitem[\citeproctext]{ref-jurenaite_setquence_2024}
Jurenaite, Neringa, Daniel LeÃ³n-PeriÃ±Ã¡n, Veronika Donath, Sunna Torge,
and RenÃ© JÃ¤kel. 2024. {``{SetQuence} \& {SetOmic}: {Deep} Set
Transformers for Whole Genome and Exome Tumour Analysis.''}
\emph{BioSystems} 235 (January): 105095.
\url{https://doi.org/10.1016/j.biosystems.2023.105095}.

\bibitem[\citeproctext]{ref-kagda_encode_2025}
Kagda, Meenakshi S., Bonita Lam, Casey Litton, Corinn Small, Cricket A.
Sloan, Emma Spragins, Forrest Tanaka, et al. 2025. {``Data Navigation on
the {ENCODE} Portal.''} \emph{Nature Communications} 16 (1): 9592.
\url{https://doi.org/10.1038/s41467-025-64343-9}.

\bibitem[\citeproctext]{ref-karczewski_gnomad_2020}
Karczewski, Konrad J., Laurent C. Francioli, Grace Tiao, Beryl B.
Cummings, Jessica AlfÃ¶ldi, Qingbo Wang, Ryan L. Collins, et al. 2020.
{``The Mutational Constraint Spectrum Quantified from Variation in
141,456 Humans.''} \emph{Nature} 581 (7809): 434--43.
\url{https://doi.org/10.1038/s41586-020-2308-7}.

\bibitem[\citeproctext]{ref-kelley_basenji2_2020}
Kelley, David R. 2020a. {``{[}{Basenji2}{]} {Cross}-Species Regulatory
Sequence Activity Prediction.''} \emph{PLOS Computational Biology} 16
(7): e1008050. \url{https://doi.org/10.1371/journal.pcbi.1008050}.

\bibitem[\citeproctext]{ref-kelley_cross-species_2020}
---------. 2020b. {``Cross-Species Regulatory Sequence Activity
Prediction.''} \emph{PLOS Computational Biology} 16 (7): e1008050.
\url{https://doi.org/10.1371/journal.pcbi.1008050}.

\bibitem[\citeproctext]{ref-kelley_basenji_2018}
Kelley, David R., Yakir A. Reshef, Maxwell Bileschi, David Belanger,
Cory Y. McLean, and Jasper Snoek. 2018. {``{[}{Basenji2}{]} {Sequential}
Regulatory Activity Prediction Across Chromosomes with Convolutional
Neural Networks.''} \emph{Genome Research} 28 (5): 739--50.
\url{https://doi.org/10.1101/gr.227819.117}.

\bibitem[\citeproctext]{ref-kircher_general_2014}
Kircher, Martin, Daniela M. Witten, Preti Jain, Brian J. O'Roak, Gregory
M. Cooper, and Jay Shendure. 2014. {``A General Framework for Estimating
the Relative Pathogenicity of Human Genetic Variants.''} \emph{Nature
Genetics} 46 (3): 310--15. \url{https://doi.org/10.1038/ng.2892}.

\bibitem[\citeproctext]{ref-krusche_happy_2019}
Krusche, Peter, Len Trigg, Paul C. Boutros, Christopher E. Mason,
Francisco M. De La Vega, Benjamin L. Moore, Mar Gonzalez-Porta, et al.
2019. {``Best {Practices} for {Benchmarking} {Germline} {Small}
{Variant} {Calls} in {Human} {Genomes}.''} \emph{Nature Biotechnology}
37 (5): 555--60. \url{https://doi.org/10.1038/s41587-019-0054-x}.

\bibitem[\citeproctext]{ref-kundaje_roadmap_2015}
Kundaje, Anshul, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela
Yen, Alireza Heravi-Moussavi, Pouya Kheradpour, et al. 2015.
{``Integrative Analysis of 111 Reference Human Epigenomes.''}
\emph{Nature} 518 (7539): 317--30.
\url{https://doi.org/10.1038/nature14248}.

\bibitem[\citeproctext]{ref-kurki_finngen_2023}
Kurki, Mitja I., Juha Karjalainen, Priit Palta, Timo P. SipilÃ¤, Kati
Kristiansson, Kati M. Donner, Mary P. Reeve, et al. 2023. {``{FinnGen}
Provides Genetic Insights from a Well-Phenotyped Isolated Population.''}
\emph{Nature} 613 (7944): 508--18.
\url{https://doi.org/10.1038/s41586-022-05473-8}.

\bibitem[\citeproctext]{ref-lambert_pgs-catalog_2021}
Lambert, Samuel A., Laurent Gil, Simon Jupp, Scott C. Ritchie, Yu Xu,
Annalisa Buniello, Aoife McMahon, et al. 2021. {``The {Polygenic}
{Score} {Catalog} as an Open Database for Reproducibility and Systematic
Evaluation.''} \emph{Nature Genetics} 53 (4): 420--25.
\url{https://doi.org/10.1038/s41588-021-00783-5}.

\bibitem[\citeproctext]{ref-landrum_clinvar_2018}
Landrum, Melissa J, Jennifer M Lee, Mark Benson, Garth R Brown, Chen
Chao, Shanmuga Chitipiralla, Baoshan Gu, et al. 2018. {``{ClinVar}:
Improving Access to Variant Interpretations and Supporting Evidence.''}
\emph{Nucleic Acids Research} 46 (D1): D1062--67.
\url{https://doi.org/10.1093/nar/gkx1153}.

\bibitem[\citeproctext]{ref-lee_g2pt_2025}
Lee, Ingoo, Zachary S. Wallace, Yuqi Wang, Sungjoon Park, Hojung Nam,
Amit R. Majithia, and Trey Ideker. 2025. {``{[}{G2PT}{]} {A}
Genotype-Phenotype Transformer to Assess and Explain Polygenic Risk.''}
bioRxiv. \url{https://doi.org/10.1101/2024.10.23.619940}.

\bibitem[\citeproctext]{ref-li_cgmega_2024}
Li, Hao, Zebei Han, Yu Sun, Fu Wang, Pengzhen Hu, Yuang Gao, Xuemei Bai,
et al. 2024. {``{CGMega}: Explainable Graph Neural Network Framework
with Attention Mechanisms for Cancer Gene Module Dissection.''}
\emph{Nature Communications} 15 (1): 5997.
\url{https://doi.org/10.1038/s41467-024-50426-6}.

\bibitem[\citeproctext]{ref-li_bwa-mem_2013}
Li, Heng. 2013. {``Aligning Sequence Reads, Clone Sequences and Assembly
Contigs with {BWA}-{MEM}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.1303.3997}.

\bibitem[\citeproctext]{ref-li_mapping_2014}
---------. 2014. {``Towards {Better} {Understanding} of {Artifacts} in
{Variant} {Calling} from {High}-{Coverage} {Samples}.''}
\emph{Bioinformatics} 30 (20): 2843--51.
\url{https://doi.org/10.1093/bioinformatics/btu356}.

\bibitem[\citeproctext]{ref-li_minimap2_2018}
---------. 2018. {``Minimap2: Pairwise Alignment for Nucleotide
Sequences.''} \emph{Bioinformatics} 34 (18): 3094--3100.
\url{https://doi.org/10.1093/bioinformatics/bty191}.

\bibitem[\citeproctext]{ref-li_codonbert_2023}
Li, Sizhen, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi,
Milad Miladi, Jacob Miner, et al. 2023. {``{CodonBERT}: {Large}
{Language} {Models} for {mRNA} Design and Optimization.''} bioRxiv.
\url{https://doi.org/10.1101/2023.09.09.556981}.

\bibitem[\citeproctext]{ref-li_mogcn_2022}
Li, Xiao, Jie Ma, Ling Leng, Mingfei Han, Mansheng Li, Fuchu He, and
Yunping Zhu. 2022. {``{MoGCN}: {A} {Multi}-{Omics} {Integration}
{Method} {Based} on {Graph} {Convolutional} {Network} for {Cancer}
{Subtype} {Analysis}.''} \emph{Frontiers in Genetics} 13 (February).
\url{https://doi.org/10.3389/fgene.2022.806842}.

\bibitem[\citeproctext]{ref-li_omnidna_2025}
Li, Zehui, Vallijah Subasri, Yifei Shen, Dongsheng Li, Yiren Zhao,
Guy-Bart Stan, and Caihua Shan. 2025. {``Omni-{DNA}: {A} {Unified}
{Genomic} {Foundation} {Model} for {Cross}-{Modal} and {Multi}-{Task}
{Learning}.''} arXiv. \url{https://doi.org/10.48550/arXiv.2502.03499}.

\bibitem[\citeproctext]{ref-liao_pangenome_2023}
Liao, Wen-Wei, Mobin Asri, Jana Ebler, Daniel Doerr, Marina Haukness,
Glenn Hickey, Shuangjia Lu, et al. 2023. {``A Draft Human Pangenome
Reference.''} \emph{Nature} 617 (7960): 312--24.
\url{https://doi.org/10.1038/s41586-023-05896-x}.

\bibitem[\citeproctext]{ref-lin_esm-2_2022}
Lin, Zeming, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting
Lu, Allan dos Santos Costa, et al. 2022. {``{[}{ESM}-2{]} {Language}
Models of Protein Sequences at the Scale of Evolution Enable Accurate
Structure Prediction.''} bioRxiv.
\url{https://doi.org/10.1101/2022.07.20.500902}.

\bibitem[\citeproctext]{ref-linder_borzoi_2025}
Linder, Johannes, Divyanshi Srivastava, Han Yuan, Vikram Agarwal, and
David R. Kelley. 2025. {``{[}{Borzoi}{]} {Predicting} {RNA}-Seq Coverage
from {DNA} Sequence as a Unifying Model of Gene Regulation.''}
\emph{Nature Genetics} 57 (4): 949--61.
\url{https://doi.org/10.1038/s41588-024-02053-6}.

\bibitem[\citeproctext]{ref-liu_life-code_2025}
Liu, Zicheng, Siyuan Li, Zhiyuan Chen, Fang Wu, Chang Yu, Qirong Yang,
Yucheng Guo, Yujie Yang, Xiaoming Zhang, and Stan Z. Li. 2025.
{``Life-{Code}: {Central} {Dogma} {Modeling} with {Multi}-{Omics}
{Sequence} {Unification}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2502.07299}.

\bibitem[\citeproctext]{ref-loh_eagle_2016}
Loh, Po-Ru, Petr Danecek, Pier Francesco Palamara, Christian
Fuchsberger, Yakir A Reshef, Hilary K Finucane, Sebastian Schoenherr, et
al. 2016. {``Reference-Based Phasing Using the {Haplotype} {Reference}
{Consortium} Panel.''} \emph{Nature Genetics} 48 (11): 1443--48.
\url{https://doi.org/10.1038/ng.3679}.

\bibitem[\citeproctext]{ref-madani_progen_2023}
Madani, Ali, Ben Krause, Eric R. Greene, Subu Subramanian, Benjamin P.
Mohr, James M. Holton, Jose Luis Olmos, et al. 2023. {``Large Language
Models Generate Functional Protein Sequences Across Diverse Families.''}
\emph{Nature Biotechnology} 41 (8): 1099--1106.
\url{https://doi.org/10.1038/s41587-022-01618-2}.

\bibitem[\citeproctext]{ref-mallal_abacavir_2008}
Mallal, Simon, Elizabeth Phillips, Giampiero Carosi, Jean-Michel Molina,
Cassy Workman, Janez TomaÅ¾iÄ, Eva JÃ¤gel-Guedes, et al. 2008.
{``{HLA}-{B}*5701 {Screening} for {Hypersensitivity} to {Abacavir}.''}
\emph{New England Journal of Medicine} 358 (6): 568--79.
\url{https://doi.org/10.1056/NEJMoa0706135}.

\bibitem[\citeproctext]{ref-manolio_finding_2009}
Manolio, Teri A., Francis S. Collins, Nancy J. Cox, David B. Goldstein,
Lucia A. Hindorff, David J. Hunter, Mark I. McCarthy, et al. 2009.
{``Finding the Missing Heritability of Complex Diseases.''}
\emph{Nature} 461 (7265): 747--53.
\url{https://doi.org/10.1038/nature08494}.

\bibitem[\citeproctext]{ref-manzo_comparative_2025}
Manzo, Gaetano, Kathryn Borkowski, and Ivan Ovcharenko. 2025.
{``Comparative {Analysis} of {Deep} {Learning} {Models} for {Predicting}
{Causative} {Regulatory} {Variants}.''} \emph{bioRxiv: The Preprint
Server for Biology}, June, 2025.05.19.654920.
\url{https://doi.org/10.1101/2025.05.19.654920}.

\bibitem[\citeproctext]{ref-marees_gwas_2018}
Marees, Andries T., Hilde de Kluiver, Sven Stringer, Florence Vorspan,
Emmanuel Curis, Cynthia Marie-Claire, and Eske M. Derks. 2018.
{``{[}{GWAS}{]} {A} Tutorial on Conducting Genome-Wide Association
Studies: {Quality} Control and Statistical Analysis.''}
\emph{International Journal of Methods in Psychiatric Research} 27 (2):
e1608. \url{https://doi.org/10.1002/mpr.1608}.

\bibitem[\citeproctext]{ref-marin_bend_2024}
Marin, Frederikke Isa, Felix Teufel, Marc Horlacher, Dennis Madsen,
Dennis Pultz, Ole Winther, and Wouter Boomsma. 2024. {``{BEND}:
{Benchmarking} {DNA} {Language} {Models} on Biologically Meaningful
Tasks.''} arXiv. \url{https://doi.org/10.48550/arXiv.2311.12570}.

\bibitem[\citeproctext]{ref-medvedev_biotoken_2025}
Medvedev, Aleksandr, Karthik Viswanathan, Praveenkumar Kanithi, Kirill
Vishniakov, Prateek Munjal, ClÃ©ment Christophe, Marco AF Pimentel,
Ronnie Rajan, and Shadab Khan. 2025. {``{BioToken} and {BioFM} --
{Biologically}-{Informed} {Tokenization} {Enables} {Accurate} and
{Efficient} {Genomic} {Foundation} {Models}.''} bioRxiv.
\url{https://doi.org/10.1101/2025.03.27.645711}.

\bibitem[\citeproctext]{ref-meier_esm-1v_2021}
Meier, Joshua, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and
Alexander Rives. 2021. {``{[}{ESM}-1v{]} {Language} Models Enable
Zero-Shot Prediction of the Effects of Mutations on Protein Function.''}
bioRxiv. \url{https://doi.org/10.1101/2021.07.09.450648}.

\bibitem[\citeproctext]{ref-morales_mane_2022}
Morales, Joannella, Shashikant Pujar, Jane E. Loveland, Alex Astashyn,
Ruth Bennett, Andrew Berry, Eric Cox, et al. 2022. {``A Joint {NCBI} and
{EMBL}-{EBI} Transcript Set for Clinical Genomics and Research.''}
\emph{Nature} 604 (7905): 310--15.
\url{https://doi.org/10.1038/s41586-022-04558-8}.

\bibitem[\citeproctext]{ref-mountjoy_open_2021}
Mountjoy, Edward, Ellen M. Schmidt, Miguel Carmona, Jeremy
Schwartzentruber, Gareth Peat, Alfredo Miranda, Luca Fumis, et al. 2021.
{``An Open Approach to Systematically Prioritize Causal Variants and
Genes at All Published Human {GWAS} Trait-Associated Loci.''}
\emph{Nature Genetics} 53 (11): 1527--33.
\url{https://doi.org/10.1038/s41588-021-00945-5}.

\bibitem[\citeproctext]{ref-mukherjee_embedgem_2024}
Mukherjee, Sumit, Zachary R. McCaw, Jingwen Pei, Anna Merkoulovitch, Tom
Soare, Raghav Tandon, David Amar, et al. 2024. {``{EmbedGEM}: A
Framework to Evaluate the Utility of Embeddings for Genetic
Discovery.''} \emph{Bioinformatics Advances} 4 (1).
\url{https://doi.org/10.1093/bioadv/vbae135}.

\bibitem[\citeproctext]{ref-naghipourfar_cdsfm_2024}
Naghipourfar, Mohsen, Siyu Chen, Mathew K. Howard, Christian B.
Macdonald, Ali Saberi, Timo Hagen, Mohammad R. K. Mofrad, Willow
Coyote-Maestas, and Hani Goodarzi. 2024. {``{[}{cdsFM} -
{EnCodon}/{DeCodon}{]} {A} {Suite} of {Foundation} {Models} {Captures}
the {Contextual} {Interplay} {Between} {Codons}.''} bioRxiv.
\url{https://doi.org/10.1101/2024.10.10.617568}.

\bibitem[\citeproctext]{ref-ng_sift_2003}
Ng, Pauline C., and Steven Henikoff. 2003. {``{SIFT}: {Predicting} Amino
Acid Changes That Affect Protein Function.''} \emph{Nucleic Acids
Research} 31 (13): 3812--14. \url{https://doi.org/10.1093/nar/gkg509}.

\bibitem[\citeproctext]{ref-nguyen_hyenadna_2023}
Nguyen, Eric, Michael Poli, Marjan Faizi, Armin Thomas, Callum
Birch-Sykes, Michael Wornow, Aman Patel, et al. 2023. {``{HyenaDNA}:
{Long}-{Range} {Genomic} {Sequence} {Modeling} at {Single} {Nucleotide}
{Resolution}.''} arXiv. \url{https://doi.org/10.48550/arXiv.2306.15794}.

\bibitem[\citeproctext]{ref-nielsen_error_2011}
Nielsen, Rasmus, Joshua S. Paul, Anders Albrechtsen, and Yun S. Song.
2011. {``Genotype and {SNP} Calling from Next-Generation Sequencing
Data.''} \emph{Nature Reviews. Genetics} 12 (6): 443--51.
\url{https://doi.org/10.1038/nrg2986}.

\bibitem[\citeproctext]{ref-notin_proteingym_2023}
Notin, Pascal, Aaron Kollasch, Daniel Ritter, Lood van Niekerk,
Steffanie Paul, Han Spinner, Nathan Rollins, et al. 2023.
{``{ProteinGym}: {Large}-{Scale} {Benchmarks} for {Protein} {Fitness}
{Prediction} and {Design}.''} \emph{Advances in Neural Information
Processing Systems} 36 (December): 64331--79.
\url{https://papers.nips.cc/paper_files/paper/2023/hash/cac723e5ff29f65e3fcbb0739ae91bee-Abstract-Datasets_and_Benchmarks.html}.

\bibitem[\citeproctext]{ref-nurk_complete_2022}
Nurk, Sergey, Sergey Koren, Arang Rhie, Mikko Rautiainen, Andrey V.
Bzikadze, Alla Mikheenko, Mitchell R. Vollger, et al. 2022. {``The
Complete Sequence of a Human Genome.''} \emph{Science} 376 (6588):
44--53. \url{https://doi.org/10.1126/science.abj6987}.

\bibitem[\citeproctext]{ref-oconnell_shapeit2_2014}
O'Connell, Jared, Deepti Gurdasani, Olivier Delaneau, Nicola Pirastu,
Sheila Ulivi, Massimiliano Cocca, Michela Traglia, et al. 2014. {``A
{General} {Approach} for {Haplotype} {Phasing} Across the {Full}
{Spectrum} of {Relatedness}.''} \emph{PLOS Genetics} 10 (4): e1004234.
\url{https://doi.org/10.1371/journal.pgen.1004234}.

\bibitem[\citeproctext]{ref-oleary_refseq_2016}
O'Leary, Nuala A., Mathew W. Wright, J. Rodney Brister, Stacy Ciufo,
Diana Haddad, Rich McVeigh, Bhanu Rajput, et al. 2016. {``Reference
Sequence ({RefSeq}) Database at {NCBI}: Current Status, Taxonomic
Expansion, and Functional Annotation.''} \emph{Nucleic Acids Research}
44 (D1): D733--45. \url{https://doi.org/10.1093/nar/gkv1189}.

\bibitem[\citeproctext]{ref-orenbuch_popeve_2025}
Orenbuch, Rose, Courtney A. Shearer, Aaron W. Kollasch, Aviv D. Spinner,
Thomas Hopf, Lood van Niekerk, Dinko Franceschi, Mafalda Dias, Jonathan
Frazer, and Debora S. Marks. 2025. {``{[}{popEVE}{]} {Proteome}-Wide
Model for Human Disease Genetics.''} \emph{Nature Genetics}, November,
1--10. \url{https://doi.org/10.1038/s41588-025-02400-1}.

\bibitem[\citeproctext]{ref-noauthor_pbsv_2025}
{``{PacificBiosciences}/Pbsv.''} 2025. PacBio.
\url{https://github.com/PacificBiosciences/pbsv}.

\bibitem[\citeproctext]{ref-patterson_population_2006}
Patterson, Nick, Alkes L. Price, and David Reich. 2006. {``Population
{Structure} and {Eigenanalysis}.''} \emph{PLOS Genetics} 2 (12): e190.
\url{https://doi.org/10.1371/journal.pgen.0020190}.

\bibitem[\citeproctext]{ref-peer_estimation_2008}
Pe'er, Itsik, Roman Yelensky, David Altshuler, and Mark J. Daly. 2008.
{``Estimation of the Multiple Testing Burden for Genomewide Association
Studies of Nearly All Common Variants.''} \emph{Genetic Epidemiology} 32
(4): 381--85. \url{https://doi.org/10.1002/gepi.20303}.

\bibitem[\citeproctext]{ref-pearce_transcriptformer_2025}
Pearce, James D., Sara E. Simmonds, Gita Mahmoudabadi, Lakshmi Krishnan,
Giovanni Palla, Ana-Maria Istrate, Alexander Tarashansky, et al. 2025.
{``{[}{TranscriptFormer}{]} {Cross}-{Species} {Generative} {Cell}
{Atlas} {Across} 1.5 {Billion} {Years} of {Evolution}: {The}
{TranscriptFormer} {Single}-Cell {Model}.''} bioRxiv.
\url{https://doi.org/10.1101/2025.04.25.650731}.

\bibitem[\citeproctext]{ref-pejaver_calibration_2022}
Pejaver, Vikas, Alicia B. Byrne, Bing-Jian Feng, Kymberleigh A. Pagel,
Sean D. Mooney, Rachel Karchin, Anne O'Donnell-Luria, et al. 2022.
{``Calibration of Computational Tools for Missense Variant Pathogenicity
Classification and {ClinGen} Recommendations for {PP3}/{BP4}
Criteria.''} \emph{American Journal of Human Genetics} 109 (12):
2163--77. \url{https://doi.org/10.1016/j.ajhg.2022.10.013}.

\bibitem[\citeproctext]{ref-poplin_deepvariant_2018}
Poplin, Ryan, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas
Colthurst, Alexander Ku, Dan Newburger, et al. 2018.
{``{[}{DeepVariant}{]} {A} Universal {SNP} and Small-Indel Variant
Caller Using Deep Neural Networks.''} \emph{Nature Biotechnology} 36
(10): 983--87. \url{https://doi.org/10.1038/nbt.4235}.

\bibitem[\citeproctext]{ref-price_pca_2006}
Price, Alkes L., Nick J. Patterson, Robert M. Plenge, Michael E.
Weinblatt, Nancy A. Shadick, and David Reich. 2006. {``Principal
Components Analysis Corrects for Stratification in Genome-Wide
Association Studies.''} \emph{Nature Genetics} 38 (8): 904--9.
\url{https://doi.org/10.1038/ng1847}.

\bibitem[\citeproctext]{ref-raffel_t5_2019}
Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. {``Exploring
the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text}
{Transformer}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.1910.10683}.

\bibitem[\citeproctext]{ref-rakowski_mifm_2025}
Rakowski, Alexander, and Christoph Lippert. 2025. {``{[}{MIFM}{]}
{Multiple} Instance Fine-Mapping: Predicting Causal Regulatory Variants
with a Deep Sequence Model.''} medRxiv.
\url{https://doi.org/10.1101/2025.06.13.25329551}.

\bibitem[\citeproctext]{ref-noauthor_rtg-core_2025}
{``{RealTimeGenomics}/Rtg-Core.''} 2025. Real Time Genomics.
\url{https://github.com/RealTimeGenomics/rtg-core}.

\bibitem[\citeproctext]{ref-regev_cell-atlas_2017}
Regev, Aviv, Sarah A Teichmann, Eric S Lander, Ido Amit, Christophe
Benoist, Ewan Birney, Bernd Bodenmiller, et al. 2017. {``The {Human}
{Cell} {Atlas}.''} Edited by Thomas R Gingeras. \emph{eLife} 6
(December): e27041. \url{https://doi.org/10.7554/eLife.27041}.

\bibitem[\citeproctext]{ref-rehm_clingen_2015}
Rehm, Heidi L., Jonathan S. Berg, Lisa D. Brooks, Carlos D. Bustamante,
James P. Evans, Melissa J. Landrum, David H. Ledbetter, et al. 2015.
{``{ClinGen} --- {The} {Clinical} {Genome} {Resource}.''} \emph{New
England Journal of Medicine} 372 (23): 2235--42.
\url{https://doi.org/10.1056/NEJMsr1406261}.

\bibitem[\citeproctext]{ref-rentzsch_cadd_2019}
Rentzsch, Philipp, Daniela Witten, Gregory M Cooper, Jay Shendure, and
Martin Kircher. 2019. {``{CADD}: Predicting the Deleteriousness of
Variants Throughout the Human Genome.''} \emph{Nucleic Acids Research}
47 (D1): D886--94. \url{https://doi.org/10.1093/nar/gky1016}.

\bibitem[\citeproctext]{ref-rives_esm_2021}
Rives, Alexander, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin,
Jason Liu, Demi Guo, et al. 2021. {``{[}{ESM}-1b{]} {Biological}
Structure and Function Emerge from Scaling Unsupervised Learning to 250
Million Protein Sequences.''} \emph{Proceedings of the National Academy
of Sciences of the United States of America} 118 (15): e2016239118.
\url{https://doi.org/10.1073/pnas.2016239118}.

\bibitem[\citeproctext]{ref-robinson_hla-db_2020}
Robinson, James, Dominic J Barker, Xenia Georgiou, Michael A Cooper,
Paul Flicek, and Steven G E Marsh. 2020. {``{IPD}-{IMGT}/{HLA}
{Database}.''} \emph{Nucleic Acids Research} 48 (D1): D948--55.
\url{https://doi.org/10.1093/nar/gkz950}.

\bibitem[\citeproctext]{ref-sakaue_hla_2023}
Sakaue, Saori, Saisriram Gurajala, Michelle Curtis, Yang Luo, Wanson
Choi, Kazuyoshi Ishigaki, Joyce B. Kang, et al. 2023. {``Tutorial: A
Statistical Genetics Guide to Identifying {HLA} Alleles Driving Complex
Disease.''} \emph{Nature Protocols} 18 (9): 2625--41.
\url{https://doi.org/10.1038/s41596-023-00853-4}.

\bibitem[\citeproctext]{ref-sanabria_grover_2024}
Sanabria, Melissa, Jonas Hirsch, Pierre M. Joubert, and Anna R. Poetsch.
2024. {``{[}{GROVER}{]} {DNA} Language Model {GROVER} Learns Sequence
Context in the Human Genome.''} \emph{Nature Machine Intelligence} 6
(8): 911--23. \url{https://doi.org/10.1038/s42256-024-00872-0}.

\bibitem[\citeproctext]{ref-schiff_caduceus_2024}
Schiff, Yair, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and
Volodymyr Kuleshov. 2024. {``Caduceus: {Bi}-{Directional} {Equivariant}
{Long}-{Range} {DNA} {Sequence} {Modeling}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2403.03234}.

\bibitem[\citeproctext]{ref-schubach_cadd_2024}
Schubach, Max, Thorben Maass, LusinÃ© Nazaretyan, Sebastian RÃ¶ner, and
Martin Kircher. 2024. {``{CADD} V1.7: Using Protein Language Models,
Regulatory {CNNs} and Other Nucleotide-Level Scores to Improve
Genome-Wide Variant Predictions.''} \emph{Nucleic Acids Research} 52
(D1): D1143--54. \url{https://doi.org/10.1093/nar/gkad989}.

\bibitem[\citeproctext]{ref-shafin_pepper_2021}
Shafin, Kishwar, Trevor Pesout, Pi-Chuan Chang, Maria Nattestad, Alexey
Kolesnikov, Sidharth Goel, Gunjan Baid, et al. 2021. {``Haplotype-Aware
Variant Calling with {PEPPER}-{Margin}-{DeepVariant} Enables High
Accuracy in Nanopore Long-Reads.''} \emph{Nature Methods} 18 (11):
1322--32. \url{https://doi.org/10.1038/s41592-021-01299-w}.

\bibitem[\citeproctext]{ref-sherry_dbsnp_2001}
Sherry, S. T., M.-H. Ward, M. Kholodov, J. Baker, L. Phan, E. M.
Smigielski, and K. Sirotkin. 2001. {``{dbSNP}: The {NCBI} Database of
Genetic Variation.''} \emph{Nucleic Acids Research} 29 (1): 308--11.
\url{https://doi.org/10.1093/nar/29.1.308}.

\bibitem[\citeproctext]{ref-siepel_phastcons_2005}
Siepel, Adam, Gill Bejerano, Jakob S. Pedersen, Angie S. Hinrichs,
Minmei Hou, Kate Rosenbloom, Hiram Clawson, et al. 2005.
{``{[}{PhastCons}{]} {Evolutionarily} Conserved Elements in Vertebrate,
Insect, Worm, and Yeast Genomes.''} \emph{Genome Research} 15 (8):
1034--50. \url{https://doi.org/10.1101/gr.3715005}.

\bibitem[\citeproctext]{ref-sirugo_diversity_2019}
Sirugo, Giorgio, Scott M. Williams, and Sarah A. Tishkoff. 2019. {``The
{Missing} {Diversity} in {Human} {Genetic} {Studies}.''} \emph{Cell} 177
(1): 26--31. \url{https://doi.org/10.1016/j.cell.2019.02.048}.

\bibitem[\citeproctext]{ref-smolka_sniffles2_2024}
Smolka, Moritz, Luis F. Paulin, Christopher M. Grochowski, Dominic W.
Horner, Medhat Mahmoud, Sairam Behera, Ester Kalef-Ezra, et al. 2024.
{``Detection of Mosaic and Population-Level Structural Variants with
{Sniffles2}.''} \emph{Nature Biotechnology} 42 (10): 1571--80.
\url{https://doi.org/10.1038/s41587-023-02024-y}.

\bibitem[\citeproctext]{ref-sollis_gwas-catalog_2023}
Sollis, Elliot, Abayomi Mosaku, Ala Abid, Annalisa Buniello, Maria
Cerezo, Laurent Gil, Tudor Groza, et al. 2023. {``The {NHGRI}-{EBI}
{GWAS} {Catalog}: Knowledgebase and Deposition Resource.''}
\emph{Nucleic Acids Research} 51 (D1): D977--85.
\url{https://doi.org/10.1093/nar/gkac1010}.

\bibitem[\citeproctext]{ref-song_t1k_2022}
Song, Li, Gali Bai, X. Shirley Liu, Bo Li, and Heng Li. 2022. {``{T1K}:
Efficient and Accurate {KIR} and {HLA} Genotyping with Next-Generation
Sequencing Data.''} bioRxiv.
\url{https://doi.org/10.1101/2022.10.26.513955}.

\bibitem[\citeproctext]{ref-sullivan_leveraging_2023}
Sullivan, Patrick F., Jennifer R. S. Meadows, Steven Gazal, BaDoi N.
Phan, Xue Li, Diane P. Genereux, Michael X. Dong, et al. 2023.
{``Leveraging Base-Pair Mammalian Constraint to Understand Genetic
Variation and Human Disease.''} \emph{Science} 380 (6643): eabn2937.
\url{https://doi.org/10.1126/science.abn2937}.

\bibitem[\citeproctext]{ref-suzek_uniref_2007}
Suzek, Baris E., Hongzhan Huang, Peter McGarvey, Raja Mazumder, and
Cathy H. Wu. 2007. {``{UniRef}: Comprehensive and Non-Redundant
{UniProt} Reference Clusters.''} \emph{Bioinformatics} 23 (10):
1282--88. \url{https://doi.org/10.1093/bioinformatics/btm098}.

\bibitem[\citeproctext]{ref-gtex_2020}
The GTEx Consortium. 2020. {``The {GTEx} {Consortium} Atlas of Genetic
Regulatory Effects Across Human Tissues.''} \emph{Science} 369 (6509):
1318--30. \url{https://doi.org/10.1126/science.aaz1776}.

\bibitem[\citeproctext]{ref-tabula_sapiens_2022}
The Tabula Sapiens Consortium. 2022. {``The {Tabula} {Sapiens}: {A}
Multiple-Organ, Single-Cell Transcriptomic Atlas of Humans.''}
\emph{Science} 376 (6594): eabl4896.
\url{https://doi.org/10.1126/science.abl4896}.

\bibitem[\citeproctext]{ref-theodoris_geneformer_2023}
Theodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina
R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023.
{``{[}{Geneformer}{]} {Transfer} Learning Enables Predictions in Network
Biology.''} \emph{Nature} 618 (7965): 616--24.
\url{https://doi.org/10.1038/s41586-023-06139-9}.

\bibitem[\citeproctext]{ref-torkamani_personal_2018}
Torkamani, Ali, Nathan E. Wineinger, and Eric J. Topol. 2018. {``The
Personal and Clinical Utility of Polygenic Risk Scores.''} \emph{Nature
Reviews Genetics} 19 (9): 581--90.
\url{https://doi.org/10.1038/s41576-018-0018-x}.

\bibitem[\citeproctext]{ref-van_der_auwera_gatk_best_2018}
Van der Auwera, Geraldine A., Mauricio O. Carneiro, Christopher Hartl,
Ryan Poplin, Guillermo del Angel, Ami Levy-Moonshine, Tadeusz Jordan, et
al. 2018. {``From {FastQ} {Data} to {High}-{Confidence} {Variant}
{Calls}: {The} {Genome} {Analysis} {Toolkit} {Best} {Practices}
{Pipeline}.''} \emph{Current Protocols in Bioinformatics} 43 (1):
11.10.1--33. \url{https://doi.org/10.1002/0471250953.bi1110s43}.

\bibitem[\citeproctext]{ref-vaswani_attention_2017}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
{``Attention {Is} {All} {You} {Need}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.1706.03762}.

\bibitem[\citeproctext]{ref-vilhjalmsson_modeling_2015}
VilhjÃ¡lmsson, Bjarni J., Jian Yang, Hilary K. Finucane, Alexander Gusev,
Sara LindstrÃ¶m, Stephan Ripke, Giulio Genovese, et al. 2015. {``Modeling
{Linkage} {Disequilibrium} {Increases} {Accuracy} of {Polygenic} {Risk}
{Scores}.''} \emph{American Journal of Human Genetics} 97 (4): 576--92.
\url{https://doi.org/10.1016/j.ajhg.2015.09.001}.

\bibitem[\citeproctext]{ref-vosa_eqtl-gen_2021}
VÃµsa, Urmo, Annique Claringbould, Harm-Jan Westra, Marc Jan Bonder,
Patrick Deelen, Biao Zeng, Holger Kirsten, et al. 2021. {``Large-Scale
Cis- and Trans-{eQTL} Analyses Identify Thousands of Genetic Loci and
Polygenic Scores That Regulate Blood Gene Expression.''} \emph{Nature
Genetics} 53 (9): 1300--1310.
\url{https://doi.org/10.1038/s41588-021-00913-z}.

\bibitem[\citeproctext]{ref-watson_rfdiffusion_2023}
Watson, Joseph L., David Juergens, Nathaniel R. Bennett, Brian L.
Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, et al. 2023. {``De
Novo Design of Protein Structure and Function with {RFdiffusion}.''}
\emph{Nature} 620 (7976): 1089--1100.
\url{https://doi.org/10.1038/s41586-023-06415-8}.

\bibitem[\citeproctext]{ref-wenger_pacbiohifi_2019}
Wenger, Aaron M., Paul Peluso, William J. Rowell, Pi-Chuan Chang,
Richard J. Hall, Gregory T. Concepcion, Jana Ebler, et al. 2019.
{``Accurate Circular Consensus Long-Read Sequencing Improves Variant
Detection and Assembly of a Human Genome.''} \emph{Nature Biotechnology}
37 (10): 1155--62. \url{https://doi.org/10.1038/s41587-019-0217-9}.

\bibitem[\citeproctext]{ref-whirl-PharmGKB_2012}
Whirl-Carrillo, M, E M McDonagh, J M Hebert, L Gong, K Sangkuhl, C F
Thorn, R B Altman, and T E Klein. 2012. {``Pharmacogenomics {Knowledge}
for {Personalized} {Medicine}.''} \emph{Clinical Pharmacology \&
Therapeutics} 92 (4): 414--17.
\url{https://doi.org/10.1038/clpt.2012.96}.

\bibitem[\citeproctext]{ref-wu_genome-wide_2024}
Wu, Yang, Zhili Zheng, Loic Thibaut2, Michael E. Goddard, Naomi R. Wray,
Peter M. Visscher, and Jian Zeng. 2024. {``Genome-Wide Fine-Mapping
Improves Identification of Causal Variants.''} \emph{Research Square},
August, rs.3.rs--4759390.
\url{https://doi.org/10.21203/rs.3.rs-4759390/v1}.

\bibitem[\citeproctext]{ref-yang_common_2010}
Yang, Jian, Beben Benyamin, Brian P. McEvoy, Scott Gordon, Anjali K.
Henders, Dale R. Nyholt, Pamela A. Madden, et al. 2010. {``Common {SNPs}
Explain a Large Proportion of the Heritability for Human Height.''}
\emph{Nature Genetics} 42 (7): 565--69.
\url{https://doi.org/10.1038/ng.608}.

\bibitem[\citeproctext]{ref-yang_xlnet_2020}
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
Salakhutdinov, and Quoc V. Le. 2020. {``{XLNet}: {Generalized}
{Autoregressive} {Pretraining} for {Language} {Understanding}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.1906.08237}.

\bibitem[\citeproctext]{ref-yeo_maxentscan_2004}
Yeo, Gene, and Christopher B. Burge. 2004. {``Maximum {Entropy}
{Modeling} of {Short} {Sequence} {Motifs} with {Applications} to {RNA}
{Splicing} {Signals}.''} \emph{Journal of Computational Biology} 11
(2-3): 377--94. \url{https://doi.org/10.1089/1066527041410418}.

\bibitem[\citeproctext]{ref-yun_accurate_2021}
Yun, Taedong, Helen Li, Pi-Chuan Chang, Michael F Lin, Andrew Carroll,
and Cory Y McLean. 2021. {``Accurate, Scalable Cohort Variant Calls
Using {DeepVariant} and {GLnexus}.''} \emph{Bioinformatics} 36 (24):
5582--89. \url{https://doi.org/10.1093/bioinformatics/btaa1081}.

\bibitem[\citeproctext]{ref-zheng_cistrome_2019}
Zheng, Rongbin, Changxin Wan, Shenglin Mei, Qian Qin, Qiu Wu, Hanfei
Sun, Chen-Hao Chen, et al. 2019. {``Cistrome {Data} {Browser}: Expanded
Datasets and New Tools for Gene Regulatory Analysis.''} \emph{Nucleic
Acids Research} 47 (D1): D729--35.
\url{https://doi.org/10.1093/nar/gky1094}.

\bibitem[\citeproctext]{ref-zheng_clair3_2022}
Zheng, Zhenxian, Shumin Li, Junhao Su, Amy Wing-Sze Leung, Tak-Wah Lam,
and Ruibang Luo. 2022. {``Symphonizing Pileup and Full-Alignment for
Deep Learning-Based Long-Read Variant Calling.''} \emph{Nature
Computational Science} 2 (12): 797--803.
\url{https://doi.org/10.1038/s43588-022-00387-x}.

\bibitem[\citeproctext]{ref-zhou_expecto_2018}
Zhou, Jian, Chandra L. Theesfeld, Kevin Yao, Kathleen M. Chen, Aaron K.
Wong, and Olga G. Troyanskaya. 2018. {``{[}{Expecto}{]} {Deep} Learning
Sequence-Based Ab Initio Prediction of Variant Effects on Expression and
Disease Risk.''} \emph{Nature Genetics} 50 (8): 1171--79.
\url{https://doi.org/10.1038/s41588-018-0160-6}.

\bibitem[\citeproctext]{ref-zhou_deepsea_2015}
Zhou, Jian, and Olga G. Troyanskaya. 2015. {``{[}{DeepSEA}{]}
{Predicting} Effects of Noncoding Variants with Deep Learning--Based
Sequence Model.''} \emph{Nature Methods} 12 (10): 931--34.
\url{https://doi.org/10.1038/nmeth.3547}.

\bibitem[\citeproctext]{ref-zhou_dnabert-2_2024}
Zhou, Zhihan, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, and
Han Liu. 2024. {``{DNABERT}-2: {Efficient} {Foundation} {Model} and
{Benchmark} {For} {Multi}-{Species} {Genome}.''} arXiv.
\url{https://doi.org/10.48550/arXiv.2306.15006}.

\bibitem[\citeproctext]{ref-zook_giab_2019}
Zook, Justin M., Jennifer McDaniel, Nathan D. Olson, Justin Wagner,
Hemang Parikh, Haynes Heaton, Sean A. Irvine, et al. 2019. {``An Open
Resource for Accurately Benchmarking Small Variant and Reference
Calls.''} \emph{Nature Biotechnology} 37 (5): 561--66.
\url{https://doi.org/10.1038/s41587-019-0074-6}.

\bibitem[\citeproctext]{ref-zvyagin_genslms_2022}
Zvyagin, Maxim, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang,
Cindy Orozco Bohorquez, Austin Clyde, et al. 2022. {``{GenSLMs}:
{Genome}-Scale Language Models Reveal {SARS}-{CoV}-2 Evolutionary
Dynamics.''} bioRxiv. \url{https://doi.org/10.1101/2022.10.10.511571}.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Deep Learning Primer}\label{sec-apx-dl}

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, opacitybacktitle=0.6, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, left=2mm, toprule=.15mm, toptitle=1mm, breakable, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-warning-color!10!white, arc=.35mm, titlerule=0mm, opacityback=0, bottomrule=.15mm, rightrule=.15mm, colback=white]

\textbf{TODO:}

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

This appendix gives a compact introduction to deep learning for readers
who are comfortable with genomics but less familiar with modern neural
networks. The goal is not to replace a full machine learning textbook,
but to provide enough background to make the models in Chapters 5--19
feel intuitive rather than magical.

We focus on:

\begin{itemize}
\tightlist
\item
  How deep models are structured (layers, parameters, activations)\\
\item
  How they are trained (loss functions, gradients, optimization)\\
\item
  Core architectures that appear throughout the book (CNNs,
  Transformers)\\
\item
  Concepts like self-supervised pretraining and transfer learning
\end{itemize}

Where possible, we connect directly to the genomic case studies in the
main text (DeepSEA, ExPecto, SpliceAI, Enformer, genomic language
models, and GFMs).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Linear Models to Deep
Networks}\label{from-linear-models-to-deep-networks}

\subsection{Models as Functions}\label{models-as-functions}

At its core, a predictive model is just a function:

\begin{equation}\phantomsection\label{eq-model}{
f_\theta: x \mapsto \hat{y}
}\end{equation}

where:

\begin{itemize}
\tightlist
\item
  \(x\) is an input (e.g., a one-hot encoded DNA sequence, variant-level
  features, or a patient feature vector).\\
\item
  \(\hat{y}\) is a prediction (e.g., probability of a histone mark, gene
  expression level, disease risk).\\
\item
  \(\theta\) are the parameters (weights) of the model.
\end{itemize}

In classical genomics workflows, \(f_\theta\) might be:

\begin{itemize}
\tightlist
\item
  \textbf{Logistic regression} (for case--control status)\\
\item
  \textbf{Linear regression} (for quantitative traits)\\
\item
  \textbf{Random forests} or \textbf{gradient boosting} (for variant
  pathogenicity scores)
\end{itemize}

Deep learning keeps the same basic structure but allows \(f_\theta\) to
be a much more flexible, high-capacity function built by composing many
simple operations.

\subsection{Linear Models vs Neural
Networks}\label{linear-models-vs-neural-networks}

A simple linear model for classification looks like:

\[
\hat{y} = \sigma(w^\top x + b),
\]

where \(w\) and \(b\) are parameters and \(\sigma(\cdot)\) is a
squashing nonlinearity (e.g., the logistic function). The model draws a
single separating hyperplane in feature space.

A \textbf{neural network} generalizes this by stacking multiple linear
transformations with nonlinear activation functions:

\[
\begin{aligned}
h_1 &= \phi(W_1 x + b_1) \\
h_2 &= \phi(W_2 h_1 + b_2) \\
&\vdots \\
\hat{y} &= g(W_L h_{L-1} + b_L)
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  Each \(W_\ell, b_\ell\) is a layer's weight matrix and bias.\\
\item
  \(\phi(\cdot)\) is a nonlinear activation (e.g., ReLU).\\
\item
  \(g(\cdot)\) is a final activation (e.g., sigmoid for probabilities,
  identity for regression).
\end{itemize}

The key idea:

\begin{quote}
By composing many simple nonlinear transformations, deep networks can
approximate very complex functions.
\end{quote}

In Chapters 5--7, DeepSEA, ExPecto, and SpliceAI implement exactly this
pattern, but with \textbf{convolutional} layers (Section 4) tailored to
1D DNA sequence instead of dense matrix multiplications (J. Zhou and
Troyanskaya 2015; J. Zhou et al. 2018; Jaganathan et al. 2019).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Training Deep Models}\label{training-deep-models}

\subsection{Data, Labels, and Loss
Functions}\label{data-labels-and-loss-functions}

To train a model, we need:

\begin{itemize}
\tightlist
\item
  A dataset of examples \(\{(x_i, y_i)\}_{i=1}^N\)\\
\item
  A model \(f_\theta\)\\
\item
  A \textbf{loss function} \(L(\hat{y}, y)\) that measures how wrong a
  prediction is
\end{itemize}

Common loss functions:

\begin{itemize}
\tightlist
\item
  \textbf{Binary cross-entropy} (for yes/no labels, e.g., ``is this
  ChIP--seq peak present?''):\\
  \[
  L(\hat{p}, y) = -\big(y \log \hat{p} + (1-y)\log(1-\hat{p})\big)
  \]
\item
  \textbf{Multiclass cross-entropy} (for one-of-K labels)\\
\item
  \textbf{Mean squared error (MSE)} (for continuous outputs, e.g., gene
  expression)
\end{itemize}

The \textbf{training objective} is to find \(\theta\) that minimizes the
average loss:

\[
\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N L\big(f_\theta(x_i), y_i\big).
\]

\subsection{2.2 Gradient-Based
Optimization}\label{gradient-based-optimization}

Deep networks may have millions to billions of parameters. We can't
search over all possibilities, but we can follow the gradient of the
loss with respect to \(\theta\):

\begin{itemize}
\tightlist
\item
  \textbf{Gradient descent} updates: \[
  \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta),
  \] where \(\eta\) is the learning rate.
\end{itemize}

In practice, we use:

\begin{itemize}
\tightlist
\item
  \textbf{Mini-batch stochastic gradient descent (SGD)}: Compute
  gradients on small batches of examples (e.g., 128 sequences at a time)
  for efficiency and better generalization.
\item
  \textbf{Adaptive optimizers} like Adam, which adjust learning rates
  per parameter.
\end{itemize}

You never compute gradients by hand; modern frameworks (PyTorch, JAX,
TensorFlow) use \textbf{automatic differentiation} to efficiently
compute \(\nabla_\theta \mathcal{L}\) even for very complex
architectures.

\subsection{Backpropagation in One
Sentence}\label{backpropagation-in-one-sentence}

\textbf{Backpropagation} is just the chain rule of calculus applied
efficiently through the layers of a network. It propagates ``blame''
from the output back to each weight, telling us how changing that weight
would change the loss.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Generalization, Overfitting, and
Evaluation}\label{generalization-overfitting-and-evaluation}

\subsection{Train / Validation / Test
Splits}\label{train-validation-test-splits}

Deep networks can memorize training data if we're not careful. To
evaluate generalization, we typically split data into:

\begin{itemize}
\tightlist
\item
  \textbf{Training set} -- used to fit parameters\\
\item
  \textbf{Validation set} -- used to tune hyperparameters (learning
  rate, depth, etc.) and perform early stopping\\
\item
  \textbf{Test set} -- held out until the end to estimate performance on
  new data
\end{itemize}

In genomics, \textbf{how we split} matters as much as \textbf{how much
data} we have:

\begin{itemize}
\tightlist
\item
  Splitting by \textbf{locus or chromosome} (to test cross-locus
  generalization)\\
\item
  Splitting by \textbf{individual or cohort} (to avoid leakage between
  related samples)\\
\item
  Splitting by \textbf{species or ancestry} when evaluating transfer
\end{itemize}

These issues are developed in more depth in the evaluation and
confounding chapters (Chapter~\ref{sec-evaluation} and
Chapter~\ref{sec-confounding}).

\subsection{Overfitting and
Regularization}\label{overfitting-and-regularization}

Signs of overfitting:

\begin{itemize}
\tightlist
\item
  Training loss keeps decreasing, but validation loss starts
  increasing.\\
\item
  Metrics like AUROC or AUPRC plateau or drop on validation data even as
  they improve on training data.
\end{itemize}

Common regularization techniques:

\begin{itemize}
\tightlist
\item
  \textbf{Weight decay / L2 regularization} -- penalize large weights.\\
\item
  \textbf{Dropout} -- randomly zero out activations during training.\\
\item
  \textbf{Early stopping} -- stop training when validation performance
  stops improving.\\
\item
  \textbf{Data augmentation} -- generate more training examples by
  transforming inputs, e.g.:

  \begin{itemize}
  \tightlist
  \item
    Reverse-complement augmentation for DNA sequences (treat sequence
    and its reverse complement as equivalent).\\
  \item
    Window jittering: randomly shifting the sequence window around a
    target site.
  \end{itemize}
\end{itemize}

\subsection{Basic Metrics}\label{basic-metrics}

You'll encounter metrics such as:

\begin{itemize}
\tightlist
\item
  \textbf{AUROC (Area Under the ROC Curve)} -- how well the model ranks
  positives above negatives.\\
\item
  \textbf{AUPRC (Area Under the Precision--Recall Curve)} -- more
  informative when positives are rare.\\
\item
  \textbf{Calibration metrics} (e.g., Brier score) and reliability
  diagrams -- especially for clinical risk prediction
  (Chapter~\ref{sec-clinical-risk}).
\end{itemize}

The model and application chapters provide details about which metrics
are appropriate for which tasks. See Chapter~\ref{sec-evaluation} for
more on evaluation metrics.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Convolutional Networks for Genomic
Sequences}\label{convolutional-networks-for-genomic-sequences}

Convolutional neural networks (CNNs) are the workhorse architecture in
early genomic deep learning models like DeepSEA, ExPecto, and SpliceAI
(J. Zhou and Troyanskaya 2015; J. Zhou et al. 2018; Jaganathan et al.
2019).

\subsection{1D Convolutions as Motif
Detectors}\label{d-convolutions-as-motif-detectors}

For a 1D DNA sequence encoded as a matrix
\(X \in \mathbb{R}^{L \times 4}\) (length \(L\), 4 nucleotides), a
\textbf{convolutional layer} applies a set of filters (kernels) of width
\(k\):

\begin{itemize}
\tightlist
\item
  Each filter is a small matrix \(K \in \mathbb{R}^{k \times 4}\).\\
\item
  At each position, the filter computes a dot product between \(K\) and
  the corresponding \(k\)-length chunk of \(X\).\\
\item
  Sliding the filter along the sequence creates an activation map that
  is high wherever the motif encoded by \(K\) is present.
\end{itemize}

Intuitively:

\begin{quote}
A 1D convolutional filter learns to recognize sequence motifs (e.g.,
transcription factor binding sites) directly from data.
\end{quote}

\subsection{Stacking Layers and Receptive
Fields}\label{stacking-layers-and-receptive-fields}

Deeper convolutional layers allow the model to ``see'' longer-range
patterns:

\begin{itemize}
\tightlist
\item
  \textbf{First layer}: short motifs (e.g., 8--15 bp).\\
\item
  \textbf{Higher layers}: combinations of motifs, motif spacing, and
  local regulatory grammar.\\
\item
  \textbf{Pooling layers} (e.g., max pooling) reduce spatial resolution
  while aggregating features, increasing the \textbf{receptive field}.
\end{itemize}

In DeepSEA, stacked convolutions and pooling allow the model to use
hundreds of base pairs of context around a locus to predict chromatin
state (J. Zhou and Troyanskaya 2015). ExPecto extends this idea by
mapping sequence to tissue-specific expression predictions (J. Zhou et
al. 2018). SpliceAI uses very deep dilated convolutions to reach
\textasciitilde10 kb of context for splicing (Jaganathan et al. 2019).

\subsection{Multi-Task Learning}\label{multi-task-learning}

Early sequence-to-function CNNs are almost always \textbf{multi-task}:

\begin{itemize}
\tightlist
\item
  A single input sequence is used to predict many outputs simultaneously
  (e.g., hundreds of TF ChIP--seq peaks, histone marks, DNase
  hypersensitivity tracks).\\
\item
  Shared convolutional layers learn \textbf{common features}, while the
  final layer has many output units (one per task).
\end{itemize}

Benefits:

\begin{itemize}
\tightlist
\item
  Efficient use of data and compute\\
\item
  Better regularization: related tasks constrain each other\\
\item
  Natural interface for variant effect prediction: you can see how a
  mutation affects many functional readouts at once
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Beyond CNNs: Recurrent Networks
(Briefly)}\label{beyond-cnns-recurrent-networks-briefly}

Before Transformers dominated sequence modeling, \textbf{recurrent
neural networks (RNNs)}---especially LSTMs and GRUs---were the default
architecture for language and time series.

Conceptually:

\begin{itemize}
\tightlist
\item
  An RNN processes a sequence one position at a time.\\
\item
  It maintains a hidden state that is updated as it moves along the
  sequence.\\
\item
  In principle, it can capture arbitrarily long-range dependencies.
\end{itemize}

In practice, for genomic sequences:

\begin{itemize}
\tightlist
\item
  Very long-range dependencies (tens to hundreds of kilobases) are
  difficult to learn with standard RNNs.\\
\item
  Training can be slow and unstable on very long sequences.\\
\item
  CNNs and attention-based models have largely displaced RNNs in genomic
  applications.
\end{itemize}

You may still see RNNs in some multi-modal or temporal settings (e.g.,
modeling longitudinal clinical data), but they are not central to this
book's architectures.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Transformers and
Self-Attention}\label{transformers-and-self-attention}

Transformers, introduced in natural language processing, have become the
dominant architecture for sequence modeling. In this book, they underpin
protein language models, DNA language models (DNABERT and successors),
and long-range models like Enformer (Ji et al. 2021; Å½. Avsec et al.
2021).

\subsection{The Idea of
Self-Attention}\label{the-idea-of-self-attention}

In a \textbf{self-attention} layer, each position in a sequence can
directly ``look at'' and combine information from every other position.

For an input sequence represented as vectors \(\{x_1, \dots, x_L\}\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each position is mapped to \textbf{query} (\(q_i\)), \textbf{key}
  (\(k_i\)), and \textbf{value} (\(v_i\)) vectors via learned linear
  projections.\\
\item
  The attention weight from position \(i\) to position \(j\) is:

  \[
  \alpha_{ij} \propto \exp\left(\frac{q_i^\top k_j}{\sqrt{d}}\right),
  \]

  followed by normalization so that \(\sum_j \alpha_{ij} = 1\).
\item
  The new representation of position \(i\) is a weighted sum of all
  value vectors:

  \[
  z_i = \sum_{j=1}^L \alpha_{ij} v_j.
  \]
\end{enumerate}

Key properties:

\begin{itemize}
\tightlist
\item
  \textbf{Content-based}: Interactions are determined by similarity of
  representations, not just distance.\\
\item
  \textbf{Global context}: Each position can, in principle, attend to
  any other position.\\
\item
  \textbf{Permutation-aware via positional encodings}: Additional
  information (sinusoidal or learned) encodes position so the model
  knows order.
\end{itemize}

\subsection{Multi-Head Attention and Transformer
Blocks}\label{multi-head-attention-and-transformer-blocks}

Real Transformer layers use \textbf{multi-head attention}:

\begin{itemize}
\tightlist
\item
  The model runs self-attention in parallel with multiple sets of
  \((Q,K,V)\) projections (heads).\\
\item
  Different heads can specialize in different patterns (e.g., local
  motif combinations, long-range enhancer--promoter contacts).
\end{itemize}

A typical Transformer block has:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multi-head self-attention\\
\item
  Add \& layer normalization\\
\item
  Position-wise feed-forward network\\
\item
  Another add \& layer normalization
\end{enumerate}

Stacking many blocks yields a deep Transformer.

\subsection{Computational Cost and Long-Range
Genomics}\label{computational-cost-and-long-range-genomics}

Naive self-attention has \(O(L^2)\) cost in sequence length \(L\). For
genomic sequences, where we might want 100 kb--1 Mb contexts, this is
expensive.

Long-range genomic models like Enformer and HyenaDNA address this with:

\begin{itemize}
\tightlist
\item
  \textbf{Hybrid designs} (CNNs + attention) to reduce sequence length
  before applying global attention (Å½. Avsec et al. 2021).\\
\item
  \textbf{Structured state space models (SSMs)} and related
  architectures that scale more gracefully with length (Nguyen et al.
  2023).
\end{itemize}

These details are treated in depth in the long-range modeling chapters;
here it suffices to know that Transformers give flexible global context
at the cost of higher computational complexity.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Self-Supervised Learning and
Pretraining}\label{self-supervised-learning-and-pretraining}

A central theme of this book is \textbf{pretraining}: training a large
model once on a broad, unlabeled or weakly-labeled task, then re-using
it for many downstream problems.

\subsection{Supervised vs
Self-Supervised}\label{supervised-vs-self-supervised}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised learning}: Each input \(x\) comes with a label
  \(y\). Examples:

  \begin{itemize}
  \tightlist
  \item
    Predicting chromatin marks from sequence (DeepSEA).\\
  \item
    Predicting splice junctions (SpliceAI).\\
  \item
    Predicting disease risk from features
    (Chapter~\ref{sec-clinical-risk}).
  \end{itemize}
\item
  \textbf{Self-supervised learning}: The model learns from raw input
  data without explicit labels, using some \textbf{pretext task}
  constructed from the data itself. Examples:

  \begin{itemize}
  \tightlist
  \item
    Masked token prediction (BERT-style): hide some nucleotides and
    train the model to predict them from surrounding context.\\
  \item
    Next-token prediction (GPT-style): predict the next base given
    previous ones.\\
  \item
    Denoising or reconstruction tasks.
  \end{itemize}
\end{itemize}

In genomics, self-supervised models treat DNA sequences as a language
and learn from the vast amount of genomic sequence without needing
curated labels.

\subsection{Masked Language Modeling on
DNA}\label{masked-language-modeling-on-dna}

DNABERT applied BERT-style masked language modeling to DNA sequences
tokenized as overlapping k-mers (Ji et al. 2021). The model:

\begin{itemize}
\tightlist
\item
  Reads sequences as k-mer tokens.\\
\item
  Randomly masks a subset of tokens.\\
\item
  Learns to predict the masked tokens given surrounding context.
\end{itemize}

Benefits:

\begin{itemize}
\tightlist
\item
  Uses essentially unlimited unlabeled genomic data.\\
\item
  Learns rich representations that can be fine-tuned for tasks like
  promoter prediction, splice site detection, and variant effect
  prediction.
\end{itemize}

Chapter~\ref{sec-dna-lm} generalizes this story to broader DNA
foundation models, including alternative tokenization schemes and
architectures.

\subsection{Pretraining, Fine-Tuning, and
Probing}\label{pretraining-fine-tuning-and-probing}

After pretraining, we can use a model in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Fine-tuning}: Initialize with pretrained weights, then
  continue training on a specific downstream task with task-specific
  labels.\\
\item
  \textbf{Linear probing}: Freeze the pretrained model, extract
  embeddings, and train a simple linear classifier on top.\\
\item
  \textbf{Prompting / adapters}: Add small task-specific modules
  (adapters) while keeping most of the model fixed.
\end{itemize}

These patterns reappear across protein LMs, DNA LMs, variant effect
models, and GFMs in Chapters 9--16.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Foundations for Evaluation and
Reliability}\label{foundations-for-evaluation-and-reliability}

While the main book has dedicated chapters for evaluation
(Chapter~\ref{sec-evaluation}), confounding
(Chapter~\ref{sec-confounding}), and clinical metrics
(Chapter~\ref{sec-clinical-risk}), it's useful to have a few basic
concepts in mind.

\subsection{Distribution Shift}\label{distribution-shift-2}

A model is trained under some data distribution (e.g., certain assays,
cohorts, ancestries) and then deployed under another (e.g., a different
hospital system or population). When these differ, we have
\textbf{distribution shift}, which can degrade performance.

Typical genomic shifts include:

\begin{itemize}
\tightlist
\item
  New sequencing technologies or lab protocols\\
\item
  New ancestries or populations\\
\item
  New tissues, diseases, or phenotypes
\end{itemize}

\subsection{Data Leakage}\label{data-leakage-1}

\textbf{Data leakage} occurs when information from the test set
``leaks'' into training (e.g., through overlapping loci or related
individuals), leading to overly optimistic estimates of performance.
Chapter~\ref{sec-evaluation} and Chapter~\ref{sec-confounding} discuss
strategies for leak-resistant splits in detail.

\subsection{Calibration and
Uncertainty}\label{calibration-and-uncertainty}

For many applications, especially in the clinic, we care not just about
whether the model is \emph{correct}, but whether its probabilities are
\textbf{well calibrated} and whether we know when the model is
uncertain. Calibration and uncertainty quantification are covered in
Chapter~\ref{sec-clinical-risk}; here, the main takeaway is that
\textbf{perfect AUROC does not imply perfect clinical utility}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{A Minimal Recipe for a Genomic Deep Learning
Project}\label{a-minimal-recipe-for-a-genomic-deep-learning-project}

To make the abstractions more concrete, here is a lightweight ``recipe''
that roughly mirrors what the case-study chapters do.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Define the prediction problem}

  \begin{itemize}
  \tightlist
  \item
    Input: e.g., 1 kb sequence around a variant, or patient-level
    features.\\
  \item
    Output: e.g., presence of a chromatin mark, change in expression,
    disease risk.
  \end{itemize}
\item
  \textbf{Choose an input representation}

  \begin{itemize}
  \tightlist
  \item
    One-hot encoding or tokenization scheme for sequences (see
    Chapter~\ref{sec-representations}).\\
  \item
    Encodings for variants, genes, or patients (e.g., aggregate from
    per-variant features).
  \end{itemize}
\item
  \textbf{Pick a model family}

  \begin{itemize}
  \tightlist
  \item
    CNN for local sequence-to-function (Chapters 5--7).\\
  \item
    Transformer or SSM for long-range or language model-style tasks
    (Chapters 8--11).\\
  \item
    Pretrained GFM + small task-specific head (Chapters 12--16).
  \end{itemize}
\item
  \textbf{Specify the loss and metrics}

  \begin{itemize}
  \tightlist
  \item
    Cross-entropy for binary classification, MSE for regression, etc.\\
  \item
    Metrics like AUROC, AUPRC, correlation, calibration.
  \end{itemize}
\item
  \textbf{Set up data splits and evaluation}

  \begin{itemize}
  \tightlist
  \item
    Decide whether to split by locus, individual, cohort, or species.\\
  \item
    Hold out a test set and use validation data to tune hyperparameters.
  \end{itemize}
\item
  \textbf{Train with regularization and monitoring}

  \begin{itemize}
  \tightlist
  \item
    Use an optimizer (SGD or Adam-like) with a learning rate schedule.\\
  \item
    Apply regularization (dropout, weight decay, augmentation).\\
  \item
    Monitor training and validation curves for overfitting.
  \end{itemize}
\item
  \textbf{Inspect and stress-test}

  \begin{itemize}
  \tightlist
  \item
    Check performance across subgroups (e.g., ancestries, assays,
    cohorts).\\
  \item
    Use interpretability tools (Chapter~\ref{sec-interpretability}) to
    see what patterns the model is using.\\
  \item
    Run robustness checks and ablations.
  \end{itemize}
\item
  \textbf{Iterate}

  \begin{itemize}
  \tightlist
  \item
    Adjust architecture, add more data, refine labels, or incorporate
    pretrained backbones.\\
  \item
    Move from model-centric tuning to system-level considerations (data
    quality, deployment environment, feedback loops).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{How This Primer Connects to the Rest of the
Book}\label{how-this-primer-connects-to-the-rest-of-the-book}

This appendix gives you the minimum vocabulary to navigate the rest of
the text:

\begin{itemize}
\tightlist
\item
  \textbf{Chapters 5--7} show how CNNs on one-hot sequence learn
  regulatory code, expression, and splicing.\\
\item
  \textbf{Chapters 8--11} extend these ideas to richer sequence
  representations, Transformers, and long-range sequence models.\\
\item
  \textbf{Chapters 12--16} frame these models as genomic foundation
  models, introduce evaluation, interpretability, and multi-omics.\\
\item
  \textbf{Chapters 17--19} show how these ingredients are assembled into
  clinical, discovery, and biotech applications.
\end{itemize}

You don't need to internalize every detail here. The goal is simply that
when you see terms like ``convolution,'' ``attention,'' ``pretraining,''
or ``fine-tuning'' in the main chapters, they feel like familiar tools
rather than mysterious jargon.

\chapter{Deployment and Compute}\label{sec-apx-compute}

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, opacitybacktitle=0.6, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, left=2mm, toprule=.15mm, toptitle=1mm, breakable, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-warning-color!10!white, arc=.35mm, titlerule=0mm, opacityback=0, bottomrule=.15mm, rightrule=.15mm, colback=white]

\textbf{TODO:}

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

This appendix gives a compact introduction to deep learning for readers
who are comfortable with genomics but less familiar with modern neural
networks. The goal is not to replace a full machine learning textbook,
but to provide enough background to make the models in Chapters 5--19
feel intuitive rather than magical.

We focus on:

\begin{itemize}
\tightlist
\item
  How deep models are structured (layers, parameters, activations)\\
\item
  How they are trained (loss functions, gradients, optimization)\\
\item
  Core architectures that appear throughout the book (CNNs,
  Transformers)\\
\item
  Concepts like self-supervised pretraining and transfer learning
\end{itemize}

Where possible, we connect directly to the genomic case studies in the
main text (DeepSEA, ExPecto, SpliceAI, Enformer, genomic language
models, and GFMs).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{From Linear Models to Deep
Networks}\label{from-linear-models-to-deep-networks-1}

\subsection{Models as Functions}\label{models-as-functions-1}

At its core, a predictive model is just a function:

\begin{equation}\phantomsection\label{eq-model}{
f_\theta: x \mapsto \hat{y}
}\end{equation}

where:

\begin{itemize}
\tightlist
\item
  \(x\) is an input (e.g., a one-hot encoded DNA sequence, variant-level
  features, or a patient feature vector).\\
\item
  \(\hat{y}\) is a prediction (e.g., probability of a histone mark, gene
  expression level, disease risk).\\
\item
  \(\theta\) are the parameters (weights) of the model.
\end{itemize}

In classical genomics workflows, \(f_\theta\) might be:

\begin{itemize}
\tightlist
\item
  \textbf{Logistic regression} (for case--control status)\\
\item
  \textbf{Linear regression} (for quantitative traits)\\
\item
  \textbf{Random forests} or \textbf{gradient boosting} (for variant
  pathogenicity scores)
\end{itemize}

Deep learning keeps the same basic structure but allows \(f_\theta\) to
be a much more flexible, high-capacity function built by composing many
simple operations.

\subsection{Linear Models vs Neural
Networks}\label{linear-models-vs-neural-networks-1}

A simple linear model for classification looks like:

\[
\hat{y} = \sigma(w^\top x + b),
\]

where \(w\) and \(b\) are parameters and \(\sigma(\cdot)\) is a
squashing nonlinearity (e.g., the logistic function). The model draws a
single separating hyperplane in feature space.

A \textbf{neural network} generalizes this by stacking multiple linear
transformations with nonlinear activation functions:

\[
\begin{aligned}
h_1 &= \phi(W_1 x + b_1) \\
h_2 &= \phi(W_2 h_1 + b_2) \\
&\vdots \\
\hat{y} &= g(W_L h_{L-1} + b_L)
\end{aligned}
\]

where:

\begin{itemize}
\tightlist
\item
  Each \(W_\ell, b_\ell\) is a layer's weight matrix and bias.\\
\item
  \(\phi(\cdot)\) is a nonlinear activation (e.g., ReLU).\\
\item
  \(g(\cdot)\) is a final activation (e.g., sigmoid for probabilities,
  identity for regression).
\end{itemize}

The key idea:

\begin{quote}
By composing many simple nonlinear transformations, deep networks can
approximate very complex functions.
\end{quote}

In Chapters 5--7, DeepSEA, ExPecto, and SpliceAI implement exactly this
pattern, but with \textbf{convolutional} layers (Section 4) tailored to
1D DNA sequence instead of dense matrix multiplications (J. Zhou and
Troyanskaya 2015; J. Zhou et al. 2018; Jaganathan et al. 2019).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Training Deep Models}\label{training-deep-models-1}

\subsection{Data, Labels, and Loss
Functions}\label{data-labels-and-loss-functions-1}

To train a model, we need:

\begin{itemize}
\tightlist
\item
  A dataset of examples \(\{(x_i, y_i)\}_{i=1}^N\)\\
\item
  A model \(f_\theta\)\\
\item
  A \textbf{loss function} \(L(\hat{y}, y)\) that measures how wrong a
  prediction is
\end{itemize}

Common loss functions:

\begin{itemize}
\tightlist
\item
  \textbf{Binary cross-entropy} (for yes/no labels, e.g., ``is this
  ChIP--seq peak present?''):\\
  \[
  L(\hat{p}, y) = -\big(y \log \hat{p} + (1-y)\log(1-\hat{p})\big)
  \]
\item
  \textbf{Multiclass cross-entropy} (for one-of-K labels)\\
\item
  \textbf{Mean squared error (MSE)} (for continuous outputs, e.g., gene
  expression)
\end{itemize}

The \textbf{training objective} is to find \(\theta\) that minimizes the
average loss:

\[
\mathcal{L}(\theta) = \frac{1}{N}\sum_{i=1}^N L\big(f_\theta(x_i), y_i\big).
\]

\subsection{2.2 Gradient-Based
Optimization}\label{gradient-based-optimization-1}

Deep networks may have millions to billions of parameters. We can't
search over all possibilities, but we can follow the gradient of the
loss with respect to \(\theta\):

\begin{itemize}
\tightlist
\item
  \textbf{Gradient descent} updates: \[
  \theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}(\theta),
  \] where \(\eta\) is the learning rate.
\end{itemize}

In practice, we use:

\begin{itemize}
\tightlist
\item
  \textbf{Mini-batch stochastic gradient descent (SGD)}: Compute
  gradients on small batches of examples (e.g., 128 sequences at a time)
  for efficiency and better generalization.
\item
  \textbf{Adaptive optimizers} like Adam, which adjust learning rates
  per parameter.
\end{itemize}

You never compute gradients by hand; modern frameworks (PyTorch, JAX,
TensorFlow) use \textbf{automatic differentiation} to efficiently
compute \(\nabla_\theta \mathcal{L}\) even for very complex
architectures.

\subsection{Backpropagation in One
Sentence}\label{backpropagation-in-one-sentence-1}

\textbf{Backpropagation} is just the chain rule of calculus applied
efficiently through the layers of a network. It propagates ``blame''
from the output back to each weight, telling us how changing that weight
would change the loss.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Generalization, Overfitting, and
Evaluation}\label{generalization-overfitting-and-evaluation-1}

\subsection{Train / Validation / Test
Splits}\label{train-validation-test-splits-1}

Deep networks can memorize training data if we're not careful. To
evaluate generalization, we typically split data into:

\begin{itemize}
\tightlist
\item
  \textbf{Training set} -- used to fit parameters\\
\item
  \textbf{Validation set} -- used to tune hyperparameters (learning
  rate, depth, etc.) and perform early stopping\\
\item
  \textbf{Test set} -- held out until the end to estimate performance on
  new data
\end{itemize}

In genomics, \textbf{how we split} matters as much as \textbf{how much
data} we have:

\begin{itemize}
\tightlist
\item
  Splitting by \textbf{locus or chromosome} (to test cross-locus
  generalization)\\
\item
  Splitting by \textbf{individual or cohort} (to avoid leakage between
  related samples)\\
\item
  Splitting by \textbf{species or ancestry} when evaluating transfer
\end{itemize}

These issues are developed in more depth in the evaluation and
confounding chapters (Chapter~\ref{sec-evaluation} and
Chapter~\ref{sec-confounding}).

\subsection{Overfitting and
Regularization}\label{overfitting-and-regularization-1}

Signs of overfitting:

\begin{itemize}
\tightlist
\item
  Training loss keeps decreasing, but validation loss starts
  increasing.\\
\item
  Metrics like AUROC or AUPRC plateau or drop on validation data even as
  they improve on training data.
\end{itemize}

Common regularization techniques:

\begin{itemize}
\tightlist
\item
  \textbf{Weight decay / L2 regularization} -- penalize large weights.\\
\item
  \textbf{Dropout} -- randomly zero out activations during training.\\
\item
  \textbf{Early stopping} -- stop training when validation performance
  stops improving.\\
\item
  \textbf{Data augmentation} -- generate more training examples by
  transforming inputs, e.g.:

  \begin{itemize}
  \tightlist
  \item
    Reverse-complement augmentation for DNA sequences (treat sequence
    and its reverse complement as equivalent).\\
  \item
    Window jittering: randomly shifting the sequence window around a
    target site.
  \end{itemize}
\end{itemize}

\subsection{Basic Metrics}\label{basic-metrics-1}

You'll encounter metrics such as:

\begin{itemize}
\tightlist
\item
  \textbf{AUROC (Area Under the ROC Curve)} -- how well the model ranks
  positives above negatives.\\
\item
  \textbf{AUPRC (Area Under the Precision--Recall Curve)} -- more
  informative when positives are rare.\\
\item
  \textbf{Calibration metrics} (e.g., Brier score) and reliability
  diagrams -- especially for clinical risk prediction
  (Chapter~\ref{sec-clinical-risk}).
\end{itemize}

The model and application chapters provide details about which metrics
are appropriate for which tasks. See Chapter~\ref{sec-evaluation} for
more on evaluation metrics.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Convolutional Networks for Genomic
Sequences}\label{convolutional-networks-for-genomic-sequences-1}

Convolutional neural networks (CNNs) are the workhorse architecture in
early genomic deep learning models like DeepSEA, ExPecto, and SpliceAI
(J. Zhou and Troyanskaya 2015; J. Zhou et al. 2018; Jaganathan et al.
2019).

\subsection{1D Convolutions as Motif
Detectors}\label{d-convolutions-as-motif-detectors-1}

For a 1D DNA sequence encoded as a matrix
\(X \in \mathbb{R}^{L \times 4}\) (length \(L\), 4 nucleotides), a
\textbf{convolutional layer} applies a set of filters (kernels) of width
\(k\):

\begin{itemize}
\tightlist
\item
  Each filter is a small matrix \(K \in \mathbb{R}^{k \times 4}\).\\
\item
  At each position, the filter computes a dot product between \(K\) and
  the corresponding \(k\)-length chunk of \(X\).\\
\item
  Sliding the filter along the sequence creates an activation map that
  is high wherever the motif encoded by \(K\) is present.
\end{itemize}

Intuitively:

\begin{quote}
A 1D convolutional filter learns to recognize sequence motifs (e.g.,
transcription factor binding sites) directly from data.
\end{quote}

\subsection{Stacking Layers and Receptive
Fields}\label{stacking-layers-and-receptive-fields-1}

Deeper convolutional layers allow the model to ``see'' longer-range
patterns:

\begin{itemize}
\tightlist
\item
  \textbf{First layer}: short motifs (e.g., 8--15 bp).\\
\item
  \textbf{Higher layers}: combinations of motifs, motif spacing, and
  local regulatory grammar.\\
\item
  \textbf{Pooling layers} (e.g., max pooling) reduce spatial resolution
  while aggregating features, increasing the \textbf{receptive field}.
\end{itemize}

In DeepSEA, stacked convolutions and pooling allow the model to use
hundreds of base pairs of context around a locus to predict chromatin
state (J. Zhou and Troyanskaya 2015). ExPecto extends this idea by
mapping sequence to tissue-specific expression predictions (J. Zhou et
al. 2018). SpliceAI uses very deep dilated convolutions to reach
\textasciitilde10 kb of context for splicing (Jaganathan et al. 2019).

\subsection{Multi-Task Learning}\label{multi-task-learning-1}

Early sequence-to-function CNNs are almost always \textbf{multi-task}:

\begin{itemize}
\tightlist
\item
  A single input sequence is used to predict many outputs simultaneously
  (e.g., hundreds of TF ChIP--seq peaks, histone marks, DNase
  hypersensitivity tracks).\\
\item
  Shared convolutional layers learn \textbf{common features}, while the
  final layer has many output units (one per task).
\end{itemize}

Benefits:

\begin{itemize}
\tightlist
\item
  Efficient use of data and compute\\
\item
  Better regularization: related tasks constrain each other\\
\item
  Natural interface for variant effect prediction: you can see how a
  mutation affects many functional readouts at once
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Beyond CNNs: Recurrent Networks
(Briefly)}\label{beyond-cnns-recurrent-networks-briefly-1}

Before Transformers dominated sequence modeling, \textbf{recurrent
neural networks (RNNs)}---especially LSTMs and GRUs---were the default
architecture for language and time series.

Conceptually:

\begin{itemize}
\tightlist
\item
  An RNN processes a sequence one position at a time.\\
\item
  It maintains a hidden state that is updated as it moves along the
  sequence.\\
\item
  In principle, it can capture arbitrarily long-range dependencies.
\end{itemize}

In practice, for genomic sequences:

\begin{itemize}
\tightlist
\item
  Very long-range dependencies (tens to hundreds of kilobases) are
  difficult to learn with standard RNNs.\\
\item
  Training can be slow and unstable on very long sequences.\\
\item
  CNNs and attention-based models have largely displaced RNNs in genomic
  applications.
\end{itemize}

You may still see RNNs in some multi-modal or temporal settings (e.g.,
modeling longitudinal clinical data), but they are not central to this
book's architectures.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Transformers and
Self-Attention}\label{transformers-and-self-attention-1}

Transformers, introduced in natural language processing, have become the
dominant architecture for sequence modeling. In this book, they underpin
protein language models, DNA language models (DNABERT and successors),
and long-range models like Enformer (Ji et al. 2021; Å½. Avsec et al.
2021).

\subsection{The Idea of
Self-Attention}\label{the-idea-of-self-attention-1}

In a \textbf{self-attention} layer, each position in a sequence can
directly ``look at'' and combine information from every other position.

For an input sequence represented as vectors \(\{x_1, \dots, x_L\}\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Each position is mapped to \textbf{query} (\(q_i\)), \textbf{key}
  (\(k_i\)), and \textbf{value} (\(v_i\)) vectors via learned linear
  projections.\\
\item
  The attention weight from position \(i\) to position \(j\) is:

  \[
  \alpha_{ij} \propto \exp\left(\frac{q_i^\top k_j}{\sqrt{d}}\right),
  \]

  followed by normalization so that \(\sum_j \alpha_{ij} = 1\).
\item
  The new representation of position \(i\) is a weighted sum of all
  value vectors:

  \[
  z_i = \sum_{j=1}^L \alpha_{ij} v_j.
  \]
\end{enumerate}

Key properties:

\begin{itemize}
\tightlist
\item
  \textbf{Content-based}: Interactions are determined by similarity of
  representations, not just distance.\\
\item
  \textbf{Global context}: Each position can, in principle, attend to
  any other position.\\
\item
  \textbf{Permutation-aware via positional encodings}: Additional
  information (sinusoidal or learned) encodes position so the model
  knows order.
\end{itemize}

\subsection{Multi-Head Attention and Transformer
Blocks}\label{multi-head-attention-and-transformer-blocks-1}

Real Transformer layers use \textbf{multi-head attention}:

\begin{itemize}
\tightlist
\item
  The model runs self-attention in parallel with multiple sets of
  \((Q,K,V)\) projections (heads).\\
\item
  Different heads can specialize in different patterns (e.g., local
  motif combinations, long-range enhancer--promoter contacts).
\end{itemize}

A typical Transformer block has:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Multi-head self-attention\\
\item
  Add \& layer normalization\\
\item
  Position-wise feed-forward network\\
\item
  Another add \& layer normalization
\end{enumerate}

Stacking many blocks yields a deep Transformer.

\subsection{Computational Cost and Long-Range
Genomics}\label{computational-cost-and-long-range-genomics-1}

Naive self-attention has \(O(L^2)\) cost in sequence length \(L\). For
genomic sequences, where we might want 100 kb--1 Mb contexts, this is
expensive.

Long-range genomic models like Enformer and HyenaDNA address this with:

\begin{itemize}
\tightlist
\item
  \textbf{Hybrid designs} (CNNs + attention) to reduce sequence length
  before applying global attention (Å½. Avsec et al. 2021).\\
\item
  \textbf{Structured state space models (SSMs)} and related
  architectures that scale more gracefully with length (Nguyen et al.
  2023).
\end{itemize}

These details are treated in depth in the long-range modeling chapters;
here it suffices to know that Transformers give flexible global context
at the cost of higher computational complexity.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Self-Supervised Learning and
Pretraining}\label{self-supervised-learning-and-pretraining-1}

A central theme of this book is \textbf{pretraining}: training a large
model once on a broad, unlabeled or weakly-labeled task, then re-using
it for many downstream problems.

\subsection{Supervised vs
Self-Supervised}\label{supervised-vs-self-supervised-1}

\begin{itemize}
\tightlist
\item
  \textbf{Supervised learning}: Each input \(x\) comes with a label
  \(y\). Examples:

  \begin{itemize}
  \tightlist
  \item
    Predicting chromatin marks from sequence (DeepSEA).\\
  \item
    Predicting splice junctions (SpliceAI).\\
  \item
    Predicting disease risk from features
    (Chapter~\ref{sec-clinical-risk}).
  \end{itemize}
\item
  \textbf{Self-supervised learning}: The model learns from raw input
  data without explicit labels, using some \textbf{pretext task}
  constructed from the data itself. Examples:

  \begin{itemize}
  \tightlist
  \item
    Masked token prediction (BERT-style): hide some nucleotides and
    train the model to predict them from surrounding context.\\
  \item
    Next-token prediction (GPT-style): predict the next base given
    previous ones.\\
  \item
    Denoising or reconstruction tasks.
  \end{itemize}
\end{itemize}

In genomics, self-supervised models treat DNA sequences as a language
and learn from the vast amount of genomic sequence without needing
curated labels.

\subsection{Masked Language Modeling on
DNA}\label{masked-language-modeling-on-dna-1}

DNABERT applied BERT-style masked language modeling to DNA sequences
tokenized as overlapping k-mers (Ji et al. 2021). The model:

\begin{itemize}
\tightlist
\item
  Reads sequences as k-mer tokens.\\
\item
  Randomly masks a subset of tokens.\\
\item
  Learns to predict the masked tokens given surrounding context.
\end{itemize}

Benefits:

\begin{itemize}
\tightlist
\item
  Uses essentially unlimited unlabeled genomic data.\\
\item
  Learns rich representations that can be fine-tuned for tasks like
  promoter prediction, splice site detection, and variant effect
  prediction.
\end{itemize}

Chapter~\ref{sec-dna-lm} generalizes this story to broader DNA
foundation models, including alternative tokenization schemes and
architectures.

\subsection{Pretraining, Fine-Tuning, and
Probing}\label{pretraining-fine-tuning-and-probing-1}

After pretraining, we can use a model in several ways:

\begin{itemize}
\tightlist
\item
  \textbf{Fine-tuning}: Initialize with pretrained weights, then
  continue training on a specific downstream task with task-specific
  labels.\\
\item
  \textbf{Linear probing}: Freeze the pretrained model, extract
  embeddings, and train a simple linear classifier on top.\\
\item
  \textbf{Prompting / adapters}: Add small task-specific modules
  (adapters) while keeping most of the model fixed.
\end{itemize}

These patterns reappear across protein LMs, DNA LMs, variant effect
models, and GFMs in Chapters 9--16.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Foundations for Evaluation and
Reliability}\label{foundations-for-evaluation-and-reliability-1}

While the main book has dedicated chapters for evaluation
(Chapter~\ref{sec-evaluation}), confounding
(Chapter~\ref{sec-confounding}), and clinical metrics
(Chapter~\ref{sec-clinical-risk}), it's useful to have a few basic
concepts in mind.

\subsection{Distribution Shift}\label{distribution-shift-3}

A model is trained under some data distribution (e.g., certain assays,
cohorts, ancestries) and then deployed under another (e.g., a different
hospital system or population). When these differ, we have
\textbf{distribution shift}, which can degrade performance.

Typical genomic shifts include:

\begin{itemize}
\tightlist
\item
  New sequencing technologies or lab protocols\\
\item
  New ancestries or populations\\
\item
  New tissues, diseases, or phenotypes
\end{itemize}

\subsection{Data Leakage}\label{data-leakage-2}

\textbf{Data leakage} occurs when information from the test set
``leaks'' into training (e.g., through overlapping loci or related
individuals), leading to overly optimistic estimates of performance.
Chapter~\ref{sec-evaluation} and Chapter~\ref{sec-confounding} discuss
strategies for leak-resistant splits in detail.

\subsection{Calibration and
Uncertainty}\label{calibration-and-uncertainty-1}

For many applications, especially in the clinic, we care not just about
whether the model is \emph{correct}, but whether its probabilities are
\textbf{well calibrated} and whether we know when the model is
uncertain. Calibration and uncertainty quantification are covered in
Chapter~\ref{sec-clinical-risk}; here, the main takeaway is that
\textbf{perfect AUROC does not imply perfect clinical utility}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{A Minimal Recipe for a Genomic Deep Learning
Project}\label{a-minimal-recipe-for-a-genomic-deep-learning-project-1}

To make the abstractions more concrete, here is a lightweight ``recipe''
that roughly mirrors what the case-study chapters do.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Define the prediction problem}

  \begin{itemize}
  \tightlist
  \item
    Input: e.g., 1 kb sequence around a variant, or patient-level
    features.\\
  \item
    Output: e.g., presence of a chromatin mark, change in expression,
    disease risk.
  \end{itemize}
\item
  \textbf{Choose an input representation}

  \begin{itemize}
  \tightlist
  \item
    One-hot encoding or tokenization scheme for sequences (see
    Chapter~\ref{sec-representations}).\\
  \item
    Encodings for variants, genes, or patients (e.g., aggregate from
    per-variant features).
  \end{itemize}
\item
  \textbf{Pick a model family}

  \begin{itemize}
  \tightlist
  \item
    CNN for local sequence-to-function (Chapters 5--7).\\
  \item
    Transformer or SSM for long-range or language model-style tasks
    (Chapters 8--11).\\
  \item
    Pretrained GFM + small task-specific head (Chapters 12--16).
  \end{itemize}
\item
  \textbf{Specify the loss and metrics}

  \begin{itemize}
  \tightlist
  \item
    Cross-entropy for binary classification, MSE for regression, etc.\\
  \item
    Metrics like AUROC, AUPRC, correlation, calibration.
  \end{itemize}
\item
  \textbf{Set up data splits and evaluation}

  \begin{itemize}
  \tightlist
  \item
    Decide whether to split by locus, individual, cohort, or species.\\
  \item
    Hold out a test set and use validation data to tune hyperparameters.
  \end{itemize}
\item
  \textbf{Train with regularization and monitoring}

  \begin{itemize}
  \tightlist
  \item
    Use an optimizer (SGD or Adam-like) with a learning rate schedule.\\
  \item
    Apply regularization (dropout, weight decay, augmentation).\\
  \item
    Monitor training and validation curves for overfitting.
  \end{itemize}
\item
  \textbf{Inspect and stress-test}

  \begin{itemize}
  \tightlist
  \item
    Check performance across subgroups (e.g., ancestries, assays,
    cohorts).\\
  \item
    Use interpretability tools (Chapter~\ref{sec-interpretability}) to
    see what patterns the model is using.\\
  \item
    Run robustness checks and ablations.
  \end{itemize}
\item
  \textbf{Iterate}

  \begin{itemize}
  \tightlist
  \item
    Adjust architecture, add more data, refine labels, or incorporate
    pretrained backbones.\\
  \item
    Move from model-centric tuning to system-level considerations (data
    quality, deployment environment, feedback loops).
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{How This Primer Connects to the Rest of the
Book}\label{how-this-primer-connects-to-the-rest-of-the-book-1}

This appendix gives you the minimum vocabulary to navigate the rest of
the text:

\begin{itemize}
\tightlist
\item
  \textbf{Chapters 5--7} show how CNNs on one-hot sequence learn
  regulatory code, expression, and splicing.\\
\item
  \textbf{Chapters 8--11} extend these ideas to richer sequence
  representations, Transformers, and long-range sequence models.\\
\item
  \textbf{Chapters 12--16} frame these models as genomic foundation
  models, introduce evaluation, interpretability, and multi-omics.\\
\item
  \textbf{Chapters 17--19} show how these ingredients are assembled into
  clinical, discovery, and biotech applications.
\end{itemize}

You don't need to internalize every detail here. The goal is simply that
when you see terms like ``convolution,'' ``attention,'' ``pretraining,''
or ``fine-tuning'' in the main chapters, they feel like familiar tools
rather than mysterious jargon.

\chapter{Data Curation}\label{sec-apx-data-curation}

\textbf{Label}: \texttt{@sec-apx-data-curation}\\
\textbf{Old source}: \textbf{NEW}

\textbf{Purpose}: Guide to constructing training sets for genomic FMs

\textbf{Content}: - Data sources and access - Quality filtering
strategies - Deduplication and redundancy reduction - Contamination
detection - Data provenance and versioning - Bias assessment

\chapter{Model Reference}\label{sec-apx-models}

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, opacitybacktitle=0.6, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, left=2mm, toprule=.15mm, toptitle=1mm, breakable, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-warning-color!10!white, arc=.35mm, titlerule=0mm, opacityback=0, bottomrule=.15mm, rightrule=.15mm, colback=white]

\textbf{TODO:}

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1795}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2564}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3077}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2564}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Category
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Chapter(s)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Citation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{DNA Language Models} & & & \\
DNABERT & DNA LM & 10, 12, App A & Ji et al. (2021) \\
DNABERT-2 & DNA LM & 10, 12 & Z. Zhou et al. (2024) \\
Nucleotide Transformer & DNA LM & 10, 12, 20 & Dalla-Torre et al.
(2023) \\
HyenaDNA & DNA LM & 10, 12, 20 & Nguyen et al. (2023) \\
Caduceus & DNA LM & 10, 12 & Schiff et al. (2024) \\
GROVER & DNA LM & 10, 12 & Sanabria et al. (2024) \\
Gene42 & DNA LM & 12 & --- \\
BioToken & DNA LM & 12 & Medvedev et al. (2025) \\
\textbf{Protein Language Models} & & & \\
ESM / ESM-2 & PLM & 9, 12 & Rives et al. (2021); Lin et al. (2022) \\
ESM-1v & PLM & 4, 9 & Brandes et al. (2023) \\
\textbf{Sequence-to-Function (Regulatory)} & & & \\
DeepSEA & Seqâ†’Func & 5, 7, 12 & J. Zhou and Troyanskaya (2015) \\
Beluga & Seqâ†’Func & 5, 6 & J. Zhou et al. (2018) \\
Sei & Seqâ†’Func & 5, 12 & K. M. Chen et al. (2022) \\
Enformer & Seqâ†’Func / GFM & 11, 12, 19, 20 & Å½. Avsec et al. (2021) \\
Basenji & Seqâ†’Func & 11 & Kelley et al. (2018) \\
Basenji2 & Seqâ†’Func & 11 & Kelley (2020a) \\
ExPecto & Seqâ†’Func & 6, 7 & J. Zhou et al. (2018) \\
Borzoi & Seqâ†’Func & 11, 19 & Linder et al. (2025) \\
\textbf{Splice Prediction} & & & \\
SpliceAI & Splice & 7, 12, 13 & Jaganathan et al. (2019) \\
MaxEntScan & Splice & 7 & Yeo and Burge (2004) \\
\textbf{Variant Effect Predictors} & & & \\
CADD & VEP (integrative) & 4, 12, 13, 20 & Rentzsch et al. (2019);
Schubach et al. (2024) \\
SIFT & VEP (conservation) & 4, 13 & Ng and Henikoff (2003) \\
PolyPhen & VEP (conservation) & 4, 13 & Adzhubei et al. (2010) \\
AlphaMissense & VEP (PLM + structure) & 9, 12, 13, 19, 20 & J. Cheng et
al. (2023) \\
GPN-MSA & VEP (alignment LM) & 13, 19, 20 & Benegas et al. (2024) \\
Evo 2 & VEP / GFM & 13, 19 & Brixi et al. (2025) \\
AlphaGenome & VEP / GFM & 13, 19, 20 & Z. Avsec, Latysheva, and Cheng
(2025) \\
\textbf{Clinical / Polygenic Models} & & & \\
Delphi & PGS (deep) & 12, 18, 19 & Georgantas, Kutalik, and Richiardi
(2024) \\
MIFM & Fine-mapping & 12, 18, 19 & Rakowski and Lippert (2025) \\
DeepRVAT & Rare variant & 19 & Clarke et al. (2024) \\
G2PT & PGS & 18 & Lee et al. (2025) \\
\textbf{Multi-omics / Graph Models} & & & \\
GLUE & Multi-omics integration & 18, 19 & Cao and Gao (2022) \\
MoGCN & Gene GNN & 19 & X. Li et al. (2022) \\
CGMega & Gene GNN & 19 & Hao Li et al. (2024) \\
\textbf{Structure Prediction} & & & \\
AlphaFold2 & Structure & 9, 13 & Jumper et al. (2021) \\
AlphaFold3 & Structure & 9 & Abramson et al. (2024) \\
\textbf{Conservation Scores} & & & \\
phyloP & Conservation & 4 & Siepel et al. (2005) \\
GERP++ & Conservation & 4 & Davydov et al. (2010) \\
\end{longtable}

\section{Category Definitions}\label{category-definitions}

\begin{itemize}
\tightlist
\item
  \textbf{DNA LM}: DNA language models using self-supervised pretraining
  on genomic sequences
\item
  \textbf{PLM}: Protein language models trained on protein sequences
\item
  \textbf{Seqâ†’Func}: Supervised sequence-to-function models predicting
  chromatin/expression from DNA
\item
  \textbf{Splice}: Specialized splice site prediction models
\item
  \textbf{VEP}: Variant effect predictors (various paradigms)
\item
  \textbf{GFM}: Genomic foundation model (broad, reusable
  representations)
\item
  \textbf{PGS}: Polygenic score or risk prediction models
\item
  \textbf{GNN}: Graph neural network for gene/pathway analysis
\end{itemize}

\chapter{Resources}\label{sec-apx-resources}

\begin{tcolorbox}[enhanced jigsaw, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}, opacitybacktitle=0.6, colframe=quarto-callout-warning-color-frame, leftrule=.75mm, left=2mm, toprule=.15mm, toptitle=1mm, breakable, coltitle=black, bottomtitle=1mm, colbacktitle=quarto-callout-warning-color!10!white, arc=.35mm, titlerule=0mm, opacityback=0, bottomrule=.15mm, rightrule=.15mm, colback=white]

\textbf{TODO:}

\begin{itemize}
\tightlist
\item
  \ldots{}
\item
  \ldots{}
\end{itemize}

\end{tcolorbox}

\section{Genomics \& Human Genetics}\label{genomics-human-genetics}

\begin{itemize}
\item
  \textbf{Thompson \& Thompson Genetics and Genomics in Medicine (9th
  ed.)}\\
  Ronald Cohn, Stephen Scherer, Ada Hamosh. Clinical-focused overview of
  human genetics and genomics for medicine, great for grounding in
  clinical genomics.
\item
  \textbf{Human Molecular Genetics (5th ed.)}\\
  Tom Strachan, Andrew Read. Higher-level molecular genetics/genomics
  text with strong coverage of mechanisms, technologies, and disease
  applications.
\end{itemize}

\section{Immunology}\label{immunology}

\begin{itemize}
\tightlist
\item
  \textbf{Janeway's Immunobiology (10th ed.)}\\
  Kenneth M. Murphy, Casey Weaver, Leslie J. Berg. Standard
  comprehensive immunology textbook, excellent for understanding immune
  system biology relevant to genomics and disease.
\end{itemize}

\section{Machine Learning \& Deep
Learning}\label{machine-learning-deep-learning}

\begin{itemize}
\item
  \textbf{Deep Learning}\\
  Ian Goodfellow, Yoshua Bengio, Aaron Courville. Comprehensive deep
  learning textbook; free online:
  \url{https://www.deeplearningbook.org/}
\item
  \textbf{Dive into Deep Learning (D2L)}\\
  Aston Zhang et al.~Interactive deep learning book with Jupyter
  notebooks and multi-framework code; free online: \url{https://d2l.ai/}
\item
  \textbf{An Introduction to Statistical Learning (ISLR, 2nd ed.)}\\
  Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. Gentle
  introduction to statistical learning methods used in ML, available
  free online: \url{https://www.statlearning.com/}
\item
  \textbf{The Elements of Statistical Learning (ESL)}\\
  Trevor Hastie, Robert Tibshirani, Jerome Friedman. More advanced,
  theory-heavy companion to ISLR; free PDF:
  \url{https://hastie.su.domains/ElemStatLearn/}
\end{itemize}

\chapter{Glossary}\label{sec-apx-glossary}

\section{CH 01}\label{ch-01}

\subsection{Sequencing Technologies \&
Data}\label{sequencing-technologies-data}

\subsubsection*{Next-generation sequencing
(NGS)}\label{glo-next-generation-sequencing-ngs}
\addcontentsline{toc}{subsubsection}{Next-generation sequencing (NGS)}

High-throughput DNA sequencing technologies that allow
rapid\ldots stretches of DNA, producing millions of short reads in
parallel\ldots.

\subsubsection*{Illumina sequencing}\label{glo-illumina-sequencing}
\addcontentsline{toc}{subsubsection}{Illumina sequencing}

A widely used NGS technology that utilizes reversible dye-terminators to
sequence DNA by synthesis

\subsubsection*{Short reads / Paired-end
reads}\label{glo-short-reads-paired-end-reads}
\addcontentsline{toc}{subsubsection}{Short reads / Paired-end reads}

DNA sequences generated by NGS technologies, typically rangi\ldots{} a
DNA fragment, providing additional information for alignment.

\subsubsection*{Long-read sequencing (PacBio HiFi, Oxford
Nanopore)}\label{glo-long-read-sequencing-pacbio-hifi-oxford-nanopore}
\addcontentsline{toc}{subsubsection}{Long-read sequencing (PacBio HiFi,
Oxford Nanopore)}

DNA sequencing technologies that produce longer reads, typic\ldots ases,
allowing for better resolution of complex genomic regions.

\subsubsection*{Circular consensus
sequencing}\label{glo-circular-consensus-sequencing}
\addcontentsline{toc}{subsubsection}{Circular consensus sequencing}

A sequencing method used by PacBio to generate highly accura\ldots{}
long reads by repeatedly sequencing the same DNA molecule.

\subsubsection*{Base calling}\label{glo-base-calling}
\addcontentsline{toc}{subsubsection}{Base calling}

The process of determining the nucleotide sequence from raw sequencing
data.

\subsubsection*{FASTQ}\label{glo-fastq}
\addcontentsline{toc}{subsubsection}{FASTQ}

A file format that stores both nucleotide sequences and their
corresponding quality scores.

\subsubsection*{Read depth / Coverage (e.g., 30Ã—,
100Ã—)}\label{glo-read-depth-coverage-e-g-30-100}
\addcontentsline{toc}{subsubsection}{Read depth / Coverage (e.g., 30Ã—,
100Ã—)}

The number of times a particular nucleotide is sequenced, indicating the
reliability of the sequencing data.

\subsection{Targeting Strategies}\label{targeting-strategies}

\subsubsection*{Targeted gene panel}\label{glo-targeted-gene-panel}
\addcontentsline{toc}{subsubsection}{Targeted gene panel}

A sequencing approach that focuses on a specific set of gene\ldots or
cost-effective analysis of known disease-associated variants.

\subsubsection*{Whole-exome sequencing
(WES)}\label{glo-whole-exome-sequencing-wes}
\addcontentsline{toc}{subsubsection}{Whole-exome sequencing (WES)}

A sequencing approach that targets all protein-coding region\ldots the
genome, providing a comprehensive view of coding variants.

\subsubsection*{Whole-genome sequencing
(WGS)}\label{glo-whole-genome-sequencing-wgs}
\addcontentsline{toc}{subsubsection}{Whole-genome sequencing (WGS)}

A sequencing approach that captures the entire genome, inclu\ldots ng
regions, providing the most comprehensive view of genetic var

\subsubsection*{Capture efficiency}\label{glo-capture-efficiency}
\addcontentsline{toc}{subsubsection}{Capture efficiency}

The effectiveness of a targeted sequencing approach in
enric\ldots interest, impacting the overall quality and coverage of the
data.

\subsection{Alignment \& Processing}\label{alignment-processing}

\subsubsection*{Read alignment /
Mapping}\label{glo-read-alignment-mapping}
\addcontentsline{toc}{subsubsection}{Read alignment / Mapping}

The process of aligning sequencing reads to a reference genome to
determine their genomic origin.

\subsubsection*{Seed-and-extend
alignment}\label{glo-seed-and-extend-alignment}
\addcontentsline{toc}{subsubsection}{Seed-and-extend alignment}

An algorithmic approach for read alignment that first identi\ldots ences
(seeds) and then extends the alignment around these seeds.

\subsubsection*{PCR duplicates}\label{glo-pcr-duplicates}
\addcontentsline{toc}{subsubsection}{PCR duplicates}

Identical sequencing reads that originate from the same DNA
\ldots esulting from PCR amplification, which can bias variant calling.

\subsubsection*{Base quality score recalibration
(BQSR)}\label{glo-base-quality-score-recalibration-bqsr}
\addcontentsline{toc}{subsubsection}{Base quality score recalibration
(BQSR)}

A process that adjusts the quality scores of sequencing read\ldots or
systematic errors made by the sequencer.

\subsubsection*{Mapping quality}\label{glo-mapping-quality}
\addcontentsline{toc}{subsubsection}{Mapping quality}

A measure of the confidence that a read is correctly aligned to the
reference genome.

\subsubsection*{Reference bias}\label{glo-reference-bias}
\addcontentsline{toc}{subsubsection}{Reference bias}

The tendency for sequencing and alignment processes to preferentially
detect alleles present in the reference genome.

\subsection{Variant Calling}\label{variant-calling}

\subsubsection*{Variant calling}\label{glo-variant-calling}
\addcontentsline{toc}{subsubsection}{Variant calling}

The process of identifying variants from sequencing data by comparing it
to a reference genome.

\subsubsection*{Genotype likelihood}\label{glo-genotype-likelihood}
\addcontentsline{toc}{subsubsection}{Genotype likelihood}

The probability of observing the sequencing data given a particular
genotype.

\subsubsection*{Pair-HMM (pair hidden Markov
model)}\label{glo-pair-hmm-pair-hidden-markov-model}
\addcontentsline{toc}{subsubsection}{Pair-HMM (pair hidden Markov
model)}

A statistical model used in variant calling to calculate the likelihood
of different alignments between reads and the reference genome.

\subsubsection*{Joint genotyping / Cohort
calling}\label{glo-joint-genotyping-cohort-calling}
\addcontentsline{toc}{subsubsection}{Joint genotyping / Cohort calling}

The process of simultaneously calling variants across multiple samples
to improve accuracy and consistency.

\subsubsection*{gVCF (genomic VCF)}\label{glo-gvcf-genomic-vcf}
\addcontentsline{toc}{subsubsection}{gVCF (genomic VCF)}

A variant call format that includes information about both
va\ldots sites, allowing for joint genotyping.

\subsubsection*{VCF (variant call
format)}\label{glo-vcf-variant-call-format}
\addcontentsline{toc}{subsubsection}{VCF (variant call format)}

A standardized file format for storing variant information, including
SNPs, indels, and structural variants.

\subsubsection*{VQSR (Variant Quality Score
Recalibration)}\label{glo-vqsr-variant-quality-score-recalibration}
\addcontentsline{toc}{subsubsection}{VQSR (Variant Quality Score
Recalibration)}

A method for improving the accuracy of variant calls by
mode\ldots lationship between variant quality scores and various
annotatio

\subsubsection*{Pileup}\label{glo-pileup}
\addcontentsline{toc}{subsubsection}{Pileup}

A summary of the base calls at each position in a set of align\ldots ing
reads, used for variant calling and visualization.

\subsection{Phasing}\label{phasing}

\subsubsection*{Haplotype phasing}\label{glo-haplotype-phasing}
\addcontentsline{toc}{subsubsection}{Haplotype phasing}

The process of determining which variants are inherited together on the
same chromosome.

\subsubsection*{Read-backed phasing}\label{glo-read-backed-phasing}
\addcontentsline{toc}{subsubsection}{Read-backed phasing}

A method of phasing that uses sequencing reads that span multiple
variants to determine their phase.

\subsubsection*{Statistical phasing}\label{glo-statistical-phasing}
\addcontentsline{toc}{subsubsection}{Statistical phasing}

A method of phasing that uses population-level genotype data and
statistical models to infer haplotypes.

\subsubsection*{Compound
heterozygosity}\label{glo-compound-heterozygosity}
\addcontentsline{toc}{subsubsection}{Compound heterozygosity}

The presence of two different variants at a particular gene locus, one
on each chromosome of a pair.

\subsubsection*{Cis vs.~trans
configuration}\label{glo-cis-vs-trans-configuration}
\addcontentsline{toc}{subsubsection}{Cis vs.~trans configuration}

Describes the relative arrangement of two variants on the same
chromosome (cis) or on different chromosomes (trans).

\subsection{Variant Types}\label{variant-types}

\subsubsection*{SNV (single nucleotide
variant)}\label{glo-snv-single-nucleotide-variant}
\addcontentsline{toc}{subsubsection}{SNV (single nucleotide variant)}

A variation in a single nucleotide that occurs at a specific position in
the genome.

\subsubsection*{Indel}\label{glo-indel}
\addcontentsline{toc}{subsubsection}{Indel}

An insertion or deletion of bases in the genome of an organism.

\subsubsection*{Structural variant}\label{glo-structural-variant}
\addcontentsline{toc}{subsubsection}{Structural variant}

A large-scale alteration in the genome, such as a deletion, duplication,
inversion, or translocation.

\subsubsection*{Multi-nucleotide variant
(MNV)}\label{glo-multi-nucleotide-variant-mnv}
\addcontentsline{toc}{subsubsection}{Multi-nucleotide variant (MNV)}

A variation that affects multiple consecutive nucleotides in the genome.

\subsubsection*{Mosaic variant}\label{glo-mosaic-variant}
\addcontentsline{toc}{subsubsection}{Mosaic variant}

A genetic variant that is present in some but not all cells of an
organism, often arising during development.

\subsubsection*{Somatic variant}\label{glo-somatic-variant}
\addcontentsline{toc}{subsubsection}{Somatic variant}

A genetic variant that occurs in non-germline cells and is not
inherited, often associated with cancer.

\subsubsection*{Germline variant}\label{glo-germline-variant}
\addcontentsline{toc}{subsubsection}{Germline variant}

A genetic variant that is present in the egg or sperm and can be passed
on to offspring.

\subsubsection*{De novo variant}\label{glo-de-novo-variant}
\addcontentsline{toc}{subsubsection}{De novo variant}

A genetic variant that arises spontaneously in an individual and is not
inherited from either parent.

\subsection{Difficult Regions}\label{difficult-regions}

\subsubsection*{Segmental duplication}\label{glo-segmental-duplication}
\addcontentsline{toc}{subsubsection}{Segmental duplication}

Large, highly similar sequences in the genome that can complicate read
alignment and variant calling.

\subsubsection*{Paralog / Paralogous
gene}\label{glo-paralog-paralogous-gene}
\addcontentsline{toc}{subsubsection}{Paralog / Paralogous gene}

A gene that is related to another gene in the same organism due to a
duplication event.

\subsubsection*{Homopolymer}\label{glo-homopolymer}
\addcontentsline{toc}{subsubsection}{Homopolymer}

A sequence of identical nucleotides in a row, which can be prone to
sequencing errors.

\subsubsection*{Low-complexity region}\label{glo-low-complexity-region}
\addcontentsline{toc}{subsubsection}{Low-complexity region}

A region of the genome with a simple sequence composition, which can be
challenging for alignment and variant calling.

\subsubsection*{HLA region / MHC}\label{glo-hla-region-mhc}
\addcontentsline{toc}{subsubsection}{HLA region / MHC}

The human leukocyte antigen (HLA) region, also known as the major
histocompatibility complex (MHC), is a highly polymorphic region
involved in immune response.

\subsection{Benchmarking}\label{benchmarking}

\subsubsection*{Precision (positive predictive
value)}\label{glo-precision-positive-predictive-value}
\addcontentsline{toc}{subsubsection}{Precision (positive predictive
value)}

The proportion of true positive variant calls among all positive calls.

\subsubsection*{Recall (sensitivity)}\label{glo-recall-sensitivity}
\addcontentsline{toc}{subsubsection}{Recall (sensitivity)}

The proportion of true positive variant calls detected among all actual
variants.

\subsubsection*{F1 score}\label{glo-f1-score}
\addcontentsline{toc}{subsubsection}{F1 score}

The harmonic mean of precision and recall, providing a single metric for
evaluating variant calling performance.

\subsubsection*{True positive (TP) / False positive (FP) / False
negative
(FN)}\label{glo-true-positive-tp-false-positive-fp-false-negative-fn}
\addcontentsline{toc}{subsubsection}{True positive (TP) / False positive
(FP) / False negative (FN)}

Metrics used to evaluate the accuracy of variant calls, where TP
represents correctly identified variants, FP represents incorrectly
identified variants, and FN represents missed variants.

\subsubsection*{High-confidence
region}\label{glo-high-confidence-region}
\addcontentsline{toc}{subsubsection}{High-confidence region}

A region of the genome where variant calls are considered to b\ldots{}
reliable, often used for benchmarking and validation.

\subsection{Key Resources/Tools (may warrant brief glossary
entries)}\label{key-resourcestools-may-warrant-brief-glossary-entries}

\subsubsection*{GIAB (Genome in a
Bottle)}\label{glo-giab-genome-in-a-bottle}
\addcontentsline{toc}{subsubsection}{GIAB (Genome in a Bottle)}

A consortium that develops reference materials and data for benchmarking
genome sequencing and variant calling.

\subsubsection*{DeepVariant}\label{glo-deepvariant}
\addcontentsline{toc}{subsubsection}{DeepVariant}

A deep learning-based variant caller developed by Google that identifies
genetic variants from sequencing data.

\subsubsection*{GLnexus}\label{glo-glnexus}
\addcontentsline{toc}{subsubsection}{GLnexus}

A tool for joint variant calling across multiple samples, designed to
work with DeepVariant outputs.

\subsubsection*{HaplotypeCaller}\label{glo-haplotypecaller}
\addcontentsline{toc}{subsubsection}{HaplotypeCaller}

A variant caller from the Genome Analysis Toolkit (GATK) that uses local
de-novo assembly of haplotypes to call variants.

\section{CH 02}\label{ch-02}

\subsection{Reference \& Coordinate
Systems}\label{reference-coordinate-systems}

\subsubsection*{Reference
genome/assembly}\label{glo-reference-genome-assembly}
\addcontentsline{toc}{subsubsection}{Reference genome/assembly}

A digital nucleic acid sequence database, assembled as a
repre\ldots example of a species' set of genes. Multiple versions exist.

\subsubsection*{GRCh37}\label{glo-grch37}
\addcontentsline{toc}{subsubsection}{GRCh37}

The 37th version of the Genome Reference Consortium human genome
assembly.

\subsubsection*{GRCh38}\label{glo-grch38}
\addcontentsline{toc}{subsubsection}{GRCh38}

The 38th version of the Genome Reference Consortium human genome
assembly.

\subsubsection*{T2T-CHM13}\label{glo-t2t-chm13}
\addcontentsline{toc}{subsubsection}{T2T-CHM13}

The Telomere-to-Telomere (T2T) CHM13 human genome assembly, r\ldots ting
a complete, gapless sequence of a human genome.

\subsubsection*{Pangenome reference}\label{glo-pangenome-reference}
\addcontentsline{toc}{subsubsection}{Pangenome reference}

A reference that represents the genetic diversity of a species, rather
than a single individual.

\subsubsection*{Gene model}\label{glo-gene-model}
\addcontentsline{toc}{subsubsection}{Gene model}

A representation of the structure of a gene, including its exons,
introns, and regulatory elements.

\subsubsection*{Canonical transcript}\label{glo-canonical-transcript}
\addcontentsline{toc}{subsubsection}{Canonical transcript}

The most biologically relevant transcript of a gene, often used as the
reference for annotation.

\subsubsection*{Alternative
transcript/isoform}\label{glo-alternative-transcript-isoform}
\addcontentsline{toc}{subsubsection}{Alternative transcript/isoform}

Different versions of a transcript produced from the same gene due to
alternative splicing or other mechanisms.

\subsubsection*{MANE Select}\label{glo-mane-select}
\addcontentsline{toc}{subsubsection}{MANE Select}

Matched Annotation from NCBI and EMBL-EBI (MANE) Select is a
\ldots cripts that are consistently annotated across databases.

\subsection{Variant Types \& Properties}\label{variant-types-properties}

\subsubsection*{Allele frequency}\label{glo-allele-frequency}
\addcontentsline{toc}{subsubsection}{Allele frequency}

The proportion of a specific allele among all alleles of a gene in a
population.

\subsubsection*{MAF (minor allele
frequency)}\label{glo-maf-minor-allele-frequency}
\addcontentsline{toc}{subsubsection}{MAF (minor allele frequency)}

The frequency at which the less common allele occurs in a given
population.

\subsubsection*{rsID}\label{glo-rsid}
\addcontentsline{toc}{subsubsection}{rsID}

A unique identifier assigned to a single nucleotide polymorphism (SNP)
in the dbSNP database.

\subsubsection*{Loss-of-function (LoF)
variant}\label{glo-loss-of-function-lof-variant}
\addcontentsline{toc}{subsubsection}{Loss-of-function (LoF) variant}

A genetic variant that results in reduced or abolished protein function.

\subsubsection*{Ultra-rare variant}\label{glo-ultra-rare-variant}
\addcontentsline{toc}{subsubsection}{Ultra-rare variant}

A genetic variant that is extremely uncommon in the population, often
with a frequency of less than 0.01\%.

\subsection{Population Genetics
Metrics}\label{population-genetics-metrics}

\subsubsection*{Linkage disequilibrium}\label{glo-ld}
\addcontentsline{toc}{subsubsection}{Linkage disequilibrium}

A non-random association of alleles at different loci in a given
population.

\subsubsection*{pLI (probability of being loss-of-function
intolerant)}\label{glo-pli-probability-of-being-loss-of-function-intolerant}
\addcontentsline{toc}{subsubsection}{pLI (probability of being
loss-of-function intolerant)}

A metric that estimates the likelihood that a gene is intolerant to
loss-of-function variants.

\subsubsection*{LOEUF (loss-of-function observed/expected upper bound
fraction)}\label{glo-loeuf-loss-of-function-observed-expected-upper-bound-fraction}
\addcontentsline{toc}{subsubsection}{LOEUF (loss-of-function
observed/expected upper bound fraction)}

A metric that quantifies the observed versus expected number of
loss-of-function variants in a gene.

\subsubsection*{Constraint metrics}\label{glo-constraint-metrics}
\addcontentsline{toc}{subsubsection}{Constraint metrics}

Metrics that assess the tolerance of a gene to functional genetic
variation.

\subsubsection*{Imputation}\label{glo-imputation}
\addcontentsline{toc}{subsubsection}{Imputation}

The process of inferring unobserved genotypes in a study sample based on
observed genotypes and a reference panel.

\subsection{Functional Genomics}\label{functional-genomics}

\subsubsection*{ChIP-seq}\label{glo-chip-seq}
\addcontentsline{toc}{subsubsection}{ChIP-seq}

Chromatin Immunoprecipitation followed by sequencing, a method used to
analyze protein-DNA interactions.

\subsubsection*{DNase-seq}\label{glo-dnase-seq}
\addcontentsline{toc}{subsubsection}{DNase-seq}

A method to identify regions of open chromatin by sequencing DNA
fragments generated by DNase I digestion.

\subsubsection*{ATAC-seq}\label{glo-atac-seq}
\addcontentsline{toc}{subsubsection}{ATAC-seq}

Assay for Transposase-Accessible Chromatin using sequencing, a technique
to study chromatin accessibility.

\subsubsection*{Hi-C}\label{glo-hi-c}
\addcontentsline{toc}{subsubsection}{Hi-C}

A method to study the three-dimensional architecture of genomes by
capturing chromatin interactions.

\subsubsection*{Chromatin
accessibility}\label{glo-chromatin-accessibility}
\addcontentsline{toc}{subsubsection}{Chromatin accessibility}

The degree to which DNA is exposed and available for binding by
proteins, often assessed by DNase-seq or ATAC-seq.

\subsubsection*{Histone modification}\label{glo-histone-modification}
\addcontentsline{toc}{subsubsection}{Histone modification}

Chemical modifications to histone proteins that can influence chromatin
structure and gene expression.

\subsubsection*{Peak calling}\label{glo-peak-calling}
\addcontentsline{toc}{subsubsection}{Peak calling}

The process of identifying regions of the genome with
signific\ldots ment of sequencing reads, often used in ChIP-seq and
ATAC-seq analyses.

\subsubsection*{Signal track}\label{glo-signal-track}
\addcontentsline{toc}{subsubsection}{Signal track}

A graphical representation of sequencing data across the genom\ldots{}
intensity of signals such as read coverage or enrichment.

\subsection{Expression Genetics}\label{expression-genetics}

\subsubsection*{eQTL (expression quantitative trait
locus)}\label{glo-eqtl-expression-quantitative-trait-locus}
\addcontentsline{toc}{subsubsection}{eQTL (expression quantitative trait
locus)}

A genomic locus that explains variation in gene expression levels.

\subsubsection*{Splicing QTL}\label{glo-splicing-qtl}
\addcontentsline{toc}{subsubsection}{Splicing QTL}

A genomic locus that affects the splicing of pre-mRNA.

\subsubsection*{Molecular QTL}\label{glo-molecular-qtl}
\addcontentsline{toc}{subsubsection}{Molecular QTL}

A quantitative trait locus that influences molecular traits such as gene
expression, protein levels, or metabolite concentrations.

\subsubsection*{Cis-regulatory}\label{glo-cis-regulatory}
\addcontentsline{toc}{subsubsection}{Cis-regulatory}

Referring to regulatory elements, such as promoters or enhanc\ldots ated
on the same molecule of DNA as the gene they regulate.

\subsubsection*{Colocalization}\label{glo-colocalization}
\addcontentsline{toc}{subsubsection}{Colocalization}

The occurrence of two or more genetic signals at the same genomic
location, suggesting a shared causal variant.

\subsubsection*{Dropout (single-cell
context)}\label{glo-dropout-single-cell-context}
\addcontentsline{toc}{subsubsection}{Dropout (single-cell context)}

The failure to detect a transcript in a single-cell RNA-seq
ex\ldots often due to low mRNA capture efficiency.

\subsection{Clinical Interpretation}\label{clinical-interpretation}

\subsubsection*{ACMG/AMP criteria}\label{glo-acmg-amp-criteria}
\addcontentsline{toc}{subsubsection}{ACMG/AMP criteria}

A set of guidelines developed by the American College of Medic\ldots ogy
(AMP) for the interpretation of sequence variants. These c\ldots vide a
framework for classifying variants into categories such path

\subsubsection*{Pathogenicity}\label{glo-pathogenicity}
\addcontentsline{toc}{subsubsection}{Pathogenicity}

The ability of a genetic variant to cause disease.

\subsubsection*{Haploinsufficiency}\label{glo-haploinsufficiency}
\addcontentsline{toc}{subsubsection}{Haploinsufficiency}

A condition in which a single functional copy of a gene is
in\ldots maintain normal function, leading to a disease phenotype.

\subsubsection*{Triplosensitivity}\label{glo-triplosensitivity}
\addcontentsline{toc}{subsubsection}{Triplosensitivity}

A condition in which an extra copy of a gene leads to a disease
phenotype.

\subsubsection*{Gene-disease validity}\label{glo-gene-disease-validity}
\addcontentsline{toc}{subsubsection}{Gene-disease validity}

The strength of evidence supporting a relationship between a gene and a
disease.

\subsubsection*{Pharmacogenomics}\label{glo-pharmacogenomics}
\addcontentsline{toc}{subsubsection}{Pharmacogenomics}

The study of how genetic variation affects an individual's response to
drugs.

\subsubsection*{Diplotype}\label{glo-diplotype}
\addcontentsline{toc}{subsubsection}{Diplotype}

The combination of alleles at multiple loci on a single chromosome that
are inherited together.

\subsection{Study Designs \& Statistics}\label{study-designs-statistics}

\subsubsection*{GWAS summary
statistics}\label{glo-gwas-summary-statistics}
\addcontentsline{toc}{subsubsection}{GWAS summary statistics}

Aggregated data from genome-wide association studies,
typicall\ldots ormation on the association between genetic variants and
traits across the genome.

\subsubsection*{Fine-mapping}\label{glo-fine-mapping}
\addcontentsline{toc}{subsubsection}{Fine-mapping}

The process of identifying the specific causal variants
within\ldots omic region associated with a trait.

\subsubsection*{Effect size}\label{glo-effect-size}
\addcontentsline{toc}{subsubsection}{Effect size}

A measure of the strength of the relationship between a genetic variant
and a trait.

\subsubsection*{Ascertainment bias}\label{glo-ascertainment-bias}
\addcontentsline{toc}{subsubsection}{Ascertainment bias}

A systematic distortion in the estimation of genetic effects
d\ldots non-random sampling of individuals or variants.

\section{CH 03}\label{ch-03}

\section{CH 04}\label{ch-04}

\section{CH 05}\label{ch-05}

\section{CH 06}\label{ch-06}

\section{CH 07}\label{ch-07}

\section{CH 08}\label{ch-08}

\section{CH 09}\label{ch-09}

\section{CH 10}\label{ch-10}

\section{CH 11}\label{ch-11}

\section{CH 12}\label{ch-12}

\section{CH 13}\label{ch-13}

\section{CH 14}\label{ch-14}

\section{CH 15}\label{ch-15}

\section{CH 16}\label{ch-16}

\section{CH 17}\label{ch-17}

\section{CH 18}\label{ch-18}

\section{CH 19}\label{ch-19}

\section{CH 20}\label{ch-20}

\section{APX A}\label{apx-a}

\section{APX B}\label{apx-b}


\backmatter


\end{document}
