# The Genomic Data Landscape {#sec-data}

## Why Genomic Data Resources Matter

We can sequence a human genome for a few hundred dollars and store the resulting terabytes of data for less than a month's streaming subscription, yet most variants discovered in that genome remain uninterpretable. A clinical laboratory receiving a patient's whole-genome sequence will identify four to five million positions that differ from the reference, but can confidently classify fewer than a hundred of those variants as medically actionable. The remaining millions occupy a gray zone where available evidence is insufficient to distinguish benign polymorphism from pathogenic mutation. This asymmetry between variant discovery and variant interpretation defines the central challenge that the data resources in this chapter were built to address.

No single dataset can resolve this gap. Instead, modern genomics depends on a mosaic of complementary resources: **reference genomes** and **gene annotations** that define coordinates and consequences, **population variant catalogs** that reveal what survives in healthy individuals, **cohort** and **biobank** datasets that link variation to phenotypes, **functional genomics** atlases that map biochemical activity, and clinical databases that aggregate expert interpretations. Each resource contributes a different type of evidence; together, they form the empirical foundation that deep learning models compress, combine, and learn from.

The models throughout this book inherit both the power and the limitations of these foundational resources. A variant effect predictor trained on ClinVar labels learns the biases embedded in clinical ascertainment. A chromatin accessibility model trained on ENCODE cell lines may not generalize to primary tissues absent from the training compendium. A constraint metric derived from European-ancestry cohorts will be less calibrated for variants private to other populations. Understanding what these resources contain, and what they systematically miss, is essential for interpreting what models learn and anticipating where they will fail.

::: {.callout-warning .content-visible when-profile="draft"}
**TODO (chapter-level):**
- Somewhere in project, discuss correlated but distinct rare variants vs gene-lethal variants vs late-onset disease variants in more depth, ideally with a simple schematic.
- Ensure consistent coverage of multi-species genomes and pangenomes (including Zoonomia-style mammalian constraint resources).
- Coordinate where deep mutational scanning (DMS) / multiplexed assays (ProteinGym, TraitGym) live across chapters so this chapter and the protein-focused chapters reference each other cleanly.
:::

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** High-level "subway map" of the data landscape, showing how reads → reference genomes → variant catalogs → cohorts → functional genomics → expression → clinical resources connect, with arrows indicating where later chapters plug in (e.g., @sec-reg, @sec-gwas, @sec-vep-classical).
:::


## Reference Genomes and Gene Annotations

A variant's biological consequence depends entirely on where it falls in the genome, and "where" is defined by coordinates that only exist relative to a reference assembly. When we say a mutation disrupts the third exon of *BRCA1*, that statement presupposes agreement on which sequence is the reference, which positions constitute exon boundaries, and which transcript model defines the canonical gene structure. Reference genomes and gene annotations are so foundational that their assumptions often become invisible, yet every downstream analysis inherits the choices embedded in these resources. A model cannot learn about a regulatory element for a transcript that does not exist in the annotation.

### Reference Assemblies

Most modern pipelines align reads to a small number of reference assemblies, predominantly GRCh38 or the newer T2T-CHM13 [@nurk_complete_2022]. A reference genome is not simply a consensus sequence; it encodes a series of consequential decisions about how to represent duplications, alternate haplotypes, and unresolved gaps. The choice of reference determines which regions are mappable by short reads, how structural variants are represented, and how comparable results will be across cohorts built on different assemblies.

Graph-based and **pangenome** references relax the assumption of a single linear reference, representing multiple haplotypes and ancestries within a unified coordinate system [@liao_pangenome_2023]. Comparative multi-species references, such as those used in mammalian constraint maps from the Zoonomia consortium [@sullivan_leveraging_2023], extend this idea across species, providing evolutionary conservation scores that feed directly into deleteriousness predictors and gene-level constraint metrics.

For most datasets used in this book, the practical reality is still GRCh37 or GRCh38 coordinates, often with incremental patches. Models trained on these resources therefore inherit their blind spots: incomplete or collapsed segmental duplications, underrepresented ancestries in pangenome construction, and uneven quality across chromosomes and regions. These limitations concentrate in precisely the regions where variant interpretation matters most (such as the HLA locus, pharmacogenes with structural variation, and segmental duplications harboring disease genes), creating a systematic mismatch between clinical importance and reference quality.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Side-by-side schematic of (1) a linear reference, (2) a graph/pangenome reference, and (3) a conservation track derived from multi-species alignment, with callouts indicating how each affects variant calling and model labels.
:::

### Gene Models

Gene annotation databases such as GENCODE and RefSeq define the biological vocabulary overlaid on reference coordinates: exon-intron structures, canonical and alternative transcripts, start and stop codons, and untranslated regions [@frankish_gencode_2019; @oleary_refseq_2016]. These annotations distinguish coding from non-coding variants, identify splice-disrupting mutations, and map functional genomics signals to genes. They also establish the units (genes, transcripts, exons) that downstream models implicitly operate on.

The MANE Select project provides a single matched transcript per protein-coding gene that is identical between GENCODE and RefSeq, simplifying clinical interpretation and variant reporting [@morales_mane_2022]. This standardization makes variant descriptions consistent across laboratories, yet it privileges a single isoform over biological complexity. In contexts where tissue-specific or developmentally regulated isoforms drive disease (alternative splicing in muscular dystrophies, isoform-specific expression in neuropsychiatric conditions), the canonical transcript may miss the relevant biology.

In practice, new isoforms continue to be discovered, alternative splicing remains incompletely cataloged, and cell-type-specific transcripts may be absent from bulk-derived annotations. Non-coding RNA genes and pseudogenes are even more unevenly annotated. These gaps propagate through every tool built on them: variant effect predictors cannot score consequences for transcripts that do not exist in their reference annotation, and expression models cannot predict isoforms they were never trained on.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Genome-browser-style panel showing (1) reference sequence, (2) two different transcript models for the same gene (GENCODE vs RefSeq), and (3) the MANE Select transcript, with a highlighted variant whose consequence changes depending on the chosen transcript.
:::


## Population Variant Catalogs and Allele Frequencies

Population variant catalogs answer a question that no amount of sequence analysis alone can resolve: has this variant been observed in healthy individuals, and at what frequency? **Allele frequency**, the proportion of chromosomes in a reference population carrying a given variant, serves as one of the most powerful priors in variant interpretation. A missense variant observed in 5% of a population is unlikely to cause severe early-onset disease; a variant never seen across hundreds of thousands of sequenced individuals demands closer scrutiny.

Beyond simple filtering, allele frequencies inform statistical frameworks for case-control association, provide training signal for deleteriousness predictors, and enable **imputation** of ungenotyped variants through **linkage disequilibrium**. The catalogs described below have progressively expanded in sample size, ancestral diversity, and annotation depth, transforming variant interpretation from an ad hoc exercise into a quantitative discipline.

A crucial nuance shapes everything that follows: these catalogs record variants that are compatible with being sampled in the first place. Gene-lethal variants that cause embryonic death or severe childhood disease rarely appear, even when they are biologically informative. Variants causing late-onset conditions (Alzheimer's risk alleles, adult-onset cancer predisposition) can persist at appreciable frequencies because selection has not had time to remove them. Throughout this book, models trained on population data can only learn from variants present in these catalogs, which means they systematically underrepresent the most severe loss-of-function mutations.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Allele-frequency spectrum for (1) common polymorphisms, (2) rare but observed variants, and (3) hypothetical gene-lethal variants that never appear, annotated with examples of disease categories and how different filters are applied in clinical interpretation.
:::

### dbSNP and the Variant Universe

The database of Single Nucleotide Polymorphisms (dbSNP) historically aggregated known **single nucleotide polymorphisms (SNPs)** and short insertions/deletions into a single catalog, providing stable identifiers (rsIDs) that serve as common currency across tools and publications [@sherry_dbsnp_2001]. Modern whole-exome and whole-genome sequencing routinely discovers millions of previously unseen variants per large cohort, but dbSNP identifiers remain the standard way to reference known polymorphisms and link disparate resources. When a GWAS publication reports an association at rs12345, that identifier traces back to dbSNP and enables integration with functional annotations, clinical databases, and other catalogs.

### 1000 Genomes and Early Reference Panels

The 1000 Genomes Project provided one of the first widely used multi-population reference panels, sampling individuals from African, European, East Asian, South Asian, and admixed American populations [@auton_1kgp_2015]. Its haplotype structure underlies many imputation servers and downstream analyses, enabling genotyping arrays with millions of markers to impute tens of millions of untyped variants through linkage disequilibrium [@yun_accurate_2021]. Although its sample size (approximately 2,500 individuals) is modest by current standards, 1000 Genomes established the template for how to build and distribute multi-population reference panels, and its samples continue to serve as benchmarks for variant calling performance.

### The Genome Aggregation Database (gnomAD)

The Genome Aggregation Database aggregates exome and genome sequencing data from research and clinical cohorts worldwide into harmonized allele frequency resources spanning hundreds of thousands of individuals [@karczewski_gnomad_2020]. gnomAD provides high-resolution allele frequencies stratified by genetic ancestry, enabling population-matched filtering that accounts for variants common in one ancestry but rare in others.

Beyond frequencies, gnomAD introduced **constraint metrics** that have become standard features in variant prioritization. The probability of loss-of-function intolerance (pLI) and loss-of-function observed/expected upper bound fraction (LOEUF) summarize how depleted a gene is for protein-truncating variants relative to expectation. Genes essential for viability show far fewer loss-of-function variants than neutral mutation rates would predict; this depletion provides evidence of selective constraint that transfers to variant interpretation. A novel truncating variant in a highly constrained gene warrants more concern than the same variant class in an unconstrained gene.

These resources are indispensable for filtering common variants in Mendelian disease diagnostics, distinguishing ultra-rare variants from recurrent ones, and providing population genetics priors for deleteriousness scores like CADD [@rentzsch_cadd_2019; @schubach_cadd_2024]. At the same time, they reflect the composition of the cohorts they aggregate: ancestry representation remains uneven despite ongoing efforts, structural variants and repeat expansions are less completely cataloged than SNVs and short indels, and individuals with severe early-onset disease are underrepresented by design. These biases propagate into every model that uses gnomAD frequencies or constraint scores as features.


## Cohorts, Biobanks, and GWAS Summary Data

Variant interpretation at the population level requires linking genetic variation to phenotypes at scale. A variant's association with disease risk, its effect on a quantitative trait, or its role in drug response can only be discovered when genotypes and phenotypes are measured together across thousands or hundreds of thousands of individuals. Yet assembling such cohorts introduces its own biases: participants must consent, provide samples, and have phenotypes recorded in standardized ways. The populations enrolled in major biobanks reflect patterns of healthcare access, research infrastructure, and historical priorities that do not represent global genetic diversity.

The overrepresentation of European-ancestry individuals in most major biobanks creates systematic gaps in variant discovery, effect-size estimation, and **polygenic score** portability that propagate through downstream analyses [@sirugo_diversity_2019]. A variant common in West African populations may be absent or rare in European-dominated catalogs, rendering it invisible to association studies and underrepresented in predictive models. These ancestry biases, and strategies for addressing them, are discussed in detail in @sec-confounding.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Flow diagram for a typical GWAS/PGS pipeline: cohort enrollment → genotyping/sequencing → QC → imputation → association testing → summary statistics → polygenic scores, with callouts for where bias enters (e.g., ancestry composition).
:::

### Large Population Cohorts

UK Biobank, with approximately 500,000 participants and deep phenotyping across thousands of traits, has become a dominant resource for methods development and benchmarking [@bycroft_ukbiobank_2018]. FinnGen leverages Finland's population history and unified healthcare records for large-scale disease association discovery [@kurki_finngen_2023]. The All of Us Research Program prioritizes diversity, aiming to enroll one million participants with deliberate oversampling of historically underrepresented groups [@null_all-of-us_2019]. Additional resources include the Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie Biobank, and emerging African genomics initiatives such as H3Africa [@sirugo_diversity_2019].

Together, these efforts enable **genome-wide association studies (GWAS)** for thousands of traits, development and evaluation of polygenic scores, and fine-mapping of causal variants and genes [@marees_gwas_2018; @mountjoy_open_2021]. From a modeling perspective, they provide the large-scale genotype-phenotype matrices that power architectures ranging from classical linear mixed models to foundation models trained on biobank-scale data. The practical reality for most GWAS and polygenic score methods in @sec-gwas is data from either array genotyping with imputation or whole-exome/whole-genome sequencing with joint calling, as in DeepVariant/GLnexus-style pipelines [@yun_accurate_2021].

### GWAS Summary Statistics

Beyond individual-level data, many resources distribute GWAS **summary statistics**: per-variant effect sizes, standard errors, and p-values aggregated across cohorts. The GWAS Catalog compiles published results across thousands of traits [@sollis_gwas-catalog_2023], while the PGS Catalog provides curated polygenic score weights and metadata for reproducibility [@lambert_pgs-catalog_2021]. Frameworks like Open Targets Genetics integrate fine-mapped signals with functional annotations to prioritize candidate causal genes at associated loci [@mountjoy_open_2021].

Summary statistics enable meta-analysis across cohorts without sharing individual-level data, transfer of genetic findings to new populations through methods like PRS-CSx, and integration with functional annotations to distinguish causal variants from linked bystanders. For deep learning, summary statistics provide a sparse, trait-level view of the genome that can be combined with richer sequence-based or functional labels, though the sparsity and noise in GWAS signals pose challenges that differ from the dense labels available in functional genomics.


## Functional Genomics and Regulatory Landscapes

Protein-coding exons constitute roughly 1.5% of the human genome, yet most disease-associated variants from GWAS fall outside coding regions. Understanding these non-coding variants requires mapping the regulatory logic that governs when, where, and how much each gene is expressed. Functional genomics assays provide this map: identifying **transcription factor** binding sites, nucleosome positioning, **chromatin accessibility**, **histone modifications**, and three-dimensional genome organization across cell types and conditions.

For this book, functional genomics datasets serve a dual role. First, they supply the biological vocabulary for interpreting non-coding variants, linking sequence changes to potential regulatory consequences. Second, and more directly, they provide the training labels for sequence-to-function deep learning models. When a model learns to predict chromatin accessibility or histone marks from DNA sequence alone, it compresses into its parameters the regulatory code implicit in thousands of functional genomics experiments.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Multi-track genome browser view showing DNA sequence, chromatin accessibility, histone marks, and transcription factor ChIP-seq peaks across two cell types, annotated to show how a sequence window becomes a multi-task training label vector for a model like DeepSEA.
:::

### ENCODE, Roadmap, and Related Consortia

The Encyclopedia of DNA Elements (ENCODE) and Roadmap Epigenomics consortia designed coordinated experimental campaigns that profiled transcription factor binding (ChIP-seq), histone modifications, chromatin accessibility (DNase-seq, ATAC-seq), and chromatin conformation (Hi-C) across cell lines and primary tissues [@kagda_encode_2025; @kundaje_roadmap_2015]. Gene Expression Omnibus (GEO) archives these and many other functional genomics datasets with standardized metadata [@edgar_geo_2002].

The significance of these consortia lies less in any individual experiment than in the scale and standardization they provide. By generating hundreds of assays across dozens of cell types with consistent protocols, ENCODE and Roadmap created canonical reference datasets that define the regulatory landscape for the cell types they profiled. Many regulatory deep learning models in @sec-reg and @sec-splice are effectively trained on these resources, learning to predict multi-task label vectors where each task corresponds to a ChIP-seq or accessibility experiment.

### The Cistrome Data Browser

ENCODE and Roadmap provide authoritative datasets for their chosen cell types and factors, but they represent only a fraction of publicly available functional genomics experiments. The Cistrome Data Browser addresses this gap by aggregating thousands of human and mouse ChIP-seq and chromatin accessibility datasets from ENCODE, Roadmap, GEO, and individual publications into a uniformly reprocessed repository [@zheng_cistrome_2019]. All datasets pass through standardized quality control and peak calling, enabling comparisons across experiments originally generated with different protocols.

Cistrome provides uniform peak calls, signal tracks, and metadata for cell type, factor, and experimental conditions. The tradeoff is heterogeneity: while reprocessing harmonizes computational steps, the underlying experiments vary in sample preparation, antibody quality, sequencing depth, and experimental design. Cistrome expands coverage at the cost of the tight experimental control found in the primary consortia, a tradeoff that matters when models learn from noisy or inconsistent labels.

### From Assays to Training Labels

Sequence-to-function models transform functional genomics resources into supervised learning problems. Models like DeepSEA (see @sec-reg) draw training labels from ENCODE, Roadmap, and Cistrome-style datasets: each genomic window is associated with binary or quantitative signals indicating transcription factor binding, histone modifications, or chromatin accessibility across hundreds of assays and cell types [@zhou_deepsea_2015; @zhou_expecto_2018].

The quality, coverage, and biases of these labels directly constrain what models can learn. Cell types absent from the training compendium cannot be predicted reliably. Factors with few high-quality ChIP-seq experiments will have noisier labels. Systematic differences between assay types (binary peak calls versus quantitative signal tracks) shape whether models learn to predict occupancy, accessibility, or something in between. These considerations become central when examining model architectures and training strategies in @sec-reg.

### Deep Mutational Scanning and Multiplexed Variant Assays

A parallel ecosystem has emerged for **deep mutational scanning (DMS)** and other multiplexed assays that measure the fitness or functional impact of thousands of protein or regulatory variants in a single experiment. Benchmarks such as ProteinGym compile large DMS datasets across proteins to evaluate variant effect predictors [@notin_proteingym_2023], while TraitGym curates multiplexed reporter assays and other high-throughput readouts of regulatory variant effects [@benegas_traitgym_2025].

These resources sit at the interface between genomic and protein-level modeling. They provide dense, quantitative labels for synthetic or near-saturated variant libraries, complementing the sparse, naturally occurring variation in gnomAD and biobanks. DMS data differ fundamentally from population catalogs: they measure functional impact directly under controlled conditions rather than inferring it from population survival. Later chapters on protein sequence models and regulatory variant prediction return to these DMS-style datasets as key benchmarks and training sources.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Cartoon of a DMS/multiplexed assay pipeline: mutagenesis → pooled library → selection/screen → sequencing → fitness score per variant, with arrows indicating how these are aggregated into ProteinGym/TraitGym-style benchmarks.
:::


## Expression and eQTL Resources

Functional genomics assays reveal where transcription factors bind and which chromatin regions are accessible, but they do not directly answer the downstream question: does regulatory activity actually change how much RNA a gene produces? Expression datasets complete this link, measuring transcript abundance across tissues, cell types, and genetic backgrounds.

**Expression quantitative trait loci (eQTLs)** formalize the genotype-expression relationship statistically, identifying genetic variants associated with changes in transcript levels. For variant interpretation, eQTLs offer mechanistic hypotheses connecting non-coding variants to specific genes and tissues: if a GWAS signal colocalizes with an eQTL for a nearby gene in a disease-relevant tissue, that gene becomes a candidate effector. For model training, expression data provide quantitative labels that integrate across many regulatory inputs converging on a single promoter.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Schematic of eQTL mapping: genotype matrix + expression matrix → association scan → locus plot linking variant, regulatory element, and target gene, with tissue labels.
:::

### Bulk Expression Atlases

The Genotype-Tissue Expression (GTEx) consortium provides the most comprehensive resource linking genetic variation to gene expression across human tissues, with RNA-seq profiles from 948 post-mortem donors across 54 tissues [@gtex_2020]. GTEx established foundational insights that inform models throughout this book: most genes harbor tissue-specific eQTLs, regulatory variants typically act in cis over distances of hundreds of kilobases, and expression variation explains a meaningful fraction of complex trait heritability.

GTEx underlies expression prediction models such as PrediXcan, which trains tissue-specific models to impute gene expression from genotypes alone [@gamazon_predixcan_2015]. Transcriptome-wide association studies (TWAS) extend this idea to associate imputed expression with phenotypes [@gusev_twas_2016]. Colocalization methods ask whether a GWAS signal and an eQTL share the same causal variant, providing evidence that the associated gene mediates the trait effect.

The GTEx design has limitations. Post-mortem collection introduces agonal stress artifacts that may not reflect living tissue biology. Sample sizes vary considerably across tissues (hundreds for some, dozens for others), affecting statistical power. Some disease-relevant tissues, such as pancreatic islets or specific brain subregions, remain undersampled. Complementary resources like the eQTLGen Consortium aggregate eQTL results from blood across much larger sample sizes, trading tissue diversity for statistical power [@vosa_eqtl-gen_2021].

### Single-Cell and Context-Specific Expression

Bulk RNA-seq averages expression across all cells in a tissue sample, obscuring the cell-type-specific programs that often mediate disease biology. A bulk eQTL in brain tissue might reflect astrocytes, neurons, microglia, or oligodendrocytes, and the causal cell type matters for understanding mechanism. Single-cell RNA-seq resolves this heterogeneity, identifying expression signatures for individual cell types, rare populations, and transitional states.

Large-scale efforts including the Human Cell Atlas and Tabula Sapiens are building reference atlases that catalog cell types across organs and developmental stages [@regev_cell-atlas_2017; @tabula_sapiens_2022]. For variant interpretation, single-cell data enable cell-type-specific eQTL mapping, revealing that a variant may influence expression in one cell type but not others within the same tissue. Spatial transcriptomics adds anatomical context, preserving tissue architecture while measuring gene expression.

These technologies introduce computational challenges: sparsity from dropout effects, batch variation across samples and technologies, and massive scale with millions of cells per study. They also offer an increasingly fine-grained view of the link between genotype, regulatory state, and cellular phenotype. Single-cell and spatial resources appear primarily in later chapters on multi-omics integration and systems-level models.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Pair of panels: (1) bulk expression averaging across mixed cell types vs (2) a UMAP of single-cell profiles colored by cell type, with an inset showing a cell-type-specific eQTL that would be invisible in bulk data.
:::


## Variant Interpretation Databases and Clinical Labels

Allele frequencies tell us what variants survive in healthy populations, and functional genomics data reveal where the genome is biochemically active, but neither directly answers the clinical question: is this variant pathogenic? That determination requires integrating multiple lines of evidence (family segregation, functional assays, computational predictions, phenotypic observations) into a structured framework that can be applied consistently.

Clinical variant interpretation databases aggregate these assessments from laboratories, expert panels, and research groups, providing labels that inform diagnostic decisions and serve as training data for machine learning models. These databases have become critical infrastructure for both clinical genomics and computational method development, though their labels carry biases and circularity that propagate through any analysis built on them.

### ClinVar and Related Resources

ClinVar aggregates assertions of variant pathogenicity from clinical laboratories and researchers worldwide [@landrum_clinvar_2018]. It provides standardized classifications following ACMG/AMP guidelines (pathogenic, likely pathogenic, benign, likely benign, **variant of uncertain significance**) that are central to diagnostic pipelines and to benchmarking variant effect predictors.

#### Strengths and Limitations of ClinVar

ClinVar has become the de facto reference for variant pathogenicity labels, but its contents reflect systematic biases that affect any downstream use.

**Submission heterogeneity** poses a fundamental challenge. Annotations come from diverse submitters, including diagnostic laboratories, research groups, expert panels, and database exports. Submitters apply varying evidentiary standards; some provide detailed supporting evidence while others offer only assertions. Conflicting interpretations are common, particularly for variants of uncertain significance (VUS).

**Version sensitivity** means that classifications evolve as evidence accumulates. A variant classified as VUS in 2018 may be reclassified as likely pathogenic by 2023 based on new functional studies or additional patient observations. Models trained on historical ClinVar snapshots may learn outdated classifications. When reporting performance, specifying the ClinVar version used is essential for reproducibility.

**Ancestry and gene coverage biases** create uneven representation. Variants in well-studied populations (particularly European ancestry) and well-characterized disease genes are heavily overrepresented. Variants from underrepresented populations are more likely to remain classified as VUS due to insufficient evidence. This creates feedback loops: predictive models perform better on European-ancestry variants because training data is richer, reinforcing the disparity [@landrum_clinvar_2018].

**Circularity with computational predictors** represents a subtle but important concern. Clinical submissions increasingly incorporate computational scores like CADD, REVEL, and AlphaMissense as supporting evidence for pathogenicity classification. When these same ClinVar labels are then used to train or evaluate computational predictors, circularity emerges [@schubach_cadd_2024]. If a laboratory used a high CADD score as supporting evidence for classifying a variant as likely pathogenic, and that variant later appears as a positive label in ClinVar, models trained on ClinVar may partly learn to reproduce CADD itself rather than discovering independent signal.

This circularity operates at two levels. **Evaluation circularity** occurs when models are assessed on benchmarks influenced by the model's own predictions. **Training circularity** occurs when features used in training derive from the same underlying information as the labels. Both forms inflate apparent performance without demonstrating genuine predictive power. We return to these issues in @sec-vep-classical and @sec-confounding.

**Variants of uncertain significance (VUS)** constitute the majority of rare variant classifications, reflecting genuinely limited evidence. These variants are both targets for predictive modeling (can computational methods resolve uncertainty?) and potential pitfalls (models trained only on confidently classified variants may not generalize to VUS with different characteristics).

Despite these limitations, ClinVar remains invaluable. The key is using it appropriately: recognizing biases when training models, accounting for version differences when comparing studies, stratifying performance by ancestry and gene coverage, and treating computational predictions as one line of evidence rather than definitive classifications.

### ClinGen and Expert Curation

The Clinical Genome Resource (ClinGen) complements ClinVar by providing expert-curated assessments at multiple levels [@rehm_clingen_2015]. ClinGen expert panels evaluate **gene-disease validity** (whether variation in a gene can cause a specific disease) and **dosage sensitivity** (whether haploinsufficiency or triplosensitivity leads to clinical phenotypes). These evaluations build on the catalog of Mendelian phenotypes maintained by OMIM, which provides curated gene-disease associations and clinical synopses [@amberger_omim_2015].

For individual variants, ClinGen Variant Curation Expert Panels apply ACMG/AMP criteria systematically, assigning levels of evidence for pathogenicity or benignity. The FDA has recognized these curations as valid scientific evidence for clinical validity [@pejaver_calibration_2022]. ClinGen also develops calibrated thresholds for computational predictors, specifying score intervals that justify different strengths of evidence (supporting, moderate, strong) for pathogenicity or benignity. These calibrations directly inform how computational scores should be incorporated into variant classification workflows.

::: {.callout-warning .content-visible when-profile="draft"}
**Visual suggestion:** Decision-graph for variant interpretation showing evidence types (population frequency, functional assays, computational scores, segregation, pharmacogenomics) converging into ClinVar/ClinGen/ClinPGx labels, with circularity highlighted (e.g., CADD used both as input evidence and training target).
:::

### ClinPGx and Pharmacogenomics Resources

ClinPGx integrates the PharmGKB knowledge base, CPIC clinical guidelines, and PharmCAT annotation tool into a unified **pharmacogenomics** resource [@whirl-PharmGKB_2012]. While most variant interpretation databases focus on rare disease-causing mutations, pharmacogenomics curates gene-drug associations that influence metabolism, efficacy, and adverse reactions.

These pharmacogenomic variants differ from typical pathogenic mutations: many are common polymorphisms rather than rare deleterious alleles, but their clinical importance for prescribing decisions makes them a distinct category of actionable genetic variation. The *CYP2D6* gene, for example, encodes an enzyme responsible for metabolizing approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants [@whirl-PharmGKB_2012]. Star-allele haplotypes (combinations of variants that travel together on a chromosome) determine metabolizer status, requiring phasing and structural variant detection that extend beyond simple SNV calling.

The CPIC guidelines provide evidence-based recommendations for adjusting drug selection or dosing based on pharmacogene diplotypes, and FDA drug labels document regulatory recognition of these associations. From a modeling perspective, pharmacogenomic resources offer a complementary type of label linking variants to molecular and clinical outcomes through different mechanisms than Mendelian disease pathogenicity.


## How Later Chapters Use These Resources

The genomic deep learning models throughout this book inherit both the strengths and limitations of the data they are trained on. @sec-gwas draws on GWAS summary statistics and biobank-scale cohorts to construct polygenic scores, inheriting the ancestry biases embedded in cohort composition. @sec-vep-classical examines how annotation-based methods compress population frequencies, conservation, and functional signals into genome-wide deleteriousness scores, propagating the biases of each input. Chapters [-@sec-reg] and [-@sec-splice] use ENCODE, Roadmap, and Cistrome-style functional data as training labels for sequence-to-function models, learning representations limited by the cell types and assays in the training compendium. Chapters [-@sec-fm-principles] through [-@sec-multi-omics] revisit these resources as inputs, labels, and priors for genomic foundation models.

By surveying the data landscape in one place, this chapter establishes a common reference that later chapters build on rather than re-introducing each resource from scratch. The recurring theme is that biases, gaps, and circularity in these foundational datasets propagate through every model trained on them. A variant effect predictor trained on ClinVar labels inherits the ascertainment biases of clinical sequencing. A chromatin model trained on ENCODE cell lines may not generalize to primary tissues. A constraint model trained on human populations systematically misses gene-lethal variants that never appear in any catalog. Understanding these foundations is essential for interpreting what models learn and anticipating where they will fail.