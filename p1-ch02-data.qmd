# The Genomic Data Landscape {#sec-data}

## Why Genomic Data Resources Matter

We can sequence a human genome for a few hundred dollars and store the resulting terabytes of data for less than a month's streaming subscription, yet most variants discovered in that genome remain uninterpretable. A clinical laboratory receiving a patient's whole-genome sequence will identify four to five million positions that differ from the reference, but can confidently classify fewer than a hundred of those variants as medically actionable. The remaining millions occupy a gray zone where available evidence is insufficient to distinguish benign polymorphism from pathogenic mutation. This asymmetry between variant discovery and variant interpretation defines the central challenge that the data resources in this chapter were built to address.

No single dataset can resolve this gap. Modern genomics depends on a mosaic of complementary resources: **reference genomes** and **gene annotations** that define coordinates and consequences, **population variant catalogs** that reveal what survives in healthy individuals, **cohort** and **biobank** datasets that link variation to phenotypes, **functional genomics** atlases that map biochemical activity, and clinical databases that aggregate expert interpretations. Each resource contributes a different type of evidence; together, they form the empirical foundation that deep learning models compress, combine, and learn from.

The models throughout this book inherit both the power and the limitations of these foundational resources. A variant effect predictor trained on ClinVar labels learns the biases embedded in clinical ascertainment. A chromatin accessibility model trained on ENCODE cell lines may not generalize to primary tissues absent from the training compendium. A constraint metric derived from European-ancestry cohorts will be less calibrated for variants private to other populations. Understanding what these resources contain, and what they systematically miss, is essential for interpreting what models learn and anticipating where they will fail.


## Reference Genomes and Gene Annotations

A family arrives at a genetics clinic after their newborn's screening reveals a potential metabolic disorder. The clinical team orders whole-genome sequencing and receives a report identifying a novel variant in a gene associated with the condition. The variant's coordinates place it at the boundary between an exon and an intron, potentially disrupting splicing. Yet whether this interpretation is correct depends on decisions made years before the child was born: which positions constitute exon boundaries, which transcript model defines the canonical gene structure, and which sequence serves as the reference against which "variant" is defined. Reference genomes and gene annotations are so foundational that their assumptions often become invisible, yet every downstream analysis inherits the choices embedded in these resources. A model cannot learn about a regulatory element for a transcript that does not exist in the annotation.

### Reference Assemblies

A patient's clinical sequencing reveals a potentially pathogenic variant in a duplicated region of chromosome 17. The variant calling pipeline reports a confident genotype, the annotation tool predicts a frameshift, and the clinical team prepares to discuss the finding with the family. Yet the "variant" may be an artifact of misalignment: reads from a paralogous sequence elsewhere in the genome mapped incorrectly because the reference assembly collapsed two distinct loci into one. Whether this error occurs, whether it can be detected, and whether the clinical interpretation has any foundation in biological reality all depend on the choice of reference genome.

Most modern pipelines align reads to a small number of reference assemblies, predominantly GRCh38 or the newer T2T-CHM13 [@nurk_complete_2022]. A reference genome is not simply a consensus sequence; it encodes a series of consequential decisions about how to represent duplications, alternate haplotypes, and unresolved gaps. These decisions determine which regions are mappable by short reads, how structural variants are represented, and how comparable results will be across cohorts built on different assemblies.

Graph-based and **pangenome** references relax the assumption of a single linear reference, representing multiple haplotypes and ancestries within a unified coordinate system [@liao_pangenome_2023]. Comparative multi-species references, such as those used in mammalian constraint maps from the Zoonomia consortium [@sullivan_leveraging_2023], extend this idea across species, providing evolutionary conservation scores that feed directly into deleteriousness predictors and gene-level constraint metrics.

For most datasets used in this book, the practical reality is still GRCh37 or GRCh38 coordinates, often with incremental patches. Models trained on these resources therefore inherit their blind spots: incomplete or collapsed segmental duplications, underrepresented ancestries in pangenome construction, and uneven quality across chromosomes and regions. These limitations concentrate in precisely the regions where variant interpretation matters most (such as the HLA locus, pharmacogenes with structural variation, and segmental duplications harboring disease genes), creating a systematic mismatch between clinical importance and reference quality.

### Gene Models

A child presents with developmental delay and muscle weakness. Whole-genome sequencing identifies a novel variant near the *DMD* gene, which encodes dystrophin and causes Duchenne muscular dystrophy when disrupted. The annotation pipeline reports the variant as intronic and unlikely to affect protein function. Yet *DMD* spans 2.2 megabases and includes 79 exons with complex alternative splicing; whether this variant disrupts a tissue-specific isoform depends entirely on which transcript model the annotation tool uses. The clinical implications are entirely different, yet the underlying sequence is identical: only the annotation changes.

Gene annotation databases such as GENCODE and RefSeq define the biological vocabulary overlaid on reference coordinates: exon-intron structures, canonical and alternative transcripts, start and stop codons, and untranslated regions [@frankish_gencode_2019; @oleary_refseq_2016]. These annotations distinguish coding from non-coding variants, identify splice-disrupting mutations, and map functional genomics signals to genes. They also establish the units (genes, transcripts, exons) that downstream models implicitly operate on.

The MANE Select project provides a single matched transcript per protein-coding gene that is identical between GENCODE and RefSeq, simplifying clinical interpretation and variant reporting [@morales_mane_2022]. This standardization makes variant descriptions consistent across laboratories, yet it privileges a single isoform over biological complexity. In contexts where tissue-specific or developmentally regulated isoforms drive disease (alternative splicing in muscular dystrophies, isoform-specific expression in neuropsychiatric conditions), the canonical transcript may miss the relevant biology.

New isoforms continue to be discovered, alternative splicing remains incompletely cataloged, and cell-type-specific transcripts may be absent from bulk-derived annotations. Non-coding RNA genes and pseudogenes are even more unevenly annotated. These gaps propagate through every tool built on them: variant effect predictors cannot score consequences for transcripts that do not exist in their reference annotation, and expression models cannot predict isoforms they were never trained on.


## Population Variant Catalogs and Allele Frequencies

A clinical geneticist evaluates a child with an undiagnosed syndrome and identifies a novel missense variant in a candidate gene. The question that determines what happens next is deceptively simple: has anyone else carried this variant? If the variant appears in thousands of healthy adults, it is almost certainly benign. If it has never been observed across hundreds of thousands of sequenced genomes, that absence becomes evidence of selective pressure against the variant, strongly suggesting functional consequence. Without population-scale variant catalogs, this inference is impossible, and every rare variant would demand the same level of scrutiny regardless of its actual likelihood of causing disease.

**Allele frequency**, the proportion of chromosomes in a reference population carrying a given variant, serves as one of the most powerful priors in variant interpretation. Beyond simple filtering, allele frequencies inform statistical frameworks for case-control association, provide training signal for deleteriousness predictors, and enable **imputation** of ungenotyped variants through **linkage disequilibrium**. The catalogs described below have progressively expanded in sample size, ancestral diversity, and annotation depth, transforming variant interpretation from an ad hoc exercise into a quantitative discipline.

A crucial nuance shapes everything that follows: these catalogs record variants that are compatible with being sampled in the first place. Gene-lethal variants that cause embryonic death or severe childhood disease rarely appear, even when they are biologically informative. Variants causing late-onset conditions (Alzheimer's risk alleles, adult-onset cancer predisposition) can persist at appreciable frequencies because selection has not had time to remove them. Throughout this book, models trained on population data can only learn from variants present in these catalogs, which means they systematically underrepresent the most severe loss-of-function mutations.

### dbSNP and Variant Identifiers

Two laboratories sequence the same patient and report their findings to a tumor board. Laboratory A describes a variant using genomic coordinates on GRCh38; Laboratory B uses HGVS nomenclature relative to a specific transcript. Are they discussing the same variant? Without standardized identifiers, this simple question can consume hours of manual reconciliation. The database of Single Nucleotide Polymorphisms (dbSNP) provides the common currency that cuts through this ambiguity: stable identifiers (rsIDs) that enable integration across tools and publications [@sherry_dbsnp_2001].

When a laboratory reports a variant, when a researcher publishes a GWAS finding, and when a clinician queries a pathogenicity database, they need a common language to ensure they are discussing the same genomic position. Modern whole-exome and whole-genome sequencing routinely discovers millions of previously unseen variants per large cohort, but dbSNP identifiers remain the standard way to reference known **single nucleotide polymorphisms (SNPs)** and link disparate resources. When a GWAS publication reports an association at rs12345, that identifier traces back to dbSNP and enables integration with functional annotations, clinical databases, and other catalogs throughout this chapter.

### 1000 Genomes and Early Reference Panels

Genotyping arrays measure only a sparse subset of genomic positions, yet disease-associated variants may lie anywhere in the genome. How can researchers infer variants at unmeasured positions? The answer lies in patterns of co-inheritance: variants that travel together on ancestral chromosome segments can be inferred from neighboring measured positions. This process of imputation depends entirely on having reference panels that capture the haplotype structure of the population being studied.

The 1000 Genomes Project provided one of the first widely used multi-population panels for imputation, sampling individuals from African, European, East Asian, South Asian, and admixed American populations [@auton_1kgp_2015]. The resulting haplotype structure underlies many imputation servers and downstream analyses, enabling genotyping arrays with millions of markers to impute tens of millions of untyped variants through linkage disequilibrium [@yun_accurate_2021]. Although its sample size (approximately 2,500 individuals) is modest by current standards, 1000 Genomes established the template for how to build and distribute multi-population reference panels, and its samples continue to serve as benchmarks for variant calling performance.

### Genome Aggregation Database (gnomAD)

A clinical geneticist evaluates a child with unexplained developmental delay. Exome sequencing reveals a missense variant in a neurodevelopmental gene. Is this the cause? The answer depends critically on whether the variant has been observed in healthy individuals. If it appears in thousands of people without developmental delay, it is unlikely to be pathogenic. If it has never been observed across hundreds of thousands of sequenced genomes, its absence becomes evidence of selective pressure against the variant. This filtering logic requires population-scale variant catalogs with sufficient sample size and ancestral diversity to distinguish genuinely rare variants from those that are simply common in underrepresented populations.

The Genome Aggregation Database (gnomAD) aggregates exome and genome sequencing data from research and clinical cohorts worldwide into harmonized allele frequency resources spanning hundreds of thousands of individuals [@karczewski_gnomad_2020]. gnomAD provides high-resolution allele frequencies stratified by genetic ancestry, enabling population-matched filtering that accounts for variants common in one ancestry but rare in others. This stratification matters because a variant observed at 1% frequency in African populations but absent from European cohorts would be incorrectly flagged as ultra-rare by a model trained predominantly on European data.

gnomAD also introduced **constraint metrics** that have become standard features in variant prioritization. The probability of loss-of-function intolerance (pLI) and loss-of-function observed/expected upper bound fraction (LOEUF) summarize how depleted a gene is for protein-truncating variants relative to expectation. Genes essential for viability show far fewer loss-of-function variants than neutral mutation rates would predict; this depletion provides evidence of selective constraint that transfers to variant interpretation. A novel truncating variant in a highly constrained gene warrants more concern than the same variant class in an unconstrained gene.

These resources are indispensable for filtering common variants in Mendelian disease diagnostics, distinguishing ultra-rare variants from recurrent ones, and providing population genetics priors for deleteriousness scores like CADD [@rentzsch_cadd_2019; @schubach_cadd_2024]. At the same time, they reflect the composition of the cohorts they aggregate: ancestry representation remains uneven despite ongoing efforts, structural variants and repeat expansions are less completely cataloged than SNVs and short indels, and individuals with severe early-onset disease are underrepresented by design. These biases propagate into every model that uses gnomAD frequencies or constraint scores as features.


## Cohorts, Biobanks, and GWAS Summary Data

A pharmaceutical company developing a new cardiac drug needs to understand which genetic variants influence drug response. A health system implementing pharmacogenomic testing needs to know which patients are at risk for adverse reactions. A researcher studying the genetics of depression needs cases and controls with standardized phenotyping. None of these questions can be answered by sequencing alone; they require linking genetic variation to phenotypes at scale, across thousands or hundreds of thousands of individuals. Yet assembling such cohorts introduces its own biases: participants must consent, provide samples, and have phenotypes recorded in standardized ways. The populations enrolled in major biobanks reflect patterns of healthcare access, research infrastructure, and historical priorities that do not represent global genetic diversity.

The overrepresentation of European-ancestry individuals in most major biobanks creates systematic gaps in variant discovery, effect-size estimation, and **polygenic score** portability that propagate through downstream analyses [@sirugo_diversity_2019]. A variant common in West African populations may be absent or rare in European-dominated catalogs, rendering it invisible to association studies and underrepresented in predictive models. This tension between scientific utility and representational equity shapes every biobank-derived resource in this chapter and is discussed in detail in @sec-confounding.

### Large Population Cohorts

A variant that increases heart disease risk by 5% requires approximately 50,000 cases and controls to detect reliably at genome-wide significance. A variant with a 1% effect on a continuous trait like blood pressure demands even larger samples. The fundamental constraint is statistical: small effect sizes, which characterize most common variant associations, disappear into noise without massive sample sizes. This explains why genetic discovery accelerated dramatically when biobanks reached the scale of hundreds of thousands of participants.

UK Biobank, with approximately 500,000 participants and deep phenotyping across thousands of traits, has become a dominant resource for methods development and benchmarking [@bycroft_ukbiobank_2018]. FinnGen leverages Finland's population history and unified healthcare records for large-scale disease association discovery [@kurki_finngen_2023]. The All of Us Research Program prioritizes diversity, aiming to enroll one million participants with deliberate oversampling of historically underrepresented groups [@null_all-of-us_2019]. deCODE genetics has genotyped a substantial fraction of Iceland's population, enabling unique studies of rare variants and founder effects in a population with detailed genealogical records [@gudbjartsson_decode_2015]. Additional resources include the Million Veteran Program, Mexican Biobank, BioBank Japan, China Kadoorie Biobank, and emerging African genomics initiatives such as H3Africa [@sirugo_diversity_2019].

Together, these efforts enable **genome-wide association studies (GWAS)** for thousands of traits, development and evaluation of polygenic scores, and fine-mapping of causal variants and genes [@marees_gwas_2018; @mountjoy_open_2021]. From a modeling perspective, they provide the large-scale genotype-phenotype matrices that power architectures ranging from classical linear mixed models to foundation models trained on biobank-scale data. The practical reality for most GWAS and polygenic score methods in @sec-gwas is data from either array genotyping with imputation or whole-exome/whole-genome sequencing with joint calling, as in DeepVariant/GLnexus-style pipelines [@yun_accurate_2021].

### GWAS Summary Statistics

Individual-level genotype and phenotype data are powerful but sensitive. Sharing such data across institutions requires complex data use agreements, institutional review board approvals, and secure computing infrastructure. These barriers would slow scientific progress if every analysis required access to raw data. Summary statistics offer an alternative: per-variant effect sizes, standard errors, and p-values that capture the essential association signal without revealing individual genotypes.

The GWAS Catalog compiles published results across thousands of traits [@sollis_gwas-catalog_2023], while the PGS Catalog provides curated polygenic score weights and metadata for reproducibility [@lambert_pgs-catalog_2021]. Frameworks like Open Targets Genetics integrate fine-mapped signals with functional annotations to prioritize candidate causal genes at associated loci [@mountjoy_open_2021].

Summary statistics enable meta-analysis across cohorts without sharing individual-level data, transfer of genetic findings to new populations through methods like PRS-CSx, and integration with functional annotations to distinguish causal variants from linked bystanders. For deep learning, summary statistics provide a sparse, trait-level view of the genome that can be combined with richer sequence-based or functional labels, though the sparsity and noise in GWAS signals pose challenges that differ from the dense labels available in functional genomics.


## Functional Genomics and Regulatory Landscapes

Protein-coding exons constitute roughly 1.5% of the human genome, yet most disease-associated variants from GWAS fall outside coding regions. A massive study identifies 100 loci associated with schizophrenia, but 90 of them lie in non-coding regions with no obvious connection to any gene. This mismatch creates a fundamental interpretability problem: we can identify non-coding loci that harbor disease risk, but we cannot easily determine which base pairs matter, which genes they regulate, or in which cell types they act. Understanding these non-coding variants requires mapping the regulatory logic that governs when, where, and how much each gene is expressed. Functional genomics assays provide this map, identifying **transcription factor** binding sites, nucleosome positioning, **chromatin accessibility**, **histone modifications**, and three-dimensional genome organization across cell types and conditions.

For this book, functional genomics datasets serve a dual role. First, they supply the biological vocabulary for interpreting non-coding variants, linking sequence changes to potential regulatory consequences. Second, and more directly, they provide the training labels for sequence-to-function deep learning models. When a model learns to predict chromatin accessibility or histone marks from DNA sequence alone, it compresses into its parameters the regulatory code implicit in thousands of functional genomics experiments.

### ENCODE, Roadmap, and Related Consortia

A single ChIP-seq experiment for one transcription factor in one cell line provides useful signal, but models that learn general regulatory grammar require thousands of such experiments spanning many factors, marks, and cell types. A researcher training a regulatory model on her own laboratory's data will produce a model that works well in her specific experimental context but fails to generalize. The key insight behind ENCODE and Roadmap was that coordinated experimental campaigns, with standardized methods and quality control, could create reference datasets serving the entire field.

The Encyclopedia of DNA Elements (ENCODE) and Roadmap Epigenomics consortia designed coordinated experimental campaigns that profiled transcription factor binding (ChIP-seq), histone modifications, chromatin accessibility (DNase-seq, ATAC-seq), and chromatin conformation (Hi-C) across cell lines and primary tissues [@kagda_encode_2025; @kundaje_roadmap_2015]. Gene Expression Omnibus (GEO) archives these and many other functional genomics datasets with standardized metadata [@edgar_geo_2002].

The significance of these consortia lies less in any individual experiment than in the scale and standardization they provide. By generating hundreds of assays across dozens of cell types with consistent protocols, ENCODE and Roadmap created canonical reference datasets that define the regulatory landscape for the cell types they profiled. Many regulatory deep learning models in @sec-regulatory and @sec-cnn are effectively trained on these resources, learning to predict multi-task label vectors where each task corresponds to a ChIP-seq or accessibility experiment. The consequence is that models trained on ENCODE data inherit ENCODE's choices about which cell types, factors, and experimental conditions merit inclusion.

### Cistrome Data Browser

ENCODE and Roadmap provide authoritative datasets for their chosen cell types and factors, but they represent only a fraction of publicly available functional genomics experiments. A researcher interested in a specific transcription factor or a disease-relevant cell type may find that ENCODE lacks the relevant data, even though dozens of laboratories have generated relevant ChIP-seq experiments. These experiments exist scattered across GEO with heterogeneous processing and quality.

The Cistrome Data Browser addresses this gap by aggregating thousands of human and mouse ChIP-seq and chromatin accessibility datasets from ENCODE, Roadmap, GEO, and individual publications into a uniformly reprocessed repository [@zheng_cistrome_2019]. All datasets pass through standardized quality control and peak calling, enabling comparisons across experiments originally generated with different protocols.

Cistrome provides uniform peak calls, signal tracks, and metadata for cell type, factor, and experimental conditions. The tradeoff is heterogeneity: while reprocessing harmonizes computational steps, the underlying experiments vary in sample preparation, antibody quality, sequencing depth, and experimental design. Cistrome expands coverage at the cost of the tight experimental control found in the primary consortia, a tradeoff that matters when models learn from noisy or inconsistent labels.

### From Assays to Training Labels

Sequence-to-function models transform functional genomics resources into supervised learning problems. Models like DeepSEA (see @sec-regulatory) draw training labels from ENCODE, Roadmap, and Cistrome-style datasets: each genomic window is associated with binary or quantitative signals indicating transcription factor binding, histone modifications, or chromatin accessibility across hundreds of assays and cell types [@zhou_deepsea_2015; @zhou_expecto_2018].

The quality, coverage, and biases of these labels directly constrain what models can learn. Cell types absent from the training compendium cannot be predicted reliably. Factors with few high-quality ChIP-seq experiments will have noisier labels. Systematic differences between assay types (binary peak calls versus quantitative signal tracks) shape whether models learn to predict occupancy, accessibility, or something in between. These considerations become central when examining model architectures and training strategies in @sec-regulatory.

### Deep Mutational Scanning and Multiplexed Variant Assays

Population variant catalogs tell us which variants survive in healthy individuals, but they cannot tell us what happens when a specific amino acid is changed to every possible alternative. Functional genomics experiments reveal where the genome is active, but they do not directly measure the consequence of each possible mutation. **Deep mutational scanning (DMS)** fills this gap by measuring the fitness or functional impact of thousands of protein or regulatory variants in a single experiment.

These assays systematically introduce mutations (often approaching saturation mutagenesis for a protein domain or regulatory element), subject the resulting library to selection or screening, and use sequencing to quantify the representation of each variant before and after selection. The result is dense, quantitative measurements of variant effects under controlled conditions. Benchmarks such as ProteinGym compile large DMS datasets across proteins to evaluate variant effect predictors [@notin_proteingym_2023], while TraitGym curates multiplexed reporter assays and other high-throughput readouts of regulatory variant effects [@benegas_traitgym_2025].

These resources sit at the interface between genomic and protein-level modeling. They provide dense, quantitative labels for synthetic or near-saturated variant libraries, complementing the sparse, naturally occurring variation in gnomAD and biobanks. DMS data differ fundamentally from population catalogs: they measure functional impact directly under controlled conditions rather than inferring it from population survival. Later chapters on protein sequence models and regulatory variant prediction return to these DMS-style datasets as key benchmarks and training sources.


## Expression and eQTL Resources

Functional genomics assays reveal where transcription factors bind and which chromatin regions are accessible, but they do not directly answer the downstream question: does regulatory activity actually change how much RNA a gene produces? A transcription factor may bind a genomic region without altering expression of nearby genes; an accessible chromatin region may not contain active regulatory elements. Regulatory binding and gene expression exist in a many-to-many relationship that cannot be resolved by either measurement alone. Expression datasets complete this link, measuring transcript abundance across tissues, cell types, and genetic backgrounds.

**Expression quantitative trait loci (eQTLs)** formalize the genotype-expression relationship statistically, identifying genetic variants associated with changes in transcript levels. For variant interpretation, eQTLs offer mechanistic hypotheses connecting non-coding variants to specific genes and tissues: if a GWAS signal colocalizes with an eQTL for a nearby gene in a disease-relevant tissue, that gene becomes a candidate effector. For model training, expression data provide quantitative labels that integrate across many regulatory inputs converging on a single promoter.

### Bulk Expression Atlases

A GWAS identifies a locus associated with coronary artery disease in a non-coding region. Dozens of genes lie within the associated interval. Which one mediates the disease risk? If the lead variant also associates with expression of a nearby gene specifically in arterial endothelial cells, that gene becomes the prime candidate. Without tissue-specific expression data linked to genotypes, this inference is impossible.

The Genotype-Tissue Expression (GTEx) consortium provides the most comprehensive resource linking genetic variation to gene expression across human tissues, with RNA-seq profiles from 948 post-mortem donors across 54 tissues [@gtex_2020]. GTEx established foundational insights that inform models throughout this book: most genes harbor tissue-specific eQTLs, regulatory variants typically act in cis over distances of hundreds of kilobases, and expression variation explains a meaningful fraction of complex trait heritability.

GTEx underlies expression prediction models such as PrediXcan, which trains tissue-specific models to impute gene expression from genotypes alone [@gamazon_predixcan_2015]. Transcriptome-wide association studies (TWAS) extend this idea to associate imputed expression with phenotypes [@gusev_twas_2016]. Colocalization methods ask whether a GWAS signal and an eQTL share the same causal variant, providing evidence that the associated gene mediates the trait effect.

The GTEx design has limitations worth acknowledging. Post-mortem collection introduces agonal stress artifacts that may not reflect living tissue biology. Sample sizes vary considerably across tissues (hundreds for some, dozens for others), affecting statistical power. Some disease-relevant tissues, such as pancreatic islets or specific brain subregions, remain undersampled. Complementary resources like the eQTLGen Consortium aggregate eQTL results from blood across much larger sample sizes, trading tissue diversity for statistical power [@vosa_eqtl-gen_2021].

### Single-Cell and Context-Specific Expression

Bulk RNA-seq averages expression across all cells in a tissue sample, obscuring the cell-type-specific programs that often mediate disease biology. A bulk eQTL in brain tissue might reflect astrocytes, neurons, microglia, or oligodendrocytes; the causal cell type matters for understanding mechanism. This averaging creates a fundamental resolution problem: variants may have strong effects in rare cell populations that are diluted to undetectability when mixed with other cell types.

Single-cell RNA-seq resolves this heterogeneity, identifying expression signatures for individual cell types, rare populations, and transitional states. Large-scale efforts including the Human Cell Atlas and Tabula Sapiens are building reference atlases that catalog cell types across organs and developmental stages [@regev_cell-atlas_2017; @tabula_sapiens_2022]. For variant interpretation, single-cell data enable cell-type-specific eQTL mapping, revealing that a variant may influence expression in one cell type but not others within the same tissue. Spatial transcriptomics adds anatomical context, preserving tissue architecture while measuring gene expression.

These technologies introduce computational challenges: sparsity from dropout effects, batch variation across samples and technologies, and massive scale with millions of cells per study. They also offer an increasingly fine-grained view of the link between genotype, regulatory state, and cellular phenotype. Single-cell and spatial resources appear primarily in later chapters on multi-omics integration and systems-level models.


## Variant Interpretation Databases and Clinical Labels

A family receives whole-exome sequencing results for their child with developmental delay. The laboratory report lists 50 rare variants in genes associated with neurodevelopmental disorders. For each variant, the clinical team must answer: is this the cause? Allele frequencies tell us what variants survive in healthy populations, and functional genomics data reveal where the genome is biochemically active, but neither directly answers this question. That determination requires integrating multiple lines of evidence (family segregation, functional assays, computational predictions, phenotypic observations) into a structured framework that can be applied consistently.

Clinical variant interpretation databases aggregate these assessments from laboratories, expert panels, and research groups. These databases have become critical infrastructure for both clinical genomics and computational method development, providing labels that inform diagnostic decisions and serve as training data for machine learning models. Their labels carry biases and circularity that propagate through any analysis built on them, yet no viable alternative exists for large-scale model training and evaluation.

### ClinVar and Clinical Assertions

A clinical laboratory sequences a patient with suspected hereditary cancer syndrome and identifies a missense variant in *BRCA2*. Before returning results, the laboratory searches ClinVar and finds that three other laboratories have evaluated this variant: two classified it as likely pathogenic, one as a variant of uncertain significance. How should this conflicting evidence inform the final report? ClinVar aggregates assertions of variant pathogenicity from clinical laboratories and researchers worldwide, making it the central clearinghouse for clinical variant interpretations [@landrum_clinvar_2018].

ClinVar provides standardized classifications following ACMG/AMP guidelines (pathogenic, likely pathogenic, benign, likely benign, **variant of uncertain significance**) that are central to diagnostic pipelines and to benchmarking variant effect predictors. It has become the de facto reference for variant pathogenicity labels, but its contents reflect systematic biases that affect any downstream use. These biases operate at multiple levels and warrant careful consideration.

Submission heterogeneity poses a fundamental challenge. Annotations come from diverse submitters, including diagnostic laboratories, research groups, expert panels, and database exports. Submitters apply varying evidentiary standards; some provide detailed supporting evidence while others offer only assertions. Conflicting interpretations are common, particularly for variants of uncertain significance.

Version sensitivity means that classifications evolve as evidence accumulates. A variant classified as VUS in 2018 may be reclassified as likely pathogenic by 2023 based on new functional studies or additional patient observations. Models trained on historical ClinVar snapshots may learn outdated classifications. When reporting performance, specifying the ClinVar version used is essential for reproducibility.

Ancestry and gene coverage biases create uneven representation. Variants in well-studied populations (particularly European ancestry) and well-characterized disease genes are heavily overrepresented. Variants from underrepresented populations are more likely to remain classified as VUS due to insufficient evidence. This creates feedback loops: predictive models perform better on European-ancestry variants because training data is richer, reinforcing the disparity [@landrum_clinvar_2018].

Circularity with computational predictors represents a subtle but important concern. Clinical submissions increasingly incorporate computational scores like CADD, REVEL, and AlphaMissense as supporting evidence for pathogenicity classification. When these same ClinVar labels are then used to train or evaluate computational predictors, circularity emerges [@schubach_cadd_2024]. If a laboratory used a high CADD score as supporting evidence for classifying a variant as likely pathogenic, and that variant later appears as a positive label in ClinVar, models trained on ClinVar may partly learn to reproduce CADD itself rather than discovering independent signal. This circularity operates at two levels: evaluation circularity (when models are assessed on benchmarks influenced by the model's own predictions) and training circularity (when features used in training derive from the same underlying information as the labels). Both forms inflate apparent performance without demonstrating genuine predictive power. We return to these issues in @sec-vep-classical and @sec-confounding.

Variants of uncertain significance constitute the majority of rare variant classifications, reflecting genuinely limited evidence. These variants are both targets for predictive modeling (can computational methods resolve uncertainty?) and potential pitfalls (models trained only on confidently classified variants may not generalize to VUS with different characteristics).

Despite these limitations, ClinVar remains invaluable. The key is using it appropriately: recognizing biases when training models, accounting for version differences when comparing studies, stratifying performance by ancestry and gene coverage, and treating computational predictions as one line of evidence rather than definitive classifications.

### Complementary Clinical Databases

ClinVar's open-access model and broad submission base make it the most widely used resource, but it is not the only source of clinical variant interpretations. The Human Gene Mutation Database (HGMD) maintains a curated collection of disease-causing mutations compiled from the published literature, with particular depth in rare Mendelian disorders [@stenson_hgmd_2017]. HGMD's professional version includes variants not yet publicly released, and its curation emphasizes literature-reported pathogenic variants rather than the full spectrum of classifications in ClinVar. The Leiden Open Variation Database (LOVD) takes a gene-centric approach, with individual databases maintained by gene experts who curate variants according to locus-specific knowledge [@fokkema_lovd_2011]. LOVD instances often capture variants and functional evidence specific to particular disease communities that may not appear in broader databases.

These resources complement ClinVar in important ways: HGMD provides literature-derived pathogenic variants that may precede ClinVar submissions, while LOVD captures expert knowledge from disease-specific research communities. For model development and benchmarking, awareness of these alternative sources matters because training exclusively on ClinVar may miss variants documented elsewhere, and apparent novel predictions may simply reflect incomplete training data rather than genuine generalization.

### ClinGen and Expert Curation

Clinical laboratories submitting to ClinVar vary enormously in expertise and evidentiary standards. A submission from a general diagnostic laboratory applying ACMG guidelines to an unfamiliar gene may differ substantially from an assessment by researchers who have studied that gene for decades. The Clinical Genome Resource (ClinGen) addresses this heterogeneity by providing expert-curated assessments at multiple levels [@rehm_clingen_2015].

ClinGen expert panels evaluate **gene-disease validity** (whether variation in a gene can cause a specific disease) and **dosage sensitivity** (whether haploinsufficiency or triplosensitivity leads to clinical phenotypes). These evaluations build on the catalog of Mendelian phenotypes maintained by OMIM, which provides curated gene-disease associations and clinical synopses [@amberger_omim_2015].

For individual variants, ClinGen Variant Curation Expert Panels apply ACMG/AMP criteria systematically, assigning levels of evidence for pathogenicity or benignity. The FDA has recognized these curations as valid scientific evidence for clinical validity [@pejaver_calibration_2022]. ClinGen also develops calibrated thresholds for computational predictors, specifying score intervals that justify different strengths of evidence (supporting, moderate, strong) for pathogenicity or benignity. These calibrations directly inform how computational scores should be incorporated into variant classification workflows.

### Pharmacogenomics Resources

Most variant interpretation focuses on rare mutations that cause or predispose to disease. **Pharmacogenomics** presents a different paradigm: common polymorphisms that individually may have no disease consequences but profoundly influence how individuals respond to medications. These variants matter not because they cause disease but because they determine whether a drug will work, fail, or cause harm.

ClinPGx integrates the PharmGKB knowledge base, CPIC clinical guidelines, and PharmCAT annotation tool into a unified pharmacogenomics resource [@whirl-PharmGKB_2012]. The *CYP2D6* gene, for example, encodes a cytochrome P450 enzyme responsible for metabolizing approximately 25% of clinically used drugs, including codeine, tamoxifen, and many antidepressants [@whirl-PharmGKB_2012]. Patients with loss-of-function *CYP2D6* variants cannot activate codeine to morphine, rendering the drug ineffective; patients with gene duplications may experience dangerous opioid toxicity from standard doses.

Star-allele haplotypes (combinations of variants that travel together on a chromosome) determine metabolizer status, requiring phasing and structural variant detection that extend beyond simple SNV calling. The CPIC guidelines provide evidence-based recommendations for adjusting drug selection or dosing based on pharmacogene diplotypes, and FDA drug labels document regulatory recognition of these associations. From a modeling perspective, pharmacogenomic resources offer a complementary type of label linking variants to molecular and clinical outcomes through different mechanisms than Mendelian disease pathogenicity.


## Connecting Data Resources to Model Development

The genomic deep learning models throughout this book inherit both the strengths and limitations of the data they are trained on. A critical distinction emerges between data used to train models, data used to evaluate them, and the distribution encountered in clinical deployment. These three contexts often differ systematically: training data may emphasize well-characterized genes and European-ancestry variants, evaluation benchmarks may inadvertently overlap with training sets, and deployed models encounter patient populations underrepresented in both. Recognizing these distributional shifts is essential for anticipating where models will succeed and where they will fail.

@sec-gwas draws on GWAS summary statistics and biobank-scale cohorts to construct polygenic scores, inheriting the ancestry biases embedded in cohort composition. @sec-vep-classical examines how annotation-based methods compress population frequencies, conservation, and functional signals into genome-wide deleteriousness scores, propagating the biases of each input. Chapters [-@sec-regulatory] and [-@sec-cnn] use ENCODE, Roadmap, and Cistrome-style functional data as training labels for sequence-to-function models, learning representations limited by the cell types and assays in the training compendium. Chapters [-@sec-dna-lm] and [-@sec-protein-lm] examine how reference genomes and protein sequence databases become training corpora for DNA and protein language models, determining which sequences and evolutionary relationships those models can represent. Chapters [-@sec-fm-principles] through [-@sec-multi-omics] revisit these resources as inputs, labels, and priors for genomic foundation models. @sec-benchmarks evaluates how benchmarks constructed from these resources shape our understanding of model performance, including the subtle ways that benchmark construction can inflate or deflate apparent capabilities.

By surveying the data landscape in one place, this chapter establishes a common reference that later chapters build on rather than re-introducing each resource from scratch. The recurring theme is that biases, gaps, and circularity in these foundational datasets propagate through every model trained on them. A variant effect predictor trained on ClinVar labels inherits the ascertainment biases of clinical sequencing. A chromatin model trained on ENCODE cell lines may not generalize to primary tissues. A constraint model trained on human populations systematically misses gene-lethal variants that never appear in any catalog. Understanding these foundations is essential for interpreting what models learn and anticipating where they will fail.