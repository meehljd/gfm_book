# RNA Structure and Function {#sec-rna}

A synonymous mutation changes the DNA codon but preserves the amino acid. By the logic of protein-centric biology, such mutations should be functionally neutral: same protein sequence, same structure, same function. Yet synonymous variants can dramatically alter gene expression, affect protein folding, and cause disease. The mechanisms operate at the RNA level: altered codon optimality changes translation speed, modified mRNA secondary structure affects ribosome processivity, disrupted regulatory motifs change transcript stability. A model that sees only DNA sequence or only protein sequence misses these effects entirely. Protein language models (@sec-protein-lm) learn evolutionary constraints on amino acid substitutions; DNA foundation models (@sec-dna-lm) learn regulatory grammar in noncoding regions. Neither captures the RNA-specific constraints that determine transcript fate.

RNA occupies a distinct position in the central dogma, essential to every step from transcription to translation, yet historically receiving less computational attention than its neighbors. The disparity reflects data availability more than biological importance. Protein sequences accumulate over billions of years of evolution, providing the massive corpora that enabled ESM to learn structure from sequence. DNA benefits from reference genomes, population sequencing, and functional genomics consortia generating petabytes of data. RNA databases remain comparatively sparse, structural annotations cover only well-characterized families, and no equivalent of AlphaFold's crystallographic training set exists for RNA tertiary structure. The result is a modeling landscape where RNA foundation models exist but remain immature relative to protein and DNA counterparts.

This chapter examines RNA-specific modeling as a bridge between the sequence-level foundation models of Part 3 and the cellular and systems contexts that follow. The foundation models examined previously all manifest their predictions through RNA intermediates: Enformer predicts RNA-seq coverage, protein models predict translation products, SpliceAI models spliceosome recognition of RNA. RNA-specific models add a distinct layer, treating RNA not merely as a readout of DNA or a precursor to protein, but as a structured molecule with its own sequence constraints, folding landscapes, and functional roles. We examine secondary structure prediction, RNA foundation models, codon-level mRNA models, and noncoding RNA classification, while confronting the data limitations that constrain current approaches.


## RNA as Molecule Versus Transcriptome Readout

Two complementary perspectives frame computational approaches to RNA. The molecular view treats RNA as a physical object with primary sequence, secondary structure through base pairing, tertiary organization in three-dimensional space, and chemical modifications that alter its properties. In this view, modeling goals include predicting which bases pair with which, how the molecule folds, which proteins bind to it, and how synthetic RNAs might be designed with desired properties. The transcriptomic view treats RNA as a cellular readout: coverage profiles along the genome, splice junction usage, isoform abundances, expression levels that vary across cell types and conditions. Here the goal is explaining how genomic sequence and chromatin state give rise to these measurements.

Models that predict transcriptomic signals from DNA sequence (Enformer, Borzoi, and related architectures covered in @sec-regulatory) operate in the second paradigm. They take genomic sequence as input and output RNA-seq or CAGE profiles as predictions. These models never see RNA sequence directly; they learn the mapping from DNA context to transcriptional output. This chapter focuses instead on the molecular perspective: models whose input is RNA sequence itself and whose outputs concern RNA structure, function, or design.

The distinction parallels the difference between protein language models and proteomics prediction models. ESM takes amino acid sequences and learns structural representations (@sec-protein-lm). A model predicting protein abundance from genomic features would be solving a different problem. Both perspectives are valuable, and both ultimately concern RNA, but they operate at different levels of the biological hierarchy and require different architectures and training strategies.


## Why Secondary Structure Creates a Distinct Modeling Challenge

### The Flat Energy Landscape Problem

RNA's defining computational challenge emerges from thermodynamics. Proteins fold into stable three-dimensional structures because their energy landscapes contain deep minima: the native state sits in a pronounced funnel that guides the folding process. RNA energy landscapes are remarkably flatter. Multiple conformations compete for occupancy, with free energy differences often smaller than thermal fluctuations at cellular temperatures. A given RNA sequence may adopt several alternative structures with similar stabilities, and the dominant conformation can shift in response to ion concentrations, temperature, protein binding, or chemical modifications.

This conformational plasticity has biological functions (riboswitches that change structure in response to ligand binding, RNA thermometers that regulate translation at different temperatures) but creates modeling difficulties. Minimum free energy predictions, which identify the single lowest-energy structure, may miss functionally relevant alternative conformations. Partition function calculations that consider the full ensemble are more complete but computationally expensive and harder to interpret. Deep learning models that predict structure from sequence must somehow capture this many-to-many relationship between sequence and conformation, a challenge that protein structure prediction largely avoided because the sequence-to-structure mapping for most proteins is effectively one-to-one.

### Base Pairing and Long-Range Dependencies

Secondary structure arises from Watson-Crick base pairing (A-U, G-C) and wobble pairs (G-U) that create stems, loops, bulges, and internal loops. Unlike protein secondary structure, where alpha helices and beta sheets are local motifs determined by nearby residues, RNA secondary structure involves long-range contacts. A base at position $i$ may pair with a base at position $j$ hundreds of nucleotides away. The intervening sequence must accommodate this pairing without introducing steric clashes or thermodynamically unfavorable arrangements.

This long-range dependency structure differs fundamentally from protein contact prediction, where most important contacts occur between residues close in primary sequence. RNA structure prediction must consider all possible pairings across the entire sequence, evaluate their compatibility, and identify the globally optimal (or near-optimal) arrangement. The number of possible secondary structures grows exponentially with sequence length, making exhaustive enumeration intractable for long RNAs.

### Pseudoknots and Tertiary Complexity

Pseudoknots occur when bases in a loop pair with bases outside that loop, creating interleaved base-pairing patterns that violate the nested structure assumed by standard secondary structure algorithms. A typical pseudoknot involves two stem regions whose base pairs cross each other when drawn in standard notation. These structures are functionally important (the telomerase RNA catalytic core contains a pseudoknot essential for activity) but algorithmically challenging. Standard dynamic programming approaches for secondary structure prediction exclude pseudoknots because their inclusion increases computational complexity from $O(n^3)$ to $O(n^6)$ or worse.

Tertiary structure involves the three-dimensional arrangement of secondary structure elements in space, including long-range interactions mediated by non-Watson-Crick base pairs, metal ion coordination, and RNA-RNA kissing loops. Predicting RNA tertiary structure remains far less developed than protein tertiary structure prediction. No RNA equivalent of AlphaFold exists, and the training data situation is dire: the Protein Data Bank contains over 200,000 protein structures but fewer than 2,000 RNA structures, many of which are ribosomal RNA fragments or tRNA variants from the same structural families.


## Classical Approaches to Structure Prediction

### Thermodynamic Folding Models

The dominant classical paradigm for RNA secondary structure prediction relies on nearest-neighbor thermodynamic models. These approaches assign free energy contributions to each base pair and structural element (loops, bulges, internal loops, multiloops) based on experimentally calibrated parameters. Given these parameters, dynamic programming algorithms identify the minimum free energy structure or compute the partition function over all possible structures.

Mfold and the ViennaRNA package represent the most widely used implementations. They achieve reasonable accuracy for short, well-behaved RNAs where the thermodynamic parameters are most reliable. Limitations emerge for longer RNAs where the flat energy landscape means many structures have similar energies, for RNAs in complex cellular environments where proteins and other factors alter folding, and for RNAs with modifications or non-canonical interactions not captured by standard parameter sets. These methods also assume equilibrium conditions that may not hold for co-transcriptional folding or kinetically trapped states.

### Comparative and Covariation Methods

For RNAs with sufficient homologous sequences, comparative approaches provide an orthogonal route to structure inference. If two positions exhibit compensatory mutations (G-C changing to A-U while maintaining complementarity), those positions likely base-pair. Databases like Rfam curate consensus secondary structures for RNA families based on these covariation signals.

Comparative methods are powerful but require multiple sequence alignments of homologous RNAs. Novel RNAs, rapidly evolving regulatory elements, or species-specific transcripts may lack sufficient homologs for reliable inference. The approach also assumes that structure is conserved across the aligned sequences, which breaks down for RNAs that have diverged in function or that adopt condition-specific alternative structures.


## Deep Learning for Secondary Structure Prediction

### From Thermodynamics to Learned Patterns

Deep learning models for RNA structure prediction frame the task as sequence-to-structure mapping, analogous to protein contact prediction. Given an RNA sequence, the model predicts base-pairing probabilities for all position pairs, contact maps indicating which bases interact, or per-nucleotide structural states (paired, unpaired, in loop, in stem).

Models like SPOT-RNA use convolutional or attention-based architectures to capture long-range dependencies in sequence. Some approaches directly predict pairing matrices as dense outputs; others output per-position classifications that are post-processed into structures. Training typically uses experimentally determined structures from databases like RNAstralign or bpRNA, supplemented by computationally predicted structures from thermodynamic models.

Performance on benchmark datasets often exceeds classical thermodynamic methods, particularly for RNAs with complex structures or pseudoknots where dynamic programming approaches struggle. The learned models can capture patterns beyond nearest-neighbor rules, potentially encoding longer-range sequence dependencies that contribute to folding but were not parameterized in classical approaches.

### Structure Probing as Supervision

High-throughput structure probing experiments provide an alternative source of supervision. SHAPE (selective 2'-hydroxyl acylation analyzed by primer extension), DMS-seq, and icSHAPE measure nucleotide accessibility or flexibility across entire transcriptomes. Positions that are base-paired or buried in tertiary structure show lower reactivity than exposed positions.

These data offer several advantages for model training. They cover far more RNAs than crystal structures, extending beyond well-characterized families to regulatory elements and novel transcripts. They capture structure in cellular context, reflecting the influence of proteins, modifications, and physiological conditions. And they provide soft constraints rather than binary pairing assignments, potentially better matching the conformational heterogeneity of real RNA populations.

Models trained on structure probing data learn to predict accessibility profiles from sequence. These predictions can be integrated with thermodynamic models (using predicted accessibility as constraints) or used directly for downstream tasks like predicting RNA-protein binding or designing stable constructs.


## RNA Foundation Models

### The Scale Gap with Protein Language Models

RNA foundation models attempt to replicate the protein language model paradigm: train large transformers on massive sequence corpora using self-supervised objectives, then transfer learned representations to downstream tasks. The approach has produced working models, but the results lag substantially behind protein LMs in both scale and demonstrated capabilities.

The comparison with ESM illustrates the gap. ESM-2 trained on over 65 million protein sequences from UniRef, spanning the known diversity of protein families. RNA-FM, one of the more successful RNA foundation models, trained on approximately 23 million noncoding RNA sequences from RNAcentral (@chen_rna-fm_2022). While not a trivial corpus, this represents an order of magnitude fewer sequences, and the RNA sequences span a narrower range of structural and functional diversity than proteins. The consequences appear in downstream performance: RNA-FM improves over baselines on secondary structure prediction and family classification, but shows nothing like the emergent structure prediction that made ESM-2's attention patterns predict contact maps without supervision.

Several factors explain the disparity. Protein sequences have accumulated over 4 billion years of evolution across all domains of life, with each functional protein family represented by thousands of homologs. RNA databases are biased toward well-characterized structural families (tRNAs, rRNAs, ribozymes) with sparser coverage of regulatory ncRNAs and lineage-specific transcripts. The epitranscriptomic modifications that alter RNA function are invisible in sequence databases, unlike protein post-translational modifications that at least occur at predictable sequence motifs.

### Architectures and Objectives

Most RNA foundation models follow the masked language modeling (MLM) paradigm established by BERT. RNA-FM uses a transformer encoder with nucleotide-level tokenization, predicting masked bases from surrounding context. The learned embeddings show some correspondence to secondary structure when probed with downstream tasks, though the correspondence is weaker than the structure-function relationship learned by protein LMs.

Alternative architectures explore different design choices. Some models incorporate explicit structure tokens or operate on sequence-structure graphs, learning joint representations over both modalities. Others use codon-level tokenization for coding RNAs (discussed in the next section) or explore state-space models and other efficient attention variants to handle longer sequences. RNAErnie and related models experiment with multi-task objectives that combine MLM with auxiliary predictions for structural features or family classification.

The field remains in active development, with no clear consensus on optimal architecture, tokenization, or training strategy. Unlike protein modeling, where ESM established a dominant paradigm that subsequent work has refined, RNA modeling still explores fundamental design choices.

### Downstream Applications

RNA foundation model embeddings support various downstream tasks. Secondary structure prediction fine-tunes the model to output pairing probabilities or SHAPE reactivity profiles. RNA-protein binding prediction uses CLIP-seq data to predict interactions with RNA-binding proteins. Family classification assigns sequences to Rfam families or functional categories (tRNA, rRNA, miRNA, lncRNA). Expression and stability tasks predict transcript half-life or steady-state levels from UTR sequences.

Performance varies substantially across tasks. For structurally constrained RNAs like tRNAs and rRNAs, where sequence motifs strongly determine structure and function, foundation model embeddings provide useful features. For regulatory lncRNAs that often lack stable secondary structures and conserved motifs, improvement over baseline methods is more modest. The diversity of RNA types and tasks complicates benchmarking, and models that excel on one task may struggle on others.


## Codon-Level Models for Coding RNA

### Beyond Nucleotide Tokenization

Coding sequences occupy a special niche where protein and nucleic acid constraints intersect. The genetic code assigns 61 sense codons to 20 amino acids, creating synonymous redundancy where multiple codons encode the same amino acid. This redundancy is not functionally neutral: synonymous codons differ in tRNA availability, translation speed, co-translational folding effects, and mRNA stability. Protein language models, which operate on amino acid sequences, cannot capture these codon-level signals.

Codon-level foundation models address this gap by tokenizing mRNA into codons rather than nucleotides. Models like cdsFM, EnCodon, and DeCodon treat each three-nucleotide codon as a single token, training on masked codon prediction and related objectives (@naghipourfar_cdsfm_2024). This tokenization encodes a biological prior: codons are the fundamental units of translation, and mutations at the codon level determine amino acid changes while mutations within synonymous codons affect expression without changing protein sequence.

The codon vocabulary contains 61 tokens (excluding stop codons) plus special tokens for noncoding regions and boundaries. This intermediate vocabulary size (between character-level nucleotide tokenization and typical BPE vocabularies of thousands of tokens) balances resolution with context length. A 300-amino-acid protein corresponds to 900 nucleotides or 300 codons, making whole-gene modeling tractable within standard transformer context windows.

### What Codon Models Add

Compared to protein language models, codon-level models enable direct modeling of mRNA design problems where amino acid sequence is fixed but codon choice is variable. They capture codon usage bias and its relationship to expression, model translation elongation dynamics that affect co-translational folding, and distinguish synonymous variants that are neutral at the protein level but affect mRNA properties.

Life-Code extends this approach into a central-dogma-wide framework, linking DNA, RNA, and protein representations through shared or aligned embedding spaces (@liu_life-code_2025). CodonBERT specifically targets mRNA design for vaccines and therapeutics, training on over 10 million mRNA sequences to learn representations that predict expression, stability, and immunogenicity (@li_codonbert_2023).

However, codon models typically ignore mRNA secondary structure and modifications. Local structure affects ribosome access and translation rate; modifications like m6A influence stability and localization. Combining codon-aware tokenization with structure-aware representations remains an open direction, less mature than the parallel integration of sequence and structure in protein modeling.

## UTR Models and Translation Regulation
### Why UTRs Dominate Expression Control
The protein output of an mRNA depends as much on its untranslated regions as on its coding sequence. A transcript's 5' UTR determines whether ribosomes find and engage the start codon; its 3' UTR controls how long the message survives and where in the cell it localizes. Two mRNAs encoding identical proteins can differ by orders of magnitude in expression if their UTRs differ. This regulatory leverage makes UTR modeling essential for both understanding endogenous gene regulation and designing synthetic mRNAs for therapeutic applications.
The 5' UTR spans from the transcription start site to the start codon, typically 50 to 200 nucleotides in human mRNAs. Within this region, secondary structure can occlude the start codon and impede ribosome scanning, upstream open reading frames (uORFs) can capture ribosomes before they reach the main coding sequence, and internal ribosome entry sites (IRES) can enable cap-independent translation under stress conditions. The Kozak consensus sequence surrounding the start codon influences initiation efficiency, but context extending dozens of nucleotides in either direction modulates this effect. Predicting translation efficiency from 5' UTR sequence requires integrating these overlapping signals.
The 3' UTR extends from the stop codon to the poly-A tail, ranging from under 100 nucleotides to over 10 kilobases. This region harbors binding sites for RNA-binding proteins and microRNAs that collectively determine mRNA half-life, localization, and translational status. AU-rich elements (AREs) recruit decay machinery in response to cellular signals. Pumilio and other RNA-binding proteins recognize specific motifs to repress or activate translation. The density and arrangement of miRNA binding sites create combinatorial regulatory logic that varies across cell types depending on which miRNAs are expressed.

### Sequence-to-Expression Models

High-throughput reporter assays have enabled systematic modeling of UTR function. Massively parallel reporter assays (MPRAs) measure expression driven by thousands of UTR variants in a single experiment, providing training data at scales previously unavailable. Sample et al. used such data to train Optimus 5-Prime, a convolutional model that predicts ribosome load from 5' UTR sequence with accuracy sufficient to guide synthetic UTR design [@sample_human_2019]. The model learned interpretable features corresponding to known regulatory elements (uORF presence, Kozak strength, secondary structure) while also capturing context-dependent interactions invisible to element-counting approaches.

For 3' UTRs, models must contend with greater length and combinatorial complexity. A 2-kilobase 3' UTR may contain dozens of potential regulatory sites whose effects depend on spacing, secondary structure context, and the expression levels of cognate binding proteins. Approaches range from motif-based models that score individual elements and sum contributions, to deep learning architectures that process entire UTR sequences and learn nonlinear interactions. Agarwal and Kelley trained models on endogenous mRNA stability measurements, demonstrating that 3' UTR sequence features explain substantial variance in half-life across the transcriptome [@agarwal_predicting_2022].

Transfer learning from RNA foundation models offers a complementary approach. Rather than training UTR-specific models from scratch, pretrained representations from RNA-FM or similar models can be fine-tuned on expression prediction tasks. The pretrained embeddings encode sequence context and potential structural features that may transfer to UTR function prediction, though systematic comparisons between foundation model transfer and task-specific training remain limited.

### Integration with mRNA Design

UTR optimization represents a distinct component of therapeutic mRNA design, complementing the codon optimization discussed in the previous section. For a vaccine or protein replacement therapy, the coding sequence determines what protein is made while the UTRs determine how much protein is made and for how long. Current mRNA therapeutics typically use UTRs borrowed from highly expressed endogenous genes (human alpha-globin and beta-globin UTRs are common choices) rather than computationally optimized sequences.

Model-guided UTR design could improve on this approach by optimizing for specific objectives: maximizing expression in target tissues, extending mRNA half-life to reduce dosing frequency, or minimizing immunogenicity by avoiding sequences that trigger innate immune sensors. The challenge lies in the combinatorial interaction between UTRs and coding sequence. Secondary structures can span the UTR-CDS boundary, and the optimal 5' UTR for one coding sequence may perform poorly for another. Integrated models that jointly optimize UTRs and coding sequence represent an active research direction, though experimental validation of computationally designed UTRs remains limited compared to the extensive optimization of coding sequences.

## mRNA Design and Optimization

### Design Objectives and Trade-offs

mRNA sequence design selects nucleotide sequences that encode a desired protein while optimizing expression, stability, safety, and manufacturability. For a 300-amino-acid protein, there are approximately $3^{300}$ possible synonymous mRNA sequences (roughly the number of synonymous codons raised to the protein length). This astronomical space cannot be exhaustively searched, motivating both classical heuristics and modern machine learning approaches.

Key objectives include high protein expression in target tissues, mRNA stability during manufacturing and in vivo, controlled translation kinetics that influence co-translational folding, and low immunogenicity for therapeutic applications. These objectives often conflict: increasing GC content may improve stability but introduce unwanted secondary structure, while avoiding rare codons may reduce expression if tRNA pools are limiting.

### Lessons from COVID-19 Vaccines

The COVID-19 mRNA vaccines provided a high-profile demonstration of mRNA design principles at unprecedented scale. The Pfizer-BioNTech and Moderna vaccines incorporated several design elements: N1-methylpseudouridine modification throughout the sequence to reduce innate immune activation, codon optimization to enhance expression in human cells, optimized 5' and 3' UTRs from highly expressed genes, and sequence modifications to stabilize the prefusion spike conformation. These choices drew on decades of basic research but were refined through empirical optimization rather than systematic model-based design.

The vaccines' success demonstrated that rationally designed mRNAs can achieve therapeutic efficacy at scale. It also revealed limitations in current understanding: the optimal combination of modifications, codons, and UTRs for a given protein target remains partly empirical, and transferring designs across proteins or therapeutic applications requires substantial optimization.

### Model-Based Design Strategies

RNA and codon foundation models enable several approaches to systematic design. Scoring and screening use pretrained models to evaluate large candidate sets for predicted expression or stability, selecting top designs for experimental validation. When models are differentiable with respect to input embeddings, gradient-based methods can guide sequence optimization toward desired objectives. Generative approaches sample diverse high-scoring sequences subject to constraints like fixed amino acid sequence or avoided motifs.

Empirical results suggest that deep models trained on high-throughput reporter assays or ribosome profiling can outperform classical codon adaptation indices like CAI or tAI, particularly for context-specific expression prediction. Classical indices rely on genome-wide codon frequencies that may not reflect the relevant cellular context, while deep models can learn local effects of codon pairs, mRNA structure, and regulatory elements. However, these models require substantial training data and may not generalize across organisms or synthetic constructs far from natural sequences.


## Noncoding RNA Classification and Function

### The Diversity of Noncoding RNA

RNA that does not encode protein spans an enormous functional and structural range. Housekeeping RNAs (tRNAs, rRNAs, snRNAs, snoRNAs) perform essential cellular functions with well-characterized structures. Regulatory RNAs (miRNAs, siRNAs, piRNAs, lncRNAs) control gene expression through diverse mechanisms. Structural and catalytic RNAs (ribozymes, riboswitches) adopt complex folds that enable enzymatic activity or ligand sensing. Circular RNAs (circRNAs) and other noncanonical species continue to expand the catalog of RNA diversity.

Each class has characteristic lengths, structural motifs, genomic contexts, and functional mechanisms. tRNAs are approximately 76 nucleotides with a conserved cloverleaf structure. miRNAs are approximately 22 nucleotides processed from longer hairpin precursors. lncRNAs span thousands of nucleotides with poorly conserved sequence and often no stable secondary structure. Unifying these classes under a single modeling framework is challenging, and models that excel on one class may fail on others.

### From Handcrafted Features to Learned Representations

Classical ncRNA classification relied on engineered features: k-mer frequencies, GC content, minimum free energy of predicted secondary structure, structural motif counts, and genomic context features like proximity to coding genes or chromatin marks. These features fed conventional classifiers (SVMs, random forests, shallow neural networks) that achieved reasonable performance for well-studied classes with strong sequence and structure signatures.

The limits of handcrafted features emerge most clearly for lncRNAs. These transcripts are defined partly by what they lack (no long open reading frame) rather than what they possess. Many lncRNAs show poor conservation, lack stable secondary structures, and have diverse, poorly characterized functions. Distinguishing functional lncRNAs from transcriptional noise remains difficult, and classical feature sets often collapse to generic statistics like length and GC content.

Foundation model embeddings offer a more flexible approach. Per-nucleotide representations can be pooled into fixed-dimensional vectors that support classification with simple downstream heads. For ncRNAs without strong sequence motifs, the pretrained embeddings may capture subtle distributional patterns learned during self-supervised training. Few-shot learning becomes possible: given a handful of newly characterized RNAs, their embeddings can seed new clusters in representation space, guiding annotation of related sequences.

## miRNA Target Prediction
MicroRNAs regulate gene expression by guiding the RNA-induced silencing complex (RISC) to complementary sites in target mRNAs, typically in the 3' UTR. A single miRNA can regulate hundreds of transcripts, and a single transcript can harbor binding sites for dozens of miRNAs. This regulatory network influences virtually every cellular process, and dysregulation of miRNA-target interactions contributes to cancer, cardiovascular disease, and neurodegeneration. Predicting which transcripts a given miRNA targets (and vice versa) has been a persistent computational challenge since the discovery of miRNA-mediated regulation.

The dominant paradigm centers on seed complementarity. Nucleotides 2 through 7 of the miRNA (the seed region) typically form perfect Watson-Crick pairs with target sites, while the remaining nucleotides contribute variably to binding affinity and regulatory effect. Classical algorithms like TargetScan identify conserved seed matches in 3' UTRs and rank targets by evolutionary conservation, site type (8mer, 7mer-m8, 7mer-A1), and local sequence context [@agarwal_predicting_2015]. Additional features including AU content flanking the site, position within the UTR, and proximity to other miRNA sites improve prediction accuracy.

Despite decades of refinement, target prediction remains noisy. Experimental validation rates for top predictions rarely exceed 50%, and many functional targets lack canonical seed matches. The disconnect arises partly from context dependence: a site may be accessible in one cell type but occluded by RNA structure or competing protein binding in another. It arises partly from the limitations of reporter assays that measure binding in artificial contexts rather than endogenous regulatory effects. And it arises from the biology itself, where weak individual sites combine additively and miRNA-target interactions are probabilistic rather than deterministic.

Deep learning approaches attempt to improve on seed-based methods by learning complex sequence features from high-throughput binding data. Models trained on CLIP-seq experiments (which crosslink miRNA-target complexes and identify bound sites transcriptome-wide) can capture non-canonical binding modes and context effects invisible to seed-matching algorithms. However, these models often overfit to cell-type-specific binding patterns and generalize poorly across contexts. The fundamental challenge is that miRNA targeting depends on factors beyond sequence: miRNA and target abundance, competition among targets for limiting RISC, and cellular state variables that no sequence-based model can capture.

For clinical applications, target prediction informs both the mechanism of disease-associated miRNAs and the design of therapeutic interventions. AntimiR oligonucleotides that sequester specific miRNAs have entered clinical trials for hepatitis C (targeting miR-122) and other indications. Predicting off-target effects of such therapeutics requires understanding the full network of targets that will be derepressed when a miRNA is inhibited. Similarly, miRNA mimics designed to replace lost tumor-suppressor miRNAs must be evaluated for potential regulation of unintended targets. In both cases, computational target prediction provides a starting point that experimental validation must refine.

## Splicing and Transcript Processing Models

### Beyond SpliceAI

SpliceAI demonstrated that deep convolutional networks could predict splice sites with near-spliceosomal precision (@sec-cnn). The model's success in identifying cryptic splice variants has made it a standard tool in clinical variant interpretation. However, splicing involves more than splice site recognition, and several extensions address aspects that SpliceAI does not fully capture.

Tissue-specific splicing patterns vary substantially across cell types and developmental stages. A splice site may be used in brain but skipped in liver due to differential expression of splicing factors. Models like Pangolin extend splice prediction by training on tissue-specific RNA-seq data, learning to predict not just whether a site is splice-competent but whether it is used in specific cellular contexts. These models enable variant interpretation that accounts for tissue-relevant splicing patterns rather than generic predictions.

Branchpoint prediction identifies the adenosine residue where the lariat intermediate forms during splicing. While SpliceAI focuses on donor and acceptor sites, branchpoint recognition involves distinct sequence features (typically a degenerate YURAY motif 18-40 nucleotides upstream of the acceptor) that specialized models can capture. Combined analysis of donor, acceptor, and branchpoint predictions provides more complete characterization of splice-altering variants.

Alternative splicing prediction moves beyond binary splice site identification to model exon inclusion rates and isoform usage. Models in this space attempt to predict not just whether an exon can be included but quantitative measures of inclusion across conditions, enabling analysis of splicing quantitative trait loci (sQTLs) and their effects on transcript diversity.


## Limitations and Open Challenges

### Sparse Structural Data

The fundamental limitation of RNA modeling is data scarcity. Protein structure prediction benefits from over 200,000 experimentally determined structures; RNA has fewer than 2,000, heavily biased toward ribosomal RNA and tRNA. This scarcity limits supervised learning for tertiary structure prediction and constrains the emergence of structural knowledge from self-supervised pretraining. Until high-throughput methods generate RNA structures at scale comparable to protein crystallography and cryo-EM, RNA tertiary structure prediction will remain a frontier problem rather than a solved one.

Secondary structure data is more abundant but still limited. Experimentally validated structures cover mainly well-characterized families, while computational predictions for novel sequences rely on thermodynamic models whose accuracy degrades for long RNAs and complex folds. Structure probing experiments provide genome-wide coverage but measure accessibility rather than pairing directly, requiring inference to convert reactivity profiles into structural models.

### Functional Annotation Gaps

For many ncRNA classes, function remains poorly characterized. LncRNA annotations often specify only genomic location and expression pattern without mechanistic understanding. Circular RNA functions are emerging but incompletely cataloged. Even for better-characterized classes like miRNAs, target prediction remains noisy and context-dependent.

This annotation gap limits supervised learning for function prediction and complicates evaluation. When ground truth is uncertain, it becomes difficult to assess whether a model's predictions reflect genuine biological insight or artifacts of incomplete training data. The field needs both experimental advances to characterize ncRNA function and computational approaches that can learn from weak or partial supervision.

### The Maturity Gap

RNA foundation models exist but have not achieved the transformative impact of protein language models. ESM-2 enabled ESMFold, providing structure prediction from single sequences that nearly matches AlphaFold. No comparable RNA breakthrough has occurred. The reasons include data scarcity, the conformational complexity of RNA, and the diversity of RNA classes that makes unified modeling difficult.

This maturity gap represents both a limitation and an opportunity. The techniques that succeeded for proteins (large-scale self-supervised learning, attention mechanisms, scaling laws) provide a roadmap. Applying that roadmap to RNA requires addressing the data challenge through structure probing, synthetic data generation, or more efficient use of limited experimental structures. It requires architectural innovations that handle RNA's long-range base pairing and conformational flexibility. And it requires benchmarks and evaluation frameworks that cover the full diversity of RNA types and tasks.


## The Bridge Between Sequence and Cell


RNA occupies a distinctive position in genomic AI, bridging the sequence-level models of Part III with the cellular perspectives that follow. Splicing models like SpliceAI operate on pre-mRNA and predict transcript processing outcomes. Codon-level models capture translation dynamics invisible to protein language models. mRNA therapeutic design demonstrates practical value through codon optimization, UTR engineering, and stability prediction. These applications proceed despite the absence of the structure prediction breakthrough that transformed protein modeling; secondary structure prediction has advanced through deep learning, but tertiary structure accuracy lags protein structure by a wide margin.

The relationship between RNA models and other modalities reflects RNA's position in the central dogma. RNA is the product of transcription that regulatory models predict, the substrate for translation that protein models assume, and the primary measurement that single-cell models use to represent cellular state. Foundation models that learn from RNA sequence capture patterns distinct from those in DNA or protein: codon usage biases, secondary structure constraints, and post-transcriptional regulatory elements that neither genomic nor protein models directly represent.

The chapters that follow extend from sequence to cellular and tissue context. Single-cell models (@sec-single-cell) treat RNA expression as the primary readout of cellular state, learning representations that capture cell type identity and perturbation response. Three-dimensional genome models (@sec-3d-genome) add spatial context that influences transcription. Network models (@sec-networks) integrate gene relationships that transcend individual sequences. RNA models provide sequence-level representations that feed into these higher-level frameworks, completing the molecular arc from DNA through RNA to protein while opening the path to systems-level integration.

## Figure Recommendations

**Figure 15.1: RNA energy landscape versus protein**. Schematic contrasting the funneled energy landscape of protein folding with the flatter landscape of RNA, illustrating multiple competing conformations for a typical RNA sequence.

**Figure 15.2: Secondary structure elements**. Diagram showing stems, loops, bulges, internal loops, and a pseudoknot with standard notation, highlighting the long-range base pairing that distinguishes RNA from protein secondary structure.

**Figure 15.3: RNA foundation model comparison**. Table or chart comparing RNA-FM, protein ESM-2, and DNA foundation models on training corpus size, model parameters, and demonstrated emergent capabilities.

**Figure 15.4: Codon-level tokenization**. Illustration showing the same protein encoded by different synonymous codons, highlighting how codon choice affects tRNA availability, translation speed, and mRNA secondary structure.

**Figure 15.5: mRNA design pipeline**. Workflow showing the progression from protein sequence target through codon optimization, UTR design, structure optimization, and modification selection for therapeutic mRNA development.