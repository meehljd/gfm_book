# Part I: Foundations {.unnumbered}

Before deep learning can predict variant effects, model regulatory grammars, or guide clinical decisions, it must contend with the raw materials of modern genomics: sequences, variants, and the biological and statistical frameworks used to interpret them. Part I establishes this foundation, surveying the technologies, datasets, and pre-deep-learning methods that any genomic foundation model must respect, integrate with, or improve upon.

The chapters that follow trace a natural arc from data generation to interpretation. @sec-ngs introduces next-generation sequencing and variant calling, the processes that transform biological samples into the VCF files of single-nucleotide variants and indels that serve as inputs to nearly all downstream analysis. Understanding these technologies reveals both their remarkable power and their systematic blind spots, from reference bias to missing structural variants, limitations that propagate into every model trained on their outputs.

@sec-data surveys the public resources that underpin modern computational genomics: reference genomes, population variation catalogs like gnomAD, functional genomics consortia such as ENCODE and Roadmap Epigenomics, and biobank-scale cohorts including the UK Biobank and GTEx. These resources serve simultaneously as training data, evaluation benchmarks, and sources of prior biological knowledge. Their coverage and biases shape what models can learn and what questions remain out of reach.

From individual variants, @sec-pgs moves to the statistical machinery of genome-wide association studies and polygenic scores. GWAS identify variant-trait associations across populations, while polygenic scores aggregate thousands of small effects into genome-wide predictors of complex traits. These methods define the classical approach to connecting genotype with phenotype, providing both baselines against which deep models are measured and conceptual frameworks that inform their design.

Finally, @sec-cadd examines pre-deep-learning variant effect prediction through the lens of CADD, the Combined Annotation Dependent Depletion framework. CADD represents a mature approach to variant prioritization through careful feature engineering, combining conservation, regulatory annotations, and population constraints into a single deleteriousness score. Its strengths and limitations, including subtle circularities with clinical databases, motivate the learned representations that subsequent chapters develop.

Together, these four chapters answer a foundational question: what data and methods form the backdrop against which genomic foundation models are built, trained, and evaluated? The models that follow do not emerge in a vacuum. They inherit the biases of sequencing technologies, learn from consortia-scale functional annotations, compete against GWAS-derived baselines, and face evaluation on benchmarks shaped by decades of prior computational and clinical work. Understanding this landscape is prerequisite to understanding what foundation models can and cannot accomplish.