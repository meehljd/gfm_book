# Part I: Data Foundations {.unnumbered}

Every genomic foundation model inherits the biases of its training data. A model trained on European-dominated biobanks will miscalibrate predictions for other populations. A variant effect predictor learning from ClinVar inherits whatever ascertainment biases clinical laboratories embedded in those classifications. A regulatory model trained on ENCODE cell lines may fail on primary tissues absent from the training compendium. The models examined throughout this book do not transcend their data sources; they compress and reflect them. Understanding what data resources contain, what they systematically miss, and what assumptions they encode is prerequisite to understanding what foundation models can and cannot accomplish.

Part I establishes this foundation by surveying the technologies, datasets, and pre-deep-learning methods that genomic foundation models must respect, integrate with, or improve upon. The chapters trace a natural arc from data generation to interpretation. @sec-ngs introduces next-generation sequencing and variant calling, the processes that transform biological samples into the VCF files that serve as inputs to nearly all downstream analysis. Understanding these technologies reveals both their remarkable power and their systematic blind spots, from reference bias to missing structural variants, limitations that propagate into every model trained on their outputs.

@sec-data surveys the public resources that underpin modern computational genomics: reference genomes, population variation catalogs like gnomAD, functional genomics consortia such as ENCODE and Roadmap Epigenomics, and biobank-scale cohorts including the UK Biobank and GTEx. These resources serve simultaneously as training data, evaluation benchmarks, and sources of prior biological knowledge. @sec-gwas introduces the statistical machinery of genome-wide association studies and polygenic scores, the classical approach to connecting genotype with phenotype that provides both baselines against which deep models are measured and conceptual frameworks that inform their design. Finally, @sec-vep-classical examines pre-deep-learning variant effect prediction through CADD and related methods, establishing what careful feature engineering achieved and where its limitations motivated the learned representations that subsequent parts develop.