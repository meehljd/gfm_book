# Evaluation Methodology {#sec-eval}

A model that achieves state-of-the-art performance on a chromatin benchmark may fail catastrophically on real clinical variants. A foundation model that improves AUROC from 0.89 to 0.90 may have learned nothing about biology and everything about data leakage. A polygenic score that predicts disease with apparent precision in British biobank participants may perform no better than random chance in African populations. The history of genomic machine learning is littered with models that worked brilliantly on paper and failed quietly in practice.

The fundamental problem is that genomic data makes it exceptionally easy to fool yourself. Sequences share evolutionary history. Variants cluster in families and populations. Experimental measurements carry batch effects invisible to the untrained eye. Training labels often derive from the very databases that will be used for evaluation, creating circular validations that inflate performance without testing genuine predictive power. Random data splits that work perfectly well for natural images become actively misleading when applied to genomes, proteins, or variants. Every shortcut that simplifies evaluation in other domains creates an opportunity for false confidence in genomics.

@sec-benchmarks catalogs the benchmark landscape in detail, describing what tasks exist, how they are constructed, and what capabilities they probe. This chapter addresses the complementary question: how to use benchmarks appropriately to draw valid conclusions about model performance. The difference between a trustworthy evaluation and a misleading one often lies not in the choice of benchmark but in the details of how that benchmark is applied. Data splitting strategies, metric selection, baseline comparisons, ablation designs, and statistical testing collectively determine whether reported results mean anything at all.

The principles developed here apply across all the benchmark categories described in @sec-benchmarks, from chromatin state prediction to clinical variant classification. By this point in the book, we have encountered genomic models deployed at almost every scale: variant calling from NGS reads (@sec-ngs), polygenic scores and GWAS (@sec-gwas), classical deleteriousness predictors (@sec-vep-classical), foundation model approaches to variant effects (@sec-vep-fm), CNN-based regulatory models (@sec-cnn), and genomic language models (@sec-dna-lm, @sec-fm-principles). Clinical risk prediction and rare disease diagnosis (@sec-clinical-risk, @sec-rare-disease) add still more evaluation considerations. What has been missing is a unified treatment of the question: what does it mean for a genomic model to work, and how do we know when it does?

## Why Random Splits Fail

The standard machine learning recipe calls for randomly partitioning data into training, validation, and test sets. For image classification or sentiment analysis, this approach works well because individual examples are approximately independent. A photograph of a cat shares no special relationship with another photograph of a different cat beyond their common label. Random assignment ensures that training and test distributions match, and performance on the test set provides an unbiased estimate of performance on new examples from the same distribution.

Genomic data violates these assumptions at every level. Consider a protein dataset where the goal is to predict stability from sequence. Proteins in the same family share evolutionary history and often similar structures. If a training set includes beta-lactamase variants from *E. coli* and the test set includes beta-lactamase variants from *Klebsiella*, the model may appear to generalize to "new" proteins while actually recognizing sequence patterns it saw during training. The test performance reflects memorization of family-specific features rather than general principles of protein stability.

The problem compounds when sequence identity is high. Two proteins sharing 80% sequence identity will typically have similar structures and functions. A model trained on one and tested on the other is not really being tested on a novel example; it is being asked to interpolate within a region of sequence space it has already explored. Even at 30% sequence identity, the so-called "twilight zone" of homology detection, proteins often share structural and functional similarities that can be exploited by sufficiently powerful models.

Variant-level data presents analogous challenges. Variants within the same gene share genomic context, and variants affecting the same protein domain share structural environment. Variants from the same individual share haplotype background. Variants from the same population share allele frequency distributions shaped by demographic history. Each of these relationships creates opportunities for models to learn shortcuts that generalize within the training distribution but fail on genuinely novel examples.

The consequence is that random splits systematically overestimate generalization. A model that achieves 0.90 AUROC with random splitting might achieve only 0.75 AUROC when evaluated on truly held-out examples, with the gap reflecting how much the model learned about biology versus how much it learned about the structure of the training data. Recognizing this problem is the first step toward solving it.

## Homology-Aware Splitting

The solution to homology-driven leakage is to explicitly account for sequence similarity when constructing data splits. Rather than random assignment, examples are clustered by sequence identity, and entire clusters are assigned to training, validation, or test sets. This ensures that no test example is "too similar" to any training example, forcing the model to demonstrate genuine generalization.

### Clustering Tools and Workflows

Two tools dominate homology-aware splitting in practice. **CD-HIT** clusters sequences by greedy incremental clustering, assigning each sequence to an existing cluster if it exceeds a similarity threshold to the cluster representative, or creating a new cluster otherwise. The algorithm is fast and scales to millions of sequences. For proteins, a typical workflow clusters at 40% sequence identity for stringent splitting or 70% for moderate splitting. For nucleotide sequences, thresholds are typically higher (80-95%) due to different evolutionary rates.

```bash
# Cluster proteins at 40% identity
cd-hit -i proteins.fasta -o proteins_clustered -c 0.4 -n 2

# Cluster nucleotides at 90% identity  
cd-hit-est -i sequences.fasta -o sequences_clustered -c 0.9 -n 8
```

**MMseqs2** offers faster clustering with similar sensitivity, becoming essential for large-scale analyses. The tool supports multiple clustering modes and can handle databases with hundreds of millions of sequences. For foundation model pretraining where deduplication affects billions of sequences, MMseqs2 is often the only practical option.

```bash
# Create database and cluster at 30% identity
mmseqs createdb proteins.fasta DB
mmseqs cluster DB DB_clu tmp --min-seq-id 0.3
mmseqs createtsv DB DB DB_clu clusters.tsv
```

The choice of identity threshold involves trade-offs. Stricter thresholds (lower identity for proteins, higher for nucleotides) create harder generalization tests but may leave insufficient data for training if clusters are small. Permissive thresholds retain more training data but allow more leakage through homologous sequences. For protein function prediction, 30-40% identity thresholds are common; for variant effect prediction within genes, even stricter gene-family-level splits may be necessary.

### Practical Considerations

Several subtleties affect the quality of homology-aware splits. **Cluster size distribution** matters: if one cluster contains half the data and is assigned to training, the remaining clusters may be too small or too biased to serve as representative test sets. Stratified sampling within clusters or careful balancing across splits can mitigate this issue.

**Transitive homology** creates hidden relationships that pairwise clustering can miss. Protein A may share 35% identity with protein B, and protein B may share 35% identity with protein C, yet A and C share only 20% identity. If A is in training and C is in testing, B serves as an indirect bridge. Connected component analysis or multi-step clustering can address these transitive relationships, though at increased computational cost.

**Domain-level homology** complicates whole-protein clustering. A multi-domain protein may share one domain with training proteins and another domain with test proteins. Whether this represents leakage depends on the prediction task: if predicting whole-protein function, shared domains matter; if predicting domain-specific properties, they matter more acutely. Domain-aware splitting assigns domains rather than whole proteins to clusters, though this requires domain annotation that may not always be available.

For genomic (non-protein) sequences, repeat elements and transposable elements create analogous challenges. A model trained to predict chromatin state may learn features of LINE elements that recur throughout the genome. Excluding repetitive regions from evaluation or explicitly accounting for repeat content can clarify what the model has actually learned about regulatory sequences versus repetitive element patterns.

## Splitting by Biological Axis

Beyond sequence homology, genomic data admits multiple axes along which splits can be constructed. The choice of axis determines what kind of generalization is being tested.

### Splitting by Individual

For tasks involving human genetic variation, ensuring that data from the same individual (or related individuals) does not appear in both training and test sets is essential. A variant effect predictor trained on variants from person A and tested on other variants from person A may learn individual-specific patterns, such as haplotype structure or ancestry-correlated allele frequencies, that do not generalize to new individuals.

Family structure creates subtler leakage. First-degree relatives share approximately 50% of their genomes identical by descent. Even distant relatives share genomic segments that can be exploited by sufficiently powerful models. Best practice involves computing kinship coefficients across all individuals and either excluding one member of each related pair or assigning entire family clusters to the same split. The UK Biobank provides pre-computed relatedness estimates; other cohorts may require explicit calculation using tools like KING or PLINK.

### Splitting by Genomic Region

Chromosome-based splits assign entire chromosomes to training or testing. This approach is common in regulatory genomics, where models trained on chromosomes 1-16 are tested on chromosomes 17-22 (or similar partitions). The advantage is simplicity and reproducibility; the disadvantage is that chromosomes are not independent. Chromosome 6 contains the HLA region with its unusual patterns of variation and selection; chromosome 21 is small and gene-poor; sex chromosomes have distinct biology. Results may vary substantially depending on which chromosomes are held out.

Region-based splits hold out contiguous segments (e.g., 1 Mb windows) distributed across the genome. This provides more uniform coverage than chromosome splits but requires careful attention to boundary effects. If a regulatory element spans the boundary between training and test regions, parts of its context may leak into training.

### Splitting by Gene or Protein Family

For variant effect prediction, holding out entire genes or protein families tests whether models learn general principles versus gene-specific patterns. A model that achieves high accuracy by memorizing that TP53 variants are often pathogenic has not demonstrated understanding of mutational mechanisms. Gene-level splits force models to generalize to genes they have never seen, providing stronger evidence of biological insight.

Family-level splits extend this logic to groups of related genes. Holding out all kinases or all GPCRs tests whether models can generalize across evolutionary families. This is particularly stringent for protein structure and function prediction, where family membership strongly predicts properties.

### Splitting by Experimental Context

Multi-task models that predict chromatin marks across cell types can be split by cell type rather than genomic position. Training on liver, lung, and brain while testing on heart and kidney assesses whether learned regulatory logic transfers across tissues. This matters because cell-type-specific factors drive much of regulatory variation; a model that has simply learned which regions are accessible in the training cell types may fail on novel cell types even when sequence features should transfer.

Similarly, models can be split by assay type (e.g., training on ATAC-seq, testing on DNase-seq), laboratory (to assess batch effects), or time point (for longitudinal data). Each split tests a different axis of generalization.

### Splitting by Ancestry

For human genomic applications, ancestry-stratified evaluation has become essential. Models trained predominantly on European ancestry cohorts often show degraded performance in African, East Asian, South Asian, and admixed populations. This degradation reflects both differences in allele frequency spectra and differences in linkage disequilibrium patterns that affect which variants are informative.

Best practice reports performance separately for each major ancestry group represented in the data. When held-out ancestry groups are available (e.g., training on Europeans and testing on Africans), this provides the strongest test of cross-population generalization. When only European data are available, this limitation should be explicitly acknowledged, and claims about generalization should be appropriately modest. The confounding effects of ancestry on genomic predictions are detailed in @sec-confounding.

### Splitting by Time

Temporal splits assign data to training and test sets based on when observations were collected, annotations were created, or variants were classified. This strategy tests whether models generalize forward in time, the actual deployment scenario for any predictive system.

For variant pathogenicity prediction, temporal splits are particularly revealing. Training on ClinVar annotations from 2018 and testing on variants first classified in 2022 asks whether the model can predict labels that did not yet exist during training. This avoids the circularity that arises when training and test labels were assigned by similar processes at similar times. Variants classified more recently may reflect updated curation standards, new functional evidence, or reclassifications of previously uncertain variants; a model that performs well on these genuinely new classifications demonstrates predictive validity rather than recapitulation of historical curation patterns.

Implementing temporal splits requires metadata that many datasets lack. ClinVar provides submission dates, enabling clean temporal partitioning. UniProt tracks annotation dates for functional assignments. Clinical cohorts with longitudinal follow-up naturally admit temporal splits based on diagnosis dates. When temporal metadata is unavailable, publication dates of source literature can serve as proxies, though these may not perfectly reflect when information became available to model developers.

The key limitation of temporal splits is non-stationarity. The distribution of variants classified in 2022 may differ systematically from those classified in 2018, not because biology changed but because research priorities, sequencing technologies, and ascertainment patterns evolved. Performance degradation on temporally held-out data may reflect distribution shift rather than genuine failure to generalize. Combining temporal splits with stratified analysis (performance by variant type, gene category, or evidence strength) helps disentangle these factors.

## Leakage Taxonomy and Detection

Even with careful splitting, leakage can enter evaluations through multiple pathways. Understanding common leakage patterns helps practitioners design cleaner evaluations and critically assess published results.

### Feature Leakage

Feature leakage occurs when input features encode information about the target that would not be available at prediction time. In genomics, conservation scores are a common source. If a model uses PhyloP scores as features and the target is pathogenicity, the model may learn that "conserved positions are more likely pathogenic" without learning anything about variant biology. This would be fine if conservation scores are intended to be part of the prediction pipeline, but problematic if the goal is to develop a model that can predict pathogenicity from sequence alone.

Similarly, population allele frequency encodes selection pressure. A model that learns "rare variants are more likely pathogenic" has discovered a useful heuristic but not necessarily a mechanistic understanding. Whether this counts as leakage depends on the intended use case. For clinical variant interpretation where allele frequency is always available, exploiting this feature is appropriate. For understanding variant biology, it may mask whether the model has learned anything beyond allele frequency.

### Label Leakage

Label leakage occurs when target labels are derived from information that the model can access through its features. The classic example is training pathogenicity predictors on ClinVar annotations while using sequence features that were used to construct ClinVar annotations. If ClinVar curators used SIFT and PolyPhen scores when classifying variants, and the model uses similar sequence features, high performance may reflect recapitulation of curation criteria rather than independent predictive power.

Temporal label leakage is subtler. A model trained on ClinVar annotations from 2020 and tested on annotations from 2023 may perform well because new annotations were informed by model-like predictions. The apparent validation is circular: the model predicts labels that were partially derived from model-like reasoning.

### Benchmark Leakage

Benchmark leakage occurs when test set construction was influenced by methods similar to those being evaluated. If a protein function benchmark was created by selecting proteins with high-confidence annotations, and those annotations were partly derived from sequence similarity searches, sequence-based models may perform well by exploiting the same similarity that guided benchmark construction.

Foundation models face particular challenges with benchmark leakage. If a DNA language model is pretrained on all publicly available genomic sequence including ENCODE data, and then evaluated on ENCODE-derived benchmarks, the pretraining has exposed the model to information about the test distribution even if specific test examples were held out. The model may have learned statistical patterns in ENCODE data that transfer to ENCODE benchmarks without reflecting genuine biological understanding.

### Detection Strategies

Several strategies help detect leakage. **Baseline analysis** asks whether simple models that could not plausibly have learned biology achieve suspiciously high performance. If a linear model using only allele frequency achieves 0.80 AUROC on a pathogenicity benchmark, and a sophisticated deep model achieves 0.82, the marginal improvement may not justify claims of biological insight.

**Feature ablation** systematically removes potentially leaky features and measures performance degradation. If removing conservation scores causes a 20-point drop in AUROC, the model was heavily dependent on conservation rather than learning independent predictors.

**Confounder analysis** explicitly models potential confounders and tests whether model predictions remain informative after conditioning. If a variant effect predictor's scores become non-predictive after controlling for gene length and expression level, the model may have learned gene-level confounders rather than variant-level effects.

**Temporal validation** evaluates models on data collected after the training data was frozen. If performance degrades substantially on newer data, the model may have been fitted to temporal artifacts in the original dataset.

## Metrics for Genomic Tasks

Metrics quantify model performance but different metrics answer different questions. Choosing appropriate metrics requires clarity about what aspect of performance matters for the intended application.

### Discrimination Metrics

For binary outcomes (pathogenic versus benign, bound versus unbound, accessible versus closed), discrimination metrics assess how well the model separates classes. The **area under the receiver operating characteristic curve (AUROC)** measures the probability that a randomly selected positive example is ranked above a randomly selected negative example. AUROC is threshold-independent and widely reported but can be misleading when classes are highly imbalanced.

The **area under the precision-recall curve (AUPRC)** better reflects performance when positives are rare. For variant pathogenicity prediction, where perhaps 1% of variants are truly pathogenic, a model achieving 0.95 AUROC might still have poor precision at useful recall levels. AUPRC directly captures the precision-recall trade-off that matters for applications requiring both high sensitivity and manageable false positive rates.

**Sensitivity**, **specificity**, **positive predictive value**, and **negative predictive value** require specifying a decision threshold. These metrics are more interpretable for specific use cases (e.g., "the model identifies 90% of pathogenic variants while flagging only 5% of benign variants as false positives") but require choosing thresholds that may not generalize across settings.

### Regression and Correlation Metrics

For continuous predictions (expression levels, effect sizes, binding affinities), correlation metrics assess agreement between predicted and observed values. **Pearson correlation** measures linear association; **Spearman correlation** measures rank association and is robust to nonlinear relationships. The **coefficient of determination (RÂ²)** measures variance explained, though interpretation requires care when baseline performance is near zero.

For predictions at genomic scale (e.g., predicted versus observed expression across thousands of genes), these metrics may obscure important patterns. A model might achieve high genome-wide correlation by correctly predicting which genes are highly expressed while failing on the genes where predictions matter most. Task-specific stratification, such as correlation within expression quantiles or among disease-relevant genes, provides more actionable information.

### Ranking and Prioritization Metrics

Many genomic workflows care about ranking rather than absolute prediction. Variant prioritization pipelines rank candidates for follow-up; gene prioritization ranks targets for experimental validation. **Top-k recall** measures the fraction of true positives captured in the top k predictions. **Enrichment at k** compares the true positive rate in the top k to the background rate. **Normalized discounted cumulative gain (NDCG)** weights ranking quality by position, penalizing relevant items placed lower in the list more than those placed near the top.

These metrics align with how predictions are actually used. If experimental capacity permits validating only 20 variants per locus, top-20 recall matters more than global AUROC. Reporting both global metrics and rank-aware metrics at relevant cutoffs provides a complete picture.

### Calibration Metrics

Calibration assesses whether predicted probabilities match observed frequencies. A well-calibrated model that predicts 0.8 probability of pathogenicity should be correct about 80% of the time across all variants receiving that score. **Reliability diagrams** plot predicted probabilities against observed frequencies within binned intervals, with deviations from the diagonal indicating miscalibration. **Expected calibration error (ECE)** summarizes miscalibration as the weighted average absolute deviation across bins.

Calibration matters whenever predictions inform decisions. A miscalibrated model that reports 0.95 probability when the true probability is 0.60 will lead to inappropriate confidence in uncertain predictions. Even models with excellent discrimination can be poorly calibrated, requiring post-hoc calibration methods such as Platt scaling or isotonic regression. @sec-uncertainty addresses calibration and uncertainty quantification in greater depth.

### Clinical Utility Metrics

For clinical applications, discrimination and calibration are necessary but not sufficient. **Decision curves** plot net benefit across decision thresholds, where net benefit weighs the value of true positives against the cost of false positives at each threshold. A model may achieve high AUROC but offer no net benefit at clinically relevant thresholds if it fails to discriminate in the region where decisions are actually made.

**Net reclassification improvement (NRI)** measures how often adding genomic features to a clinical model changes risk classifications in the correct direction. This directly addresses whether genomics adds clinical value beyond existing predictors. @sec-clinical-risk provides detailed treatment of clinical evaluation frameworks.

## Baseline Selection

Baseline comparisons determine the meaning of reported performance. A model achieving 0.85 AUROC might represent a major advance if the best prior method achieved 0.70, or a trivial improvement if simple heuristics achieve 0.83. Choosing appropriate baselines is as important as choosing appropriate metrics.

### Strong Baselines, Not Straw Men

The temptation to compare against weak baselines inflates apparent contributions. A deep learning model compared against a naive prior or a deliberately crippled baseline will appear impressive regardless of whether it offers genuine value. Strong baselines force honest assessment of improvement.

For sequence-based predictions, position weight matrices (PWMs) and k-mer logistic regression provide classical baselines that capture sequence composition without deep learning. If a convolutional model barely outperforms logistic regression on k-mer counts, the convolutional architecture may not be contributing as much as claimed.

For variant effect prediction, simple features like allele frequency, conservation scores, and amino acid properties provide baselines that any sophisticated model should substantially exceed. CADD (@sec-vep-classical) serves as a well-calibrated baseline that combines many hand-crafted features; outperforming CADD demonstrates that learning provides value beyond feature engineering.

For foundation models, comparisons should include both randomly initialized models of similar architecture (to isolate the value of pretraining) and simpler pretrained models (to isolate the value of scale or architectural innovations). Claiming that pretraining helps requires demonstrating improvement over training from scratch on the same downstream data.

### Historical Baselines and Progress Tracking

Comparing to methods from five years ago may demonstrate progress but overstates the contribution of any single method. Comparisons should include the best currently available alternatives, not just historically important ones. When prior work is not directly comparable (different data, different splits, different metrics), reimplementing baselines on common benchmarks provides fairer comparison.

Field-wide progress tracking benefits from persistent benchmarks with frozen test sets. Once test set results for a benchmark are published, that benchmark becomes less useful for future model development because the test set is no longer truly held out. Periodic benchmark refresh with new held-out data helps maintain evaluation integrity.

### Non-Deep-Learning Baselines

Deep learning models should be compared against strong non-deep alternatives. Gradient-boosted trees, random forests, and regularized linear models often achieve competitive performance with far less computation. If a 100-million-parameter transformer barely outperforms XGBoost on tabular features, the complexity may not be justified.

This comparison is especially important for clinical deployment, where simpler models may be preferred for interpretability, computational efficiency, or regulatory approval. Demonstrating that deep learning provides substantial gains over strong non-deep baselines strengthens the case for adoption.

## Ablation Studies

Ablation studies systematically remove or modify model components to understand their contributions. Where baselines compare across methods, ablations investigate within a method, revealing which design choices actually matter.

### Component Isolation

Standard ablations remove individual components: attention layers, skip connections, normalization schemes, specific input features. If removing attention heads causes minimal performance degradation, the model may not be exploiting long-range dependencies as claimed. If removing a particular input modality has no effect, that modality may not be contributing useful information.

Ablations should be designed to test specific hypotheses. If the claim is that a foundation model learns biologically meaningful representations, ablating pretraining (comparing to random initialization) directly tests this claim. If the claim is that cross-attention between modalities enables integration, ablating cross-attention while retaining separate encoders tests whether integration provides value.

### Hyperparameter Sensitivity

Reporting performance across hyperparameter ranges reveals robustness. A model that achieves state-of-the-art performance only at a narrow learning rate range with specific regularization may be overfit to the evaluation setup. Consistent performance across reasonable hyperparameter variations provides stronger evidence of genuine capability.

### Architecture Search Confounds

When model development involves extensive architecture search, reported performance conflates the value of the final architecture with the value of search on the validation set. The validation set is no longer truly held out; it has been used to select among architectures. Final evaluation on a completely untouched test set, with the architecture fixed before test set examination, provides cleaner assessment.

### Reporting Standards

Ablation tables should clearly indicate what was changed in each condition, the number of random seeds or runs, and measures of variance. Single-run ablations can produce misleading results due to training stochasticity. Reporting means and standard deviations across multiple runs reveals whether observed differences exceed random variation.

## Statistical Rigor

Performance differences between models may reflect genuine capability differences or random variation in training and evaluation. Statistical analysis distinguishes signal from noise.

### Significance Testing

For classification metrics, significance tests ask whether observed differences exceed what would be expected from sampling variation. **Bootstrap confidence intervals** resample the test set with replacement, recompute metrics on each resample, and report the distribution of metric values. Non-overlapping 95% confidence intervals suggest significant differences. **Permutation tests** shuffle predictions between models and measure how often shuffled differences exceed observed differences.

For comparing multiple models across multiple benchmarks, correction for multiple testing becomes important. Without correction, 20 pairwise comparisons will produce an expected one false positive at the 0.05 level even when all models perform equally. The **Bonferroni correction** divides the significance threshold by the number of tests; the **Benjamini-Hochberg procedure** controls false discovery rate with more power than Bonferroni.

### Effect Sizes

Statistical significance does not imply practical significance. A difference of 0.001 AUROC might be statistically significant with millions of test examples while being practically meaningless. **Effect sizes** quantify the magnitude of differences independent of sample size. Cohen's d for continuous outcomes and odds ratios for binary outcomes provide standardized measures of effect magnitude.

Reporting both significance tests and effect sizes provides complete information. A result that is statistically significant with a tiny effect size warrants different interpretation than one that is significant with a large effect size.

### Confidence Intervals on Metrics

Point estimates of AUROC or correlation should be accompanied by confidence intervals. **DeLong's method** provides analytical confidence intervals for AUROC; **bootstrap methods** provide distribution-free intervals for any metric. Reporting "AUROC = 0.85 (95% CI: 0.82-0.88)" is more informative than "AUROC = 0.85" alone.

### Variance Across Random Seeds

Deep learning models are sensitive to initialization and optimization stochasticity. Training the same architecture with different random seeds can produce substantially different results. Best practice trains multiple runs and reports means and standard deviations. If the standard deviation across runs exceeds the difference between methods, claimed improvements may not be reproducible.

## Evaluating Foundation Models

Genomic foundation models (@sec-fm-principles) admit multiple evaluation paradigms, each testing different aspects of learned representations.

### Zero-Shot Evaluation

In zero-shot evaluation, the pretrained model is applied without any task-specific training. For masked language models, this typically means using token probabilities to score variants or classify sequences. A variant that disrupts a position the model predicts with high confidence may indicate functional importance.

Zero-shot performance tests whether pretraining captures task-relevant structure without explicit supervision. Strong zero-shot performance suggests the pretraining objective aligned with the evaluation task; weak zero-shot performance suggests misalignment. Comparing zero-shot performance to simple baselines (e.g., conservation scores for variant effects) calibrates whether the foundation model provides value beyond what simpler approaches achieve.

### Linear Probing

Linear probing freezes the foundation model and trains only a linear classifier on extracted embeddings. This isolates representation quality from fine-tuning capacity. If a linear probe on foundation model embeddings substantially outperforms a linear probe on random embeddings, the foundation model has learned useful features.

Layer-wise probing reveals where information is encoded. Early layers may capture local sequence features while later layers capture more abstract patterns. If the information needed for a task is extractable from early layers, the model may not require the full depth of the architecture for that application.

### Fine-Tuning Evaluation

Full fine-tuning adapts all model parameters to the downstream task. This provides the best performance but conflates representation quality with adaptation capacity. A foundation model might achieve high fine-tuned performance through the capacity of its architecture rather than the quality of its pretrained representations.

Comparing fine-tuned foundation models to equivalently architected models trained from scratch isolates the value of pretraining. If both approaches converge to similar performance given sufficient downstream data, pretraining provides label efficiency (less data needed to reach a given performance level) rather than improved final performance. Data efficiency curves, plotting performance against downstream training set size, reveal this trade-off.

### Transfer Across Tasks

Foundation models justify their "foundation" designation by transferring to diverse downstream tasks. Evaluating on a single task, however well-designed, cannot assess breadth of transfer. Multi-task evaluation across regulatory prediction, variant effects, protein properties, and other applications reveals whether foundation models provide general-purpose representations or excel only on tasks similar to their pretraining objective.

Transfer across species, tissues, and experimental modalities provides additional evidence of generalization. A DNA language model that transfers from human to mouse, or from blood cells to neurons, demonstrates that its representations capture biological principles rather than species-specific or tissue-specific patterns.

## Calibration Essentials

Even models with strong discrimination may produce probability estimates that mislead decisions. Calibration assesses whether predicted probabilities match observed frequencies, a property essential for rational decision-making.

### Assessing Calibration

**Reliability diagrams** bin predictions by predicted probability and plot the mean predicted probability against the observed frequency within each bin. Perfect calibration produces points along the diagonal; deviations reveal systematic over-confidence (points below the diagonal) or under-confidence (points above).

**Expected calibration error** summarizes miscalibration as a single number: the weighted average absolute difference between predicted and observed probabilities across bins. Lower ECE indicates better calibration. However, ECE is sensitive to binning choices and should be reported alongside reliability diagrams for interpretability.

**Calibration by subgroup** reveals whether miscalibration varies across populations or conditions. A model might be well-calibrated overall but systematically overconfident for rare variant classes or underrepresented ancestries. Stratified calibration analysis identifies these disparities.

### Recalibration Methods

Post-hoc recalibration adjusts predicted probabilities to improve calibration without retraining the model. **Temperature scaling** divides logits by a learned temperature parameter before applying softmax, compressing or expanding the probability distribution. **Platt scaling** fits a logistic regression from model outputs to true labels. **Isotonic regression** fits a monotonic function that maps model outputs to calibrated probabilities.

These methods require held-out calibration data distinct from training and test sets. Calibrating on test data and then evaluating calibration on the same test data produces overoptimistic estimates.

For detailed treatment of calibration and uncertainty quantification, including epistemic versus aleatoric uncertainty and selective prediction, see @sec-uncertainty.

## Putting It All Together

When designing or evaluating a genomic model assessment, working through a systematic checklist helps identify gaps and potential problems.

**Level of decision**: Is the model intended for molecular prediction, variant prioritization, patient risk stratification, or clinical action? Metrics should align with the actual decision context. Enrichment metrics suit variant ranking; net benefit matters for clinical decisions.

**Data splits**: Are individuals, genomic regions, gene families, and ancestries appropriately separated? Has homology-aware clustering been applied with appropriate identity thresholds? Is there any plausible pathway for leakage or circularity?

**Baselines**: Are comparisons made against the best available alternatives, not just historical or deliberately weak baselines? Do non-deep-learning baselines establish floors? Does the improvement over baselines justify the complexity?

**Metrics**: Are multiple metrics reported to capture discrimination, calibration, and ranking quality? Are metrics computed with confidence intervals? Are subgroup-stratified metrics reported for fairness assessment?

**Ablations**: Have component contributions been isolated through systematic ablation? Is performance robust across hyperparameter ranges and random seeds?

**Statistical rigor**: Are significance tests applied with multiple testing correction? Are effect sizes reported alongside p-values? Are confidence intervals provided for key metrics?

**Foundation model specifics**: For foundation models, is performance reported across zero-shot, probing, and fine-tuning regimes? Do data efficiency curves reveal where pretraining value lies? Has transfer been tested across diverse tasks?

**Robustness**: How does performance vary across cohorts, platforms, and ancestries? How does the model behave under distribution shift, missing data, or label noise?

This checklist is not exhaustive but covers the most common evaluation pitfalls. Working through it systematically at the design stage can prevent problems that are difficult to fix retrospectively. Reviewers and readers can use the same checklist to critically assess published work.

## Looking Forward

Rigorous evaluation requires sustained effort at every stage, from data splitting through statistical analysis. The shortcuts that accelerate research in other domains, random splits, single-metric comparisons, significance tests without effect sizes, produce misleading conclusions when applied to genomic data. Homology, population structure, batch effects, and label circularity create countless opportunities for self-deception.

The chapters that follow address complementary aspects of reliability. @sec-confounding examines how confounders, biases, and data artifacts can produce evaluation results that evaporate under deployment. Population stratification, batch effects, and benchmark leakage can all create illusions of performance that careful evaluation might fail to detect. @sec-uncertainty develops the theory and practice of uncertainty quantification, extending the calibration discussion here to cover epistemic versus aleatoric uncertainty, selective prediction, and communication of model confidence. @sec-interpretability addresses the complementary question of not just whether models work but why they work, developing tools to distinguish genuine biological insight from pattern matching on confounded data.

Together with the benchmark survey in @sec-benchmarks, these chapters provide the critical apparatus for engaging with genomic model claims. @sec-confounding examines how population stratification, batch effects, and ascertainment bias can produce results that evaporate under deployment. @sec-uncertainty extends calibration to epistemic versus aleatoric uncertainty and selective prediction. @sec-interpretability addresses whether models have learned biology or exploited confounded patterns. For clinical applications specifically, @sec-clinical-risk develops evaluation frameworks tailored to risk prediction, where net benefit and decision curves supplement standard discrimination metrics. The question is never simply "what is the AUROC?" but rather "what has been demonstrated, and how much should we trust it?" The answer depends on the details.