# Convolutional Models for Genomic Sequence {#sec-cnn}

In 2015, a convolutional neural network trained on ENCODE chromatin data learned to recognize transcription factor binding motifs that matched entries in the JASPAR database, despite never being shown those motifs during training [@zhou_deepsea_2015]. The network had discovered, through gradient descent on raw sequence, patterns that took decades of experimental work to catalog. This result announced a new era for computational genomics: deep learning could extract biologically meaningful representations directly from DNA sequence, bypassing the hand-crafted features that had defined the field since its inception.

The significance extended beyond mere accuracy improvements. Classical variant effect predictors like CADD (@sec-vep-classical) achieved their performance by aggregating dozens of annotation features, each encoding human assumptions about what mattered for function: conservation scores, protein domain overlaps, known regulatory element positions. These features worked, but they imposed a ceiling. Models could only recognize patterns that humans had already identified and encoded. Convolutional networks promised something different: learning which sequence features mattered directly from functional data, potentially discovering patterns that had escaped prior notice.

This chapter examines the convolutional architectures that established the sequence-to-function paradigm between 2015 and 2019. DeepSEA demonstrated that CNNs could predict chromatin accessibility and transcription factor binding across hundreds of cell types. Basset and DanQ explored architectural variations that influenced subsequent work. ExPecto extended the paradigm to gene expression prediction by integrating chromatin signals across 40 kilobases of promoter-proximal sequence. SpliceAI achieved near-spliceosomal accuracy for splice site prediction, enabling clinical identification of cryptic splice variants invisible to annotation-based methods. Together, these models established core principles that persist in modern genomic AI: raw sequence as input, multi-task learning for shared representations, and variant effect prediction through in silico mutagenesis. They also revealed a fundamental limitation that would motivate the architectural innovations examined in subsequent chapters.


## Convolutions as Sequence Pattern Detectors

Before examining specific models, understanding how convolutional operations process biological sequence clarifies what these architectures can and cannot learn. A convolutional filter slides across an input sequence, computing similarity scores at each position. For genomic applications, the input is typically one-hot encoded DNA (a binary matrix with four rows for A, C, G, T and columns for each position), and filters learn weight patterns that respond to specific nucleotide arrangements.

Consider a filter of width 8 nucleotides. At each position along the sequence, the filter computes a weighted sum of the underlying nucleotides, producing high activation when the sequence matches its learned pattern and low activation otherwise. This operation is mathematically equivalent to scanning a position weight matrix (PWM) across the sequence, but with a crucial difference: the filter weights are learned during training rather than derived from aligned binding site sequences.

The first layer of a genomic CNN typically contains hundreds of such filters, each learning to detect different local patterns. Analysis of trained filters consistently reveals that many correspond to known transcription factor binding motifs. The CTCF insulator motif, the ETS family consensus sequence, the AP-1 binding site: these patterns emerge from training on chromatin data without any explicit motif supervision. The network discovers them because they predict the training labels.

Deeper layers operate on the output of earlier layers rather than raw sequence. A second-layer filter might learn to detect specific arrangements of first-layer motifs: two ETS sites within 20 base pairs, or a CTCF motif flanked by particular spacing patterns. This hierarchical feature learning enables CNNs to capture regulatory grammar beyond individual motifs, including spacing constraints, orientation preferences, and combinatorial requirements that govern transcription factor cooperativity.

Pooling operations between convolutional layers reduce spatial resolution while increasing the receptive field. Max pooling selects the strongest activation within a window, achieving a form of position-invariant detection: the network cares that a motif is present somewhere in a region, not its exact position. This property suits regulatory genomics, where binding site positions within an enhancer often matter less than their presence and combination.

The receptive field of a convolutional network defines how much input sequence can influence a single output prediction. For a network with kernel width $k$, pooling factor $p$, and $L$ layers, the receptive field grows with depth but remains fundamentally limited by architecture. A three-layer network with typical parameters might integrate information from 200 to 1,000 base pairs. Reaching further requires either more layers (increasing computational cost and training difficulty) or dilated convolutions (spacing filter weights to sample larger regions). This receptive field limitation becomes critical when biological dependencies span tens of kilobases.


::: {.callout-warning .content-visible when-profile="draft"}
**VISUAL SUGGESTION: Convolution mechanics**

Three-panel figure: (A) A single filter sliding across one-hot DNA, producing activation scores; (B) Comparison of learned filter weights to JASPAR motif logo; (C) Receptive field diagram showing how pooling + stacking expands the region influencing each output.
:::


## DeepSEA: Regulatory Prediction from Sequence {#sec-deepsea}

### The Noncoding Variant Problem

Genome-wide association studies had, by 2015, mapped thousands of loci to complex diseases and traits, yet a persistent pattern complicated interpretation: the vast majority of associated variants fell in noncoding regions [@maurano_noncode_2012]. Only a small fraction directly altered protein sequences. The remainder landed in introns, intergenic regions, and putative regulatory elements where their functional consequences remained obscure.

Existing approaches to noncoding variant interpretation relied on overlap with functional annotations. If a variant fell within a ChIP-seq peak or DNase hypersensitive site, it might be flagged as potentially regulatory. This strategy had obvious appeal since it grounded predictions in experimental observations. But overlap-based annotation could not predict whether a variant would strengthen or weaken regulatory activity, could not score variants in regions lacking experimental coverage, and provided no mechanism for quantifying effect magnitude. A variant might fall within an enhancer, but would it matter? The data said where regulatory elements were; they did not say how sequence changes would affect them.

DeepSEA, introduced by Zhou and Troyanskaya, reframed the problem: rather than asking whether a variant overlaps known annotations, ask what regulatory activities a sequence encodes and how mutations would alter them [@zhou_deepsea_2015]. The shift from annotation lookup to sequence-based prediction enabled scoring any variant in any genomic context, including regions never assayed in any experiment.


### Architecture and Training

The DeepSEA architecture was deliberately simple by contemporary standards. Input sequences of 1,000 base pairs, one-hot encoded, passed through three convolutional layers with 320, 480, and 960 filters respectively. Max pooling after each convolution compressed spatial dimensions. A fully connected layer with 925 units integrated information across the compressed representation, and a final output layer with 919 sigmoid units produced independent probability predictions for each chromatin profile.

Training data came from ENCODE and Roadmap Epigenomics: 690 transcription factor binding profiles, 104 histone modification profiles, and 125 DNase I hypersensitivity profiles spanning diverse cell types [@kagda_encode_2025; @kundaje_roadmap_2015]. For each 1,000-bp input, the model predicted whether the central 200-bp region exhibited each chromatin feature. Chromosome 8 was held out for evaluation.

The multi-task formulation proved essential. Predicting 919 features simultaneously forced the network to learn shared representations useful across many prediction problems. The first convolutional layer learns general sequence patterns (GC content, common dinucleotides, ubiquitous motifs); these representations then feed task-specific combinations in later layers. Joint training provides implicit regularization, preventing overfitting to any single task while amortizing the cost of learning basic sequence features across all outputs.


### What DeepSEA Learned

Analysis of first-layer filters revealed learned patterns matching known transcription factor motifs. The network had independently discovered sequence preferences cataloged in JASPAR and TRANSFAC, confirming that the training objective (predicting chromatin state) induced biologically meaningful feature extraction. This interpretability distinguished deep learning from prior black-box approaches and suggested that the models captured genuine regulatory logic rather than spurious correlations. Systematic methods for extracting and visualizing these learned representations, from filter analysis to attribution mapping, are examined in @sec-interpretability.

Beyond individual motifs, DeepSEA implicitly learned aspects of regulatory grammar. Deeper layers combined first-layer patterns into more complex representations, potentially capturing motif spacing requirements, orientation preferences, and cooperative binding arrangements. The network encoded relationships between sequence features that position weight matrices, operating independently at each motif, could not represent.

DeepSEA outperformed gkm-SVM (gapped k-mer support vector machines) on nearly all transcription factor binding prediction tasks. More revealing was the pattern with respect to context: gkm-SVM showed no improvement when given longer input sequences, while DeepSEA performance improved substantially with additional context. The difference reflects a fundamental limitation of k-mer counting methods: they cannot learn relationships between patterns at different positions.


### Variant Effect Prediction

With a trained sequence-to-chromatin model, variant scoring becomes straightforward: predict chromatin profiles for reference and alternative sequences, compute the difference. This produces a vector describing predicted changes across all 919 features. Critically, the model never sees variant data during training; effect prediction emerges from learned sequence-function relationships applied to mutations the model has never encountered.

Validation used allelic imbalance data from digital genomic footprinting. For variants showing allele-specific DNase I sensitivity, DeepSEA predictions correlated with experimentally observed biases: variants predicted to increase accessibility tended to show higher accessibility on the corresponding allele. This correlation would not exist if the model merely learned coarse sequence features insensitive to point mutations.

In silico saturation mutagenesis extends single-variant scoring to systematic characterization. By predicting effects of all possible substitutions across a regulatory element, one identifies positions where mutations most strongly perturb function. These critical positions typically correspond to transcription factor binding motifs, providing a form of motif discovery that emerges from learned representations rather than explicit sequence alignment.


::: {.callout-warning .content-visible when-profile="draft"}
**VISUAL SUGGESTION: DeepSEA architecture and validation**

Three-panel figure: (A) Architecture schematic showing convolutional stack, pooling, and 919-output vector; (B) First-layer filter aligned to JASPAR motif; (C) Scatter plot of predicted vs. observed allelic imbalance.
:::


## Architectural Variations: Basset and DanQ

DeepSEA established the paradigm; subsequent models explored architectural variations with different trade-offs. Basset, introduced by Kelley et al. in 2016, focused specifically on predicting chromatin accessibility from sequence [@kelley_basset_2016]. Rather than the diverse chromatin features of DeepSEA, Basset predicted DNase-seq peaks across 164 cell types, enabling detailed analysis of regulatory element activity.

Basset's architecture introduced several refinements. Batch normalization after convolutional layers stabilized training and enabled deeper networks. The model used larger filters in early layers (19 nucleotides in the first layer) to capture longer motifs directly. A key contribution was demonstrating that in silico saturation mutagenesis profiles from trained models could identify causal variants underlying disease-associated haplotypes, moving beyond simple peak overlap to mechanistic variant prioritization.

DanQ combined convolutional and recurrent components, reasoning that regulatory grammar involves sequential dependencies that convolutions alone might miss [@quang_danq_2016]. After initial convolutional layers extracted local features, a bidirectional LSTM processed the resulting feature sequence, potentially learning ordering constraints and long-range patterns. The hybrid architecture achieved modest improvements on chromatin prediction benchmarks, though the recurrent components added computational cost and training complexity.

These variations illustrated a broader point: multiple architectures could learn useful regulatory representations from sequence. The specific choices (filter sizes, layer depths, recurrent components) mattered less than the fundamental framework of learning from one-hot encoded sequence to predict chromatin labels. This robustness suggested that the underlying signal (sequence determinants of regulatory activity) was strong enough to be captured by diverse architectural approaches.


## ExPecto: From Chromatin to Expression {#sec-expecto}

### Beyond Intermediate Phenotypes

Chromatin accessibility and transcription factor binding are intermediate phenotypes, means to an end rather than the end itself. The ultimate functional readout for most regulatory variants is their effect on gene expression. A variant might disrupt a transcription factor binding site, but does that site actually regulate a nearby gene? In which tissues? By how much?

ExPecto, introduced by Zhou et al. in 2018, addressed these questions by extending sequence-to-chromatin prediction toward tissue-specific gene expression [@zhou_expecto_2018]. The framework predicts expression levels across 218 tissues and cell types by integrating predicted chromatin signals across a 40-kilobase promoter-proximal window. This context expansion, from DeepSEA's 1 kb to ExPecto's 40 kb, represented a significant architectural commitment: expression prediction requires integrating regulatory signals from distances far exceeding typical motif sizes.


### The Modular Architecture

ExPecto comprises three sequential components, each addressing a distinct computational challenge. The first component, an enhanced CNN called Beluga, predicts 2,002 chromatin profiles from 2,000-bp input sequences. Beluga incorporated architectural improvements over DeepSEA: six convolutional layers with residual connections, expanded chromatin targets, and broader cell-type coverage. This CNN scans the 40-kb region surrounding each transcription start site with a moving window, generating chromatin predictions at 200 spatial positions and producing over 400,000 features per gene.

The second component transforms these high-dimensional features through spatial aggregation. Ten exponential decay functions, applied separately to upstream and downstream regions, encode the prior belief that nearby elements contribute more than distant ones. This transformation reduces dimensionality while preserving spatial relationships, producing approximately 20,000 features per gene that capture both which chromatin features are predicted and where they occur relative to the TSS.

The final component comprises 218 L2-regularized linear regression models, one per tissue, predicting log expression from spatially-transformed features. Linear models were chosen deliberately: they provide interpretability, prevent overfitting given the high-dimensional feature space, and enable coefficient analysis to identify which chromatin features drive expression in each tissue. The combination of a shared sequence-to-chromatin CNN with separate tissue-specific linear heads cleanly separates sequence-level regulatory grammar from tissue-specific regulatory programs.


### Expression Prediction and Variant Effects

ExPecto achieved 0.819 median Spearman correlation between predicted and observed expression across tissues. Analysis of model coefficients revealed automatic learning of cell-type-relevant features: the liver expression model weighted HepG2-derived transcription factor features most heavily; breast tissue models emphasized estrogen receptor features from breast cancer cell lines. These tissue-specific patterns emerged purely from learning to predict expression, without tissue identity information provided to the chromatin model.

Variant effect prediction follows the same logic as DeepSEA: compare expression predictions for reference and alternative sequences. Because the model never trains on variant data, predictions are unconfounded by linkage disequilibrium. ExPecto correctly predicted expression change direction for 92% of the strongest GTEx eQTL variants, and experimental validation confirmed that model-prioritized variants (not the GWAS lead SNPs) showed allele-specific regulatory activity in reporter assays.

The 40-kb window represents an empirically optimized trade-off. Smaller windows decreased performance; larger windows showed negligible improvement. This suggests that most promoter-proximal regulatory information lies within 40 kb of the TSS, at least within ExPecto's linear modeling framework. Distal enhancers beyond this window, while biologically important, require more sophisticated integration. This limitation points toward the longer-context architectures examined in @sec-regulatory.


::: {.callout-warning .content-visible when-profile="draft"}
**VISUAL SUGGESTION: ExPecto pipeline**

Three-component diagram: (1) Beluga CNN scanning 40 kb window; (2) Spatial feature transformation with decay functions; (3) Per-tissue linear models. Include example predicted expression values and delta scores.
:::


## SpliceAI: Clinical-Grade Splicing Prediction {#sec-spliceai}

### The Cryptic Splice Problem

While DeepSEA and ExPecto addressed chromatin state and expression, a distinct class of functional variants operates through disruption of pre-mRNA splicing. The clinical stakes are substantial. Splice-disrupting mutations represent a major mechanism of Mendelian disease, yet variants affecting splicing outside canonical GT/AG dinucleotides are systematically underascertained. A patient's exome may reveal no obvious coding variant explaining their phenotype, while a cryptic splice variant in an intron silently disrupts gene function.

Prior splice prediction methods captured essential splice site motifs but could not model the long-range determinants contributing to splicing specificity. MaxEntScan operates on approximately 9 bp of context around donor/acceptor sites [@yeo_maxentscan_2004]. These methods produced many false positives and missed variants acting through distal mechanisms. The limitations paralleled those of pre-deep-learning variant effect predictors: hand-crafted features impose ceilings that learned representations can transcend.

SpliceAI, introduced by Jaganathan et al. in 2019, demonstrated that deep neural networks could learn splicing rules with near-spliceosomal precision [@jaganathan_spliceai_2019]. The model predicts splice site locations directly from pre-mRNA sequence using 10,000 nucleotides of context, an order of magnitude beyond prior methods. This context expansion enabled recognition of distant determinants like branch points, exonic splicing enhancers, and intron length constraints that previous models could not see.


### Architecture: Depth and Dilation

SpliceAI employs an ultra-deep residual network with 32 convolutional layers. Residual connections address the vanishing gradient problem:

$$
\text{output} = \text{input} + F(\text{input})
$$

This design enables training depths impossible with earlier architectures. Skip connections from every fourth residual block feed directly to the penultimate layer, further stabilizing gradients.

Dilated convolutions expand the receptive field efficiently. A dilated convolution with rate $d$ samples input positions at intervals of $d$ rather than consecutively. Stacking convolutions with increasing dilation rates allows the network to integrate information across the full 10-kb window while maintaining sensitivity to local patterns. Standard convolutions with small kernels would require impractical depth to achieve equivalent receptive fields.

For each position in the pre-mRNA sequence, SpliceAI outputs three probabilities: splice acceptor, splice donor, or neither. This per-position classification enables fine-grained predictions across entire transcripts. Training used GENCODE annotations, with odd and even chromosomes split for training and testing.


### Performance and Validation

SpliceAI achieved 95% top-k accuracy for splice site identification (compared to 57% for MaxEntScan) and 0.98 precision-recall AUC. Even complex genes exceeding 100 kb are often reconstructed perfectly to nucleotide precision. Performance improved dramatically with context length:

| Model Variant | Context (each side) | PR-AUC |
|---------------|---------------------|--------|
| SpliceAI-80nt | 40 bp | 0.87 |
| SpliceAI-400nt | 200 bp | 0.93 |
| SpliceAI-2k | 1,000 bp | 0.96 |
| SpliceAI-10k | 5,000 bp | 0.98 |

This progression confirms that distal sequence features contribute meaningfully to splicing decisions.

The delta score quantifies variant effects by comparing predictions for reference and alternative sequences:

$$
\Delta\text{score} = \max_{|p - v| \leq 50} \left| P_{\text{alt}}(p) - P_{\text{ref}}(p) \right|
$$

Validation against GTEx RNA-seq showed that mutations with higher delta scores showed higher validation rates at novel splice junctions: approximately 50% at Δ ≥ 0.2, 75% at Δ ≥ 0.5, and 85% at Δ ≥ 0.8. Population genetics provided orthogonal support: predicted cryptic splice variants showed 78% depletion at common allele frequencies, nearly matching the depletion of frameshift and stop-gain variants.


### Clinical Impact

SpliceAI's most significant contribution may be identifying cryptic splice mutations as a major, previously underappreciated cause of rare genetic disorders. Analysis of de novo mutations in over 4,000 individuals with intellectual disability found significant enrichment of predicted splice-disrupting variants compared to unaffected controls (1.51-fold, p = 4.2×10⁻⁴). Approximately 9% of pathogenic de novo mutations in intellectual disability act through cryptic splicing. Including these variants in gene discovery analyses identified additional candidate genes that would have fallen below discovery thresholds when considering only protein-coding mutations.

This clinical utility explains SpliceAI's rapid adoption. Illumina integrated SpliceAI into their annotation pipelines. Clinical genetics laboratories worldwide use delta scores to flag potential splice-affecting variants for RNA-seq follow-up. The model exemplifies how task-specific deep learning can achieve clinical-grade accuracy on well-defined problems. We return to SpliceAI's role in modern variant interpretation workflows in @sec-vep-fm.


::: {.callout-warning .content-visible when-profile="draft"}
**VISUAL SUGGESTION: SpliceAI clinical application**

Case-style figure: (A) Intronic variant location in a disease gene; (B) Delta score track showing predicted cryptic splice activation; (C) RNA-seq sashimi plot confirming the aberrant junction; (D) Comparison to MaxEntScan prediction (missed).
:::


## The Receptive Field Ceiling {#sec-receptive-field}

The models examined in this chapter share a fundamental limitation rooted in their architecture: convolutional networks can only integrate information within their receptive fields. DeepSEA's three-layer architecture effectively considers roughly 1 kb of context. ExPecto's Beluga component operates on 2-kb windows, aggregated across a 40-kb region by the spatial transformation layer. SpliceAI pushes to 10 kb through dilated convolutions and 32 layers. Each expansion required significant architectural engineering, and each reached a practical ceiling.

The limitation matters because genomic regulation operates across scales these models cannot reach. Enhancers routinely regulate promoters 50 to 500 kilobases away. The beta-globin locus control region sits 40 to 60 kb from the genes it activates. Polycomb-mediated repression involves chromatin contacts spanning megabases. Topologically associating domains organize regulatory interactions across hundreds of kilobases. When regulatory elements and their targets lie beyond a model's receptive field, the model cannot learn their relationship regardless of how much training data is available.

This creates a systematic blind spot. Variants within distal enhancers may have profound effects on gene expression, but a model with a 10-kb receptive field cannot connect the enhancer sequence to its target promoter. The model might correctly predict that the enhancer sequence contains regulatory features, but it cannot predict which gene those features regulate or how strongly. Regulatory effect prediction requires seeing both the regulatory element and its target simultaneously.

The architectural response to this challenge takes two forms. One approach, exemplified by Enformer (@sec-regulatory), combines convolutional feature extraction with attention mechanisms that enable direct interaction between distant positions. By computing pairwise relationships across a 200-kb context, Enformer can model enhancer-promoter communication that pure convolutions cannot reach. The other approach uses efficient attention variants or state space models that reduce the quadratic cost of full attention, enabling even longer contexts.

The progression from DeepSEA's 1 kb to Enformer's 200 kb represents a 200-fold expansion in effective context. This expansion did not come from better convolutions; it came from abandoning the assumption that local operations were sufficient and embracing architectures designed for long-range dependency modeling. Understanding why this architectural shift was necessary, and what CNNs could not provide, establishes the foundation for the attention mechanisms examined in @sec-attention.


## Architectural Themes and Lasting Contributions

The convolutional models examined here established paradigms that persist in modern genomic AI, even as transformer architectures have become dominant for many applications.

**Raw sequence as input**: One-hot encoded nucleotides, with no hand-crafted features, are sufficient to learn complex regulatory signals. This end-to-end learning approach forces models to discover relevant patterns from data rather than encoding human assumptions about what matters.

**Multi-task training**: Jointly predicting hundreds of related chromatin features improves both accuracy and generalization compared to training separate models. Shared representations in early layers benefit all tasks, and joint prediction provides implicit regularization against overfitting.

**Variant scoring through in silico mutagenesis**: Comparing predictions for reference and alternative sequences enables effect prediction without training on variant data. This ab initio capability avoids linkage disequilibrium confounding and enables scoring of rare variants, de novo mutations, and hypothetical sequences never observed in any population.

**Task-specific architectures achieve remarkable accuracy**: SpliceAI's focus on splicing enabled near-spliceosomal precision that broad foundation models have not yet matched for this specific task. When the prediction target is well-defined and training data are abundant, specialized models remain competitive.

These models also revealed what CNNs cannot do. The receptive field ceiling limits modeling of long-range regulatory interactions. Expression prediction requires integrating signals across distances that pure convolutions cannot efficiently span. The fundamental operation of convolution, local pattern detection with limited spatial context, suits some problems better than others.

The tension between specialized and general-purpose models recurs throughout this book. For clinical applications requiring high accuracy on specific tasks (splice site prediction, chromatin accessibility), specialized CNNs may remain preferred. For discovery applications requiring broad coverage of molecular mechanisms across diverse prediction targets, the foundation model approach examined in Part III offers different trade-offs. The architectures examined in @sec-attention provide mechanisms for long-range dependency modeling that extend the principles established here while transcending the receptive field limitations that constrained these pioneering models.


::: {.callout-warning .content-visible when-profile="draft"}
**VISUAL SUGGESTION: Receptive field comparison**

Diagram comparing effective context windows: DeepSEA (1 kb), ExPecto/Beluga (2 kb windows, 40 kb aggregated), SpliceAI (10 kb), Enformer (200 kb). Overlay typical enhancer-promoter distances and TAD sizes to illustrate what each architecture can and cannot see.
:::