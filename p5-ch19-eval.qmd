# Model Evaluation {#sec-eval}

Genomic foundation models are evaluated on an increasingly diverse landscape of benchmarks spanning molecular predictions, variant effects, and complex traits. @sec-benchmarks catalogs these benchmark suites in detail, describing what tasks they include, how they are constructed, and what aspects of model capability they probe. This chapter focuses on the complementary question: **how to use benchmarks appropriately** to draw valid conclusions about model performance.

Proper evaluation requires attention to data splitting strategies, metric selection, statistical testing, and interpretation of results. It requires understanding when good benchmark performance translates to real-world utility and when it reflects overfitting, data leakage, or confounding. The principles developed here apply across all the benchmark categories described in @sec-benchmarks, from chromatin state prediction to clinical variant classification.

By this point in the book, we have seen genomic models deployed at almost every scale. Variant calling from NGS reads (@sec-ngs), polygenic scores and GWAS (@sec-pgs), deleteriousness scores and variant effect predictors (@sec-cadd, @sec-vep), CNN-based sequence-to-function models (@sec-reg through @sec-splice), and genomic language models and foundation models (@sec-dna, @sec-foundation) have each introduced their own metrics and benchmarks. Clinical risk prediction and pathogenic variant discovery (@sec-clinical, @sec-variants) add still more evaluation considerations. What has been missing is a single place to answer a deceptively simple question: what does it mean for a genomic model to work, and how should we systematically evaluate it?

This chapter provides that unifying view. We describe the major families of evaluation metrics and show how they map to typical genomic tasks. We organize evaluation across four levels, from molecular readouts through variant-level predictions to trait-level risk scores and finally to clinical decisions. We discuss data splitting, leakage, and robustness, the mechanics that make or break benchmarks regardless of how sophisticated the underlying architecture may be. We explain how to evaluate foundation models across different usage regimes, from zero-shot scoring through linear probing to full fine-tuning. Finally, we connect evaluation to the broader theme of reliability, linking forward to the detailed treatments of confounders in @sec-confound and interpretability in @sec-interp.

Throughout, the theme is that architecture and scale matter, but evaluation choices often matter more. A state-of-the-art model evaluated on a leaky benchmark tells us less than a modest model evaluated on a clean one. A foundation model that achieves impressive perplexity but fails to improve downstream variant interpretation has not demonstrated clinical utility. Getting evaluation right is prerequisite to knowing whether any of the sophisticated methods covered in this book actually work.

## Evaluation as a Multi-Scale Problem

Genomic models are deployed at very different scales, and understanding this hierarchy is essential for designing appropriate evaluations. It helps to keep a simple mental pyramid in mind, with molecular readouts at the base and clinical decisions at the apex.

At the **molecular and regulatory level**, models take local sequence and epigenomic context as input and predict outputs such as chromatin accessibility, histone marks, transcription factor binding, splicing outcomes, or expression levels. Representative models at this level include DeepSEA-style chromatin predictors, SpliceAI for splice site prediction, and Enformer for long-range regulatory modeling. Evaluation here typically involves comparing predicted tracks or binary annotations against experimental measurements.

At the **variant level**, models take a specific variant (whether SNV, indel, or structural variant) and its surrounding context as input, producing outputs such as pathogenicity scores, predicted molecular impact, or fine-mapping posterior probabilities. Examples include CADD-style deleteriousness scores, AlphaMissense-like variant effect predictors, and Bayesian fine-mapping methods. Evaluation focuses on concordance with clinical annotations, allele frequency patterns, or experimental measurements of variant effects.

At the **trait and individual level**, models take a person's genotype or sequence along with other features as input and produce risk scores for complex traits, predicted phenotypes, or endophenotypes. Classical polygenic scores and GFM-augmented risk models (@sec-pgs, @sec-clinical) operate at this level. Evaluation compares predicted risk against observed outcomes in held-out cohorts, often with attention to calibration and discrimination across ancestry groups.

At the **clinical and decision level**, the inputs are model predictions combined with contextual factors such as guidelines, utility assumptions, and patient preferences. The outputs are actual decisions: whether to treat or not treat, screen or not screen, include a patient in a trial or exclude them. Examples include screening strategies, clinical decision support tools, and trial enrichment protocols. Evaluation at this level requires moving beyond accuracy metrics to consider decision curves, net benefit, and prospective validation.

Good evaluation starts from the intended level of action. If the goal is variant prioritization in a rare disease pipeline, improvement in AUROC on a chromatin benchmark is only indirectly relevant. If the goal is clinical risk stratification, better perplexity on a DNA language model test set is useful only insofar as it leads to more discriminative, better calibrated risk scores. The rest of the chapter climbs this pyramid while keeping a few core metric families in view.

## Metric Families Across Genomic Tasks

Most evaluation in this book falls into four broad metric families, each suited to different types of predictions and scientific questions.

### Classification Metrics

For binary or multi-class outputs such as pathogenic versus benign, open versus closed chromatin, or presence versus absence of a histone mark, the standard metrics derive from the confusion matrix. The area under the receiver operating characteristic curve (AUROC or simply AUC) measures the probability that a randomly chosen positive example is ranked above a randomly chosen negative example, providing a threshold-independent summary of discrimination. The area under the precision-recall curve (AUPRC) is more informative when positives are rare, as is typically the case when identifying pathogenic variants among many benign ones or causal variants among many correlated candidates. Simple metrics like accuracy, sensitivity, and specificity are intuitive but sensitive to class imbalance and require choosing specific decision thresholds.

In practice, variant effect predictors and clinical risk models typically report AUROC and AUPRC for prioritization tasks. Regulatory prediction models often report per-task AUROC averaged over hundreds of chromatin assays, sometimes with weighting schemes that emphasize difficult or clinically relevant targets.

### Regression and Correlation Metrics

For continuous outputs such as expression levels, log-odds of accessibility, or quantitative traits, the standard metrics measure association between predicted and observed values. Pearson correlation measures linear association, while Spearman correlation measures rank-based association and is robust to monotone transformations of the data. The coefficient of determination ($R^2$) measures the fraction of variance explained, often computed against a simple baseline such as a mean-only model. Mean-squared error and root mean-squared error provide absolute measures of prediction error in the original units.

Sequence-to-expression models and multi-omics integrations frequently use correlation between predicted and observed tracks, as in Enformer-style evaluations that compare predicted and measured gene expression across cell types. Polygenic score performance is often reported as incremental $R^2$, the additional variance explained by genomic features over and above clinical covariates.

### Ranking and Prioritization Metrics

Many genomics workflows are fundamentally about ranking rather than absolute prediction. The goal may be to prioritize variants in a locus for follow-up, rank genes or targets for experimental validation, or select individuals at highest risk for screening. While AUROC and AUPRC capture some aspects of ranking quality, additional metrics can be more directly relevant.

Top-k recall or enrichment measures the fraction of true positives captured in the top k predictions, directly addressing questions like "how many real causal variants would land in our top 20 candidates?" Enrichment over baseline measures how much more likely a high-scoring bucket is to contain true positives compared to random expectation. Normalized discounted cumulative gain (NDCG) emphasizes getting highly relevant items near the top of the ranked list, with diminishing returns for items placed lower. These metrics often align better with practical questions about how predictions will actually be used in experimental workflows where resources limit validation efforts to a small number of top candidates.

### Generative and Language Model Metrics

Self-supervised genomic language models (@sec-dna) introduce their own metrics related to the pretraining objective. Perplexity and cross-entropy on masked-token reconstruction tasks measure how well the model predicts held-out sequence content. Bits-per-base for next-token prediction or compression-style objectives provides a related measure of the model's ability to capture sequence statistics.

These metrics are important for assessing representation quality and for comparing pretraining runs, but they come with important caveats. They are distribution-specific, tied to the particular pretraining corpus and task, which limits comparability across models trained on different data. More importantly, improvements in perplexity do not automatically translate into better variant or trait predictions. A model might achieve excellent perplexity by capturing abundant patterns in the genome, such as repetitive elements and sequence composition, that are largely irrelevant for functional prediction. As a result, generative metrics should always be paired with downstream task metrics to assess real utility.

## Levels of Evaluation: From Base Pairs to Bedside

We now walk through the pyramid from molecular readouts to clinical decisions, focusing on what good evaluation looks like at each level and the common pitfalls that can undermine it.

### Molecular and Regulatory-Level Evaluation

At the molecular level, the core tasks include predicting chromatin accessibility, histone marks, and transcription factor binding profiles; predicting splicing outcomes such as percent spliced in (PSI) values or transcription start and termination sites; and predicting readouts from functional assays like massively parallel reporter assays (MPRAs) or CRISPR perturbation screens.

Common evaluation setups involve multi-task classification, where AUROC or AUPRC is computed for each assay and then averaged with or without weighting across assays. Track-wise regression computes Pearson or Spearman correlation between predicted and observed signal profiles across genomic positions. Out-of-cell-type prediction trains on some cell types and tests on others to assess generalization beyond the training distribution.

Several design choices shape the meaning of reported metrics. The granularity of labels matters: base-resolution predictions present a different challenge than predictions averaged over 128-base-pair bins. The size of context windows determines whether the evaluation tests local sequence features or long-range regulatory architecture. The definition of held-out biology, whether new transcription factors, new cell types, or entirely new genomic loci, determines what kind of generalization is actually being tested.

Common pitfalls include overfitting to specific assays or idiosyncratic lab protocols and inadvertent leakage when nearby genomic regions or replicate experiments are split across train and test sets. A model might appear to generalize to new regions while actually leveraging sequence similarity or chromatin context shared with training examples.

For noisy assays, reporting performance relative to replicate concordance provides important context. If technical replicates of the same experiment correlate at $r = 0.85$, a model achieving $r = 0.80$ may be approaching the practical ceiling imposed by measurement noise. Reporting fraction of explainable variance, computed as the ratio of model performance to replicate concordance, can be more informative than raw correlation values.

### Variant-Level Evaluation

At the variant level, tasks include classifying variants as pathogenic versus benign or damaging versus tolerated, predicting functional impact such as effects on splicing, expression, or protein stability, and fine-mapping to assign posterior probabilities of causality to variants in associated loci.

Common benchmarks derive from clinical labels in ClinVar and HGMD (see @sec-data for detailed properties and @sec-benchmarks for benchmark-specific considerations), from curated variant sets assembled by diagnostic laboratories, from population-based labels using allele frequency strata in gnomAD-like resources, and from functional assays including saturation mutagenesis, MPRAs, and deep mutational scanning experiments. The choice of benchmark profoundly shapes what the evaluation measures.

Metrics typically include AUROC and AUPRC on binary labels, correlation or rank metrics against experimental effect sizes, and calibration-style metrics for probabilistic outputs. Reliability diagrams for pathogenicity probabilities or fine-mapping posteriors assess whether variants scored at 80% pathogenic are truly pathogenic about 80% of the time.

Several design questions deserve careful attention. The definition of the negative class matters enormously: common and presumably benign variants, frequency-matched controls, synonymous variants, or synthetic negatives as in CADD (@sec-cadd) each create different evaluation contexts with different biases. The choice of what is held out determines the kind of generalization being tested. Holding out entire genes tests whether the model has learned general principles about variant effects versus gene-specific patterns. Holding out specific loci tests whether the model can extrapolate to new genomic contexts. Holding out particular variant types (for example, only evaluating on frameshifts or splice-disrupting variants) tests whether the model has learned type-specific consequences.

For fine-mapping and similar tasks where multiple variants per locus compete for causal status, evaluating top-k recall of causal variants per risk locus is often more informative than global AUC across all variants. In practice, researchers follow up only a handful of variants per locus, so knowing that the causal variant consistently ranks in the top three is more valuable than knowing the model achieves high genome-wide discrimination.

This level is also where issues of circularity become especially acute. Scores trained on ClinVar and then evaluated on overlapping or highly correlated variants create feedback loops that inflate apparent performance without demonstrating real predictive power. Similarly, models that incorporate features derived from the same underlying data as the evaluation labels can appear to work well while providing no incremental utility beyond what those features already captured. We return to these problems in detail in @sec-confound.

### Trait- and Individual-Level Evaluation

At the trait and individual level, tasks include predicting quantitative traits such as LDL cholesterol, height, or estimated glomerular filtration rate from genotypes and other features, case-control risk prediction for complex diseases like coronary artery disease or type 2 diabetes, and multi-trait and multi-task risk modeling that jointly predicts related phenotypes.

For quantitative traits, incremental $R^2$ measures the variance explained by genomic features over and above clinical covariates, directly quantifying what genetics adds to prediction. For binary or time-to-event outcomes, AUROC, AUPRC, and the concordance index (C-index) measure discrimination. Net reclassification improvement (NRI) asks how often individuals are moved across clinically meaningful risk thresholds in the correct direction, a metric more directly tied to clinical utility than discrimination alone.

Critical evaluation settings include within-ancestry versus cross-ancestry performance. Models trained predominantly on European ancestry cohorts often show substantial performance degradation when applied to African, East Asian, or admixed populations. The technical causes and fairness implications of these portability failures are detailed in @sec-confound. Reporting ancestry-stratified metrics is now considered essential for polygenic score evaluations, with best practices requiring separate reporting of discrimination, calibration, and incremental value for each major ancestry group.

Within-cohort versus external validation compares models trained and tested in the same biobank against models validated in entirely separate cohorts with different recruitment, sequencing, and clinical practices. External validation provides stronger evidence of generalizability but is often not feasible until after model publication. When external validation is not possible, careful documentation of the training cohort's characteristics, including detailed ancestry composition, and explicit caveats about likely performance in other settings become especially important.

Joint versus marginal contribution of genetics examines how much predictive information comes from genomic features when combined with electronic health records and other multi-omic data (@sec-systems). A model that achieves high absolute performance might contribute little beyond what clinical variables already provide. Reporting both absolute metrics and incremental gains over strong non-genomic baselines is essential for understanding real impact.

Even for purely research models, reporting absolute performance alongside incremental gain over strong baselines is essential for understanding real impact. A polygenic score that achieves 0.65 AUROC for a disease sounds moderately impressive until one learns that clinical variables alone achieve 0.63 AUROC. The incremental value may still be scientifically interesting, but the practical utility for clinical decision-making is limited.

### Clinical and Decision-Level Evaluation

Clinical risk models, treatment response predictors, and trial enrichment models (@sec-clinical) ultimately need to be evaluated in terms of decisions, not just scores. Beyond discrimination and calibration, several additional concepts become important.

Decision curves and net benefit compare different decision thresholds or policies by weighting true positives versus false positives according to clinical utilities. A model that achieves high AUROC but offers no net benefit at clinically relevant thresholds has not demonstrated clinical value. The net benefit framework explicitly incorporates the relative costs of false positives versus false negatives, allowing evaluation to reflect real-world trade-offs in screening, diagnosis, or treatment decisions.

Cost-sensitive and utility-aware evaluation explicitly models different misclassification costs, recognizing that missing a high-risk patient has different consequences than unnecessary screening. For cancer screening, a false negative might delay diagnosis by years with substantial mortality consequences, while a false positive leads to additional imaging or biopsy. These asymmetric costs should be reflected in evaluation metrics and decision thresholds.

Prospective and interventional evaluation through randomized trials, pragmatic trials, and observational implementations with careful monitoring provides the strongest evidence for clinical utility but is expensive and time-consuming. Retrospective validation on historical data can identify promising models but cannot fully account for how clinician behavior, patient adherence, or health system workflows will change in response to model predictions. The gap between retrospective performance and prospective impact is often substantial.

This chapter provides only a high-level overview of clinical evaluation. @sec-clinical goes deeper into clinical metrics and deployment considerations, while @sec-variants discusses evaluation of variant-centric discovery workflows in rare disease and oncology settings.

## Data Splits, Leakage, and Robustness

Metrics mean little without well-designed data splits. In genomics, the usual approach of randomly assigning 80% of examples to training, 10% to validation, and 10% to testing often fails to test the kind of generalization we actually care about. The structure of genomic data, with its hierarchical organization from bases to variants to individuals to populations, creates many opportunities for subtle information leakage.

### Axes of Splitting

Several axes exist along which we can and often should split data. Splitting by individual ensures that genomes from the same person or family do not appear in both training and test sets, preventing models from memorizing individual-specific patterns. This is essential for trait prediction and clinical risk modeling but may be less relevant for purely sequence-based regulatory models that do not use individual-level labels.

Splitting by locus or region holds out contiguous genomic segments such as specific chromosomes or megabase windows, testing whether models can generalize to entirely new genomic contexts. Chromosome-based splits are common in regulatory genomics, where models trained on chromosomes 1 through 16 are tested on chromosomes 17 through 22. This approach reduces sequence similarity between train and test sets and forces models to rely on general principles rather than memorizing local patterns.

Splitting by gene or target holds out entire genes or protein families for variant effect and protein models, testing whether the model has learned general principles versus gene-specific idiosyncrasies. For example, a protein model evaluated on entirely held-out protein families provides stronger evidence of biological understanding than a model evaluated on random variants across all proteins, some of which may have close homologs in the training set.

Splitting by assay, cell type, or tissue trains on some experimental contexts and tests on unseen ones, assessing whether learned regulatory logic transfers across biological conditions. This is particularly relevant for multi-task models like Enformer that predict regulatory readouts across many cell types. Holding out entire cell types or tissue contexts tests whether the model has learned general regulatory principles or has simply memorized cell-type-specific patterns.

Splitting by ancestry or cohort trains in one population or recruitment setting and evaluates in others, testing whether models generalize across human diversity. This is essential for assessing model fairness and for understanding performance degradation in underrepresented populations. As discussed in @sec-pgs and @sec-confound, ancestry-aware evaluation has become a standard requirement for genomic risk prediction.

Different scientific questions imply different splitting strategies. The question "Can this model generalize to new loci in the same cell type?" calls for locus or chromosome-based splits. The question "Can it generalize to new cell types?" requires cell-type splits. The question "Can it generalize to different populations or clinical settings?" demands ancestry and cohort splits. Matching the split to the intended use case is essential for meaningful evaluation.

### Types of Leakage

Leakage arises when information about the test set sneaks into training, inflating apparent performance without improving real-world generalization. Several forms of leakage are common in genomics.

Duplicate or near-duplicate sequences across splits can occur when overlapping windows around the same variant appear in both training and test sets. In regulatory genomics, if training windows overlap with test windows by even 100 base pairs, sequence similarity may allow models to effectively memorize test examples through their training neighbors. Careful attention to window boundaries and minimum separation distances between train and test regions is required.

Shared individuals or families across train and test can happen when different cohorts containing related individuals are combined without careful deduplication. Even distant relatives share genomic segments identical by descent, and models can exploit this structure if related individuals appear in both training and test partitions. Pedigree-aware splitting or explicit removal of relatives below a kinship threshold is necessary to prevent this leakage.

Benchmark construction leakage occurs when evaluation labels are derived from resources that also guided model design or pretraining. For example, if a foundation model is pretrained on all publicly available chromatin data including ENCODE, and then evaluated on ENCODE-derived benchmarks, the pretraining has seen information about the test distribution even if the exact test examples were held out. This subtle form of leakage is difficult to avoid entirely but should at least be acknowledged and quantified when possible.

Hyperparameter tuning leakage results from repeatedly evaluating on the test set while choosing checkpoints or model configurations, gradually overfitting to the test distribution. Best practice maintains a completely untouched final test set, using only training and validation data for all model development decisions. When iterative model development requires feedback, the validation set should be used for intermediate decisions, with the test set reserved for final reporting only.

The practical takeaway is straightforward in principle but demanding in practice: always define the split to match the generalization you care about, then audit carefully for potential linkage and dataset overlap. This often requires detailed provenance tracking of where every training and test example originated and whether any pathways exist for information to flow from test back to training.

### Robustness and Distribution Shift

Robustness is evaluated by deliberately shifting the data distribution beyond what the model encountered during training. Technical shifts involve new sequencing platforms, different coverage levels, or altered assay protocols. A model trained on Illumina short-read data may perform poorly on PacBio long-read data if it has learned platform-specific artifacts. Similarly, models trained on high-coverage whole genome sequencing may degrade substantially when applied to lower-coverage exome sequencing.

Biological shifts involve new species, tissues, disease subtypes, or ancestry groups not represented in training. Cross-species evaluation tests whether regulatory logic learned from human data transfers to mouse or other model organisms. Cross-tissue evaluation tests whether a model trained on blood and brain can generalize to liver or kidney. Cross-ancestry evaluation tests whether patterns learned predominantly from European populations apply to African, East Asian, or admixed individuals.

Clinical shifts involve new hospitals, different care patterns, or later time periods with evolving patient populations and medical practices. A risk model trained on academic medical center data may perform differently in community hospitals with different patient demographics and care protocols. Temporal validation, where models trained on earlier time periods are evaluated on later ones, can reveal degradation due to changes in diagnostic coding, treatment guidelines, or population health trends.

Robustness evaluations typically involve training on one platform or cohort and testing on another, comparing performance across subgroups such as ancestry-stratified AUROC, and stress-testing models under label noise or missing data. These experiments often reveal that performance on curated, independently and identically distributed benchmarks overestimates usefulness in messy real-world settings, especially for high-stakes clinical decisions.

A model that performs well on curated benchmarks may still struggle in real-world deployment for several reasons. Population diversity issues arise when training corpora underrepresent certain ancestries, leading to biased variant scoring (@sec-data, @sec-confound). Assay heterogeneity means that experimental conditions, laboratories, and technologies in deployment differ from the curated datasets used in training. Different labs use different antibodies for ChIP-seq, different enzymes for ATAC-seq, and different sequencing depths, all of which can affect the mapping between sequence and measured regulatory activity. Phenotypic complexity reflects the reality that many clinically relevant phenotypes involve long causal chains from variant to molecular consequence to tissue-level effect to disease, and models may capture only part of this cascade.

For these reasons, genomic model evaluation increasingly includes cross-population robustness testing, out-of-distribution evaluation on new tissues, cell types, or species, and end-to-end assessments on clinically relevant endpoints often combined with traditional statistical genetics tools. Reporting performance stratified by potential sources of distribution shift has become expected practice, particularly for models intended for clinical deployment.

## Evaluating Foundation Models: Zero-Shot, Probing, and Fine-Tuning

Genomic foundation models (@sec-foundation) complicate evaluation because there are multiple ways to use them, each testing different aspects of the learned representations. The appropriate evaluation regime depends on the downstream application and available resources.

### Zero-Shot and Few-Shot Evaluation

In zero-shot settings, we apply the pretrained model without any task-specific training. Examples include using masked-token probabilities to rank variants by predicted deleteriousness and using embedding similarities to cluster sequences or annotate motifs. Evaluation in this regime focuses on how well these raw scores correlate with functional or clinical labels and whether few-shot adaptation with small linear heads trained on limited labeled data already yields strong performance.

Zero-shot performance serves as a stress test of representation quality and inductive biases. Strong zero-shot performance suggests that the pretraining objective has captured biologically relevant structure that transfers without explicit supervision. For example, if masked language model probabilities on sequence variations correlate strongly with experimentally measured variant effects without any fine-tuning, this indicates that the model has internalized functional constraints during pretraining.

Weak zero-shot performance combined with strong fine-tuned performance suggests that pretraining provides useful initialization but the learned representations are not directly interpretable for the task. This pattern is common when the pretraining objective (for example, next-token prediction) differs substantially from the downstream task (for example, clinical pathogenicity classification). The representations may still be useful as features for subsequent learning, but they do not encode the target concept directly.

Few-shot evaluation examines how quickly models can adapt to new tasks with minimal labeled data. This is particularly relevant for rare diseases, underrepresented populations, or novel assays where large labeled datasets are impractical. If a foundation model can achieve competitive performance with 100 labeled examples while a model trained from scratch requires 10,000 examples, this demonstrates substantial practical value even if the final saturated performance is similar.

### Probing and Linear Evaluation

A common evaluation pattern freezes the foundation model, extracts embeddings for sequences, variants, or loci, and trains simple probes such as linear models or shallow MLPs on downstream labels. This approach isolates the usefulness of learned representations from the model's capacity to adapt during fine-tuning.

Key evaluation questions in the probing regime include how much label efficiency is gained compared to training from scratch, how stable probe results are across random seeds and small dataset variations, and whether probes perform well across diverse tasks or only on those similar to the pretraining objectives. Linear probing provides a clean measure of how much useful information is linearly decodable from model representations.

Layer-wise probing analysis can reveal how information is organized within the model. Early layers of a genomic transformer might encode local motifs and k-mer statistics, while deeper layers encode more abstract patterns like regulatory grammar or evolutionary constraints. Observing which layers are most informative for which tasks provides insight into the model's internal representations and can guide feature extraction for downstream applications.

Probing also enables diagnosing failure modes. If a task requires information that should be present in the training data but probes fail to decode it, this suggests either that the pretraining objective did not incentivize learning that information or that it is encoded in a non-linear or distributed way that simple probes cannot access. This diagnostic capability makes probing valuable both for understanding models and for improving them.

### Full Fine-Tuning and Task-Specific Heads

For high-value tasks, practitioners often fine-tune the foundation model end-to-end, adding task-specific heads for classification, regression, or ranking and adapting to new modalities or clinical contexts. Evaluation then looks similar to classic deep model evaluation but with additional questions specific to the foundation model paradigm.

Transfer versus from-scratch baselines ask whether fine-tuning a foundation model meaningfully outperforms training a comparable architecture from scratch on the same downstream data. If the fine-tuned foundation model and from-scratch baseline converge to similar performance given sufficient data, the primary benefit of pretraining is data efficiency rather than improved final performance. This distinction matters for resource allocation: if labeled data are abundant, pretraining may offer limited advantage, but if labeled data are scarce, pretraining can be essential.

Catastrophic forgetting asks whether fine-tuning degrades performance on other tasks, and whether that degradation matters for the intended use. A model fine-tuned aggressively on pathogenicity prediction might lose the ability to predict splicing effects or regulatory activity. If the deployment scenario requires multi-task performance, techniques to mitigate forgetting such as multi-task fine-tuning or parameter-efficient adaptation become important.

Robustness and fairness ask whether foundation model features inherit or amplify biases present in the pretraining data or introduced during fine-tuning. If pretraining data are dominated by European ancestry samples, fine-tuning on a more diverse dataset may not fully overcome the ancestral imbalance in the learned representations. Explicit evaluation of performance across demographic groups is necessary to detect and mitigate these issues.

Across all evaluation regimes, it is helpful to report absolute performance, the delta compared to strong baselines, and data efficiency curves showing how performance varies with the amount of labeled data. This comprehensive reporting reveals whether pretraining provides genuine benefit or merely matches well-tuned task-specific models. Data efficiency curves are particularly informative: plotting performance as a function of training set size often shows that foundation models achieve with 1% of the data what from-scratch models require 10% or more to achieve, even when both eventually converge to similar asymptotic performance.

## Uncertainty, Calibration, and Reliability

Metrics like AUROC summarize ranking quality but say little about how trustworthy individual predictions are. For many applications, especially those involving clinical decisions, we care not only about whether the model is correct on average but also about whether its confidence estimates are meaningful.

Calibration refers to the property that predicted probabilities match observed frequencies. A variant scored at 0.8 probability of being pathogenic should truly be pathogenic about 80% of the time. Well-calibrated models support rational decision-making because the probability scores can be interpreted at face value. Poorly calibrated models, even if they rank examples correctly, provide misleading confidence estimates that can lead to inappropriate decisions.

The distinction between epistemic and aleatoric uncertainty is also important. Epistemic uncertainty arises from limited data and could in principle be reduced by gathering more training examples. A model might express high epistemic uncertainty about variants in an underrepresented ancestral population simply because the training data contained few similar examples. Aleatoric uncertainty reflects inherent noise in the problem and cannot be reduced by additional data. Measurement noise in assays, stochastic biological processes, and incomplete penetrance of genetic variants all contribute to aleatoric uncertainty.

Models that can distinguish these uncertainty types provide more actionable predictions, flagging cases where more data might help versus cases where uncertainty is irreducible. Ensemble methods, Bayesian neural networks, and other uncertainty quantification techniques can decompose total predictive uncertainty into epistemic and aleatoric components, though these decompositions are approximate and depend on modeling assumptions.

Selective prediction or abstention allows models to say "I don't know" when confidence is low, focusing predictions on cases where the model is reliable. This capability is particularly valuable in clinical settings where the cost of errors is high. A variant interpretation tool that abstains on 20% of variants but achieves 99% accuracy on the remaining 80% may be more useful than a tool that attempts to classify all variants at 90% accuracy, because the high-confidence predictions can be trusted while the abstained cases are flagged for manual review.

Evaluation tools for uncertainty and calibration include reliability diagrams that plot predicted probabilities against observed frequencies, Brier scores that combine calibration and discrimination in a single metric, and calibration curves stratified by subgroup to identify differential calibration across ancestry, sex, or clinical site. Coverage versus accuracy curves for selective prediction show how accuracy changes as the model restricts predictions to increasingly confident cases: if the model predicts only on the 50% most confident samples, how accurate is it?

Reliability diagrams are constructed by binning predictions into intervals (for example, 0 to 0.1, 0.1 to 0.2, and so on), computing the mean predicted probability and empirical frequency within each bin, and plotting one against the other. A perfectly calibrated model produces points along the diagonal. Systematic deviations reveal patterns of over-confidence (predictions above the diagonal) or under-confidence (predictions below the diagonal).

Expected calibration error (ECE) provides a scalar summary by computing the weighted average absolute difference between predicted probabilities and empirical frequencies across bins. Lower ECE indicates better calibration. However, ECE is sensitive to bin size and binning strategy, so it should be reported alongside reliability diagrams for interpretability.

For clinical risk models, @sec-clinical covers calibration and uncertainty in more depth. For variant-centric tasks, similar tools apply to pathogenicity probabilities or fine-mapping posteriors, which must be interpreted cautiously in light of confounders discussed in @sec-confound. Even well-calibrated models can give misleading risk estimates if the training data systematically differ from the deployment population in ways that affect the relationship between features and outcomes.

## Benchmark Suites and Their Limitations

Standardized benchmark suites, detailed in @sec-benchmarks, have accelerated progress by enabling direct comparison of models under controlled conditions. However, benchmark-centric evaluation culture has well-documented pitfalls that require careful navigation.

**Overfitting to benchmarks** occurs when models are tuned aggressively on a small panel of tasks, achieving impressive headline numbers while degrading on tasks outside the benchmark. This is particularly problematic when the same benchmark is used for both model development and final evaluation, creating incentives to optimize for benchmark-specific quirks rather than general capability.

**Narrow task coverage** is common. A model that achieves state-of-the-art performance on chromatin benchmarks may perform no better than baselines on splice site prediction or pathogenicity classification. Relying solely on benchmark rankings from any single suite without examining breadth of capability can give a misleading picture. This is why @sec-benchmarks organizes evaluation by multiple levels: molecular, variant, and trait.

**Misaligned incentives** can emerge when the community prizes fractional improvements in AUROC over more important but harder-to-measure gains in robustness, calibration, or fairness. A model that improves AUROC from 0.89 to 0.90 on a saturated benchmark may receive more attention than a model that maintains 0.88 AUROC while dramatically improving calibration, cross-ancestry performance, and uncertainty quantification. Yet the latter may be far more valuable for real-world deployment.
## Putting It All Together: An Evaluation Checklist

When designing or reviewing an evaluation for a genomic model, walking through a systematic checklist can help identify gaps and potential problems.

The first question concerns the **level of decision**. Is the model intended for molecular assay design, variant prioritization, patient risk stratification, or clinical action? The answer should determine which metrics are reported and how they are interpreted. Enrichment metrics make sense for variant ranking. Net benefit matters for clinical decisions. Choosing metrics aligned with the actual decision context ensures that evaluation measures what matters.

The second question concerns **baselines**. What are the comparison points? Strong non-deep baselines like logistic regression and classical polygenic scores establish floors that any sophisticated model should exceed. Prior deep models such as DeepSEA, SpliceAI, Enformer, and earlier foundation models establish the relevant state of the art. Reporting both absolute performance and gains over these baselines provides necessary context. A model achieving 0.85 AUROC might represent substantial progress if baselines are at 0.70, or minimal progress if baselines are at 0.83.

The third question concerns **split design**. Are individuals, loci, genes, assays, and ancestries appropriately separated between training and test sets? Is there any plausible path for leakage or circularity? These questions require careful auditing of data provenance and split construction. Documenting exactly how splits were constructed and what overlap checks were performed builds confidence in evaluation validity.

The fourth question concerns **robustness**. How does performance vary across cohorts, ancestries, platforms, and time? How does the model behave under label noise or missing data? Robustness evaluations reveal whether benchmark performance translates to real-world utility. Reporting stratified metrics by subgroup and evaluating on external datasets from different sources tests whether apparent performance generalizes beyond the specific training distribution.

The fifth question concerns **uncertainty and calibration**. For probabilistic outputs, are calibration and decision-level trade-offs reported? Are subgroup-specific metrics examined to identify differential performance across populations? Models deployed in high-stakes settings require not just good average performance but reliable confidence estimates that support appropriate action.

The sixth question concerns **usage regimes for foundation models**. How does the model perform in zero-shot, probing, and fine-tuning settings? Does pretraining help when labeled data are scarce, as measured by data efficiency curves? Understanding where the value of pretraining comes from (better final performance, faster convergence, improved data efficiency) helps determine whether the investment in large-scale pretraining is justified.

The seventh question concerns the **story beyond the benchmark**. Does improved performance actually change downstream decisions or experimental design? For models intended for clinical deployment, are there plans for prospective or interventional evaluation? The ultimate test of model utility is whether it enables better science or better care, not just higher numbers on a leaderboard.

This checklist is not exhaustive but covers the most common evaluation pitfalls in genomics. Working through it systematically at the design stage can prevent problems that are difficult or impossible to fix after the fact. Reviewers and readers can use the same checklist to critically evaluate published work.

## Looking Forward

This chapter has provided a framework for thinking about evaluation across the full range of genomic models. The subsequent chapters flesh out specific aspects of reliability that evaluation alone cannot address.

@sec-confound examines confounders, bias, and fairness in detail, showing how evaluation can mislead when data are structured in problematic ways. Population stratification, batch effects, label circularity, and benchmark leakage can all create illusions of performance that evaporate in deployment. Understanding these failure modes is essential for interpreting evaluation results critically.

@sec-interp focuses on interpretability and mechanisms, turning models from black boxes into sources of testable biological hypotheses. When evaluation shows that a model works, interpretability helps us understand why it works and whether the reasons are biologically meaningful or artifacts of confounded data.

Together, these chapters aim to equip readers with the critical perspective needed to engage with the emerging literature on genomic foundation models. The question is never simply "what is the AUROC?" but rather "what has really been demonstrated, and how much should we trust it?" With careful attention to evaluation design, data splitting, robustness testing, and calibration assessment, we can distinguish models that represent genuine advances from those that merely perform well on convenient benchmarks.