# Pretraining Strategies {#sec-pretraining}

The choice of pretraining objective is not merely technical; it encodes assumptions about what matters in biological sequence. Masked language modeling encourages bidirectional context integration: the model learns to predict missing tokens using information from both upstream and downstream sequence. Next-token prediction builds autoregressive capabilities: the model learns to generate sequence one position at a time, enabling design of novel proteins or regulatory elements. Contrastive learning teaches invariance: the model learns that functionally equivalent sequences should map to similar representations regardless of species or polymorphism. Each objective produces a different model, and those differences propagate to downstream performance. A model pretrained with masked language modeling may excel at variant effect prediction (where context on both sides matters) but struggle at sequence generation. A model pretrained for generation may produce plausible sequences but provide representations less suited for classification tasks. Understanding what each objective teaches, and what assumptions each encodes, is prerequisite to selecting the right foundation model for a given application.

Self-supervised pretraining addresses a fundamental asymmetry in genomic data. Reference genomes span billions of nucleotides across thousands of species. Population sequencing projects catalog genetic variation in millions of individuals. Functional genomics consortia measure chromatin accessibility and gene expression across hundreds of cell types. Yet experimental labels remain sparse: for any given sequence, we typically lack direct measurements of its regulatory function, its effect on splicing, or its contribution to disease risk. Self-supervised objectives extract training signal from the sequences themselves, without requiring experimental labels. The resulting models learn representations that capture evolutionary constraints, sequence grammar, and functional relationships, all from the patterns present in unlabeled data. When these representations are applied to downstream tasks with limited labels, the pretrained knowledge makes scarce labeled data go further.

This chapter examines the major pretraining strategies employed by genomic foundation models, from masked language modeling in DNABERT and Nucleotide Transformer through autoregressive objectives in generative models. The focus is on understanding what each objective teaches the model to learn, what biological patterns emerge from each training approach, and how objective choice shapes downstream capabilities. The goal is not encyclopedic coverage of every pretraining variant but principled understanding of the design space: which objectives suit which applications, what tradeoffs each involves, and how to reason about novel objectives as the field continues to develop.

::: {#fig-pretraining-objectives layout-ncol=3}
![**FIGURE PLACEHOLDER A**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20A)

![**FIGURE PLACEHOLDER B**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20B)

![**FIGURE PLACEHOLDER C**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20C)

[Essential] Three-panel schematic comparing major pretraining objectives on the same input sequence. Panel A (MLM): Sequence with masked positions (shown as [MASK]), bidirectional context arrows from both sides, prediction targets at masked positions. Panel B (Next-token prediction): Causal/unidirectional arrows, each position predicting the next, with sampling process for generation. Panel C (Contrastive): Anchor sequence, positive pair (augmented version), negative samples, mapped to embedding space with distance relationships.
:::


## Masked Language Modeling

Consider predicting whether a splice site variant in *DMD* will cause exon skipping in Duchenne muscular dystrophy. The model must recognize the canonical GT-AG splice signals, understand how flanking sequences modulate splicing efficiency, and integrate information from both the upstream exon and downstream intron. A model trained only on labeled splice variants would see perhaps a few hundred *DMD* examples across the entire clinical literature. A model pretrained on billions of nucleotides learns splice grammar across the entire genome, then applies that knowledge to the specific clinical question. **Masked language modeling** provides this pretraining by teaching models to predict missing sequence content from surrounding context, and the bidirectional attention it requires captures exactly the upstream-downstream integration that splice prediction demands.

MLM treats sequences as partially observed and trains models to reconstruct missing content. The procedure is straightforward: randomly mask portions of an input sequence, feed the corrupted sequence to the model, and train the model to predict the original tokens at masked positions. A **masking strategy** replaces selected tokens with a special `[MASK]` token, leaving the surrounding context intact. The model processes the masked sequence through its layers and produces predictions for the masked positions, typically optimizing cross-entropy loss over the vocabulary at each masked location.

The key insight is that accurate prediction requires learning genuine sequence structure. To predict a masked position in a transcription factor binding site, the model must recognize the surrounding motif context. To predict masked splice donor sequences, the model must encode the consensus GT dinucleotide and the flanking patterns that modulate splicing strength. Over millions of training examples, models build distributed representations of motifs, compositional rules, and sequence constraints that transfer to tasks never seen during pretraining. The *DMD* splice variant can be evaluated using patterns learned from every splice site in the genome.

MLM encourages bidirectional context integration, and this bidirectionality has direct clinical relevance. Unlike autoregressive models that condition only on preceding tokens, MLM models see both left and right context when predicting masked positions. For genomics, this matches biological reality: regulatory function depends on patterns both upstream and downstream of any given position. A transcription factor binding site is recognized through flanking sequences on both sides. Splicing signals require coordination between donor and acceptor sites separated by hundreds of bases. Missense variants disrupt protein function through effects that depend on the entire domain context, not just the preceding amino acids. The bidirectional attention in MLM naturally captures these dependencies.

### Masking Strategies and Their Implications

::: {#fig-masking-strategies layout-ncol=2}
![**FIGURE PLACEHOLDER A**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20A)

![**FIGURE PLACEHOLDER B**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20B)

[High] Two-panel visualization comparing masking strategies on a regulatory sequence containing a TF binding motif. Panel A (Random token masking): Individual tokens masked throughout, including partial motif masking; show how local context allows easy prediction. Panel B (Span masking): Entire motif region masked as contiguous span; show how prediction requires reasoning from distal regulatory context. Include attention patterns for each, showing how span masking forces longer-range dependencies.
:::


Predicting whether a regulatory variant disrupts an entire transcription factor binding site or merely alters its affinity requires models that learn compositional patterns, not just local nucleotide statistics. The tension between local and compositional learning plays out in masking strategy design.

Random masking of individual tokens creates predictions that are relatively local: each masked position can often be inferred from immediately adjacent nucleotides. This approach is efficient but may not force models to learn higher-order structure. **Span masking**, which masks contiguous blocks of tokens, forces models to infer longer-range dependencies and compositional patterns. If an entire transcription factor binding motif is masked, the model cannot rely on partial motif information and must instead recognize the motif's role from surrounding regulatory context. For clinical variant interpretation, span masking may better capture the compositional grammar that determines whether a regulatory variant disrupts an entire binding site or merely modulates its affinity.

Masking rates present a fundamental tradeoff between supervision density and prediction difficulty. Higher masking rates (30-40% of tokens) provide more supervision per sequence but make prediction harder and may destabilize training. Lower masking rates (10-15%) produce more stable training but require more data to achieve equivalent coverage. The standard 15% rate from BERT [@devlin_bert_2019] represents a reasonable compromise, though genomic models have explored values ranging from 10% to 40% depending on context length and tokenization granularity. *DNABERT* used 15% masking on 6-mer tokens [@ji_dnabert_2021], while later models have experimented with adaptive masking rates that increase as training progresses, starting conservatively and becoming more aggressive as the model's predictions improve.

Tokenization interacts with masking in ways that affect what biological patterns models learn. *DNABERT* pioneered MLM for genomic sequences by applying it to overlapping k-mer tokens: rather than treating DNA as individual nucleotides, *DNABERT* tokenizes sequences into all possible 6-mers with overlapping windows [@ji_dnabert_2021]. Masking then operates at the k-mer level, with entire 6-mers masked as units. This design encourages learning of k-mer level patterns corresponding to transcription factor binding motifs (typically 6-12 base pairs) and other short functional elements. *DNABERT-2* adopted **byte-pair encoding** tokenization, which learns a vocabulary of variable-length subword units from the training corpus [@zhou_dnabert2_2023]. BPE tokens represent single nucleotides, common motifs, or repeated elements depending on their frequency. MLM with BPE balances flexibility with compositional structure, though the learned vocabulary may not align with biological functional units in interpretable ways.

The design decisions explored by *DNABERT* and *DNABERT-2* established patterns that subsequent DNA language models have built upon and refined. @sec-dna-lm examines how these architectural and tokenization choices have evolved as the field has scaled to longer contexts and larger training corpora.

### What Masked Language Models Learn

MLM objectives drive models to capture multiple levels of sequence organization, from local nucleotide statistics to long-range regulatory grammar. At the lowest level, models learn base composition and local constraints: CpG dinucleotide frequencies, GC content biases, and simple repeat patterns. These basic properties are necessary but not sufficient for biological function prediction.

At higher levels, MLM captures motif patterns and sequence grammar. Predicting masked positions in regulatory regions requires recognizing transcription factor binding sites, understanding how motifs combine in enhancers and promoters, and learning context-dependent usage patterns. If certain transcription factor motifs co-occur at specific distances (as they do in developmental enhancers where factors like *HOX* proteins bind cooperatively), masking one motif and predicting it from the other reinforces this grammatical relationship. This compositional learning is difficult to achieve with supervised learning alone, which typically provides coarse binary labels ("enhancer" versus "non-enhancer") rather than fine-grained structural information about sequence organization.

MLM also captures evolutionary conservation patterns implicitly, and this has direct relevance for clinical variant interpretation. Conserved sequences are constrained because mutations would disrupt function. By learning to predict conserved patterns from surrounding context, models encode which sequence features are under selection. This knowledge transfers to variant effect prediction, where the model recognizes when a mutation disrupts a learned conserved pattern. A variant that replaces a highly predictable position (one the model confidently fills in during MLM) is more likely to be damaging than one at a position where the model is uncertain. The connection between pretraining on raw sequence and downstream variant interpretation illustrates how self-supervised objectives capture biologically meaningful structure without explicit functional labels.


## Next-Token Prediction

Designing a novel promoter sequence for gene therapy requires generating DNA that respects learned regulatory grammar while achieving specific expression characteristics. Masked language modeling can evaluate whether a candidate sequence looks "natural," but it cannot generate sequences from scratch. A gene therapy team optimizing a CAR-T construct needs promoter variants to test; they cannot simply evaluate candidates one by one when the search space spans $4^{500}$ possible 500-base-pair sequences. Next-token prediction provides the generative capability missing from MLM, learning to predict each token given only preceding tokens and thereby acquiring the ability to sample coherent novel sequences that respect learned biological constraints.

**Next-token prediction** represents an alternative paradigm where models learn to predict each token in a sequence given only the preceding tokens. This **autoregressive** approach, popularized by GPT-style language models, treats sequence generation as a core capability rather than a secondary feature. For a sequence of length $T$, the model predicts token $t$ from tokens $1$ through $t-1$, maximizing the likelihood of the observed sequence under the model's learned distribution. The probability of a sequence factors as the product of conditional probabilities for each token given its predecessors:

$$P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^{T} P(x_t | x_1, \ldots, x_{t-1})$$

Algorithmically, next-token prediction requires **causal masking** in the attention mechanism. Each position attends only to earlier positions, ensuring predictions at position $t$ depend exclusively on positions $1$ through $t-1$. The loss function is cross-entropy over the vocabulary, computed at every position rather than only at masked locations. During training, **teacher forcing** allows efficient parallel computation: the model predicts all positions simultaneously by feeding in the ground truth sequence shifted by one position. Generation at inference time is inherently sequential, predicting one token at a time and conditioning each prediction on all previous outputs.

The fundamental difference from MLM lies in what the model can see during prediction. Autoregressive models build representations from unidirectional context, learning to generate sequences that respect learned constraints. This makes autoregressive pretraining attractive for sequence design applications. Sampling new sequences proceeds naturally: predict the first token, condition on it to predict the second, and continue token by token. The generation process directly uses the learned conditional distributions without requiring additional architectural modifications or iterative refinement procedures.

### Genomic Applications

DNA sequences present a complication that natural language does not: they have no inherent directionality. Both strands encode information, and regulatory function is often strand-agnostic. A transcription factor binding site functions identically whether read 5'-to-3' or on the reverse complement strand. This contrasts with natural language, where left-to-right reading order carries meaning. Early autoregressive genomic models addressed this by training separate models for forward and reverse strands or by augmenting training data with reverse-complement sequences. More recent approaches treat strand symmetry as an architectural constraint, ensuring that forward and reverse complement sequences produce equivalent representations through weight sharing or explicit symmetrization.

*Evo* represents a large-scale autoregressive genomic model trained on whole genomes with long-context architectures [@nguyen_sequence_2024]. Using StripedHyena layers to achieve contexts exceeding 100 kilobases, *Evo* learns long-range dependencies including gene structure, repeat organization, and regulatory architecture spanning tens of kilobases. This enables generating coherent synthetic genomes that respect higher-order structure, not just local motif patterns. For therapeutic applications, *Evo*'s generative capability could design synthetic regulatory circuits, generate diverse candidate sequences for directed evolution, or produce training data through synthetic augmentation when real labeled data is scarce.

Protein sequence models benefit from autoregressive pretraining with clearer biological justification. The N-terminus to C-terminus directionality of protein synthesis provides a natural left-to-right ordering: ribosomes translate mRNA sequentially, and co-translational folding means that protein structure emerges progressively from the N-terminus. *ESM* models and protein design systems like *ProtGPT2* predict amino acid sequences autoregressively, learning protein grammar and evolutionary constraints that transfer to structure prediction and function annotation [@ferruz_protgpt2_2022]. For designing therapeutic proteins (antibodies, enzymes, peptide drugs), autoregressive generation produces candidates that respect learned constraints on foldability and function.

### Comparing MLM and Autoregressive Objectives

::: {#fig-bidirectional-vs-autoregressive layout-ncol=2}
![**FIGURE PLACEHOLDER A**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20A)

![**FIGURE PLACEHOLDER B**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER%20B)

[Essential] Two-panel figure showing information flow during prediction. Panel A (MLM): Position in center of sequence with arrows coming from BOTH left and right context, showing bidirectional conditioning. Annotate: "Sees full context → better for understanding." Panel B (Autoregressive): Same position with arrows only from left (preceding tokens), showing causal restriction. Annotate: "Sees only past → enables generation."
:::



The tension between bidirectional understanding and generative capability represents the fundamental tradeoff between these objectives. For tasks requiring understanding of full sequence context, MLM's bidirectional attention provides richer representations. Predicting transcription factor binding at a specific location benefits from seeing both upstream and downstream sequence, information that autoregressive models cannot access during inference. Variant effect prediction similarly benefits from full context: a missense variant's impact depends on the entire domain, not just the preceding residues.

Autoregressive models offer more principled generation. Their sequential prediction structure matches the generation process exactly, whereas generating from MLM models requires iterative masking and filling procedures that were not part of pretraining. A promoter design task using MLM would require starting with random sequence, masking positions, predicting fills, remasking, and iterating until convergence. This procedure is ad hoc and may not produce sequences that lie on the learned distribution. Autoregressive generation is direct: sample token by token from learned conditionals.

Training efficiency differs between objectives in ways that affect practical decisions. MLM predicts only 15% of tokens per sequence but uses bidirectional context for each prediction. Autoregressive models predict all tokens but with unidirectional context. The effective supervision per sequence is higher for autoregressive training, but each prediction is less informed. For fixed compute budgets, the tradeoffs roughly balance, with optimal choice depending on downstream applications rather than training efficiency alone.

Task-specific performance depends on alignment between pretraining and downstream objectives. If the downstream task involves predicting missing information from context (variant effect prediction, binding site identification, conservation scoring), MLM pretraining provides better transfer. If the downstream task involves generation or sequential decision-making (sequence design, sampling from conditional distributions, therapeutic protein generation), autoregressive pretraining aligns more naturally. For applications requiring both understanding and generation, hybrid architectures that combine bidirectional encoding with autoregressive decoding offer a middle ground, though these add complexity.


## Span Corruption and Denoising

Clinical variant interpretation must be robust to sequencing errors, population polymorphisms, and batch effects between discovery and validation cohorts. A pathogenic variant identified in a research study must remain classifiable as pathogenic when sequenced on a different platform in a clinical laboratory, surrounded by different technical artifacts and population-specific polymorphisms. A model trained only on pristine reference sequence may fail when encountering the noise and variation present in real patient data. **Denoising objectives** address this by training models on corrupted inputs, building tolerance to the kinds of perturbations that occur in clinical genomics pipelines.

**Span corruption** generalizes masked language modeling by introducing more complex forms of input degradation. The T5 model popularized this approach for natural language [@raffel_exploring_2020], and the principles transfer to genomic sequences with biological adaptations. Rather than masking individual tokens, span corruption masks contiguous spans of variable length and replaces each span with a single sentinel token. The model then generates the original content of all masked spans in sequence, learning to reconstruct substantial missing regions rather than isolated positions.

This objective teaches different aspects of sequence structure than standard MLM. Reconstructing entire spans requires understanding longer-range dependencies and compositional patterns. If a span encompasses an entire transcription factor binding motif (typically 6-12 base pairs), the model cannot infer the motif from partial information and must instead reason about the motif's role from surrounding regulatory context. Span lengths are typically sampled from a distribution (geometric or uniform) with a mean around 3-5 tokens, creating a mix of short and long reconstruction challenges within each training example.

Denoising objectives extend beyond masking to include other forms of corruption that mirror real-world data degradation. **Token substitution** replaces input tokens with random tokens from the vocabulary, creating corrupted sequences that resemble sequencing errors or natural variation. The model learns to distinguish correct from incorrect tokens based on surrounding context, encouraging representations that capture local consistency and motif structure. **Deletion and insertion corruptions** remove or add tokens at random positions, teaching models about position-invariant features that remain identifiable despite surrounding changes. For genomics, insertions and deletions are biologically realistic mutation types (indels account for approximately 15% of pathogenic variants in ClinVar [@landrum_clinvar_2018]), and models that handle them during pretraining may better predict their effects downstream.

### Biologically Motivated Corruption

The most effective corruption strategies mirror actual sources of noise in clinical genomics data. Simulating sequencing errors provides corruption strategies that match experimental reality. Base miscalls follow platform-specific patterns: Illumina sequencing shows characteristic substitution biases (favoring certain nucleotide transitions over transversions, with error rates of 0.1-1% depending on read position and quality score), while nanopore sequencing exhibits distinct error profiles concentrated in homopolymer regions where the signal for consecutive identical bases becomes ambiguous. Training with corruptions that mimic these error patterns may improve generalization to real sequencing data with platform-specific artifacts.

**Variant augmentation** introduces biologically realistic sequence changes based on population variation. Randomly substituting alleles at known polymorphic sites or injecting variants from databases like gnomAD [@karczewski_mutational_2020] creates corrupted sequences reflecting natural genetic diversity. This teaches models that common polymorphisms are normal variation rather than errors to be corrected, potentially improving robustness for variant effect prediction where distinguishing pathogenic variants from benign polymorphisms is the central challenge. A model trained only on reference sequence might flag any deviation as potentially damaging; a model trained with variant augmentation learns which deviations are within normal population variation.

Structural variation simulation models larger-scale genomic changes: tandem duplications, copy number variation, and segmental rearrangements. These corruptions are harder to implement but capture realistic sources of genomic diversity beyond single-nucleotide changes. Models trained with structural variation corruptions may better understand how gene dosage changes, enhancer duplications, or domain boundary disruptions affect function. For clinical applications involving copy number variants (which underlie conditions ranging from developmental disorders like DiGeorge syndrome to cancer predisposition in hereditary breast cancer), this training signal could improve predictive accuracy.

The benefit of denoising pretraining extends to robustness under distribution shift. If downstream applications involve sequences from different populations, experimental platforms, or tissue contexts than the pretraining corpus, models pretrained with appropriate corruptions can maintain performance despite distribution mismatch. This matters in clinical genomics, where validation cohorts often differ from discovery cohorts in ancestry composition, sequencing technology, or phenotyping protocols. A model trained with corruptions spanning these sources of variation generalizes more reliably than one trained only on pristine reference sequence.


## Contrastive Learning

Cross-population generalization presents a persistent challenge in clinical genomics. A variant classifier trained on European ancestry cohorts may perform poorly on African ancestry patients due to different patterns of linkage disequilibrium and background polymorphism. The classifier learned to recognize pathogenic variants against a European genetic background; African genomes present the same functional variants but surrounded by different neutral polymorphisms. **Contrastive learning** addresses this by teaching models to recognize functional equivalence despite sequence-level differences, producing representations where a regulatory element is recognizable regardless of the population-specific variants surrounding it.

Contrastive learning takes a fundamentally different approach to self-supervised pretraining than reconstruction-based objectives. Rather than recovering corrupted inputs, contrastive objectives train models to produce similar representations for different views of the same sequence while distinguishing them from representations of unrelated sequences. The intuition is that augmented versions of a sequence (with minor corruptions, reverse complementation, or variants) should map to nearby points in representation space, while unrelated sequences should map to distant points. This teaches invariance to transformations that do not change function.

The algorithmic framework constructs **positive pairs** and **negative samples**. For a given anchor sequence, positive pairs are created through augmentation: reverse complementation, random cropping, variant injection, or other transformations that preserve functional identity. Negative samples are drawn from other sequences in the training batch. The model produces **embeddings** for all sequences, and the contrastive loss encourages anchor and positive embeddings to be similar (high cosine similarity) while pushing apart anchor and negative embeddings.

**InfoNCE loss** is the most common contrastive objective [@oord_representation_2018]. For an anchor embedding $z_i$ and positive embedding $z_i^+$, InfoNCE maximizes:

$$\mathcal{L} = -\log \frac{\exp(z_i \cdot z_i^+ / \tau)}{\sum_j \exp(z_i \cdot z_j / \tau)}$$

where the sum runs over the positive and all negative samples, and $\tau$ is a temperature parameter controlling the sharpness of the distribution. Lower temperatures make the model more discriminative, requiring cleaner separation between positives and negatives. The objective is equivalent to classifying the positive pair among all possible pairs, and the model learns representations that make this classification easy.

### Augmentation Design for Genomic Sequences

A CTCF binding site must be recognizable whether it appears on a European or African genetic background, whether read on the forward or reverse strand, and whether the surrounding sequence contains common polymorphisms or reference alleles. Augmentation design is critical for contrastive learning because augmentations must preserve functional identity while introducing variability. If augmentations change function, the contrastive objective will learn meaningless invariances. Several augmentation strategies are biologically grounded and preserve the functional relationships that matter for downstream clinical applications.

**Reverse complementation** is the simplest and most reliable augmentation. DNA is double-stranded, and many regulatory elements function identically on either strand. Training the model to treat forward and reverse complement sequences as equivalent captures strand symmetry inherent in molecular biology. This augmentation is universally applicable and introduces no risk of changing functional identity.

**Random cropping** extracts overlapping windows from longer sequences. If a transcription factor binding site appears in multiple cropped windows, the model learns that the binding site is the functionally relevant feature regardless of absolute position or surrounding context. This teaches position-invariant representations useful for tasks where genomic coordinates matter less than local sequence content. A binding site predictor benefits from learning that the core motif is what matters, not its position within the input window.

**Variant injection** introduces common polymorphisms or simulated mutations. If the variants are neutral (common variants from gnomAD with high allele frequency, which are unlikely to be damaging), treating variant and reference sequences as positive pairs teaches robustness to genetic variation. This is particularly valuable for cross-population generalization, where models must recognize functional elements despite surrounding sequence polymorphism that differs between ancestry groups. A model trained with variant augmentation learns that a CTCF binding site is functionally equivalent whether it appears surrounded by European or African background variants.

Negative sampling strategies also affect what models learn. Random genomic sequences provide straightforward negatives but may be too easy to distinguish: any functional regulatory sequence is readily separable from random intergenic sequence. Harder negatives, such as sequences from paralogous genes, pseudogenes, or orthologous regions in distant species, provide more informative supervision that forces the model to learn subtle discriminative features.

### Cross-Species Contrastive Learning

::: {#fig-cross-species-contrastive}
![**FIGURE PLACEHOLDER**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER)

[Enhancing] Illustration of contrastive pretraining using orthologous sequences. Show: Human enhancer sequence and mouse ortholog (aligned, showing nucleotide divergence but conserved functional elements). Both mapped to embedding space as nearby points (positive pair). Non-orthologous sequence mapped to distant point (negative). Include phylogenetic context showing ~75 million years of divergence. Annotate: "Same function despite sequence divergence → learns species-invariant features."
:::

Leveraging evolutionary relationships for self-supervision enables a particularly powerful form of contrastive learning. Orthologous sequences from different species share functional identity despite nucleotide divergence accumulated over millions of years of evolution. Treating orthologous pairs as positives and non-orthologous pairs as negatives teaches the model to extract species-invariant functional features. A human enhancer and its mouse ortholog should map to similar embeddings despite 75 million years of sequence divergence, while unrelated sequences should map to distant embeddings.

This approach has direct implications for drug development and therapeutic translation. Many drug targets are validated in mouse models before human clinical trials; roughly 95% of cancer drugs that succeed in mouse models fail in human trials, often because the models do not adequately capture human biology. A model pretrained with human-mouse contrastive pairs may generalize better to predicting drug response in humans based on mouse efficacy data, or to transferring regulatory circuit designs from model organisms to human cell types. The evolutionary record provides implicit labels about functional equivalence that would be expensive to obtain through direct experimental annotation.

Sequence **embedding** quality improves with contrastive pretraining in ways that benefit clinical applications. Models trained contrastively produce embedding spaces where functionally similar sequences cluster together, enabling nearest-neighbor search for annotating novel variants (finding similar characterized variants), sequence retrieval for identifying regulatory homologs, and unsupervised clustering of regulatory elements. For variant effect prediction, contrastive pretraining improves robustness: if the model learns that sequences differing only by neutral variants are functionally equivalent, it will better distinguish truly disruptive variants from benign polymorphisms.


## Multi-Task Pretraining

Predicting variant pathogenicity requires integrating multiple lines of evidence: evolutionary conservation, protein structure effects, splicing changes, and regulatory disruption. A variant in *TTN* (the gene encoding titin, mutated in 25% of dilated cardiomyopathy cases) might be pathogenic because it disrupts protein folding, because it alters splicing, or because it affects regulatory binding sites. No single assay captures all these dimensions. **Multi-task pretraining** addresses this by jointly optimizing for diverse prediction tasks, learning representations that capture the multiple facets of genomic function relevant to clinical interpretation.

Multi-task pretraining combines multiple related objectives during the same training run, jointly optimizing for several prediction tasks. Different tasks provide complementary supervision signals: masking captures local sequence patterns, chromatin prediction captures regulatory function, conservation scoring captures evolutionary constraint, and expression prediction captures transcriptional consequences. Representations that satisfy all tasks simultaneously develop richer and more general features than any single objective alone.

**Task selection** is the first design decision. Ideally, tasks should be diverse enough to provide distinct supervision signals but related enough to benefit from shared representations. For genomic models, effective combinations include masked language modeling for general sequence structure, chromatin accessibility prediction for regulatory function, gene expression prediction for transcriptional output, evolutionary conservation scoring for functional constraint, and variant frequency prediction from population databases. Each task operates on the same input sequence but predicts different outputs using task-specific head layers. The shared backbone encoder processes the sequence into intermediate representations, and separate prediction heads map these representations to task-specific outputs.

**Task weighting** determines how much each task contributes to the total loss. With $\mathcal{L}_1, \ldots, \mathcal{L}_K$ representing individual task losses, the multi-task loss combines them:

$$\mathcal{L}_{\text{total}} = \sum_{k=1}^K w_k \mathcal{L}_k$$

where $w_k$ are task weights. Equal weighting is simple but may lead to imbalanced learning if tasks have different scales or difficulties. A task with high variance loss may dominate gradient updates, starving other tasks of learning signal. **Dynamic weighting** approaches adjust weights during training based on learning progress, using uncertainty estimation, gradient norms, or task-specific validation performance as signals for rebalancing.

### Large-Scale Multi-Task Examples

*Enformer* exemplifies large-scale multi-task pretraining for genomics [@avsec_effective_2021]. The model predicts over 5,000 genomic assays simultaneously: ChIP-seq signals for hundreds of transcription factors and histone marks, DNase-seq and ATAC-seq accessibility across cell types, CAGE transcription initiation profiles, and more. This massive multi-task objective (covering 674 DNase-seq, 4,675 ChIP-seq, and 638 CAGE experiments from ENCODE and Roadmap Epigenomics [@encode_project_integrated_2012]) forces the model to learn representations capturing diverse regulatory signals.

The task diversity in *Enformer* provides supervision far richer than any single assay. A model trained only on DNase-seq learns general accessibility patterns but misses transcription factor specificity: it cannot distinguish which factors bind to accessible regions. A model trained only on H3K27ac ChIP-seq captures active enhancers but misses repressive marks that indicate silenced regulatory elements. Training on all assays jointly allows the model to disentangle overlapping and complementary signals, learning representations that generalize across regulatory contexts. For clinical variant interpretation, this means *Enformer* can predict how a regulatory variant affects enhancer activity, chromatin state, transcription factor binding, and gene expression simultaneously.

::: {#fig-multitask-pretraining}
![**FIGURE PLACEHOLDER**](https://placehold.co/600x400?text=FIGURE%20PLACEHOLDER)

[High] Diagram showing shared encoder with multiple prediction heads. Structure: Sequence input → Convolutional layers → Transformer layers (labeled "Shared Backbone") → Branching to separate heads for: Chromatin accessibility (DNase-seq, ATAC-seq), Histone modifications (H3K27ac, H3K4me3, etc.), Transcription factor binding (hundreds of factors), Gene expression (CAGE). Annotate approximate task counts from ENCODE: "674 DNase + 4,675 ChIP-seq + 638 CAGE."
:::

*Borzoi* extends this paradigm to full RNA-seq coverage prediction, jointly modeling transcription initiation, splicing, and transcript abundance [@linder_borzoi_2023]. By predicting continuous coverage across gene bodies rather than just expression levels, *Borzoi* captures splicing patterns that are invisible to models predicting only total expression. This has direct clinical relevance: many pathogenic variants act through splicing disruption rather than protein-coding changes, and models that capture splicing patterns can identify variants that traditional expression-based approaches miss.

Combining MLM with functional prediction represents another multi-task configuration. The model predicts masked tokens through a language modeling head while simultaneously predicting chromatin accessibility or other functional readouts through regression heads. This hybrid objective balances sequence-level pretraining with functional supervision. The MLM component ensures the model learns general sequence patterns even in regions without functional annotations (the majority of the genome lacks chromatin or expression measurements in any given cell type), while the functional prediction component focuses learning on biologically relevant features.

### When Multi-Task Helps and When It Hurts

**Task interference** is the primary concern with multi-task learning. If tasks require conflicting representations, jointly optimizing for both may compromise performance on each compared to single-task baselines. In genomics, this might occur if one task benefits from very local features (splice site prediction, which depends on short consensus sequences spanning roughly 10 base pairs) while another requires long-range context (enhancer activity prediction, which depends on distant promoter interactions spanning 100 kilobases). The shared backbone must compromise, potentially learning suboptimal representations for both.

**Negative transfer** occurs when adding a task actually hurts downstream performance compared to training without it. This can happen if the additional task introduces noise (poorly measured assays with high experimental variance), if task weights are poorly balanced (causing one task to dominate gradients), or if the auxiliary task shifts learned representations away from features useful for target applications. The risk of negative transfer increases with task diversity: distantly related tasks are more likely to require conflicting representations.

The benefits of multi-task pretraining are largest when tasks are complementary and data for individual tasks is limited. If chromatin data is sparse for a particular cell type but gene expression data is abundant, jointly training on both may improve performance on both compared to single-task models. The shared representations allow information to flow between tasks, compensating for data scarcity in any single modality. When functional labels exist at scale and tasks are genuinely related, multi-task pretraining consistently outperforms single-task alternatives.


## Data Strategies for Pretraining

Corpus construction establishes the foundation for pretraining and determines what patterns the model can learn. A clinical variant classifier is only as good as the evolutionary and population diversity captured in its pretraining corpus. If the training data underrepresents African genetic variation (African populations harbor more genetic diversity than all other continental populations combined, yet constitute a small fraction of most reference panels), the resulting model will underperform on African ancestry patients. These data decisions have direct consequences for health equity and clinical utility.

**Reference genomes** are the standard starting point. Human genome assemblies like GRCh38 provide high-quality, contiguous sequence spanning all chromosomes (roughly 3.1 billion base pairs of assembled sequence, representing about 92% of the full genome before telomere-to-telomere completion). Training on the reference genome allows models to learn patterns characteristic of human DNA: base composition, repeat structure, gene organization, and regulatory architecture. The reference genome represents a single haploid consensus, missing variation present in human populations, but provides the foundation for most pretraining approaches.

Population-scale variation can be incorporated through variant databases. Rather than training only on reference sequence, injecting variants at observed population frequencies creates synthetic diploid genomes reflecting real genetic diversity. This teaches models that common polymorphisms are normal variation, potentially improving robustness and variant effect prediction. gnomAD provides allele frequencies across over 800,000 individuals spanning diverse ancestries [@karczewski_mutational_2020], enabling population-aware training. **Pan-genome** approaches extend this by representing multiple high-quality assemblies from diverse individuals, capturing structural variation and population-specific haplotypes that a single reference cannot represent.

Repeat handling impacts pretraining in ways that depend on downstream applications. Simple repeats, tandem repeats, and transposable elements occupy roughly half of the human genome but contribute less directly to protein-coding function than unique sequences. **Hard-masking** repeats (replacing them with Ns) reduces training data but may discard information relevant to some tasks; many regulatory elements derive from transposable elements, and some disease-associated repeats (like the CGG expansion in *FMR1* causing Fragile X syndrome, or the CAG expansion in *HTT* causing Huntington disease) are clinically important. **Soft-masking** retains sequence information while marking repetitive regions, allowing models to learn differential representations for repeats and unique sequences.

**Multi-species pretraining** incorporates genomes from model organisms and related species. Including mouse, zebrafish, and other commonly used experimental organisms enables models to learn evolutionary conservation patterns and may improve transfer between species. For therapeutic development that relies on animal model data, multi-species pretraining provides the foundation for cross-species generalization.

**Data augmentation** artificially increases training diversity. Reverse complementation exploits DNA strand symmetry, effectively doubling training data without collecting new sequences. Random cropping extracts variable-length windows, teaching position-invariant features. Variant injection simulates genetic variation, building robustness to population diversity. These augmentations are typically applied on-the-fly during training rather than pre-computed, maintaining flexibility in the training pipeline.


## Optimization and Scaling

Training a model to predict variant effects in genes like *BRCA1* requires not just the right objective but also stable optimization that converges to useful representations. A model that diverges during training or gets stuck in poor local minima will fail clinically regardless of how well-designed its architecture may be. The optimization details that seem merely technical have direct consequences for whether the final model can reliably distinguish pathogenic from benign variants.

**Learning rate warmup** gradually increases the learning rate from near-zero over the first several thousand steps. This prevents early training instability when the model has random initializations and large gradient variance. After warmup, **cosine decay** schedules reduce the learning rate following a cosine curve from peak to near-zero over training, providing aggressive learning early when gradients are most informative and gentle refinement late as the model approaches convergence.

**Gradient clipping** prevents training instability from occasional large gradients. Clipping by global norm scales all gradients proportionally when the total norm exceeds a threshold (typically 1.0), maintaining gradient direction while controlling magnitude. This is standard practice for transformer models where exploding gradients can occur, particularly with long sequences where attention matrices span many positions.

**Mixed precision training** uses lower-precision arithmetic (`float16` or `bfloat16` instead of `float32`) to reduce memory consumption and accelerate computation on modern GPUs. Loss scaling prevents numerical underflow in `float16`, and careful handling of gradient updates ensures stability. Mixed precision is now standard for large-scale pretraining, roughly doubling throughput with minimal impact on model quality.

Pretraining scales with model size, sequence length, and dataset size in predictable ways that have profound implications for what models can learn. Larger models with more parameters capture more complex patterns but require more data and compute to train. *ESM-2*'s largest variant has 15 billion parameters [@lin_evolutionary_2023] (roughly one parameter for every two amino acids in its training corpus), enabling it to capture subtle evolutionary constraints invisible to smaller models. Longer sequence contexts enable learning of long-range dependencies but increase memory requirements quadratically for standard attention. More diverse training data improves generalization but requires proportionally more training time.

The relationships between scale and capability follow power laws that predict optimal resource allocation [@hoffmann_training_2022]. For a fixed computational budget, there exists an optimal balance between model size and training data: models that are too large undertrain on available data, while models that are too small cannot capture the complexity present in abundant data. These **scaling laws**, first characterized systematically for language models [@kaplan_scaling_2020], appear to hold for genomic foundation models as well, though the precise exponents and constants differ. Understanding these relationships guides decisions about when to scale up versus when to improve data quality or model architecture. @sec-fm-principles examines these scaling relationships in detail, formalizing the observations introduced here into quantitative laws that define the foundation model paradigm.

Beyond smooth improvements in loss, scale produces qualitative changes in model capabilities that were absent at smaller scales. Language models exhibit **emergent behaviors** (in-context learning, chain-of-thought reasoning, few-shot generalization) that appear only above certain parameter thresholds. Whether genomic models exhibit analogous emergent capabilities remains an active research question with early evidence suggesting they do. *ESM-2*, trained on evolutionary sequence databases containing hundreds of millions of protein sequences from UniRef [@suzek_uniref_2015], develops structural understanding of proteins despite receiving no explicit structural supervision: the three-dimensional contacts emerge from predicting amino acid sequences alone. *Evo*, trained autoregressively on genomes, learns to generate sequences with realistic gene structure and regulatory organization. These emergent properties cannot be predicted by extrapolating from smaller models, making them both scientifically interesting and practically difficult to anticipate.


## Monitoring and Debugging

A two-week pretraining run that fails on day thirteen represents not just wasted compute but delayed clinical deployment and missed opportunities for patient benefit. Early detection of training issues is essential for avoiding wasted computation and ensuring models achieve the representations necessary for clinical utility.

**Training loss curves** should decrease smoothly in early stages, eventually plateauing as the model approaches convergence. Sudden spikes suggest numerical instability (often from learning rate issues or gradient explosion), inappropriate optimization hyperparameters, or corrupted data batches. Persistent plateaus may indicate insufficient model capacity, inappropriate objectives, or learning rates that prevent further improvement. Tracking loss on held-out validation data monitors generalization: if training loss decreases while validation loss increases, the model is **overfitting** to the training corpus.

**Gradient norms** indicate whether optimization is proceeding normally. Very small gradients suggest the **vanishing gradient problem**, preventing effective learning in early layers. Very large gradients suggest instability that gradient clipping should catch. Tracking per-layer gradient norms helps diagnose where problems originate in deep networks.

**Probing tasks** provide functional sanity checks during pretraining. Simple downstream evaluations (predicting known splice sites, identifying transcription factor binding motifs, distinguishing exons from introns) can be run periodically on intermediate checkpoints to verify that learned representations capture biologically meaningful patterns. If probing performance plateaus or degrades while pretraining loss continues improving, the model may be learning patterns that do not transfer to downstream tasks.


## Choosing the Right Strategy

A clinician asking "will this *BRCA1* variant cause disease?" needs a model pretrained with objectives that capture protein function and evolutionary constraint. A synthetic biologist asking "can you design me a promoter with 10-fold higher expression?" needs generative capabilities that MLM does not provide. Selecting a pretraining approach involves matching computational investment to the clinical or research questions the model must ultimately answer.

For most general-purpose DNA or protein models, MLM pretraining provides a strong default. It learns bidirectional context, scales efficiently, and transfers well to diverse downstream tasks. *DNABERT* and *DNABERT-2* exemplify this approach for genomics, while *ESM* models demonstrate its effectiveness for proteins. Start with MLM unless there is a specific reason to prefer alternatives.

Next-token prediction is preferred when generation is the primary goal. If designing sequences from scratch (therapeutic proteins, synthetic promoters, regulatory circuits), sampling from autoregressive models produces coherent outputs respecting learned grammar. *Evo* and similar models demonstrate this for genomic sequence generation. The autoregressive structure makes conditional generation straightforward, enabling design applications that MLM does not naturally support.

Multi-task pretraining makes sense when functional labels are available at scale and tasks are complementary. *Enformer*'s success with thousands of chromatin assays demonstrates the power of multi-task learning when data supports it. The infrastructure requirements are higher (handling heterogeneous data, balancing losses across tasks, maintaining separate prediction heads), but the resulting representations capture functional information that pure sequence-based objectives miss.

Contrastive learning is valuable for cross-species applications or when robustness to variation is critical. If transferring models trained on model organisms to related species, or improving robustness to genetic polymorphism across human populations, contrastive pretraining on orthologous pairs or variant-augmented sequences provides targeted benefits.

When deciding whether to pretrain from scratch or start from existing models, starting from pretrained checkpoints is almost always preferable if an appropriate model exists. **Fine-tuning** a *DNABERT-2* checkpoint on a new task is faster and more data-efficient than training from scratch. Pretraining from scratch is necessary when using new tokenization schemes (incompatible vocabularies prevent weight transfer), targeting species without suitable existing models, or experimenting with fundamentally different architectures where pretrained weights cannot transfer.


## Pretraining in Practice: Case Studies

Examining how successful models were pretrained provides concrete lessons and design patterns that inform new projects. Each case study illustrates how architectural choices, data decisions, and optimization strategies combine to produce models with distinct capabilities.

*DNABERT* introduced MLM pretraining to genomics by adapting BERT's architecture to DNA sequences with overlapping k-mer tokenization [@ji_dnabert_2021]. The model was pretrained on the human genome with 6-mer tokens, masking 15% of tokens at random. Standard BERT hyperparameters proved effective: AdamW optimizer with warmup, dropout regularization, and layer normalization. The key lessons include the importance of tokenization choice (k-mers capture motif-level patterns better than single nucleotides for regulatory prediction), the value of reverse complement augmentation for strand symmetry, and the transferability of representations across tasks never seen during pretraining.

*HyenaDNA* demonstrated that efficient long-range architectures enable pretraining on extremely long contexts [@nguyen_hyenadna_2023]. By using Hyena layers with subquadratic complexity, *HyenaDNA* scaled to contexts spanning one million bases (compared to typical transformer limits of a few thousand bases), far beyond standard transformers. Pretraining used single-nucleotide next-token prediction with a **curriculum** that progressively increased context length from shorter windows to full million-base sequences. This curriculum learning proved essential: training directly on long contexts without warmup led to instability. The lessons include the feasibility of million-base contexts with appropriate architectures, the benefits of curriculum learning for context scaling, and the emergence of long-range regulatory patterns when models have sufficient receptive field.

*Enformer* pioneered multi-task chromatin prediction at scale [@avsec_effective_2021]. The model was pretrained jointly on over 5,000 assays from ENCODE, Roadmap Epigenomics, and related consortia, using a hybrid convolutional-transformer architecture with 200 kilobase context (spanning typical enhancer-promoter distances in mammalian genomes). Task weighting was balanced to prevent any single assay from dominating. Key insights include the power of large-scale multi-task learning for capturing diverse regulatory signals, the effectiveness of combining convolutions for local patterns with transformers for long-range interactions, and the interpretability benefits of attention patterns that reveal learned enhancer-promoter relationships.

*ESM-2* represents the state of the art for protein language models, scaling to 15 billion parameters trained on UniRef databases containing sequences from hundreds of millions of protein families [@lin_evolutionary_2023]. Pretraining used standard MLM on amino acid sequences at unprecedented scale. The lessons include the continued benefit of scaling (larger models and more data improve even at billions of parameters, with no plateau in sight), the value of evolutionary diversity (pretraining on distinct protein families captures constraints invisible in any single genome), and the emergence of structural understanding from sequence alone (*ESM-2* representations encode three-dimensional contacts despite no explicit structural supervision during pretraining).


## Open Questions

Despite rapid progress, fundamental questions about genomic pretraining remain open, and resolving them will determine whether the next generation of models can achieve clinical-grade reliability.

Optimal objective combinations remain unclear: should we jointly train with MLM and chromatin prediction, or train sequentially? How many auxiliary tasks help before diminishing returns? Do contrastive and generative objectives complement each other or interfere? These questions have different answers for different downstream applications, and systematic characterization is incomplete.

Incorporating biological priors versus learning from scratch presents a design tension. Known motifs, pathway structure, and evolutionary constraints could be encoded in model architecture or initialization. Hand-engineered features risk encoding false assumptions, but pure data-driven learning may rediscover basic biology inefficiently. Hybrid approaches combining priors with learned representations remain underexplored.

**Continual pretraining** as new data arrives is increasingly relevant. As sequencing technologies improve and new assays emerge, updating pretrained models without catastrophic forgetting of prior knowledge presents challenges. Online learning and elastic weight consolidation are potential solutions that remain largely untested in genomics at scale.

The relationship between pretraining scale and downstream performance follows predictable patterns that are still being characterized for genomic models. Understanding these relationships more precisely would guide resource allocation and set realistic expectations for what different scales of pretraining can achieve. These scaling considerations connect to the broader foundation model paradigm examined in @sec-fm-principles.


## From Sequence Statistics to Biological Knowledge

The fundamental insight underlying self-supervised pretraining is that patterns relevant to biological function are embedded in sequence statistics themselves. A model that learns to predict masked nucleotides must implicitly capture the evolutionary constraints, regulatory grammar, and structural requirements that determine what sequences are viable. A model that learns to generate plausible protein sequences must internalize the constraints that distinguish functional proteins from random polymers. These objectives extract biological knowledge from sequence without requiring explicit functional labels, transforming abundant unlabeled data into learned representations that improve data efficiency for downstream applications.

The choice of pretraining objective shapes what models learn in ways that propagate to clinical utility. Masked language modeling teaches bidirectional sequence understanding, making it the natural choice for variant interpretation and regulatory prediction where full flanking context informs the prediction. Next-token prediction teaches generative capabilities essential for therapeutic protein design and synthetic sequence generation. Contrastive learning teaches invariance to perturbations, building robustness that transfers across species and populations. Aligning pretraining objectives with intended applications improves transfer; misalignment creates representational gaps that fine-tuning may struggle to bridge.

Self-supervised pretraining has become the default approach for building genomic foundation models. The DNA language models in @sec-dna-lm, protein language models in @sec-protein-lm, and regulatory sequence models in @sec-regulatory each employ variants of these objectives tailored to their sequence modalities and downstream applications. The transfer learning methods examined next determine how effectively pretrained representations can be adapted to specific clinical and research tasks, completing the pipeline from raw sequence through learned representation to deployed application.