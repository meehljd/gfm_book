# Pretraining Objectives and Strategies {#sec-pretraining}

Genomics generates sequence data far faster than we can annotate it. Reference genomes span billions of nucleotides across thousands of species. Population sequencing projects catalog genetic variation in millions of individuals. Functional genomics consortia measure chromatin accessibility, transcription factor binding, and gene expression across hundreds of cell types. Yet experimental labels remain sparse: for any given sequence, we typically lack direct measurements of its regulatory function, its effect on splicing, or its contribution to disease risk. The gap between available sequence and available annotation grows with each passing year.

This asymmetry between data abundance and label scarcity defines the central opportunity for self-supervised learning in genomics. Rather than training models from scratch on small labeled datasets, we can first learn general-purpose sequence representations from unlabeled genomes, then adapt these representations to specific tasks through fine-tuning or few-shot learning. The intuition is that patterns relevant to regulatory function, splice site recognition, and protein folding are embedded in sequence statistics themselves. A model that learns to predict missing nucleotides or adjacent sequence context must implicitly capture motifs, constraints, and compositional structure that generalize across tasks. Self-supervised pretraining transforms raw sequence abundance into learned representations that make downstream labeled data go further.

The choice of pretraining objective fundamentally shapes what a model learns. Masked language modeling encourages bidirectional context integration, teaching models to recognize patterns from both upstream and downstream sequence. Next-token prediction builds generative capabilities, enabling models to sample novel sequences that respect learned grammar. Contrastive learning teaches invariance, producing representations where functionally equivalent sequences map to nearby embeddings regardless of superficial differences. Multi-task objectives combine supervision signals from diverse functional assays, learning representations that capture chromatin state, gene expression, and evolutionary conservation simultaneously. Each objective encodes different assumptions about what matters in biological sequence, and these assumptions propagate through to downstream performance.

## Masked Language Modeling

Masked language modeling treats sequences as partially observed and trains models to reconstruct missing content from surrounding context. The objective is straightforward: randomly mask portions of an input sequence, feed the corrupted sequence to the model, and train the model to predict the original tokens at masked positions. A masking strategy replaces selected tokens with a special `[MASK]` token, leaving the surrounding context intact. The model processes the masked sequence through its layers and produces predictions for the masked positions, typically optimizing cross-entropy loss over the vocabulary at each masked location.

The key insight is that accurate prediction requires learning genuine sequence structure. To predict a masked position in a transcription factor binding site, the model must recognize the surrounding motif context. To predict masked splice donor sequences, the model must encode the consensus patterns characteristic of exon-intron boundaries. Over millions of training examples, models build distributed representations of motifs, compositional rules, and sequence constraints that transfer to tasks never seen during pretraining.

MLM encourages bidirectional context integration. Unlike autoregressive models that condition only on past tokens, MLM models see both left and right context when predicting masked positions. For genomics, this is biologically appropriate: regulatory function depends on patterns both upstream and downstream of any given position. A transcription factor binding site is recognized through flanking sequences on both sides. Splicing signals require coordination between donor and acceptor sites separated by hundreds of bases. The bidirectional attention in MLM naturally captures these dependencies.

### Masking Strategies and Their Implications

The choice of masking strategy significantly impacts what models learn. Random masking of individual tokens creates predictions that are relatively local, where each masked position can often be inferred from immediately adjacent nucleotides. Span masking, which masks contiguous blocks of tokens, forces models to infer longer-range dependencies and compositional patterns. If an entire transcription factor binding motif is masked, the model cannot rely on partial motif information and must instead recognize the motif's role from surrounding regulatory context.

Masking rates present a fundamental tradeoff. Higher masking rates (30-40% of tokens) provide more supervision per sequence but make prediction harder and may destabilize training. Lower masking rates (10-15%) produce more stable training but require more data to achieve equivalent coverage. The standard 15% rate from BERT represents a reasonable compromise, though genomic models have explored a range of values depending on context length and tokenization granularity. DNABERT used 15% masking on 6-mer tokens, while later models have experimented with adaptive masking rates that increase as training progresses.

Tokenization interacts with masking in important ways. DNABERT pioneered MLM for genomic sequences by applying it to overlapping k-mer tokens: rather than treating DNA as individual nucleotides, DNABERT tokenizes sequences into all possible 6-mers with overlapping windows. Masking then operates at the k-mer level, with entire 6-mers masked as units. This design encourages learning of k-mer level patterns corresponding to transcription factor binding motifs and other short functional elements. DNABERT-2 adopted byte-pair encoding tokenization, which learns a vocabulary of variable-length subword units from the training corpus. BPE tokens represent single nucleotides, common motifs, or repeated elements depending on their frequency. MLM with BPE balances flexibility with compositional structure, though the learned vocabulary may not align with biological functional units.

The design decisions explored by DNABERT and DNABERT-2 established patterns that subsequent DNA language models have built upon and refined. @sec-dna-lm examines how these architectural and tokenization choices have evolved as the field has scaled to longer contexts and larger training corpora, tracing the lineage from these early MLM models through modern approaches like Nucleotide Transformer and HyenaDNA.

### What Masked Language Models Learn

MLM objectives drive models to capture multiple levels of sequence organization. At the lowest level, models learn nucleotide statistics and local constraints: CpG dinucleotide frequencies, GC content biases, and simple repeat patterns. These basic properties are necessary but not sufficient for biological function.

At higher levels, MLM captures motif patterns and sequence grammar. Predicting masked positions in regulatory regions requires recognizing transcription factor binding sites, understanding how motifs combine, and learning context-dependent usage patterns. If certain transcription factor motifs co-occur at specific distances, masking one motif and predicting it from the other reinforces this grammatical relationship. This compositional learning is difficult to achieve with supervised learning alone, which typically provides coarse binary labels rather than fine-grained structural information about sequence organization.

MLM also captures evolutionary conservation patterns implicitly. Conserved sequences are constrained because mutations would disrupt function. By learning to predict conserved patterns from surrounding context, models encode which sequence features are under selection. This knowledge transfers to variant effect prediction, where the model recognizes when a mutation disrupts a learned conserved pattern. The connection between pretraining on raw sequence and downstream variant interpretation illustrates how self-supervised objectives can capture biologically meaningful structure without explicit functional labels.

## Next-Token Prediction

Next-token prediction represents an alternative paradigm where models learn to predict each token in a sequence given only the preceding tokens. This autoregressive approach, popularized by GPT-style language models, treats sequence generation as a core capability rather than a secondary feature. For a sequence of length $T$, the model predicts token $t$ from tokens $1$ through $t-1$, maximizing the likelihood of the observed sequence under the model's learned distribution.

Algorithmically, next-token prediction requires causal masking in the attention mechanism. Each position attends only to earlier positions, ensuring predictions at position $t$ depend exclusively on positions $1$ through $t-1$. This matches the conditional probability factorization inherent in the objective: the probability of a sequence factors as the product of conditional probabilities for each token given its predecessors. The loss function is cross-entropy over the vocabulary, computed at every position rather than only at masked locations.

The fundamental difference from MLM lies in what the model can see during prediction. Autoregressive models build representations from unidirectional context, learning to generate sequences that respect learned constraints. This makes autoregressive pretraining attractive for sequence design applications. Sampling new sequences proceeds naturally: predict the first token, condition on it to predict the second, and continue token by token. The generation process directly uses the learned conditional distributions without requiring additional architectural modifications.

### Genomic Applications

DNA sequences present a complication: they have no inherent directionality. Both strands encode information, and regulatory function is often strand-agnostic. This contrasts with natural language, where left-to-right reading order is meaningful. Early autoregressive genomic models addressed this by training separate models for forward and reverse strands or by augmenting training data with reverse-complement sequences. More recent approaches treat strand symmetry as an architectural constraint, ensuring that forward and reverse complement sequences produce equivalent representations.

Evo represents a large-scale autoregressive genomic model trained on whole genomes with long-context architectures. By predicting next tokens across chromosome-length sequences, Evo learns long-range dependencies and can generate coherent synthetic genomes. This capability enables designing regulatory circuits, generating training data through synthetic augmentation, and exploring sequence space beyond observed genomes. The autoregressive formulation makes conditional generation straightforward: to generate sequences with desired properties, incorporate conditioning information into the context at each prediction step.

Protein sequence models also benefit from autoregressive pretraining. The N-terminus to C-terminus directionality of protein synthesis provides biological justification for left-to-right prediction. ESM models and their successors predict amino acid sequences autoregressively, learning protein grammar and evolutionary constraints that transfer to structure prediction and function annotation. The success of protein language models demonstrates that autoregressive objectives can capture deep biological structure when training data is sufficiently diverse.

The protein language modeling paradigm, encompassing both autoregressive and masked objectives, has produced some of the most striking successes in computational biology. @sec-protein-lm explores this family of models in depth, examining how evolutionary sequence databases provide training signal equivalent to billions of implicit functional experiments, and how the resulting representations transfer to structure prediction, function annotation, and protein design.

### Comparing MLM and Autoregressive Objectives

The choice between MLM and next-token prediction involves several considerations that depend on intended downstream applications. For tasks requiring understanding of full sequence context, MLM's bidirectional attention provides richer representations. Predicting transcription factor binding at a specific location benefits from seeing both upstream and downstream sequence, information that autoregressive models cannot access during inference.

For generative tasks, autoregressive models are more principled. Their sequential prediction structure matches the generation process exactly, whereas generating from MLM models requires iterative masking and filling procedures or auxiliary generative heads that were not part of pretraining. Autoregressive models also handle variable-length sequences naturally and can process streaming data where sequence length is not known in advance.

Training efficiency differs between objectives in subtle ways. MLM predicts only 15% of tokens per sequence but uses bidirectional context for each prediction. Autoregressive models predict all tokens but with unidirectional context. During training, teacher forcing allows efficient parallel computation for autoregressive models: the model predicts all positions simultaneously by feeding in the ground truth sequence shifted by one position. Generation at inference time is inherently sequential and slower, but pretraining speed is comparable to MLM.

Task-specific performance depends on alignment between pretraining and downstream objectives. If the downstream task involves predicting missing information from context (variant effect prediction, binding site identification), MLM pretraining provides better transfer. If the downstream task involves generation or sequential decision-making (sequence design, sampling from conditional distributions), autoregressive pretraining aligns more naturally. For applications requiring both understanding and generation, hybrid approaches that combine bidirectional encoding with autoregressive decoding offer a middle ground, though these add architectural complexity.

## Span Corruption and Denoising

Span corruption generalizes masked language modeling by introducing more complex forms of input degradation. The T5 model popularized this approach for natural language, and the principles transfer to genomic sequences. Rather than masking individual tokens, span corruption masks contiguous spans of variable length and replaces each span with a single sentinel token. The model then generates the original content of all masked spans in sequence, learning to reconstruct substantial missing regions rather than isolated positions.

This objective teaches different aspects of sequence structure than standard MLM. Reconstructing entire spans requires understanding longer-range dependencies and compositional patterns. If a span encompasses an entire transcription factor binding motif, the model cannot infer the motif from partial information and must instead reason about the motif's role from surrounding regulatory context. Span lengths are typically sampled from a distribution (geometric or uniform) with a mean around 3-5 tokens, creating a mix of short and long reconstruction challenges within each training example.

Denoising objectives extend beyond masking to include other forms of corruption. Token substitution replaces input tokens with random tokens from the vocabulary, creating corrupted sequences that resemble sequencing errors or natural variation. The model learns to distinguish correct from incorrect tokens based on surrounding context, encouraging representations that capture local consistency and motif structure. Deletion and insertion corruptions remove or add tokens at random positions, teaching models about position-invariant features that remain identifiable despite surrounding changes. For genomics, insertions and deletions are biologically realistic mutation types, and models that handle them during pretraining may better predict their effects downstream.

### Biologically Motivated Corruption

Simulating sequencing errors provides corruption strategies grounded in experimental reality. Base miscalls follow platform-specific patterns: Illumina sequencing shows characteristic substitution biases, while nanopore sequencing exhibits distinct error profiles concentrated in homopolymer regions. Training with corruptions that mimic these error patterns may improve generalization to real sequencing data with platform-specific artifacts.

Variant augmentation introduces biologically realistic sequence changes based on population variation. Randomly substituting alleles at known polymorphic sites or injecting variants from databases like gnomAD creates corrupted sequences reflecting natural genetic diversity. The model learns that common polymorphisms represent normal variation rather than errors to be corrected, potentially improving robustness for variant effect prediction where distinguishing pathogenic variants from benign polymorphisms is the central challenge.

Structural variation simulation models larger-scale genomic changes: tandem duplications, copy number variation, and segmental rearrangements. These corruptions are harder to implement but capture realistic sources of genomic diversity beyond single-nucleotide changes. Models trained with structural variation corruptions may better understand how gene dosage changes, enhancer duplications, or domain boundary disruptions affect function.

The benefit of denoising pretraining extends to robustness under distribution shift. If downstream applications involve sequences from different populations, experimental platforms, or tissue contexts than the pretraining corpus, models pretrained with appropriate corruptions can maintain performance despite distribution mismatch. This is valuable in clinical genomics, where validation cohorts often differ from discovery cohorts in ancestry, sequencing technology, or phenotyping protocols.

## Contrastive Learning

Contrastive learning takes a fundamentally different approach to self-supervised pretraining. Rather than reconstructing corrupted inputs, contrastive objectives train models to produce similar representations for different views of the same sequence while distinguishing them from representations of unrelated sequences. The intuition is that augmented versions of a sequence (with minor corruptions or transformations) should map to nearby points in representation space, while unrelated sequences should map to distant points.

The algorithmic framework constructs positive pairs and negative samples. For a given anchor sequence, positive pairs are created through augmentation: reverse complementation, random cropping, variant injection, or other transformations that preserve functional identity. Negative samples are drawn from other sequences in the training batch. The model produces embeddings for all sequences, and the contrastive loss encourages anchor and positive embeddings to be similar (high cosine similarity) while pushing apart anchor and negative embeddings.

InfoNCE loss is the most common contrastive objective. For an anchor embedding $z_i$ and positive embedding $z_i^+$, InfoNCE maximizes:

$$\log \frac{\exp(z_i \cdot z_i^+ / \tau)}{\sum_j \exp(z_i \cdot z_j / \tau)}$$

where the sum runs over the positive and all negative samples, and $\tau$ is a temperature parameter controlling the sharpness of the distribution. Lower temperatures make the model more discriminative, requiring cleaner separation between positives and negatives.

### Augmentation Design for Genomic Sequences

Augmentation design is critical for contrastive learning because augmentations must preserve functional identity while introducing variability. If augmentations change function, the contrastive objective will learn meaningless invariances. Several augmentation strategies are biologically grounded and preserve the functional relationships that matter for downstream tasks.

Reverse complementation is the simplest and most reliable augmentation. DNA is double-stranded, and many regulatory elements function identically on either strand. Training the model to treat forward and reverse complement sequences as equivalent captures strand symmetry inherent in molecular biology. This augmentation is universally applicable and introduces no risk of changing functional identity.

Random cropping extracts overlapping windows from longer sequences. If a transcription factor binding site appears in multiple cropped windows, the model learns that the binding site is the functionally relevant feature regardless of absolute position or surrounding context. This teaches position-invariant representations useful for tasks where genomic coordinates matter less than local sequence content.

Variant injection introduces common polymorphisms or simulated mutations. If the variants are neutral or do not disrupt function, treating variant and reference sequences as positive pairs teaches robustness to genetic variation. This is valuable for cross-population generalization, where models must recognize functional elements despite surrounding sequence polymorphism that differs between ancestry groups.

Negative sampling strategies also affect what models learn. Random genomic sequences provide straightforward negatives but may be too easy to distinguish: any functional regulatory sequence is readily separable from random intergenic sequence. Harder negatives, such as sequences from orthologous regions in related species or sequences with similar motif content but different functional annotations, provide more informative supervision that forces the model to learn subtle discriminative features.

### Cross-Species Contrastive Learning

Cross-species contrastive learning leverages evolutionary relationships for self-supervision. Orthologous sequences from different species share functional identity despite nucleotide divergence accumulated over millions of years of evolution. Treating orthologous pairs as positives and non-orthologous pairs as negatives teaches the model to extract species-invariant functional features. A human enhancer and its mouse ortholog should map to similar embeddings despite sequence differences, while unrelated sequences should map to distant embeddings.

This approach can improve cross-species transfer. A model pretrained with human-mouse contrastive pairs may generalize better to rat, primate, or other mammalian sequences by learning to ignore species-specific sequence differences while preserving functionally relevant patterns. The evolutionary record provides implicit labels about functional equivalence that would be expensive to obtain through experimental annotation.

Sequence embedding quality improves with contrastive pretraining in ways that benefit downstream applications. Models trained contrastively produce embedding spaces where functionally similar sequences cluster together, enabling nearest-neighbor search, sequence retrieval, and unsupervised clustering of regulatory elements. Variant effect prediction benefits through improved robustness: if the model learns that sequences differing only by neutral variants are functionally equivalent, it will better distinguish truly disruptive variants from benign polymorphisms.

## Multi-Task Pretraining

Multi-task pretraining combines multiple related objectives during the same training run, jointly optimizing for several prediction tasks. Different tasks provide complementary supervision signals: masking captures local patterns, chromatin prediction captures regulatory function, and conservation scoring captures evolutionary constraint. Representations that satisfy all tasks simultaneously may develop richer and more general features than any single objective alone.

Task selection is the first design decision. Ideally, tasks should be diverse enough to provide distinct supervision signals but related enough to benefit from shared representations. For genomic models, effective combinations include masked language modeling for general sequence structure, chromatin accessibility prediction for regulatory function, gene expression prediction for transcriptional output, evolutionary conservation scoring for functional constraint, and variant frequency prediction from population databases. Each task operates on the same input sequence but predicts different outputs using task-specific head layers. The shared backbone encoder processes the sequence into intermediate representations, and separate prediction heads map these representations to task-specific outputs.

Task weighting determines how much each task contributes to the total loss. With $L_1, \ldots, L_K$ representing individual task losses, the multi-task loss combines them:

$$L_{\text{total}} = \sum_{k=1}^K w_k L_k$$

where $w_k$ are task weights. Equal weighting is simple but may lead to imbalanced learning if tasks have different scales or difficulties. Dynamic weighting approaches adjust weights during training based on learning progress, using the magnitude of task losses or gradient norms as signals for rebalancing.

### Large-Scale Multi-Task Examples

Enformer and Borzoi exemplify large-scale multi-task pretraining for genomics. Enformer predicts over 5,000 genomic assays simultaneously: ChIP-seq signals for hundreds of transcription factors and histone marks, DNase-seq and ATAC-seq accessibility, CAGE transcription initiation, and more. This massive multi-task objective forces the model to learn representations capturing diverse regulatory signals across cell types and experimental conditions.

The task diversity in Enformer provides supervision far richer than any single assay. A model trained only on DNase-seq learns general accessibility patterns but misses transcription factor specificity. A model trained only on H3K27ac ChIP-seq captures active enhancers but misses repressive marks. Training on all assays jointly allows the model to disentangle overlapping and complementary signals, learning representations that generalize across regulatory contexts. Borzoi extends this paradigm to full RNA-seq coverage prediction, jointly modeling transcription initiation, splicing, and transcript abundance.

Combining MLM with functional prediction represents another multi-task configuration. The model predicts masked tokens through a language modeling head while simultaneously predicting chromatin accessibility or other functional readouts through regression heads. This hybrid objective balances sequence-level pretraining with functional supervision, potentially combining the benefits of both approaches. The MLM component ensures the model learns general sequence patterns even in regions without functional annotations, while the functional prediction component focuses learning on biologically relevant features.

### When Multi-Task Helps and When It Hurts

Task interference is the primary concern with multi-task learning. If tasks require conflicting representations, jointly optimizing for both may compromise performance on each compared to single-task baselines. In genomics, this might occur if one task benefits from very local features while another requires long-range context, forcing the model to choose suboptimal representations for both.

Negative transfer occurs when adding a task actually hurts downstream performance compared to training without it. This can happen if the additional task introduces noise, if task weights are poorly balanced, or if the auxiliary task shifts learned representations away from features useful for target applications. The risk of negative transfer increases with task diversity: distantly related tasks are more likely to require conflicting representations.

The benefits of multi-task pretraining are largest when tasks are complementary and data for individual tasks is limited. If chromatin data is sparse for a particular cell type but gene expression data is abundant, jointly training on both may improve performance on both compared to single-task models. The shared representations allow information to flow between tasks, compensating for data scarcity in any single modality. When functional labels exist at scale and tasks are genuinely related, multi-task pretraining consistently outperforms single-task alternatives.

## What Different Objectives Teach

The choice of pretraining objective encodes assumptions about what matters in biological sequence, and these assumptions propagate through to downstream performance. Understanding what each objective teaches helps practitioners select appropriate strategies for their applications.

Masked language modeling teaches bidirectional sequence understanding. Models learn to recognize patterns from both directions, capturing the full context that determines biological function at any position. This makes MLM well-suited for classification and interpretation tasks: predicting whether a sequence is a binding site, identifying regulatory elements, or scoring variant effects based on sequence context. The bidirectional representations naturally capture dependencies that span the prediction target.

Next-token prediction teaches generative sequence modeling. Models learn the distribution over next tokens given preceding context, enabling coherent sequence generation that respects learned grammar. This makes autoregressive pretraining ideal for sequence design applications: generating novel promoters, designing therapeutic proteins, or creating synthetic regulatory circuits. The generative capability is inherent to the objective rather than an afterthought.

Contrastive learning teaches invariant representations. Models learn which sequence features matter for functional identity and which are incidental details that vary across examples. This makes contrastive pretraining valuable for cross-species transfer, robustness to genetic variation, and applications where functional equivalence matters more than sequence identity. The learned embedding spaces cluster functionally similar sequences regardless of superficial differences.

Multi-task learning teaches shared structure across diverse functional readouts. Models learn representations that simultaneously predict chromatin state, gene expression, conservation, and other outputs, discovering latent structure that underlies multiple aspects of genome function. This makes multi-task pretraining effective when downstream tasks involve predicting functional properties that relate to the pretraining tasks, though the benefits depend on task complementarity.

Denoising objectives teach robustness to corruption and noise. Models learn to recover original sequences from degraded inputs, building tolerance to sequencing errors, natural variation, and distribution shift. This makes denoising pretraining valuable when downstream applications involve data from different platforms, populations, or experimental conditions than the training corpus.

## Data Strategies for Pretraining

Corpus construction establishes the foundation for pretraining. The choice of training data determines what patterns the model can learn and how well it will generalize. For genomic models, this involves decisions about reference genomes, population variation, repeat handling, and chromosome segmentation.

Reference genomes are the standard starting point. Human genome assemblies like GRCh38 provide high-quality, contiguous sequence spanning all chromosomes. Training on the reference genome allows models to learn patterns characteristic of human DNA: base composition, repeat structure, gene organization, and regulatory architecture. The reference genome represents a single haploid consensus, missing variation present in human populations, but provides the foundation for most pretraining approaches.

Population-scale variation can be incorporated through variant databases. Rather than training only on reference sequence, injecting variants at observed population frequencies creates synthetic diploid genomes reflecting real genetic diversity. This teaches models that common polymorphisms are normal variation, potentially improving robustness and variant effect prediction. Pan-genome approaches extend this by representing multiple high-quality assemblies from diverse individuals, capturing structural variation and population-specific haplotypes that a single reference cannot represent.

Repeat handling impacts pretraining in ways that depend on downstream applications. Simple repeats, tandem repeats, and transposable elements occupy substantial genomic fractions but contribute less to regulatory function than unique sequences. Hard-masking repeats (replacing them with Ns) reduces training data but may discard information relevant to some tasks. Soft-masking retains sequence information while marking repetitive regions, allowing models to learn differential representations for repeats and unique sequences.

Data augmentation artificially increases training diversity. Reverse complementation exploits DNA strand symmetry, doubling effective training data. Random cropping extracts variable-length windows, teaching position-invariant features. Variant injection simulates genetic variation, building robustness to population diversity. These augmentations are typically applied on-the-fly during training rather than pre-computed, maintaining flexibility in the training pipeline.

## Optimization and Scaling

Effective pretraining requires careful attention to optimization details that become critical at scale. Learning rate schedules, batch sizing, gradient handling, and numerical precision all affect training stability and final model quality.

Learning rate warmup gradually increases the learning rate from near-zero over the first several thousand steps. This prevents early training instability when the model has random initializations and large gradient variance. After warmup, cosine decay schedules reduce the learning rate following a cosine curve from peak to near-zero over training, providing aggressive learning early when gradients are most informative and gentle refinement late as the model approaches convergence.

Gradient clipping prevents training instability from occasional large gradients. Clipping by global norm scales all gradients proportionally when the total norm exceeds a threshold, maintaining gradient direction while controlling magnitude. This is standard practice for transformer models where exploding gradients can occur, particularly with long sequences where attention matrices span many positions.

Mixed precision training uses lower-precision arithmetic (float16 or bfloat16 instead of float32) to reduce memory consumption and accelerate computation on modern GPUs. Loss scaling prevents numerical underflow in float16, and careful handling of gradient updates ensures stability. Mixed precision is now standard for large-scale pretraining, roughly doubling throughput with minimal impact on model quality.

Pretraining scales with model size, sequence length, and dataset size in predictable ways that have profound implications for what models can learn. Larger models with more parameters capture more complex patterns but require more data and compute to train. Longer sequence contexts enable learning of long-range dependencies but increase memory requirements quadratically for standard attention. More diverse training data improves generalization but requires proportionally more training time. These relationships are not merely practical constraints; they reflect fundamental properties of how neural networks extract structure from data.

The relationships between scale and capability follow power laws that predict optimal resource allocation given a compute budget. For a fixed computational budget, there exists an optimal balance between model size and training data: models that are too large undertrain on available data, while models that are too small cannot capture the complexity present in abundant data. These scaling laws, first characterized systematically for language models, appear to hold for genomic foundation models as well, though the precise exponents and constants differ across domains. Understanding these relationships guides decisions about when to scale up versus when to improve data quality or model architecture.

Beyond smooth improvements in loss, scale produces qualitative changes in model capabilities that were absent at smaller scales. Language models exhibit emergent behaviors including in-context learning, chain-of-thought reasoning, and few-shot generalization that appear only above certain parameter thresholds. Whether genomic models exhibit analogous emergent capabilities remains an active research question. Early evidence suggests that sufficiently large DNA and protein models develop unexpected capabilities: predicting three-dimensional structure from sequence alone, generalizing across species without explicit training, or capturing regulatory logic that smaller models miss entirely. These emergent properties cannot be predicted by extrapolating from smaller models, making them both exciting and difficult to anticipate.

The interplay between pretraining objective and scale creates a rich design space. A small model trained with MLM learns local motif patterns. The same architecture at larger scale, trained on more diverse genomes, begins to capture long-range regulatory dependencies and cross-species conservation patterns. Autoregressive models at small scale generate locally coherent but globally inconsistent sequences; at sufficient scale, they produce chromosome-length sequences respecting higher-order genome organization. The pretraining objective determines what capabilities are possible; scale determines which of those possibilities are realized. @sec-fm-principles examines these scaling relationships in detail, formalizing the observations introduced here into quantitative laws that define the foundation model paradigm.

## Monitoring and Debugging

Pretraining runs span days to weeks, making early detection of issues essential for avoiding wasted computation. Careful monitoring tracks training progress and identifies problems before they become catastrophic.

Training loss curves should decrease smoothly in early stages, eventually plateauing as the model approaches convergence. Sudden spikes suggest numerical instability, inappropriate learning rates, or corrupted data batches. Persistent plateaus may indicate insufficient model capacity, inappropriate objectives, or optimization hyperparameters that prevent further improvement. Tracking loss on held-out validation data monitors generalization: if training loss decreases while validation loss increases, the model is overfitting to the training corpus.

Gradient norms indicate whether optimization is proceeding normally. Very small gradients suggest the vanishing gradient problem, preventing effective learning in early layers. Very large gradients suggest instability that gradient clipping should catch. Tracking per-layer gradient norms helps diagnose where problems originate in deep networks.

Probing tasks provide functional sanity checks during pretraining. Simple downstream evaluations can be run periodically on intermediate checkpoints to verify that learned representations capture biologically meaningful patterns. If probing performance plateaus or degrades while pretraining loss continues improving, the model may be learning patterns that do not transfer to downstream tasks.

## Choosing the Right Strategy

Selecting a pretraining approach involves balancing computational budget, target downstream tasks, data availability, and model architecture constraints. No single strategy is universally optimal, so understanding when each approach excels guides practical decisions.

For most general-purpose DNA or protein models, MLM pretraining provides a strong default. It learns bidirectional context, scales efficiently, and transfers well to diverse downstream tasks. DNABERT and DNABERT-2 exemplify this approach for genomics, while ESM models demonstrate its effectiveness for proteins. Start with MLM unless there is a specific reason to prefer alternatives.

Next-token prediction is preferred when generation is the primary goal. If designing sequences from scratch, sampling from autoregressive models produces coherent outputs respecting learned grammar. Evo and similar models demonstrate this for genomic sequence generation. The autoregressive structure makes conditional generation straightforward, enabling design applications that MLM does not naturally support.

Multi-task pretraining makes sense when functional labels are available at scale and tasks are complementary. Enformer's success with thousands of chromatin assays demonstrates the power of multi-task learning when data supports it. The infrastructure requirements are higher, including handling heterogeneous data and balancing losses across tasks, but the resulting representations capture functional information that pure sequence-based objectives miss.

Contrastive learning is valuable for cross-species applications or when robustness to variation is critical. If transferring models trained on model organisms to related species, or improving robustness to genetic polymorphism, contrastive pretraining on orthologous pairs or variant-augmented sequences provides targeted benefits that other objectives do not address directly.

When deciding whether to pretrain from scratch or start from existing models, starting from pretrained checkpoints is almost always preferable if an appropriate model exists. Fine-tuning a DNABERT-2 checkpoint on a new task is faster and more data-efficient than training from scratch. Pretraining from scratch is necessary when using new tokenization schemes (incompatible vocabularies), targeting species without suitable existing models, or switching to fundamentally different architectures where pretrained weights cannot transfer.

## Pretraining in Practice: Case Studies

Examining how successful models were pretrained provides concrete lessons and design patterns that inform new projects.

DNABERT introduced MLM pretraining to genomics by adapting BERT's architecture to DNA sequences with overlapping k-mer tokenization. The model was pretrained on the human genome with 6-mer tokens, masking 15% of tokens at random. Standard BERT hyperparameters proved effective: AdamW optimizer with warmup, dropout regularization, and layer normalization. The key lessons include the importance of tokenization choice (k-mers capture motif-level patterns better than single nucleotides), the value of reverse complement augmentation for strand symmetry, and the transferability of representations (pretrained DNABERT generalized to diverse regulatory tasks despite training only on raw genome sequence).

HyenaDNA demonstrated that efficient long-range architectures enable pretraining on extremely long contexts. By using Hyena layers with subquadratic complexity, HyenaDNA scaled to contexts spanning one million bases, far beyond standard transformers. Pretraining used single-nucleotide next-token prediction with a curriculum that progressively increased context length. The lessons include the feasibility of million-base contexts with appropriate architectures, the benefits of curriculum learning for context scaling, and the emergence of long-range regulatory patterns when models have sufficient receptive field.

Enformer pioneered multi-task chromatin prediction at scale. The model was pretrained jointly on over 5,000 assays from ENCODE, Roadmap Epigenomics, and related consortia, using a hybrid convolutional-transformer architecture with 200 kilobase context. Task weighting was balanced to prevent any single assay from dominating. The key insights include the power of large-scale multi-task learning for capturing diverse regulatory signals, the effectiveness of combining convolutions for local patterns with transformers for long-range interactions, and the interpretability benefits of attention patterns that reveal learned enhancer-promoter relationships.

ESM-2 represents the state of the art for protein language models, scaling to 15 billion parameters trained on evolutionary databases containing billions of protein sequences. Pretraining used standard MLM on amino acid sequences at unprecedented scale. The lessons include the continued benefit of scaling (larger models and more data improve even at billions of parameters), the value of evolutionary diversity (pretraining on hundreds of millions of protein families captures constraints invisible in individual genomes), and the emergence of structural understanding from sequence alone (ESM-2 representations encode 3D structure despite no explicit structural supervision).

## Open Questions

Despite rapid progress, fundamental questions about genomic pretraining remain open. Optimal objective combinations are unclear: should we jointly train with MLM and chromatin prediction, or train sequentially? How many auxiliary tasks help before diminishing returns? Do contrastive and generative objectives complement each other or interfere?

Incorporating biological priors versus learning from scratch presents a design tension. Known motifs, pathway structure, and evolutionary constraints could be encoded in model architecture or initialization. Hand-engineered features risk encoding false assumptions, but pure data-driven learning may rediscover basic biology inefficiently. Hybrid approaches combining priors with learned representations are underexplored.

Continual pretraining as new data arrives is increasingly relevant. As sequencing technologies improve and new assays emerge, updating pretrained models without catastrophic forgetting of prior knowledge presents challenges. Online learning and elastic weight consolidation are potential solutions largely untested in genomics.

The relationship between pretraining scale and downstream performance follows predictable patterns that are still being characterized for genomic models. Understanding these relationships more precisely would guide resource allocation and set realistic expectations for what different scales of pretraining can achieve. These scaling considerations connect to the broader foundation model paradigm examined in @sec-fm-principles.

## Summary

Pretraining objectives transform abundant unlabeled sequence into learned representations that improve data efficiency and generalization for downstream tasks. Masked language modeling teaches bidirectional sequence understanding, making it the default choice for most genomic applications. Next-token prediction teaches generative capabilities essential for sequence design. Contrastive learning teaches invariance and robustness. Multi-task pretraining captures functional information when diverse labeled data is available at scale. Denoising objectives build tolerance to noise and distribution shift.

The choice of objective shapes what models learn in ways that propagate to downstream performance. Aligning pretraining objectives with intended applications improves transfer: bidirectional objectives for classification and interpretation, autoregressive objectives for generation, contrastive objectives for cross-species transfer and robustness. Data strategies, optimization details, and scaling considerations all affect final model quality.

Self-supervised pretraining has become the standard approach for building genomic foundation models. The next chapter (@sec-transfer) examines how to adapt pretrained models to specific downstream tasks through fine-tuning, probing, and parameter-efficient methods that leverage pretraining investment for new applications.