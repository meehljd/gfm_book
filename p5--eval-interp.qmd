# Part V: Evaluation and Trust {.unnumbered}

Evaluating genomic models presents challenges that distinguish this domain from natural language processing or computer vision. Biological sequences contain evolutionary history: a model tested on homologous sequences may appear to generalize when it has merely memorized. Population structure creates spurious associations: a variant predictor may learn ancestry rather than pathogenicity. Nested functional hierarchies obscure what models actually capture: impressive aggregate performance can coexist with systematic failure on the variants that matter most clinically. Standard machine learning evaluation practices, developed for domains where training and test examples are approximately independent and identically distributed, become actively misleading when applied to genomic data without careful adaptation.

This part develops the frameworks and methodologies that determine whether genomic foundation models deliver on their promises. @sec-benchmarks surveys established benchmarks across protein, DNA, regulatory, and clinical domains, examining their construction, scope, and limitations. What do these benchmarks actually measure? When does benchmark success predict deployment value? How do we recognize when a benchmark has outlived its usefulness? @sec-evaluation addresses how to use benchmarks appropriately, developing principles for data splitting, metric selection, and statistical testing that produce trustworthy conclusions rather than inflated claims.

@sec-confounding confronts the systematic biases and data leakage pathways that pervade genomic datasets, with particular attention to population stratification, batch effects, and the hidden correlations that models exploit. @sec-uncertainty examines calibration and uncertainty quantification, the methods that determine whether model outputs can be trusted at face value or require careful interpretation. @sec-interpretability explores how to move beyond black-box prediction toward mechanistic understanding, distinguishing explanations that accurately reflect model computation from those that merely satisfy human intuition. Together, these chapters provide the critical toolkit needed to evaluate claims about genomic AI and to deploy models responsibly in research and clinical settings.