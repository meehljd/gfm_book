# Preface {.unnumbered}

Genomics is in the middle of a quiet revolution.

For years, “machine learning in genetics” meant hand-crafted features, linear models, and carefully tuned statistical pipelines. Variant calling relied on logistic regression and hidden Markov models. Genome-wide association studies (GWAS) and polygenic risk scores (PRS) summarized common variant effects. Deleteriousness scores like CADD distilled curated annotations into prioritization heuristics. Functional genomics resources—ENCODE, GTEx, large biobanks—arrived in waves, but each new dataset was bolted onto bespoke tools and task-specific models.

Over roughly the last decade, that picture has changed. Convolutional neural networks (CNNs) began to learn regulatory code directly from sequence. Transformers and protein language models showed how self-supervision on massive unlabeled corpora could extract rich biological structure. Genomic foundation models (GFMs) now promise reusable representations of DNA, RNA, protein, and multi-omic context that can be adapted to everything from variant effect prediction to clinical risk scoring.

This book is my attempt to make sense of that transition—*from sequence variation to genomic foundation models and back again to clinical and biological questions*—in a way that is coherent, historically grounded, and practically useful.

---

## Who This Book Is For

This book is written for readers who sit somewhere in the triangle between **genomics**, **statistics**, and **machine learning**:

- **Computational biologists and statistical geneticists**  
  who want to understand how deep learning and foundation models fit alongside GWAS, PRS, rare variant association, and functional genomics.

- **Machine learning researchers and engineers**  
  who are comfortable with CNNs, transformers, and language models, and want a pragmatic guide to what makes genomic data different from text or images.

- **Clinicians and translational researchers**  
  who increasingly see “AI scores,” “foundation models,” or “genomic LLMs” in papers, reports, and grant proposals, and want a grounded view of what these tools can and cannot do in practice.

Some familiarity with probability, linear models, and basic genomics (variants, genes, regulatory elements) will help, but the early chapters are designed to be a gentle ramp rather than a gate.

---

## How the Book Is Organized

The book is organized into five parts that mirror how the field itself has evolved—from data and pre-deep-learning methods to CNNs, transformers, foundation models, and finally clinical and translational applications. :contentReference[oaicite:0]{index=0}

- **Part I: Data & Pre-DL Methods (Chapters 1–4)**  
  We start from the raw materials: next-generation sequencing and variant calling, GWAS and PRS basics, classical deleteriousness scores, and foundational functional genomics resources. This part establishes the statistical and biological context that later deep models must respect.

- **Part II: CNN Seq-to-Function Models (Chapters 5–7)**  
  We then move into supervised sequence-to-function models like DeepSEA, ExPecto, and SpliceAI. These chapters show how CNNs can learn regulatory code, link chromatin to expression, and predict splicing, and they surface early lessons about architecture, context windows, and training data.

- **Part III: Transformer Models (Chapters 8–11)**  
  With that foundation, we broaden to representation learning and long-range modeling. We discuss sequence tokenization and context representation, protein language models, DNA foundation models, and hybrid architectures that couple CNNs and transformers to capture long-range regulatory interactions.

- **Part IV: GFMs & Multi-omics (Chapters 12–16)**  
  Here we step back and ask: what makes a genomic model a *foundation model*? We examine principles for designing GFMs, how variant effect prediction is being reshaped by foundation models, and how confounding, interpretability, and multi-omic integration become more critical as models grow larger and more general.

- **Part V: Applications (Chapters 17–19)**  
  Finally, we move from methods to workflows: clinical risk prediction, pathogenic variant discovery, and drug discovery/biotech applications. These chapters focus less on novel architectures and more on how GFMs plug into broader pipelines, evaluation strategies, and decision-making processes.

You do not have to read the book linearly. If you are already comfortable with GWAS and classical variant scores, you might skim Part I and start with the CNN or transformer chapters. If you care primarily about clinical or biotech applications, you may wish to read Part V early, referring back to earlier chapters as needed.

---

## Key Themes Across the Book

Several themes recur throughout the chapters, and the rest of the book can be read as a deepening of each of these:

### Data → Architecture Evolution

We trace how **data availability and representation** drove model design:

- From hand-engineered features and shallow models (e.g., CADD-style scores)  
- To supervised CNNs that learn regulatory motifs and grammars directly from sequence  
- To transformers and structured state-space models (SSMs) that operate on longer contexts and richer tokenizations  
- To GFMs trained to be reused across tasks, tissues, and species  

At each stage, the goal is not simply “more complex models,” but better alignment between *what the data contain* and *what the model can express*.

### Context Length Scaling: 1 kb → 100 kb → 1 Mb

Many of the most interesting genomic questions are **non-local**:

- Enhancers regulate genes hundreds of kilobases away.  
- Chromatin conformation creates long-range dependencies.  
- Polygenic traits aggregate effects across the entire genome.

We follow how context windows grew from ~1 kb promoter-centric CNNs, to tens of kilobases in ExPecto-style models, to 100 kb–1 Mb in modern hybrid and SSM-based architectures—and how each jump in context length changes both modeling tactics and biological interpretation.

### Self-Supervision: Leveraging Unlabeled Genomic Data

Unlike labeled regulatory or clinical datasets, the **raw genome is cheap and abundant**. We explore how self-supervised objectives—masked language modeling, next-token prediction, denoising tasks—allow models to learn useful representations from unlabeled sequences, and how these representations transfer to:

- Variant effect prediction  
- Regulatory element annotation  
- Cross-species generalization  
- Downstream clinical prediction tasks

### Transfer Learning: Protein LMs → DNA LMs; Human → Cross-Species

Genomic foundation models are fundamentally about **transfer**:

- From protein language models to variant effect prediction and structure-informed tasks  
- From human-focused models to cross-species models that incorporate evolutionary signal  
- From pretraining on large unlabeled corpora to fine-tuning on smaller, task-specific or cell-type-specific datasets

We emphasize both the opportunities and the pitfalls: where transfer learning works remarkably well, and where naive reuse can bake in biases or violate domain assumptions.

### Clinical Translation: VEP, PRS, Rare Disease, and Beyond

Throughout the book we keep returning to a basic question:

> When, if ever, do these models *change decisions* in medicine and biology?

We examine how GFMs augment or reshape:

- Variant effect prediction pipelines for rare disease diagnosis  
- Polygenic and multi-omic risk prediction for complex traits  
- Fine-mapping and gene prioritization in drug target discovery  
- Biomarker development, patient stratification, and trial design  

The focus is on workflows and evaluation: calibration, robustness, fairness, and reproducibility in real clinical and translational settings.

### Interpretability: From Black Box to Mechanistic Insight

Finally, we treat interpretability not as an afterthought, but as a central design constraint. As models grow larger and more flexible, we ask:

- Can we extract motifs, grammars, and regulatory programs from sequence models in a way that aligns with experimental biology?  
- How can we distinguish genuine mechanism from confounded shortcuts and benchmark leakage?  
- What tools—from saliency and attribution to mechanistic interpretability—are actually useful in genomics practice?

These themes tie together discussions of confounders, evaluation, and mechanistic modeling across Parts II–V.

---

## What This Book Is *Not*

To keep the narrative focused, this book deliberately avoids trying to be:

- A **comprehensive survey** of every published model or dataset.  
- A **software manual** for specific tools, APIs, or cloud platforms.  
- A **proof-heavy textbook** on statistical genetics or deep learning theory.

Instead, the goal is to provide a **conceptual map and a set of worked examples** that will help you navigate the rapidly evolving space of genomic foundation models, understand their assumptions and limitations, and decide where they fit into your own research or clinical practice.

Where detailed tutorials, code, or benchmarks are helpful, I point to external resources rather than reproducing them in full.

---

## How to Read and Use This Book

A few practical suggestions:

- **Use the early chapters as shared vocabulary.**  
  Even if you are already familiar with NGS pipelines or GWAS, skimming Chapters 1–4 ensures we share terminology and assumptions.

- **Pick application-driven paths.**  
  If your primary interest is clinical risk prediction, you might follow a path like: Chapters 2–3 → 5–7 → 10–13 → 17. For variant discovery: Chapters 1–4 → 9–13 → 18. For biotech and drug discovery: Chapters 3–4 → 9–12 → 16–19.

- **Move back and forth between methods and applications.**  
  The later application chapters are written to be readable on their own, but they gain depth if you revisit relevant method chapters (e.g., SpliceAI in Chapter 7 when thinking about splice-disrupting variants in Chapter 18).

- **Treat the references as a roadmap.**  
  The citations are curated to point to primary papers and representative follow-ups rather than exhaustive lists. Following those threads is often the fastest way to get from this conceptual overview to the cutting edge in a particular niche.

---

## Acknowledgments

This book grew directly out of work with the Mayo Clinic GenAI team, and I owe a special debt of gratitude to the colleagues who made that environment so generative.

To the **principal investigators and clinicians** who grounded our models in real clinical questions:  
Drs. **Shant Ayanian**, **Elena Myasoedova**, and **Alexander Ryu**.

To **leadership** for carving out the space, support, and vision for this work:  
Dr. **Matthew Callstrom**, Dr. **Panos Korfiatis**, and **Matt Redlon**.

To my **Data Science and Machine Learning Engineering colleagues**, whose ideas, code, and patience shaped many of the workflows and examples in this book:  
**Bridget Toomey**, **Carl Molnar**, **Zach Jensen**, and **Marc Blasi**.

I am also grateful for the architectural creativity and technical depth of our **collaborators at Cerebras**:  
**Natalia Vassilieva**, **Jason Wolfe**, **Omid Shams Solari**, **Vinay Pondenkandath**, **Bhargav Kanakiya**, and **Faisal Al-khateeb**.

And to our **collaborators at GoodFire**, whose partnership helped push these ideas closer to interpretable and deployable systems:  
**Daniel Balsam**, **Nicholas Wang**, **Michael Pearce**, and **Mark Bissell**.

I also want to thank my former colleagues at **LGC** for foundational work on protein language models and for the conversations that helped shape my thinking about PLMs:  
**Prasad Siddavatam** and **Robin Butler**.

Finally, beyond these named groups, this book owes a great deal to the broader community of people building and using genomic models: the teams who generated large-scale sequencing and functional genomics datasets; the authors of classical tools like CADD and PGS; the many groups pushing forward CNNs, transformers, and foundation models for DNA, RNA, and protein; and the clinicians, statisticians, and experimental biologists who keep all of us honest about what actually matters.

If this book helps you connect a new model to a real biological question, or use genomic foundation models a bit more thoughtfully in your own work, then it will have done its job.

— *Josh Meehl*
