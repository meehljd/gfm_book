::: {.callout-warning .content-visible when-profile="draft"}
**TODO:**

- Add figure: Attribution method comparison showing ISM, DeepLIFT, and integrated gradients on the same regulatory sequence with a known CTCF motif
- Add figure: TF-MoDISco pipeline schematic showing seqlet extraction → clustering → motif derivation → grammar inference
- Add figure: Attention pattern visualization from a genomic language model showing operon-like structure or enhancer-promoter linkages
- Add figure: Sei sequence class UMAP colored by regulatory program (promoter, enhancer, repressive, etc.) with example variants mapped
- Add figure: Faithfulness vs plausibility illustration showing a motif that "looks biological" but fails counterfactual deletion tests
- Add table: Comparison of attribution methods with columns for computational cost, reference dependency, noise characteristics, and typical use cases
- Consider adding BPNet case study as concrete example of motif discovery workflow
- Add code snippet or pseudocode for ISM calculation
:::


# Interpretability & Mechanisms {#sec-interp}

## Why Interpretability Matters for Genomic Models

Deep learning models in genomics increasingly operate as systems-level surrogates for biology. They predict chromatin features, gene expression, and variant effects directly from sequence, achieving accuracy that would have seemed implausible a decade ago. When such models drive mechanistic hypotheses or inform clinical decisions, understanding how they make predictions becomes as important as understanding how well they perform.

Interpretability in this context serves several distinct but interconnected roles. The most scientifically compelling is mechanistic insight: extracting sequence motifs, regulatory grammars, and long-range interaction patterns directly from trained models. A well-designed interpretability analysis can turn a black-box predictor into a source of candidate mechanisms that can be tested experimentally. When a model trained to predict chromatin accessibility learns filters that match known transcription factor binding motifs, this validates that the model has discovered biologically meaningful patterns. When the same analysis reveals novel motif variants or unexpected spacing constraints, it generates hypotheses that extend beyond what was known before training.

Interpretability also serves as a tool for model debugging and confounder detection. Deep networks can achieve high benchmark accuracy by learning spurious correlations rather than genuine regulatory signals. A model might learn that certain k-mers correlate with peak calls because of batch effects in the training data, or that GC content predicts chromatin accessibility because GC-rich regions tend to be more mappable and thus better covered by sequencing. Interpretability methods can reveal such shortcuts by showing what features the model actually relies upon. This diagnostic function complements the data-level confounder analyses discussed in @sec-confound by interrogating model internals directly.

In clinical and translational settings, interpretability supports variant interpretation workflows by explaining why specific rare or de novo variants are predicted to be damaging. A pathogenicity score alone may be insufficient for clinical decision-making; knowing that a variant disrupts a specific transcription factor binding motif in a disease-relevant enhancer provides interpretable evidence that can be combined with family history, functional assays, and literature review. Interpretability tools that produce such explanations bridge the gap between computational predictions and actionable clinical reasoning.

Finally, interpretability enables scientific communication by condensing high-dimensional latent representations into human-readable abstractions. Motifs, regulatory sequence classes, and interaction graphs can be shared across laboratories and applications in ways that raw model weights cannot. A published motif vocabulary derived from a foundation model becomes a reusable resource for the community, even if the original model is computationally expensive to run or subject to access restrictions.

This chapter surveys the main interpretability tools developed for genomic models, from convolutional filter analysis and saliency maps to global regulatory vocabularies and attention patterns in genomic language models. Throughout, the emphasis is on mechanistic interpretability: moving from correlational explanations ("what features correlate with the prediction?") to causal hypotheses ("what regulatory mechanism does the model imply?").


## Interpreting Convolutional Filters as Motifs

Convolutional neural networks remain a workhorse for modeling cis-regulatory sequence, as described in Chapters 5 through 7. In many of these models, first-layer convolutional filters act as motif detectors. A filter slides along the one-hot encoded sequence, computing a dot product between its learned weights and the local sequence window at each position. High activation indicates that the subsequence closely matches the filter's preferred pattern.

### From Filters to Motif Logos

Converting learned filters into interpretable motifs follows a standard workflow. The trained model is run on a large sequence set, typically the training data or genome-wide tiles, and for each filter the positions where its activation exceeds a threshold are recorded. The fixed-length windows around these high-activation positions are then extracted and aligned, and base frequencies at each position are computed to build a position weight matrix (PWM). This PWM can be visualized as a sequence logo, where letter heights reflect information content, and compared to known motif databases like JASPAR or HOCOMOCO using similarity scores. Filters that produce PWMs resembling characterized transcription factors can be annotated with candidate TF identities.

This procedure has been applied extensively to models like DeepSEA and its successors, demonstrating that early convolutional layers learn motifs for canonical transcription factors and chromatin-associated patterns. Such validation confirms that models are discovering biologically meaningful sequence features rather than arbitrary patterns that happen to correlate with training labels.

### Beyond First-Layer Filters

Deeper convolutional layers aggregate lower-level motifs into more complex representations. These layers can encode combinatorial motifs that respond to pairs or clusters of transcription factor binding sites, grammar patterns involving distance or orientation constraints, and contextual preferences that depend on surrounding sequence composition like GC content or nucleosome positioning signals. However, directly interpreting deeper layers becomes increasingly difficult because receptive fields expand and nonlinearities accumulate. The activation of a deep-layer filter depends on intricate combinations of early-layer patterns, making it hard to summarize what the filter "means" in simple biological terms. This interpretive challenge motivates attribution-based approaches that trace predictions back to individual input bases rather than trying to interpret intermediate representations.


## Attribution Methods: Connecting Bases to Predictions

Attribution methods assign an importance score to each input base, reflecting how much that position contributes to a prediction for a specific task and sequence. If a model $f(x)$ predicts some output from sequence $x$, attribution methods estimate the contribution of each base $x_i$ to $f(x)$, typically for a specific output neuron such as chromatin accessibility in a particular cell type. The resulting attribution maps can reveal which sequence positions drive a prediction, highlighting candidate motifs and regulatory elements.

### In Silico Mutagenesis

In silico mutagenesis (ISM) is conceptually the most straightforward attribution method and works with any model, regardless of architecture. For each position $i$ and alternative base $b$, ISM creates a mutated sequence $x^{(i \rightarrow b)}$ and computes the change in prediction: $\Delta f_{i,b} = f(x^{(i \rightarrow b)}) - f(x)$. These changes can be aggregated across non-reference alleles to obtain a per-base importance score, typically by taking the maximum or mean absolute change.

ISM provides true counterfactual information about how the model responds to sequence perturbations. Unlike gradient-based methods that estimate local sensitivity, ISM directly measures what happens when a base is changed. This makes ISM the gold standard for faithfulness: if ISM shows that mutating a position changes the prediction, that is a direct observation rather than an approximation.

The primary limitation of ISM is computational cost. Scoring all possible single-nucleotide substitutions requires $L \times 3$ forward passes for a sequence of length $L$, which becomes expensive for long sequences or large models. Variants of ISM can reduce this cost by focusing on specific regions of interest or by using saturation mutagenesis only in targeted windows. For variant effect prediction specifically, ISM reduces to computing the difference between reference and alternative allele predictions, which requires only two forward passes per variant.

### Gradient-Based Methods

Gradient-based methods approximate how much the prediction would change if each input base were perturbed, using backpropagation rather than explicit perturbation. The simplest approach computes the gradient of the output with respect to the input: $s_i = \partial f(x) / \partial x_i$. With one-hot encoding, this gradient can be interpreted as the sensitivity to changing the nucleotide at position $i$. A common variant multiplies the gradient by the input to focus on positions where the current nucleotide (rather than hypothetical alternatives) is important.

Vanilla gradients require only a single backward pass per sequence, making them computationally efficient. However, they are susceptible to gradient saturation, where gradients vanish in regions where the model is already confident. Saturated regions may be functionally important but show near-zero gradients because small perturbations do not change the prediction.

DeepLIFT (Deep Learning Important FeaTures) addresses saturation by comparing neuron activations between an input and a reference sequence, distributing differences back to inputs using layer-wise rules rather than raw gradients. This approach avoids gradient saturation and enforces a consistency constraint: the sum of input contributions matches the difference in output between input and reference. DeepLIFT has been widely used for genomic models, particularly in conjunction with TF-MoDISco, where its base-level importance scores serve as inputs for motif discovery.

Integrated gradients (IG) compute the path integral of gradients along a linear interpolation from a reference sequence $x'$ to the input $x$: 
$$\text{IG}_i(x) = (x_i - x'_i) \int_{\alpha=0}^1 \frac{\partial f\left(x' + \alpha(x - x')\right)}{\partial x_i} d\alpha.$$
This integral is approximated via a Riemann sum over discrete interpolation steps. Integrated gradients satisfy desirable theoretical properties including sensitivity (if changing an input changes the output, that input receives nonzero attribution) and implementation invariance (functionally equivalent networks produce identical attributions). In practice, IG tends to be less noisy than raw gradients.

All gradient-based methods require choosing a reference sequence, which significantly affects the resulting attributions. Common choices include random genomic sequence, dinucleotide-shuffled versions of the input that preserve local composition, or an average "non-functional" sequence. Different references emphasize different aspects of the signal. A shuffled reference highlights features that differ from random sequence with matched composition, while a zero reference (all bases equally weighted) treats any informative position as important.


## From Attributions to Motifs: TF-MoDISco

Attribution maps highlight where the model focuses, but they do not automatically yield consistent motifs or regulatory grammars. A DeepLIFT attribution track might show high importance at scattered positions throughout a sequence without revealing that those positions collectively form instances of the same transcription factor binding site. TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) was developed to bridge this gap by discovering motifs from attribution scores rather than from raw sequences.

The core insight of TF-MoDISco is that operating on importance-weighted sequences rather than raw sequences focuses motif discovery on positions the model actually uses. Traditional motif discovery algorithms applied to regulatory sequences must contend with the fact that most positions are not part of functional motifs. By extracting "seqlets" (short windows where total importance exceeds a threshold) and clustering them based on both sequence and importance profiles, TF-MoDISco identifies the specific patterns that drive model predictions.

The workflow begins by computing importance scores for many sequences using DeepLIFT, ISM, or integrated gradients. Local windows where total importance exceeds a threshold are extracted as seqlets, each representing a candidate motif instance. These seqlets are then compared using similarity metrics that consider both sequence content and the importance score profile, and clustered into groups corresponding to putative motifs. Within each cluster, seqlets are aligned and consolidated into position weight matrices and importance-weighted logos. The resulting motifs can be matched to known transcription factor binding sites or flagged as novel patterns.

Beyond individual motifs, TF-MoDISco enables grammar inference by analyzing how motifs co-occur within sequences. Mapping motif instances back onto the genome reveals patterns of co-occurrence, characteristic spacing between motif pairs, and orientation preferences. These grammatical rules can be validated through in silico experiments: inserting or removing motifs in synthetic sequences and observing whether predictions change as expected.

When applied to models like BPNet trained on ChIP-seq data, TF-MoDISco has recovered known transcription factor motifs, discovered novel sequence variants, and revealed grammars such as directional spacing constraints that have been validated with synthetic reporter assays. In the context of genomic foundation models, an analogous workflow applies: use the model to produce base-level attributions for a downstream task, run TF-MoDISco to extract a task-specific motif vocabulary, and analyze how motif usage varies across cell types, conditions, or species.


## Interpreting Attention and Long-Range Context

Transformer-based models use self-attention to mix information across long genomic contexts, enabling them to capture distal regulatory interactions and genomic organization that are invisible to models with narrow receptive fields. Interpretability for these models often centers on attention patterns and long-range attribution, asking which distant positions influence predictions at a given location.

### Attention in Genomic Language Models

Genomic language models (gLMs) trained on prokaryotic genomes treat genes or genomic tokens as elements of a sequence and learn to predict masked tokens, analogous to protein or text language models. Work on gLMs trained on millions of metagenomic scaffolds has shown that these models learn non-trivial genomic structure that can be read out from attention patterns.

Certain attention heads specialize in connecting genes that are part of the same operon or functional module. When attention weights are visualized as edges between gene positions, they reveal networks of co-regulated genes that often align with known operon boundaries. Other heads capture functional semantics, with attention patterns that cluster genes by enzymatic function or gene ontology category. Still others encode taxonomic signals, separating clades and capturing clade-specific gene neighborhood patterns.

These findings suggest that the model has inferred a "syntax" of gene neighborhoods: which genes tend to co-occur, in what order, and conditioned on phylogenetic context. While attention weights are not universally faithful explanations of model decisions (high attention need not correspond to large causal influence), attention analysis in genomic language models reveals emergent mechanistic structure that is consistent with known biological organization.

### Distal Regulatory Elements in Enformer-Like Models

Enformer and related models predict chromatin features and gene expression from large genomic windows spanning 100 kb or more, combining convolutional layers for local feature extraction with transformer blocks for long-range integration. A central interpretability question for these models is which distal enhancers drive predicted expression at a given transcription start site, and how variants in distal elements propagate to gene-level outputs.

Gradient-based attributions can be computed over the entire input window, producing importance tracks that span tens to hundreds of kilobases. Visualizing these tracks alongside gene annotations reveals putative enhancers and silencers: positions where the model places high importance for predicting expression. Attention pattern analysis complements gradient methods by identifying attention heads that consistently link distal positions to transcription start site regions. These high-attention edges can be compared to chromatin conformation data from Hi-C experiments to assess whether the model has learned biologically plausible enhancer-promoter interactions.

In silico perturbation experiments provide additional validation. Candidate enhancers identified by attribution or attention can be deleted or scrambled in the input sequence, and the resulting change in predicted expression quantifies how much that element contributes to the model's output. Inserting synthetic motifs or strengthening existing motif scores can test dose-response relationships, asking whether enhancing a putative regulatory element produces the expected increase in predicted expression.

Together, these analyses can reveal candidate enhancer-promoter links and the transcription factor motifs that the model deems critical for gene regulation. They help translate raw attention weights and attribution scores into mechanistic hypotheses that can be tested experimentally.


## Global Regulatory Vocabularies: Sei Sequence Classes

Most motif-based interpretation operates at the local level, asking which motifs appear in a particular sequence and how they contribute to a specific prediction. Sei takes a complementary global approach by learning a vocabulary of regulatory sequence classes that summarize the vast diversity of chromatin profiles across the genome.

### The Sei Framework

Sei trains a deep sequence model to predict tens of thousands of chromatin profiles covering transcription factor binding, histone modifications, and chromatin accessibility across many cell types. The key interpretability step is to compress these thousands of outputs into a few dozen sequence classes, each representing a characteristic regulatory activity pattern.

Sequence classes are derived by clustering genome-wide predictions. For each of millions of genomic positions, Sei computes predicted chromatin profiles and projects them into a lower-dimensional space using principal component analysis. These projections are then clustered to identify recurrent patterns of regulatory activity. The resulting classes include promoter-like patterns enriched for H3K4me3 and TSS proximity, enhancer-like patterns with H3K27ac and H3K4me1, repressive patterns dominated by H3K27me3 or H3K9me3, and cell-type-specific modules corresponding to neuronal, immune, or other lineage-specific regulatory programs.

Each input sequence or variant can be scored against all sequence classes, effectively mapping it to a point in a low-dimensional regulatory activity space. This representation has several interpretability advantages. Instead of reasoning about thousands of raw chromatin predictions, one can describe a sequence in terms of human-interpretable categories: "this variant increases neuronal enhancer activity while decreasing polycomb repressive marks." Variants can be summarized by their shifts in sequence-class scores, yielding concise functional descriptions. GWAS loci can be enriched for specific sequence classes, revealing which tissues and regulatory programs are most relevant to a disease.

This notion of a regulatory vocabulary parallels word embeddings or topic models in natural language processing. It provides a bridge between highly multivariate model outputs and mechanistically interpretable axes of variation that can be communicated across studies and applications.


## A Case Study: From Base-Pair Attributions to Regulatory Grammar

Putting the pieces together, a typical mechanistic interpretability pipeline for a CNN or transformer-based regulatory model proceeds through several connected stages.

The starting point is a trained predictive model, for example one that predicts chromatin accessibility or transcription factor ChIP-seq tracks from sequence. For sequences where the model makes confident predictions in a target cell type, base-level attributions are computed using DeepLIFT or integrated gradients. These attributions are fed into TF-MoDISco, which extracts seqlets from high-attribution regions, clusters them, and derives motifs. The resulting motifs are matched to known transcription factors where possible, and novel motifs are flagged for further investigation.

Grammar inference follows from analyzing motif instances across the full set of high-confidence predictions. Motif co-occurrence patterns reveal which factors tend to operate together. Spacing distributions between motif pairs identify characteristic distances that may reflect cooperative binding or nucleosome constraints. Orientation analysis determines whether certain motif pairs require specific relative orientations to function. In silico knock-in and knock-out experiments confirm these grammatical dependencies: if the model predicts that two motifs must co-occur for high accessibility, deleting either motif from a sequence should reduce the prediction, while inserting both into a neutral background should increase it.

The local motif grammar can then be connected to global regulatory context. Motif-rich regions can be mapped to Sei sequence classes to understand what broader regulatory programs they participate in. For transformer-based models, attention patterns or long-range attributions can link local motif clusters to distal elements, revealing enhancer-promoter architectures or chromatin domain boundaries.

Validation closes the loop by connecting model-derived hypotheses to external evidence. Do motif disruptions align with reporter assay effects or allelic imbalance measured in functional genomics experiments? Do inferred enhancer-promoter links correspond to contacts observed in Hi-C or to effects measured in CRISPR perturbation screens? This integrated approach moves beyond descriptive saliency maps toward testable hypotheses about regulatory logic.


## Evaluating Interpretations: Faithfulness versus Plausibility

Not all explanations are equally trustworthy. Effective interpretability work must grapple with the distinction between plausibility (does the explanation "look" biological?) and faithfulness (does the explanation accurately reflect the internal computation of the model?).

An explanation is plausible if it matches prior biological knowledge. Discovering a motif that resembles CTCF is plausible because CTCF is a well-characterized chromatin organizer. Plausibility provides reassuring sanity checks but does not guarantee that the model actually uses the plausible feature. An explanation is faithful if perturbing the identified features changes the model's output as predicted. If removing a putative CTCF site from a sequence causes the model's chromatin accessibility prediction to drop, the explanation has some degree of faithfulness.

Several pitfalls complicate the relationship between plausibility and faithfulness. Attention weights in transformer models need not correspond to large changes in output; high attention may reflect information routing rather than causal influence on predictions. A model might attend strongly to certain positions for bookkeeping purposes without those positions driving the final output. Combining attention analysis with attribution or perturbation experiments yields more reliable insights by checking whether high-attention positions are also high-importance positions under counterfactual tests.

Gradient-based attribution methods can produce noisy maps or miss important features in saturated regions where gradients are near zero. Comparing multiple methods (ISM, DeepLIFT, integrated gradients) and checking for consistency helps identify robust signals. If different methods agree that a position is important, confidence increases; if they disagree, the discrepancy warrants investigation.

Perhaps most problematically, models may learn shortcut features that produce clean, plausible-looking motifs but are not mechanistically meaningful. A model might learn that certain k-mers correlate with peak calls because of barcode sequences in the training data, or that GC content predicts accessibility because of mappability biases. These shortcuts can produce interpretable patterns that are biologically vacuous.

Recommended practices for validating interpretations include sanity checks where model weights are randomized (attributions should degrade to noise) or training labels are scrambled (derived motifs should disappear or lose predictive power). Counterfactual tests delete or scramble high-attribution regions to confirm that predictions drop accordingly, or insert discovered motifs into neutral backgrounds to test gain-of-function effects. Benchmarking on synthetic datasets with known ground-truth grammar provides controlled settings where the ability of interpretability methods to recover planted motifs and interactions can be quantified.


## A Practical Interpretability Toolbox

For practitioners working with genomic foundation models and their fine-tuned derivatives, several interpretability strategies form a practical toolbox.

Local effect estimation focuses on individual variants or short sequence windows. For variant effect prediction, comparing reference and alternative allele scores provides direct effect estimates, while small-window ISM around variants reveals which nearby positions modulate the effect. Per-base attributions can be aggregated into per-variant or per-motif scores for summary statistics.

Motif and grammar discovery begins with computing base-level attributions for sequences where the model makes high-confidence predictions. Running TF-MoDISco or similar algorithms builds a motif vocabulary that can be compared across tasks, cell types, or training conditions. Grammar analysis examines motif co-occurrence, spacing, and orientation to infer combinatorial rules.

Global context visualization applies to transformer-based models, where attention patterns can reveal which distant positions the model considers when making predictions at a given location. For hybrid architectures like Enformer, combining long-range attributions with contact maps helps hypothesize regulatory architectures that span tens to hundreds of kilobases.

Regulatory vocabularies and embeddings use frameworks like Sei to project sequences into interpretable regulatory activity spaces. Clustering variants, enhancers, or genomic regions by their sequence-class profiles reveals shared regulatory programs and enables compact summaries of complex predictions.

Model and dataset auditing uses interpretability tools to identify reliance on confounded or undesirable features. Cross-referencing with the confounder taxonomy from @sec-confound helps design deconfounded training and evaluation schemes. If interpretability reveals that a model relies heavily on GC content or batch-specific signals, this diagnoses a problem that evaluation metrics alone might miss.

Human-in-the-loop analysis integrates motif and sequence-class outputs into visualization tools such as genome browsers with attribution tracks, motif annotations, and class scores. Domain experts can then iteratively refine hypotheses, identifying patterns that merit experimental follow-up and flagging predictions that seem biologically implausible.


## Outlook: From Explanations to Mechanistic Models

Interpretability in genomic deep learning is evolving from post hoc explanation toward model-assisted mechanistic discovery. Foundation models provide rich latent spaces and long-range context that capture regulatory information at unprecedented scale. Attribution and motif discovery tools translate those representations into candidate regulatory grammars that can be tested experimentally. Global vocabularies like Sei's sequence classes offer interpretable axes spanning thousands of assays, enabling systematic characterization of regulatory programs across the genome.

Attention analysis in genomic language models reveals emergent gene-level organization, suggesting that models trained on raw sequence implicitly learn operon structure, co-regulation patterns, and phylogenetic context. These findings hint at scalable ways to capture systems-level biology from sequence alone, complementing the multi-omic integration approaches discussed in @sec-systems.

The next frontier is to close the loop between interpretability and model development. Insights from interpretability (motifs, grammars, sequence classes) can inform better architectures and training objectives. Experimentally validated grammars can be fed back into models as inductive biases, constraining the hypothesis space to biologically plausible solutions. Evaluation frameworks can measure not only predictive accuracy but also mechanistic fidelity: how well do model-derived hypotheses align with the causal structure of regulatory biology revealed by perturbation experiments?

In this sense, interpretability is not merely a diagnostic for black-box models. It is a central tool for turning genomic foundation models into engines of biological discovery, capable of bridging the gap between sequence-level predictions and the mechanistic understanding that underpins robust clinical translation. When a model's explanations match experimental observations and generate validated predictions, it becomes more than a predictor: it becomes a hypothesis-generating system that accelerates the scientific enterprise.