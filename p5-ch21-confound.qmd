# Confounders in Model Training {#sec-confound}

In previous chapters, we have largely treated model performance curves and ROC plots as if they faithfully reflected how well a model captured biology. Under that view, higher AUC or lower loss is always better, and improvements on a benchmark mean real progress.

This chapter interrogates that assumption.

Genomic datasets are riddled with hidden structure: ancestry, family relatedness, sequencing center, capture kit, hospital system, recruitment year, label curation protocol, and more. These factors correlate with both the features (genotypes, readouts, gene expression, epigenomic marks) and the labels (disease status, variant pathogenicity, expression levels). When such *confounders* are not explicitly controlled, models, especially large, flexible foundation models, can easily learn shortcuts that exploit them.

Population structure, technical batch effects, benchmark leakage, and label bias are not unique to deep learning; they affect linear regression and logistic models just as much. What makes them particularly dangerous in the foundation model era is scale: larger datasets and more expressive architectures make it easier for models to discover subtle shortcuts that are invisible in standard diagnostics but dramatically hurt out-of-distribution performance.

Our goal in this chapter is not to provide a complete causal inference textbook. Instead, we offer a practical guide for genomic modelers. We examine how to recognize common confounders in genomic and clinical datasets, how these confounders show up in benchmarks and cross-cohort evaluation, how to design splits and experiments that probe real generalization rather than memorization or shortcut learning, how to mitigate confounding via study design, modeling choices, and post-hoc analysis, and how to report results transparently so that readers can assess robustness and fairness.

We will repeatedly cross-reference the benchmark-focused discussion in @sec-benchmarks and the methodological guidance in @sec-eval. Here, the emphasis is on pitfalls: ways models can look impressive on paper while failing to learn the desired biology.


## Definitions: Confounding, Bias, and Leakage

Before diving into genomics-specific examples, it is helpful to clarify terminology.

A **confounder** is a variable that influences both the input features and the label. For example, ancestry affects allele frequencies across the genome and disease risk via environmental and socioeconomic pathways. If ancestry is not explicitly modeled or controlled, a model trained to predict disease may in fact be learning ancestry.

**Bias** refers to a systematic deviation from the quantity we intend to estimate or predict. Bias can result from confounding, but also from measurement error, label definitions, sampling procedures, or deployment differences (for instance, different prevalence in clinical practice versus a case–control study).

**Leakage** occurs when information about the test set inadvertently influences model training or selection. Leakage can occur through overlapping individuals or variants, shared families, duplicated samples, or indirect channels such as pretraining on a resource that is later used as a benchmark.

**Distribution shift** is a mismatch between the data distribution used for training and the distribution encountered during evaluation or deployment. Shift can be driven by changes in ancestry composition, sequencing technology, clinical coding practices, or temporal trends in care.

These phenomena are intertwined. Confounders create biases in estimated effects and predictions. Leakage can hide those biases by making held-out performance look better than it should. Distribution shifts then cause performance to collapse when a model is deployed outside the environment in which its shortcuts were learned.

For genomic foundation models, these risks are magnified by three features. First, high-dimensional structure means that genomes encode ancestry, relatedness, and assay conditions in thousands of subtle ways, even if we never explicitly provide those labels. Second, powerful pattern detection means that shallow models might miss some shortcuts, but large transformers will find them if they help optimize the training objective. Third, complex training regimes involving pretraining on biobank-scale data, followed by fine-tuning on curated labels and evaluation on community benchmarks, create many opportunities for direct and indirect leakage.


## A Taxonomy of Confounders in Genomic Data

Confounders in genomic modeling can be grouped into several broad categories. The same variable, say recruitment site, may simultaneously induce ancestry differences, batch effects, and label bias.

**Population structure and relatedness** includes continental and sub-continental ancestry, family relatedness (siblings, parent–offspring, cryptic relatedness), and founder effects with local haplotype structure.

**Technical batch and platform effects** arise from sequencing center, instrument, capture kit, and library preparation protocol. Different read lengths, coverage patterns, aligners, and variant callers introduce systematic differences. Distinct assay chemistries for DNA, RNA, or epigenomic profiling further complicate matters.

**Cohort and institution effects** reflect hospital systems with different patient populations and coding practices, biobank-specific inclusion and exclusion criteria, and ascertainment patterns (referral centers versus population-based studies).

**Label and curation bias** stems from clinical labels derived from billing codes, problem lists, or registry entries. Variant pathogenicity labels from ClinVar and similar databases reflect expert curation, test panels, and past literature biases [@landrum_clinvar_2018]. Expression, regulatory, or splicing labels derived from specific tissues or cell lines carry their own limitations [@gtex_2020; @kagda_encode_2025].

**Temporal drift** includes changes in clinical practice, diagnostic criteria, or coding over time, evolving sequencing technologies and QC pipelines, and shifts in population behavior or environment.

**Knowledge-base and benchmark leakage** involves using resources like gnomAD [@karczewski_gnomad_2020] or UK Biobank [@bycroft_ukbiobank_2018] in both model training and evaluation, or reusing curated variant sets across multiple publications and tasks.

Many of these factors are present simultaneously in large-scale genomic resources and biobanks. Ignoring them leads to deceptively strong benchmark performance and brittle behavior when models are deployed in new contexts.

::: {.callout-note}
**Visual suggestion:** A comprehensive table summarizing confounder categories (population, technical, cohort, label, temporal, knowledge-base leakage) with columns for "Example variables", "Common manifestations in genomic data", "Diagnostics", and "Mitigation strategies" would provide readers with a practical reference.
:::


## Population Structure and Ancestry as Shortcuts

### How Ancestry Becomes a Confounder

Human genetic variation is structured by ancestry: allele frequencies, haplotype blocks, and linkage disequilibrium patterns differ across populations. Principal components (PCs) computed from genome-wide genotypes provide a low-dimensional summary of this structure and are now standard in genome-wide association studies (GWAS) to correct for stratification [@patterson_population_2006; @price_pca_2006].

Ancestry, however, is not just a convenient statistical nuisance. It is intertwined with geography, environment, socioeconomic status, and access to healthcare. These factors directly impact disease risk, phenotyping, and the likelihood of receiving genetic testing. Thus, ancestry influences both features (the genome) and labels (phenotypes and clinical annotations), creating classic confounding.

For example, a rare disease clinic serving primarily individuals of European ancestry may contribute most pathogenic variants in ClinVar, while variants observed predominantly in other ancestries remain labelled as variants of uncertain significance (VUS) [@landrum_clinvar_2018]. Biobanks enriched for particular ancestries, birth cohorts, or health systems introduce subtle differences in both genotype and phenotype distributions [@bycroft_ukbiobank_2018].

A model trained on such data may appear to excel at predicting disease risk or variant pathogenicity, while in reality it has learned to infer ancestry and exploit its correlation with label definitions and clinical practice.

::: {.callout-note}
**Visual suggestion:** A causal diagram or schematic showing ancestry influencing both genotype and environment/clinical practice, which in turn influence disease labels, would clarify where a model might pick up shortcuts (predicting ancestry instead of biology).
:::

### Ancestry and Foundation Models

Foundation models trained directly on nucleotide sequences or variant streams clearly see ancestry information: the distribution of k-mers and haplotypes differs by population [@he_nucleic_2023]. When such models are fine-tuned to predict disease risk, expression, or pathogenicity, they may leverage this ancestry signal as an easy shortcut.

Multi-ancestry GWAS and polygenic score studies underscore the magnitude of these issues. Risk scores derived from primarily European cohorts often transfer poorly to other ancestries, with attenuated performance and systematic miscalibration [@ishigaki_multi-ancestry_2022]. Similar pitfalls arise when genomic foundation models are trained or validated in ancestry-skewed datasets.

Crucially, increasing model capacity does not automatically solve ancestry bias; it can make it worse by making it easier to detect and exploit subtle ancestry-linked features. Robust evaluation must therefore probe performance within and across ancestry groups, not just overall metrics.


## Technical Batch and Platform Effects

Technical pipelines are complex. Each step from sample collection through library preparation, sequencing, alignment, and variant calling can introduce systematic differences that act as confounders.

Different sequencing centers use distinct instruments, reagents, and QC thresholds. Library preparation protocols vary in GC bias and coverage profiles. Capture kits and read lengths change the probability of calling variants in specific genomic regions. Assay platforms for epigenomics, expression, or chromatin conformation have distinct noise and bias characteristics.

When samples from a particular batch or platform are disproportionately drawn from a specific label class (say, cases sequenced at one center and controls at another), models can learn to distinguish batches rather than biology. In high-dimensional feature spaces, even subtle batch-specific artifacts (coverage dips, variant density patterns, adapter content) can be exploited.

Foundation models are particularly sensitive: pretraining on raw reads, read piles, or coverage tracks makes it easy to detect batch signatures unless preprocessing and normalization are carefully handled.

Common patterns include clustering by batch or sequencing center when embedding outputs or PCs are visualized, strong predictive performance that collapses when evaluated on data from a new center, instrument, or protocol, and models that can nearly perfectly predict batch ID, platform, or capture kit from the inputs alone.

::: {.callout-note}
**Visual suggestion:** A PCA or UMAP of sample embeddings colored by batch or sequencing center (left panel) versus labeled phenotype (right panel) would illustrate strong batch structure relative to signal-of-interest.
:::


## Cohort, Institution, and Label Bias

Beyond ancestry and technical factors, cohort and institution choices introduce additional confounding.

**Cohort effects** arise because population-based cohorts differ from hospital-based or referral cohorts in terms of disease prevalence, comorbidities, and socio-demographic composition.

**Institutional practices** vary across hospitals and clinics, which use distinct coding practices, diagnostic thresholds, and follow-up schedules. These alter labels derived from electronic health records.

**Ascertainment and testing bias** means that individuals who receive genome sequencing or panel testing may be more severely affected, more affluent, or preferentially from particular ancestry groups.

Label sources further complicate matters. Claims-based phenotypes and ICD codes can be noisy and incomplete. Registries and expert-curated variant databases may lag behind current knowledge and reflect historical focus areas [@landrum_clinvar_2018]. Functional assay readouts often depend on specific cell lines, overexpression systems, or environmental conditions, which may not generalize.

For genomic foundation models that aspire to downstream clinical utility, these biases can dominate the training signal. Without careful design, models may simply reproduce institutional practices and historical biases rather than underlying biology.


## Data Splitting and Benchmark Leakage

Data splitting is one of the primary tools we have to assess generalization. However, naive splits can silently allow leakage and confounding.

### Types of Splits

Common splitting strategies include several approaches. **Random individual-level splits** assign individuals randomly to train, validation, and test sets. **Family-aware splits** keep families together to avoid relatedness leakage across splits. **Locus-level splits** keep variants at the same genomic position, and often in close proximity, within the same split to prevent models from memorizing site-specific patterns. **Region or chromosome splits** hold out entire genomic regions or chromosomes to assess long-range generalization. **Cohort or site splits** hold out entire cohorts, sequencing centers, or hospitals to probe robustness across institutions. **Time-based splits** use earlier data for training and hold out later data to simulate prospective performance and account for temporal drift.

Each strategy targets specific forms of leakage and confounding. Unfortunately, many benchmarks rely on random individual-level splits that allow information leakage across families, loci, or cohorts.

::: {.callout-note}
**Visual suggestion:** A figure comparing different splitting strategies (random individual, family-aware, locus-level, chromosome, cohort, time-based) and highlighting which leakage pathways each prevents would be instructive. Consider a timeline or schematic genome diagram with color-coded splits.
:::

### Overlap and Indirect Leakage

Leakage need not be explicit. A few common patterns in variant effect prediction and risk modeling include variant overlap across resources, where variants used for training may also appear in external benchmarking datasets (for instance, ClinVar variants used during model development, then again in downstream evaluation), even if the exact label is not reused. Relatedness across splits allows close relatives of test individuals to appear in the training set, letting models memorize segments of the genome and inflate apparent performance on rare variants. Knowledge-base pretraining occurs when models pre-trained on resources like gnomAD allele frequencies or UK Biobank genotype–phenotype associations [@karczewski_gnomad_2020; @bycroft_ukbiobank_2018] are later evaluated on benchmarks built from the same data without explicitly acknowledging that this constitutes leakage.

These issues are particularly acute for community benchmarks that reuse widely popular variant sets. Without rigorous deduplication and splitting at the locus or individual level, performance may primarily reflect memorization.

::: {.callout-note}
**Case study suggestion:** An example of benchmark leakage in a variant effect prediction task, illustrating how deduplicating variants across training and benchmark sets causes a noticeable drop in AUC, would reveal prior over-optimism and make the concept concrete.
:::


## Manifestations of Confounding in Genomic Models

How can we tell when confounding is driving performance?

Some common signatures include performance gaps across subgroups, where models exhibit high overall performance but much worse metrics in under-represented ancestry groups, sequencing centers, or time periods. Prediction–confounder correlation occurs when model outputs are strongly correlated with ancestry PCs, batch indicators, or recruitment site, even after conditioning on the label. Performance collapse under cohort or time-based splits happens when models that perform well under random splits show sharp degradation when evaluated on held-out cohorts or later time windows. Simple confounder-only baselines performing comparably means that logistic regression on ancestry PCs, batch IDs, or site indicators alone achieves performance close to that of a complex model.

These signatures are not proofs of confounding, but they are red flags that should trigger deeper investigation.

For genomic foundation models, additional manifestations include token and embedding structure dominated by confounders, where sequence or variant embeddings cluster by ancestry or batch rather than by biological similarity. Contextual embeddings may track institutional labels, with models fine-tuned on EHR-linked genomics encoding institutional coding practices more strongly than underlying phenotypes.

The key lesson is that good performance on a single benchmark, especially with random splits and without subgroup analysis, is insufficient evidence that a model has learned robust, biologically meaningful patterns.


## Diagnostics and Sanity Checks

Because confounding is often subtle, we need systematic diagnostics.

### Baseline Models Using Confounders Only

An essential check is to train simple models using only potential confounders. This includes logistic regression on ancestry PCs and basic covariates, gradient boosting trees on technical variables (batch, sequencing center, capture kit), and models using only cohort or site indicators.

If these baselines approach the performance of complex genomic models, a substantial portion of the signal is likely driven by confounding. Reporting these baselines alongside genomic models makes hidden shortcuts more visible.

### Subgroup Performance and Calibration

Performance should be reported stratified by ancestry or population group, sequencing center or platform, cohort, institution, or country, and time period.

This includes both discrimination metrics (AUC, precision–recall) and calibration diagnostics. Poor calibration or systematic over- or under-prediction in specific subgroups often reveals confounding or distribution shift.

### Prediction–Confounder Association

Plotting model predictions against potential confounders can be revealing. Look for correlations between predicted risk and ancestry PCs after adjusting for true status, differences in mean predicted risk across batches, sites, or time periods within the same label class, and association tests using regression or mutual information between predictions and confounders.

Strong residual associations indicate that the model is encoding confounders beyond what is needed to predict the label.

### Split Sensitivity Analyses

Varying the splitting strategy is a powerful diagnostic. Re-evaluate performance under locus-level or chromosome-level splits, hold out entire cohorts, sites, or time windows, and exclude families or closely related individuals from test sets.

Large drops in performance under stricter splits suggest that initial results were inflated by leakage or confounding.

### Negative Controls and Perturbations

Finally, negative controls and simple perturbations can help. Use outcome labels known to be unrelated to genomics as negative controls; good performance suggests confounding. Randomize labels within batches or ancestry strata and confirm that models cannot achieve meaningful performance. Remove suspected confounder variables (when explicitly present) or residualize features and compare performance.

These diagnostics do not remove confounding, but they help quantify its impact and guide mitigation efforts.


## Mitigation Strategies

No mitigation strategy is perfect, and trade-offs between bias, variance, and coverage are inevitable. Nonetheless, several practical approaches can substantially reduce confounding.

### Study Design and Cohort Construction

Good design beats clever modeling. Match cases and controls on age, sex, ancestry PCs, and recruitment site where possible to reduce confounding by these factors. Use balanced sampling to down-sample dominant groups or up-sample under-represented groups within mini-batches to prevent models from over-relying on majority patterns. For new studies, plan recruitment prospectively to ensure diversity across ancestries, institutions, and environments.

Matched designs and balanced sampling reduce the incentive for models to use confounders, though they may limit effective sample size.

### Covariate Adjustment and Residualization

Explicitly modeling confounders can help. Include ancestry PCs, batch indicators, or site variables as covariates in regression or generalized linear models. Residualize phenotypes with respect to known confounders before training genetic models, while being cautious not to remove genuine biological signal. Use mixed models or hierarchical structures to model institution or batch as random effects.

These approaches are well-established in GWAS and can be adapted to genomic foundation model pipelines.

### Domain Adaptation and Invariance

More advanced approaches aim to learn representations that are invariant to confounders. Adversarial training involves training a feature extractor such that a discriminator cannot recover batch, site, or ancestry labels from the learned representation, promoting invariance. Domain adaptation uses techniques such as domain adversarial networks or importance weighting to align distributions across batches or cohorts. Group-robust optimization optimizes worst-group performance (for instance, worst-ancestry AUC) rather than average performance, encouraging robust models.

These methods are not a substitute for careful design, but they can reduce reliance on confounders when distribution alignment is feasible.

::: {.callout-note}
**Visual suggestion:** A small schematic illustrating adversarial domain adaptation for batch correction (encoder → predictor and encoder → batch discriminator, with gradient reversal on the discriminator branch) would clarify this technique.
:::

### Data Curation and Benchmark Design

Mitigation also depends on how we build benchmarks and curate datasets. Deduplicate individuals, families, and variants across training, validation, and benchmark sets. Use locus-level or chromosome-level splits in variant effect prediction to avoid memorization of sites. Construct benchmarks that explicitly include diverse ancestries, cohorts, and platforms, rather than reusing a single dominant dataset. Document known overlaps or shared resources between training data and benchmarks, clearly flagging potential leakage.

These principles complement the methodological guidance in @sec-benchmarks and @sec-eval, focusing them on confounding and bias.


## Fairness, External Validity, and Causal Thinking

Confounding and bias are intimately connected to questions of fairness and external validity.

Models that primarily serve majority ancestry groups exacerbate health disparities, even if they show excellent overall performance. Biases in ClinVar-like resources and biobank recruitment can lock in historical inequities, with pathogenic variants in under-studied groups remaining poorly characterized [@landrum_clinvar_2018; @karczewski_gnomad_2020]. Multi-ancestry GWAS and polygenic score studies highlight both the need for broad representation and the difficulty of building models that generalize across ancestries [@ishigaki_multi-ancestry_2022].

Causal representation learning aims to separate invariant mechanisms from spurious correlations [@chen_causal_2025]. While still an active research area, this perspective is useful: robust genomic foundation models should capture relationships that hold across ancestries, cohorts, and technical platforms, rather than those specific to a single dataset.

In practice, this means prioritizing cross-cohort and cross-ancestry evaluation, designing objectives and benchmarks that emphasize out-of-distribution robustness, and being explicit about which populations and settings a model is expected to work in, and where uncertainty remains.


## A Practical Checklist

To close, here is a concise checklist to apply when designing, training, and evaluating genomic models.

**Population structure and relatedness**

- Quantify ancestry and relatedness via PCs, kinship estimates, or other appropriate methods.
- Decide whether to match, stratify, or adjust for ancestry and relatedness, and justify that choice.
- Report performance stratified by ancestry and, when relevant, by family structure (for instance, presence of close relatives in the training data).

**Data splits and leakage**

- Ensure that individuals, families, and loci do not cross the train–validation–test boundaries for the target tasks.
- Consider stricter splits (locus-level, chromosome-level, cohort- or time-based) to probe for leakage.
- Check for overlap with external databases or benchmarks used in evaluation and clearly document any shared resources.

**Batch, platform, and cohort effects**

- Catalog technical variables (center, instrument, protocol, assay) and cohort or institution identifiers.
- Evaluate whether these variables align with labels or subgroups of interest.
- Use diagnostics (embeddings, PCs, simple classifiers) to detect batch or cohort signatures and mitigate via design, adjustment, or domain adaptation.

**Label quality and curation bias**

- Understand how labels were defined and what processes (billing codes, expert review, registry inclusion) produced them.
- Quantify label noise where possible, and consider robust training strategies when labels are noisy.
- Be explicit about how curated resources (for instance, ClinVar, functional assay datasets) may reflect historical biases.

**Cross-group performance and fairness**

- Report metrics for each major subgroup (ancestry, sex, age, cohort, platform) rather than only overall metrics.
- Examine calibration and error patterns across groups, not just discrimination metrics.
- Discuss the ethical and clinical implications of residual performance gaps, including whether deployment might exacerbate existing disparities.

**Reproducibility and transparency**

- Fully document dataset construction, inclusion criteria, and all splitting strategies used.
- Release code and configuration files for preprocessing, model training, and evaluation where feasible.
- Clearly describe which confounders were measured, how they were handled, and what limitations remain.

By systematically addressing these points, we can move beyond models that merely perform well on convenient benchmarks to models that reveal genuine biology and behave reliably in diverse clinical and scientific settings.