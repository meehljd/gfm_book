# The Foundation Model Paradigm {#sec-fm-principles}

A curious asymmetry defines modern computational genomics. We can train models to predict chromatin accessibility with remarkable accuracy, yet these same models fail entirely when asked about splicing. We can predict splice site recognition with near-perfect precision, yet learn nothing transferable about transcription factor binding. Each model excels within its narrow domain while remaining blind to the broader genomic landscape. The result is an ecosystem of specialized tools, each requiring its own training data, computational infrastructure, and domain expertise, collectively covering only fragments of the questions we need to answer.

Foundation models offer a different approach: train once on vast genomic data, then adapt to many downstream tasks. This paradigm, which transformed natural language processing and protein structure prediction, promises to unify fragmented genomic modeling efforts under shared representations. A single foundation model might provide embeddings useful for regulatory prediction, variant interpretation, and sequence design simultaneously. The efficiency gains would be substantial: rather than curating labeled datasets for each new question, researchers could fine-tune existing models on modest task-specific data.

Yet foundation models carry their own costs and limitations. Pretraining at scale requires computational resources beyond most academic budgets. Adaptation to specific tasks demands expertise in transfer learning techniques. Predictions from general-purpose models may lack the precision of specialized alternatives. The decision to use, adapt, or build foundation models involves trade-offs that depend on available resources, target applications, and acceptable uncertainty. This chapter provides the conceptual framework for navigating these decisions, establishing what foundation models are, how they scale, what emerges at scale, and when they represent the right tool for a given genomic question.

## From Task-Specific Models to Foundation Models

The history of computational genomics reveals a consistent pattern: models become more general while maintaining or improving task-specific performance. Hand-crafted scores such as CADD and SIFT established that integration of diverse genomic annotations could improve variant pathogenicity prediction [@rentzsch_cadd_2019; @schubach_cadd_2024]. These approaches relied on expert feature engineering, combining conservation scores, functional annotations, and population frequency data through ensemble methods or logistic regression.

Task-specific deep learning models demonstrated that neural networks could learn relevant features directly from sequence. DeepSEA predicted chromatin accessibility and transcription factor binding from 1 kb sequences using convolutional architectures [@zhou_deepsea_2015]. ExPecto extended this approach to gene expression prediction by modeling regulatory elements across multiple cell types [@zhou_expecto_2018]. Sei organized regulatory predictions into interpretable sequence classes through unsupervised clustering [@chen_deepsea_2022]. SpliceAI achieved near-perfect splice site prediction through dilated convolutions over 10 kb contexts (@sec-cnn). Enformer scaled sequence-to-function modeling to 200 kb windows and thousands of chromatin tracks through transformer architectures [@avsec_enformer_2021].

These models succeeded within their specific domains but remained difficult to repurpose. Training a DeepSEA model required chromatin accessibility data. Using SpliceAI for regulatory prediction would require complete retraining on different labels. Each application domain needed its own model, trained from scratch on task-specific data. **The fundamental limitation was not model capacity but training paradigm: supervised learning on narrow tasks produced narrow capabilities.**

Sequence language models introduced the self-supervised pretraining paradigm (@sec-pretraining) to genomics. DNABERT applied masked language modeling to DNA sequences, demonstrating that general representations could be learned without task-specific labels [@ji_dnabert_2021]. ESM and ESM-2 showed that protein language models pretrained on sequence alone could transfer effectively to structure prediction, variant effect prediction, and protein design [@rives_esm_2021; @lin_esm-2_2022]. The Nucleotide Transformer family scaled DNA language modeling to cross-species training corpora [@dalla-torre_nucleotide_2023]. HyenaDNA used implicit convolutions to reach million-token contexts at single-nucleotide resolution [@nguyen_hyenadna_2023].

The transition from task-specific to foundation models changes the relationship between model developers and users. Task-specific models deliver predictions as their primary product. Foundation models deliver representations that users adapt to their own tasks through the transfer learning techniques examined in @sec-transfer. This distinction affects everything from model architecture design to evaluation strategies to deployment infrastructure.

## Defining Genomic Foundation Models

The term "foundation model" appears frequently in genomics literature, sometimes applied to any large neural network trained on biological sequences. Establishing clear criteria helps separate true genomic foundation models from ordinary deep learning approaches that happen to operate on DNA or protein sequences.

### Essential Properties

A genomic foundation model satisfies several key properties that distinguish it from task-specific architectures.

**Large-scale pretraining with minimal supervision.** Foundation models train on entire genomes, pan-genomic sequence collections, or large assay compendia. The pretraining objectives include masked language modeling, next-token prediction, denoising, or multi-task sequence-to-function prediction. Critically, these objectives do not require dense task-specific labels for every training example. A model that requires annotated enhancers or curated pathogenic variants for every training instance does not qualify as a foundation model under this criterion.

**General-purpose representations.** Foundation models produce embeddings that prove useful across many downstream tasks. These representations can be extracted through forward passes and reused with simple linear probes or lightweight adapter modules rather than requiring full model retraining. The representations should encode biological information at multiple scales, from local sequence motifs to long-range regulatory grammar.

**Broad transfer capability.** Foundation models support diverse downstream applications without architectural modifications or complete retraining. Transfer occurs across multiple dimensions: different assays (from chromatin accessibility to gene expression), different tissues and cell types, different species, and different variant types (from SNVs to structural variants). Evidence of broad transfer requires evaluation across multiple benchmarks rather than demonstration of performance on a single task.

**Scale along at least one dimension.** Foundation models operate at a scale that would be impractical for task-specific training. Some scale context length, as HyenaDNA scales to million-token windows at single-nucleotide resolution. Others scale parameter count, as the ESM and Nucleotide Transformer families reach billions of parameters. Still others scale data diversity through pan-genomic pretraining across hundreds of species or integration of many assays and cell types. The scaling dimension chosen reflects the model's intended applications and architectural constraints.

**Standardized interfaces.** Foundation models typically expose consistent APIs for common operations. These include embedding extraction for sequences or variants, sequence probability scoring, and mask-based in-silico mutagenesis for variant effect prediction. Models distributed through repositories such as Hugging Face often include documented recipes for downstream fine-tuning and example notebooks demonstrating common use cases.

### What Doesn't Count

Many excellent genomic models fail one or more of these criteria and should not be classified as foundation models. Early versions of DeepSEA trained specifically on chromatin accessibility data from a limited set of cell types lack the generality and standardized interface of foundation models, though later iterations that integrate many assays begin to approach foundation model territory [@zhou_deepsea_2015]. SpliceAI predicts splicing outcomes exceptionally well but was designed for that specific task and provides neither general-purpose embeddings nor easy transfer to other genomic prediction problems [@jaganathan_spliceai_2019]. Even a very large Enformer-like model trained solely on human chromatin tracks remains bound to its specific prediction interface despite its scale and sophistication [@avsec_enformer_2021].

The distinction matters for several reasons. It affects evaluation strategy, since foundation models must be assessed across families of tasks rather than single benchmarks. It affects integration into existing pipelines, since foundation models serve as feature extractors while task-specific models typically provide end-to-end predictions. It affects how we think about model development, since foundation model training requires different infrastructure and data curation than task-specific supervised learning.

### The Metaphor Itself

The term "foundation" carries implications worth examining. Architectural foundations are static, load-bearing, and invisible once construction proceeds. Genomic foundation models share only the load-bearing property: they support downstream applications that would otherwise require independent construction. Yet unlike architectural foundations, these models remain visible and modifiable throughout their use. Fine-tuning adjusts the foundation itself rather than building atop an immutable base. The metaphor also implies that foundations precede and enable all subsequent work, but genomic foundation models often coexist with task-specific alternatives that outperform them on narrow benchmarks.

A more accurate metaphor might be "foundation" in the educational sense: a broad base of knowledge that enables specialized learning but continues to develop alongside it. The pretraining phase establishes general competence; adaptation refines that competence for specific purposes without abandoning the original learning. This framing better captures the dynamic relationship between pretrained representations and downstream tasks, though the architectural metaphor has become standard terminology.

## Scaling Laws and Compute-Optimal Training

The success of foundation models in natural language processing rests partly on empirical scaling laws: predictable relationships between model size, training data, computational budget, and performance. Understanding these relationships guides resource allocation and model development decisions.

### The Scaling Law Framework

Scaling laws describe how model performance (typically measured as loss on held-out data) varies with three key quantities: the number of model parameters (N), the amount of training data (D), and the total compute budget (C). The Chinchilla scaling analysis demonstrated that for a fixed compute budget, there exists an optimal balance between model size and training data [@hoffmann_chinchilla_2022]. Training a model that is too large on too little data, or too small on too much data, yields worse performance than compute-optimal allocation.

The practical implications are significant. Many early large language models were undertrained relative to their parameter count. GPT-3's 175 billion parameters were trained on roughly 300 billion tokens, while Chinchilla-optimal training would suggest either fewer parameters or more data. The Chinchilla model itself matched GPT-3 performance with only 70 billion parameters trained on 1.4 trillion tokens.

For genomic foundation models, scaling relationships are less well characterized but increasingly important. Several key questions arise: Do genomic sequences exhibit the same scaling behavior as natural language? How does the finite size of reference genomes constrain data scaling? What role does sequence diversity (cross-species, population variation) play in data scaling? Can synthetic data augmentation extend effective training data beyond natural sequences?

### Empirical Scaling in Genomic Models

Several genomic foundation model families have reported scaling experiments, though systematic scaling laws comparable to NLP remain elusive.

The Nucleotide Transformer family provides perhaps the clearest genomic scaling data [@dalla-torre_nucleotide_2023]. Performance on downstream benchmarks improves consistently with parameter count across models from 50 million to 2.5 billion parameters. The largest models (trained on multi-species data) outperform smaller models trained on human sequences alone, suggesting that cross-species data provides effective scaling even when human-specific performance is the target. Training compute scaled from approximately 10^19 to 10^21 FLOPs across the model family.

ESM-2 demonstrated similar scaling for protein language models, with performance on structure prediction and variant effect tasks improving smoothly from 8 million to 15 billion parameters [@lin_esm-2_2022]. The largest ESM-2 models approach the structure prediction accuracy of AlphaFold2 using only single-sequence input, a capability entirely absent in smaller models. This represents a qualitative capability threshold crossed through scale.

HyenaDNA focused on context length scaling rather than parameter scaling, demonstrating that million-token contexts at single-nucleotide resolution could be achieved through sub-quadratic architectures [@nguyen_hyenadna_2023]. The relationship between context length and downstream performance is task-dependent: some tasks (local motif recognition) show saturation at kilobase scales, while others (long-range regulatory prediction) continue improving with longer contexts.

### Compute-Optimal Decisions for Genomics

The scaling law framework has direct implications for model development decisions in genomics.

**Data-constrained regimes.** Unlike natural language, where text data is effectively unlimited, genomic sequence data faces hard constraints. Reference genomes for well-studied species total perhaps 10^11 to 10^12 nucleotides. Population-level variant data can expand this somewhat, but the effective diversity may be lower than raw counts suggest. In data-constrained regimes, smaller models trained to convergence may outperform larger models that are undertrained.

**Compute-constrained regimes.** Academic groups typically face stricter compute constraints than industry labs. Given fixed compute budgets, the Chinchilla framework suggests allocating resources toward longer training of smaller models rather than abbreviated training of larger models. A 500 million parameter model trained for 10 epochs on diverse genomic data may outperform a 5 billion parameter model trained for 1 epoch on the same data.

**Task-specific considerations.** Not all downstream tasks benefit equally from foundation model scale. Tasks that depend primarily on local sequence patterns (transcription factor motif recognition, splice site identification) may show diminishing returns beyond modest model sizes. Tasks requiring integration of long-range context or rare variant interpretation may continue benefiting from larger models and longer contexts.

**Data curation versus model scaling.** For many practical applications, investment in data curation and quality may yield better returns than model scaling. A foundation model trained on carefully curated, diverse, high-quality sequences may outperform a larger model trained on noisier data. The relative value of these investments is task-dependent and not well characterized by current scaling law frameworks.

### Emergent Capabilities

Perhaps the most intriguing aspect of foundation model scaling is the emergence of qualitatively new capabilities at sufficient scale. Emergence refers to abilities that are absent or negligible in smaller models but appear discontinuously as models grow.

In large language models, emergent capabilities include multi-step reasoning, code generation, and in-context learning. These capabilities appear at model scales of roughly 10^10 parameters and above, with no clear precursor in smaller models.

Genomic foundation models exhibit analogous emergence, though the capability thresholds are less well characterized.

**Structural understanding from sequence.** ESM-2 at sufficient scale produces contact maps and secondary structure predictions from single sequences with accuracy approaching multiple sequence alignment methods [@lin_esm-2_2022]. Smaller ESM models show no meaningful structural understanding. This capability emerges at approximately 650 million parameters and continues improving with scale.

**Cross-species transfer.** Larger Nucleotide Transformer models transfer more effectively to novel species not seen during training [@dalla-torre_nucleotide_2023]. The ability to generalize beyond training species appears to require sufficient model capacity to learn abstract regulatory principles rather than memorizing species-specific patterns.

**Zero-shot variant effect prediction.** Foundation models at sufficient scale can predict variant effects without task-specific fine-tuning, using only the difference in likelihood between reference and alternative sequences. This zero-shot capability requires models large enough to capture subtle sequence dependencies.

**In-context learning.** The most recent genomic foundation models show preliminary evidence of in-context learning: the ability to adapt to new tasks based on examples provided in the input context without parameter updates. This capability, central to the utility of large language models, remains nascent in genomic models but may emerge with further scaling.

The practical implication is that capability thresholds exist: models below certain scales may be fundamentally incapable of certain tasks regardless of fine-tuning. Identifying these thresholds helps guide model selection and prevents wasted effort fine-tuning models that lack necessary capacity.

## A Taxonomy of Genomic Foundation Models

The landscape of genomic foundation models can be organized into four broad families. Each family exhibits distinct characteristics, strengths, limitations, and typical application domains.

### DNA Language Models

DNA language models learn sequence representations from raw nucleotide strings through self-supervised objectives. These models treat DNA as a language to be modeled without explicit functional labels, discovering patterns through statistical regularities in genomic sequence.

**Core characteristics.** DNA language models typically use masked language modeling or autoregressive next-token prediction as their pretraining objective. They train on reference genomes or pan-genomic sequence collections spanning multiple species. The resulting models produce per-position or pooled sequence embeddings that can be extracted and used for downstream tasks. Critically, these embeddings are not tied to specific assays or cell types, making them applicable to any task that benefits from general sequence context.

**Representative models.** DNABERT and DNABERT-2 apply BERT-style masked language modeling to DNA sequences, using overlapping k-mers as tokens [@ji_dnabert_2021; @zhou_dnabert-2_2024]. The Nucleotide Transformer family scales this approach to larger parameter counts and cross-species training [@dalla-torre_nucleotide_2023]. HyenaDNA achieves subquadratic complexity through implicit convolutions, enabling context lengths up to one million nucleotides [@nguyen_hyenadna_2023]. Caduceus incorporates bidirectional processing and reverse-complement equivariance as architectural inductive biases. Evo-2 combines long-range attention with biological tokenization strategies. GROVER integrates learned BPE-style tokenization with training on regulatory tracks in addition to raw sequence [@sanabria_grover_2024].

**Strengths and limitations.** DNA language models provide truly general representations not bound to specific assays, cell types, or experimental conditions. They can process novel sequences not present in reference genomes. Their self-supervised training requires only genome sequences, making them scalable. Without explicit functional grounding, however, they may not capture subtle regulatory patterns that manifest only under specific cellular conditions. Performance on tasks requiring fine-grained functional discrimination may lag models trained with functional supervision.

**Typical applications.** Sequence classification (promoters, enhancers, transposons), motif discovery, variant effect prediction through embedding perturbation, sequence generation for synthetic biology, and transfer learning to new species with limited labeled data.

### Sequence-to-Function Foundation Models

Sequence-to-function models predict molecular readouts directly from sequence through supervised or semi-supervised training on assay compendia. These models blur into foundation model territory when their output space is sufficiently broad and their internal representations prove useful for tasks beyond the original assay set.

**Core characteristics.** These models map DNA sequences to high-dimensional vectors of molecular measurements, including chromatin accessibility, histone modifications, transcription factor binding, and gene expression levels. Training uses large collections of functional genomics assays spanning many cell types. The models learn regulatory grammar through supervised prediction of molecular phenotypes.

**Representative models.** Enformer predicts thousands of chromatin and expression tracks from 200 kb sequence windows through transformer attention [@avsec_enformer_2021]. Borzoi extends this with refined architectures and expanded coverage. Sei organizes predictions into interpretable sequence classes through unsupervised clustering [@chen_deepsea_2022]. Earlier models including DeepSEA and Basset established the paradigm at smaller scales.

**Strengths and limitations.** Explicit functional supervision provides mechanistic grounding. Predictions can be interpreted through comparison to experiments. The models naturally support variant effect prediction by computing reference-alternative differences. Models remain tied to training assays and cell types. Extension to new contexts typically requires retraining or new data collection.

**Typical applications.** Regulatory variant interpretation in well-studied cell types, eQTL fine-mapping, enhancer identification, transcription factor binding prediction, and regulatory mechanism discovery.

### Variant Effect Prediction Models

Models optimized specifically for predicting functional or clinical consequences of genetic variants. These take a variant and predict its effect on molecular phenotypes, organismal fitness, or disease risk.

**Core characteristics.** Variant effect prediction models integrate sequence context with evolutionary information, population genetics signals, and sometimes structural or functional annotations. They output pathogenicity scores, effect size estimates, or functional consequence predictions. Training combines multiple data sources: clinical labels from ClinVar, population frequency from gnomAD, functional assays such as deep mutational scanning, and evolutionary constraint metrics.

**Representative examples.** AlphaMissense applies protein language models to predict pathogenicity of missense variants [@cheng_alphamissense_2023]. ESM-1v uses evolutionary context for protein variant effect prediction. EVE combines evolutionary and structural information. Genomic foundation models like DNABERT and Enformer provide variant effect predictions through in silico mutagenesis. The architecture, training, evaluation, and clinical deployment of variant effect predictors are covered comprehensively in @sec-vep-fm.

### Multi-Omic Foundation Models

Models that natively integrate multiple molecular modalities, jointly processing DNA sequence, chromatin state, gene expression, protein abundance, 3D genome structure, or phenotypic descriptions.

**Core characteristics.** Multi-omic models employ architectures handling heterogeneous input types: transformer variants with cross-attention, graph neural networks, or modality-specific encoders with fusion layers. Training objectives encourage cross-modal alignment through contrastive learning, joint prediction, or generative modeling of multiple data types.

**Representative models.** Omni-DNA uses transformer-based autoregressive models with vocabulary expansion and multi-task finetuning, unifying diverse genomic tasks under an instruction-response paradigm [@li_omnidna_2025]. Models integrating Hi-C data capture 3D genome organization. Cross-modal architectures align DNA embeddings with chromatin or expression predictions.

**Strengths and limitations.** Unified representations enable cross-modal queries. Joint training can improve performance through multi-task effects. Data engineering becomes substantially more complex, with different modalities requiring different measurement technologies and quality control. The field is early, with few models reaching production maturity.

## Design Dimensions

Within and across families, individual models differ along orthogonal design dimensions that affect suitability for specific tasks.

### Data Composition

**Species coverage.** Human-only training focuses on clinically relevant patterns. Cross-species training encourages learning of conserved elements and evolutionary constraints, potentially improving generalization but risking dilution of human-specific signals.

**Sequence diversity.** Training on reference genomes alone provides clean sequences but limited exposure to population variation. Incorporating variant data improves robustness but requires careful design to avoid learning spurious associations.

**Annotation integration.** Models may train on raw sequence alone or incorporate functional annotations. The degree of integration trades generality against functional grounding.

### Architecture Choices

**Transformer variants.** Encoder-only models (DNABERT, Nucleotide Transformer) excel at classification and embedding tasks. Decoder-only models (GROVER) support generative applications. Full and sparse attention patterns, linear approximations, and Flash attention implementations affect computational efficiency.

**Sub-quadratic architectures.** Hyena-based models and state space models achieve subquadratic scaling, enabling longer contexts than standard transformers with comparable parameters.

**Hybrid architectures.** CNN-transformer combinations use local convolutions followed by global attention, as in Enformer. Multi-scale approaches process sequences at multiple resolutions.

### Context Length

**Short context (under 1 kb)** captures local patterns: motifs, splice sites, promoter elements. **Medium context (1-10 kb)** spans complete genes with proximal regulatory regions. **Long context (10-200 kb)** represents enhancer-promoter interactions and TAD-scale organization. **Ultra-long context (over 200 kb)** enables chromosomal domain modeling and complex structural variant interpretation. The effective use of long context requires appropriate tokenization and positional encoding.

### Tokenization

**Character-level** maintains single-base resolution but imposes longest sequence lengths. **K-mer tokenization** reduces length by a factor approaching k, with vocabulary reaching 4,096 for 6-mers. **Learned tokenization** (BPE-style) discovers schemes from data, potentially allocating vocabulary more efficiently [@medvedev_biotoken_2025]. The choice should align with both computational constraints and biological resolution requirements.

## Build Versus Use Decisions

The availability of pretrained foundation models creates strategic choices about when to use existing models, when to adapt them, and when to train from scratch.

### When to Use Existing Models

Existing foundation models provide immediate utility when the target application aligns with model capabilities, labeled data is limited, and computational resources are constrained.

**Embedding extraction.** For tasks where general sequence representations suffice, frozen foundation model embeddings with simple downstream classifiers often perform competitively with fine-tuned alternatives. This approach requires minimal compute (single forward passes), no gradient computation through large models, and modest labeled data (hundreds to thousands of examples). Applications include sequence classification, clustering, and similarity search.

**Zero-shot inference.** Some foundation models support zero-shot variant effect prediction through likelihood ratio scoring. This requires no task-specific training and produces calibrated scores for novel variants immediately. Zero-shot approaches work well when the pretraining objective aligns with the target task and when fine-tuning data is unavailable or unreliable.

**Rapid prototyping.** Foundation model APIs enable quick assessment of whether a modeling approach is viable before committing resources to custom development. Testing variant effect prediction with ESM-1v takes hours rather than the weeks required to train a custom model.

### When to Adapt Existing Models

Adaptation through fine-tuning or lightweight methods (LoRA, adapters, prefix tuning) makes sense when downstream tasks require specialized behavior beyond what frozen embeddings provide, sufficient labeled data exists (typically thousands to tens of thousands of examples), and the target domain falls within the pretraining distribution.

**Parameter-efficient fine-tuning.** Methods like LoRA update a small fraction of model parameters (often under 1%) while keeping the foundation model frozen [@hu_lora_2021]. This preserves general knowledge while allowing task-specific adaptation. Compute requirements are modest: a few GPU-hours for most genomic tasks. The approach works well when the foundation model's representations are largely appropriate but need refinement for specific applications.

**Full fine-tuning.** Updating all parameters typically achieves the best single-task performance but requires more data (tens of thousands of examples), more compute (GPU-days to weeks), and careful regularization to prevent overfitting. Full fine-tuning makes sense for high-stakes applications where maximum accuracy justifies the investment.

### When to Train from Scratch

Building custom foundation models requires substantial justification given the resources involved.

**Novel domains.** When target sequences differ fundamentally from existing model pretraining data (novel species, synthetic sequences, non-standard nucleotides), existing models may provide poor transfer. Custom pretraining on domain-specific data may be necessary.

**Specialized architectures.** If the application requires architectural features absent from existing models (specific attention patterns, custom tokenization, multi-modal inputs), building from scratch may be unavoidable.

**Scale requirements.** For applications requiring larger models or longer contexts than available options, custom training is necessary. This applies primarily to well-resourced groups with specific performance requirements.

**Proprietary data advantages.** Organizations with unique large-scale datasets (clinical biobanks, pharmaceutical screening data) may achieve better performance through custom pretraining than public models allow. The data advantage must be substantial to justify training costs.

### Cost-Benefit Analysis

The decision framework involves comparing expected performance against resource requirements.

**Compute costs.** Training a foundation model from scratch requires 10^20 to 10^22 FLOPs, translating to thousands of GPU-hours and tens of thousands of dollars at current cloud prices. Fine-tuning requires 10^16 to 10^18 FLOPs, often achievable in hours on single GPUs. Inference with frozen embeddings requires only forward passes. Detailed compute estimates for specific model scales and hardware configurations appear in @app-compute.

**Data costs.** Foundation model pretraining requires billions of tokens. Fine-tuning requires thousands to tens of thousands of labeled examples. Zero-shot and embedding approaches require only evaluation data.

**Performance expectations.** For well-studied tasks with abundant labeled data, fine-tuned models typically outperform frozen embeddings by 5-15% on standard metrics. Zero-shot approaches often achieve 70-90% of fine-tuned performance. Custom foundation models rarely outperform existing options by large margins unless the application involves genuinely novel domains.

**Time costs.** Using existing models takes hours to days. Fine-tuning takes days to weeks. Training from scratch takes weeks to months. For time-sensitive applications, using existing models often dominates even if custom training would eventually yield better results.

The practical recommendation for most applications: start with frozen embeddings from the most appropriate existing foundation model. If performance is insufficient, try parameter-efficient fine-tuning. Train from scratch only if adaptation fails and the application justifies the investment.

## Evaluation Principles

Foundation models resist evaluation on single tasks. Their value lies in transfer across many applications, making comprehensive evaluation substantially more complex than benchmarking task-specific models.

### Multi-Task Assessment

A genomic foundation model should be evaluated across families of related tasks rather than isolated benchmarks. For DNA language models, this includes sequence classification tasks, variant effect prediction across multiple variant types, motif discovery, and cross-species transfer. For sequence-to-function models, evaluation should span prediction of held-out assays, transfer to novel cell types, and consistency with experimental measurements.

The diversity of evaluation tasks complicates comparison across models. A model excelling at promoter classification may underperform on eQTL fine-mapping. Direct comparisons require controlling for differences in training data, model scale, and evaluation protocols.

### Transfer Versus Pretraining Performance

Foundation models are intended for transfer, making pretraining loss only moderately predictive of downstream utility. A model with slightly worse masked language modeling loss may produce better embeddings if its training objective better aligns with useful representations. Evaluation should explicitly test transfer through zero-shot performance, few-shot learning, cross-domain transfer, and robustness to distribution shift.

Detailed discussion of benchmark suites, evaluation protocols, and methodological best practices appears in @sec-benchmarks and @sec-eval.

## The Foundation Model Ecosystem

Genomic foundation models exist within a broader ecosystem of infrastructure, community resources, and shared practices.

### Model Distribution

Most models are distributed through centralized repositories. Hugging Face hosts many DNA and protein language models with documented APIs. GitHub repositories accompany publications with weights, code, and examples. Standardized formats reduce friction in adoption, enabling rapid benchmarking and experimentation.

### Documentation Requirements

Responsible distribution requires comprehensive documentation: training data provenance, preprocessing procedures, architecture details, hyperparameters, evaluation protocols, and known limitations. Data provenance is particularly important given population-specific biases and use restrictions in genomic datasets.

### Industry and Academic Contributions

Both academic and industry groups develop genomic foundation models. Academic models emphasize reproducibility and open access. Industry models may offer superior performance through proprietary data or compute but with limited transparency. Notable industry contributions include NVIDIA's BioNeMo platform and Microsoft's Azure genomics integration. Users should review license terms before clinical or commercial deployment.

## Open Questions

Despite rapid progress, fundamental challenges remain.

**Convergence or divergence.** Whether the field converges toward unified architectures or maintains specialized families remains unclear. The diversity of genomic scales, resolution requirements, and functional contexts may preclude the convergence seen in NLP.

**Causal reasoning.** Existing models learn correlations without distinguishing causal from spurious relationships. Integrating causal structure could improve robustness and enable counterfactual reasoning.

**Rare variant interpretation.** Models trained on reference genomes and common variants may not calibrate well for ultra-rare or de novo variants. Improved integration of structural and evolutionary constraints could strengthen rare variant interpretation.

**Clinical deployment.** Translation to clinical use requires robust cross-population performance, calibrated uncertainty, interpretability for clinicians, prospective validation, and regulatory approval. These requirements extend well beyond benchmark performance.

## Summary

This chapter established a conceptual framework for genomic foundation models. We defined foundation models through five essential properties: large-scale pretraining with minimal supervision, general-purpose representations, broad transfer capability, scale along at least one dimension, and standardized interfaces.

Scaling laws describe predictable relationships between parameters, data, compute, and performance. Compute-optimal training allocates resources based on these relationships, while emergent capabilities represent qualitative thresholds that appear only at sufficient scale. The practical implication: some capabilities require minimum model scales regardless of fine-tuning effort.

Four model families serve different needs: DNA language models for general sequence representations, sequence-to-function models for regulatory prediction with mechanistic grounding, variant effect models for clinical interpretation, and multi-omic models for systems-level integration. The build-versus-use framework guides resource allocation: use existing models when possible, adapt when needed, train from scratch only when justified by unique requirements.

Subsequent chapters apply this framework to specific domains. @sec-dna-lm and @sec-protein-lm examine DNA and protein language models in detail. @sec-regulatory covers long-context regulatory models. @sec-vep-fm addresses variant effect prediction as a capstone application integrating multiple model families. Throughout, the principles established here guide model selection, evaluation, and deployment decisions.