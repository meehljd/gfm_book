# Long-Context Regulatory Models {#sec-regulatory}

Gene regulation in higher eukaryotes poses a fundamental modeling challenge: the elements that control a gene's expression often lie tens or hundreds of kilobases away from the gene itself. An enhancer 80 kilobases upstream of a promoter can drive tissue-specific expression; an insulator 50 kilobases downstream can block inappropriate activation. Disease-associated variants discovered through genome-wide association studies frequently map to these distal regulatory regions, yet short-context models treat such elements as noise. The convolutional architectures examined in @sec-cnn excel at detecting local motifs but cannot span the distances over which mammalian gene regulation operates. Pure transformer architectures (@sec-attention) could theoretically attend across arbitrary distances, but quadratic attention costs make naive application to hundred-kilobase windows computationally prohibitive.

Hybrid architectures resolve this tension by combining the strengths of both paradigms. A convolutional front-end efficiently extracts local sequence features and compresses the input to a manageable length, while a transformer backbone propagates information across the full window through attention. This design enables models like Enformer to process 200 kilobase windows and predict chromatin state, transcription initiation, and gene expression from sequence alone. The result is a new class of regulatory models that can capture enhancer-promoter interactions, predict the effects of distal variants on gene expression, and provide mechanistic hypotheses about long-range regulation. These models do not merely extend receptive fields; they reformulate what sequence-based regulatory prediction can accomplish.

This chapter examines the major long-context regulatory architectures, their training regimes, and their applications to variant effect prediction. We begin with the biological motivation for long-range context, then trace the development from Enformer through Borzoi to AlphaGenome and related approaches like Sei. Each model makes distinct architectural choices that shape what it can and cannot capture about regulatory biology.


## The Long-Range Regulation Problem

Consider a canonical mammalian gene with complex tissue-specific expression. The promoter sits at the transcription start site, but the sequences that determine when and where the gene is expressed may be scattered across a 200 kilobase neighborhood. Multiple enhancers drive expression in different tissues; silencers suppress expression in inappropriate contexts; insulators demarcate regulatory domains. Chromatin looping brings these distal elements into physical proximity with the promoter, but the loops themselves are dynamic and cell-type-specific.

Short-context models face an information-theoretic barrier in this setting. A model with a 2 kilobase receptive field cannot distinguish a variant in an enhancer 50 kilobases upstream from a variant in neutral sequence at the same distance. Both fall outside the model's effective context. Stacking more convolutional layers or using dilated convolutions can expand the receptive field, but the computational path between distant positions grows long, and gradients attenuate over many layers. Models like Basenji2 [@kelley_basenji_2018] pushed convolutional receptive fields to tens of kilobases through aggressive pooling, but purely convolutional architectures struggle to propagate information across hundreds of kilobases without impractical depth.

The scale of the problem becomes concrete when examining enhancer-promoter distances in the human genome. Median enhancer-promoter distances in many tissues span 20 to 50 kilobases, with substantial fractions exceeding 100 kilobases [@gasperini_genome-wide_2019]. Topologically associating domains (TADs), which define the neighborhoods within which regulatory elements typically interact, range from hundreds of kilobases to several megabases. A model that cannot span these distances cannot fully capture the regulatory grammar of the genome.

Attention mechanisms offer a direct solution: by computing pairwise interactions between all positions, attention can model dependencies across arbitrary distances in a single layer. The cost is quadratic scaling with sequence length. A naive transformer operating on 200,000 base pairs at single-nucleotide resolution would require attention matrices with 40 billion entries, far exceeding practical memory limits. Hybrid architectures sidestep this constraint by using convolutions to compress the sequence before attention, reducing the effective sequence length to a few thousand tokens while preserving the information needed for long-range modeling.


## Enformer: Attention Meets Regulatory Genomics

Enformer [@avsec_enformer_2021] demonstrated that combining convolutional compression with transformer attention could dramatically improve expression prediction from sequence. The model processes 200 kilobase windows of DNA and predicts thousands of chromatin and transcription tracks across cell types and species, establishing a template that subsequent models have extended and refined.

### Architecture

The Enformer architecture consists of three stages that progressively transform raw sequence into multi-task predictions.

The convolutional stem takes one-hot encoded DNA (four channels for A, C, G, T) and applies a series of convolutional blocks with residual connections. Each block includes convolutions that detect local patterns, batch normalization and nonlinearities, and pooling operations that reduce sequence length while increasing channel depth. By the end of the stem, a 200 kilobase input has been compressed to roughly 1,500 tokens, each representing approximately 128 base pairs of underlying sequence. This compression is crucial: it reduces the attention computation from quadratic in 200,000 to quadratic in 1,500, a reduction of roughly 17,000-fold in memory requirements.

The transformer trunk operates on the compressed sequence through a stack of self-attention layers. Each layer computes attention scores between all pairs of positions, allowing information to flow directly between any two locations in the 200 kilobase window. Relative positional encodings preserve information about the distances between elements, which matters for regulatory biology where the spacing between motifs often carries functional significance. The combination of multi-head attention and feed-forward layers enables the model to learn complex, position-dependent relationships across the full window.

Task-specific output heads branch from the shared transformer backbone. Separate heads predict different types of outputs: DNase accessibility and ATAC-seq signal (chromatin openness), histone modifications including H3K4me3, H3K27ac, and other marks, CAGE signal reflecting transcription initiation, and additional functional genomics readouts where training data is available. Each head consists of convolutional and linear layers that transform the shared representation into track-specific predictions.

The multi-task design serves multiple purposes. Different assays provide complementary supervision: chromatin accessibility reflects regulatory potential, histone marks indicate active enhancers and promoters, and CAGE captures transcriptional output. Training on all assays jointly encourages the backbone to learn representations that capture the full regulatory cascade from accessible chromatin through enhancer activation to transcription initiation.

### Training Data and Cross-Species Learning



Enformer trains on functional genomics data from both human and mouse, spanning hundreds of assays and cell types. The chromatin accessibility, histone modification, and transcription initiation assays introduced in @sec-data provide the supervision signals: DNase-seq and ATAC-seq measure regulatory potential, ChIP-seq for histone marks identifies active enhancers and promoters, and CAGE captures where transcription begins. Human training data derives largely from ENCODE and Roadmap Epigenomics consortia, supplemented by CAGE data from FANTOM and additional chromatin profiling studies. Mouse data from analogous consortia provides complementary supervision.

Cross-species training confers several advantages. Regulatory sequences that are functionally constrained evolve more slowly than neutral sequence, so mouse and human share many regulatory motifs and principles despite 80 million years of divergence. Training on both species helps the model distinguish conserved regulatory logic from species-specific noise, reduces overfitting to idiosyncrasies of human data, expands the effective training set without requiring additional human samples, and implicitly emphasizes evolutionarily conserved patterns that are more likely to be functionally important.

The training objective combines losses across all tracks, positions, and species. Count-based likelihoods (Poisson or negative binomial) handle sequencing-derived signals, while correlation-based objectives ensure the model captures the overall shape of coverage profiles. Per-track weighting prevents abundant assays from dominating gradients.

### Variant Effect Prediction

The clinical and scientific value of Enformer lies substantially in its ability to predict how sequence variants alter regulatory activity. The procedure follows a straightforward logic: extract a 200 kilobase window containing the variant, compute predictions for the reference allele, compute predictions for the alternative allele, and compare the outputs across all tracks and positions.

The resulting variant effect scores span thousands of dimensions, one for each assay and cell type. A variant might increase predicted DNase accessibility in one cell type while decreasing predicted CAGE signal in another, suggesting context-dependent regulatory effects. By aggregating predictions around gene promoters, researchers can estimate variant effects on gene expression in specific tissues.

Validation against GTEx expression quantitative trait loci (eQTLs) demonstrated that Enformer's predictions correlate with observed genetic effects on expression [@avsec_enformer_2021]. Variants with large predicted effects on promoter-proximal CAGE signal were enriched among significant eQTLs. Notably, this correlation extended to distal variants: sequence changes 50 kilobases or more from a gene's transcription start site still showed predictive power when they fell in regions of predicted regulatory activity. This long-range predictive capacity distinguishes Enformer from short-context models and validates the architectural investment in extended context windows.


## Borzoi: From Chromatin to Transcriptome

While Enformer predicts transcription initiation through CAGE, RNA-seq captures a richer picture of gene expression: not just where transcription begins, but how the transcript is spliced, which isoforms dominate, where transcription terminates, and how stable the resulting mRNA is. Borzoi [@linder_borzoi_2025] extends the hybrid architecture paradigm to predict full RNA-seq coverage profiles, enabling a unified view of how sequence variation affects the entire transcriptional program.

### Motivation

A single gene can produce multiple transcript isoforms through alternative promoter usage, alternative splicing, and alternative polyadenylation. These isoforms may have different stabilities, different translation efficiencies, and different functions. A variant that shifts isoform ratios without changing total expression could have substantial phenotypic consequences: a switch from a cytoplasmic to a nuclear isoform, for instance, or inclusion of a premature stop codon in the predominant transcript.

CAGE and chromatin assays cannot capture these complexities. They measure where transcription might begin and what the chromatin environment looks like, but they do not reveal how RNA polymerase traverses the gene body, where splicing occurs, or which 3' end is selected. RNA-seq coverage profiles encode all of this information: exon boundaries appear as coverage drops at intron junctions, alternative splicing manifests as variable junction usage, and polyadenylation site choice appears in the coverage pattern near gene 3' ends.

### Architecture and Training

Borzoi builds on an Enformer-style backbone with modifications tailored to RNA-seq prediction. The convolutional stem and transformer trunk follow similar principles, compressing long input windows and propagating information through attention. Output heads predict stranded RNA-seq coverage across the window, with additional heads for complementary signals like PRO-seq (nascent transcription), CAGE, and other assays when available.

Training on RNA-seq coverage imposes different demands than training on chromatin marks. Coverage varies over orders of magnitude between introns and exons; the model must capture both the overall expression level and the fine structure of the coverage profile. Junction reads that span splice sites provide particularly informative supervision, as they directly constrain the model to learn splicing patterns. The loss function balances accurate prediction of coverage levels with faithful reproduction of the coverage shape, including sharp transitions at exon boundaries.

### Applications Beyond Expression Level

By predicting full RNA-seq coverage, Borzoi enables analyses that go beyond simple expression quantification. Splicing variant effects can be assessed by comparing predicted coverage at exons and junctions under reference and alternative alleles. A variant that reduces predicted junction reads for a particular exon suggests exon skipping; increased junction reads to a cryptic splice site suggests aberrant splicing. These predictions complement specialized splicing models like SpliceAI (@sec-cnn), providing additional context about how splicing changes fit within the broader transcriptional program.

Alternative promoter usage becomes visible through coverage patterns near transcription start sites. A variant that increases coverage downstream of one TSS while decreasing it downstream of another suggests a shift in promoter preference. Such shifts can alter the 5' UTR of the resulting transcript, affecting translation efficiency and regulatory motif content.

Polyadenylation site choice affects 3' UTR length and content. Shorter 3' UTRs may escape microRNA-mediated repression; longer ones may include additional regulatory elements. Borzoi's coverage predictions around annotated polyadenylation sites can reveal variants that shift site usage, potentially explaining effects on mRNA stability and translation that would be invisible to chromatin-based models.


## Sei: A Regulatory Vocabulary from Sequence

While Enformer and Borzoi predict continuous coverage tracks, Sei [@chen_sequence_2022] takes a complementary approach: learning a discrete vocabulary of sequence classes that capture distinct regulatory activities. Rather than predicting thousands of individual assays, Sei maps sequences to a reduced set of regulatory states, each associated with characteristic chromatin and transcription patterns.

### Approach

Sei builds on observations that chromatin states cluster into interpretable categories: active promoters, strong enhancers, poised enhancers, heterochromatin, and so forth. Previous methods like ChromHMM defined such states from observed chromatin marks in specific cell types. Sei learns to predict sequence class membership directly from DNA, asking what regulatory identity a sequence carries based on its intrinsic properties.

The model predicts 40 sequence classes derived from clustering patterns across chromatin accessibility, histone modifications, and transcription factor binding. Each class corresponds to a recognizable regulatory state: promoter-like sequences, enhancer-like sequences, CTCF binding sites, repressed regions, and various intermediate states. The output is not a single class assignment but a probability distribution over classes, reflecting the observation that many sequences have context-dependent regulatory potential.

### Complementary to Track Prediction

Sei and Enformer-style models serve complementary purposes. Enformer provides detailed, quantitative predictions across specific assays and cell types; Sei provides a compressed, interpretable summary of regulatory identity. For variant interpretation, both perspectives can be valuable. Enformer might reveal that a variant reduces predicted H3K27ac signal in liver but not heart; Sei might reveal that the same variant shifts sequence class membership from "strong enhancer" toward "weak enhancer," a more immediately interpretable characterization.

The regulatory vocabulary approach also facilitates systematic analysis across many variants. Rather than tracking changes in thousands of individual tracks, researchers can ask how a set of variants affects the distribution of regulatory classes, identifying patterns that might be obscured in high-dimensional track space.


## AlphaGenome: Unifying Modalities at Megabase Scale

AlphaGenome [@avsec_alphagenome_2025] extends the hybrid modeling paradigm in two directions: longer context windows (approximately one megabase) and broader output modalities spanning chromatin, expression, splicing, and three-dimensional contacts. The goal is a single model that provides a comprehensive view of how sequence determines regulatory state.

### Architectural Extensions

The megabase context window pushes against computational limits even with hybrid architectures. AlphaGenome addresses this through efficient attention mechanisms that reduce the quadratic cost, hierarchical processing that handles different output modalities at appropriate resolutions, and architectural refinements accumulated from Enformer and Borzoi development.

The output repertoire spans chromatin accessibility and histone modifications (following Enformer), gene expression and RNA coverage (following Borzoi), splicing predictions including exon inclusion and junction usage, and contact predictions reflecting three-dimensional chromatin organization.

Unifying these modalities in a single model offers several advantages. The backbone representation must capture information relevant to all outputs, encouraging learning of features that connect chromatin state to transcription to RNA processing. Variant effect predictions become coherent across modalities: a single forward pass reveals how a variant affects chromatin, expression, splicing, and contacts, rather than requiring separate runs through independent models.

### Access and Practical Considerations

AlphaGenome is primarily available through an API interface rather than as a downloadable model. This arrangement simplifies use for many applications: researchers can score variants without managing large model weights or specialized hardware. It also introduces constraints around data privacy, customization, and integration with local pipelines. Clinical applications that cannot send patient sequence data to external services may be unable to use API-only models directly, motivating interest in openly available alternatives.

From the perspective of variant interpretation workflows, AlphaGenome provides a comprehensive set of predictions from a single query. A variant can be assessed for effects on local chromatin state, expression of nearby genes, splicing of overlapping transcripts, and potential disruption of chromatin contacts, all from the same underlying model. The challenge lies in synthesizing these multiple outputs into actionable conclusions, a topic addressed further in @sec-vep-fm.


## What Hybrid Architectures Accomplish

The progression from DeepSEA through Enformer, Borzoi, and AlphaGenome reflects accumulating solutions to specific limitations. Each model addresses constraints that bounded its predecessor's utility.

### Spanning Enhancer-Promoter Distances

The most direct contribution is enabling long-range interaction modeling. A 200 kilobase context window encompasses the distances over which most cis-regulatory interactions occur. Attention mechanisms allow the model to learn direct relationships between enhancers and promoters without requiring information to propagate through many intermediate layers. Empirically, this translates to improved prediction of expression and better correlation with eQTLs, particularly for variants in distal regulatory elements.

### Multi-Task Regularization

Training on hundreds of assays jointly constrains the model to learn representations that generalize across regulatory modalities. A feature useful only for predicting H3K4me3 in one cell type provides less gradient signal than a feature useful across chromatin, transcription, and accessibility. This multi-task pressure steers the model toward learning fundamental regulatory logic rather than assay-specific artifacts.

### Cross-Species Constraints

Training on human and mouse together further regularizes the model. Species-specific binding site variants, repetitive elements, and technical artifacts in training data affect one species but not the other. Features that generalize across species are more likely to reflect conserved regulatory mechanisms. This provides a form of evolutionary validation built into the training process.

### Unified Variant Effect Prediction

Perhaps most practically valuable, hybrid models provide a unified framework for variant effect prediction on expression and related phenotypes. Rather than assembling scores from multiple specialized models, researchers can query a single model for comprehensive predictions. The outputs span cell types and assays, enabling tissue-specific interpretation of regulatory variants.


## Limitations and Open Challenges

Despite their power, long-context regulatory models face fundamental limitations that bound their current utility and define directions for future development.

### Training Data Constraints

Functional genomics data is biased in coverage, overrepresenting well-studied cell types (embryonic stem cells, K562, HepG2, lymphoblastoid cell lines) while leaving many tissue types and disease-relevant cell states poorly covered. Models trained on available data will perform better in represented contexts and may fail silently in underrepresented ones. Ancestry bias compounds the problem: most functional genomics studies derive from individuals of European descent, limiting the diversity of haplotypes and regulatory variants represented in training data.

These biases propagate to variant effect predictions. A variant in a regulatory element active primarily in pancreatic beta cells may receive poor predictions if beta cell data is sparse in training. A variant on a haplotype common in African populations but rare in Europeans may fall outside the model's effective training distribution. Users must recognize that prediction confidence varies with representation in training data, a consideration that current models do not explicitly communicate.

### Finite Context

Even megabase-scale windows capture only local regulation. Trans-acting factors, three-dimensional contacts spanning multiple megabases, and whole-chromosome organization fall outside model context. Structural variants that rearrange large genomic segments, duplicate enhancers, or create novel fusion genes cannot be modeled within fixed-window architectures. The reference genome assumption underlying these models further limits their applicability to complex haplotypes and populations with substantial structural variation relative to the reference.

### Missing Three-Dimensional Context

Linear sequence models treat DNA as a one-dimensional string, but gene regulation occurs in three-dimensional nuclear space. Chromatin loops bring distal elements into proximity; nuclear compartmentalization segregates active and repressed regions; phase-separated condensates concentrate regulatory factors. While AlphaGenome predicts some contact features, current hybrid models do not fully integrate three-dimensional chromatin organization. The relationship between linear sequence, three-dimensional structure, and regulatory output remains incompletely captured. We return to this gap in @sec-3d-genome, which examines models that explicitly address chromatin architecture.

### Correlation Versus Causation

Hybrid models learn correlations between sequence and functional readouts, not causal mechanisms. A variant might receive a high predicted effect score because it disrupts a motif correlated with expression in training data, not because the motif causally drives expression. Attribution methods can identify which sequence features contribute to predictions, but attribution is not validation. High-confidence predictions require experimental confirmation through approaches like massively parallel reporter assays, CRISPR perturbation, or allelic series analysis.

### Interpretability Challenges

The scale of these models (hundreds of millions of parameters) makes mechanistic interpretation difficult. Attention patterns provide some insight into which positions the model considers related, but attention weights are not guaranteed to reflect the model's actual computational strategy. Attribution methods (saliency maps, integrated gradients) can highlight important input positions, but the features the model constructs from those positions remain opaque. Chapter @sec-interpretability examines these interpretability methods and their limitations in detail.


## Relationship to Foundation Models

Long-context regulatory models occupy an interesting position in the genomic foundation model landscape. They share key characteristics with foundation models: large scale, broad training data, strong performance across tasks, and utility as feature extractors for downstream applications. Yet they differ from self-supervised DNA language models in their heavy reliance on supervised, task-specific training signals.

Enformer and its descendants can be viewed as highly specialized foundation models, pre-trained on the specific task of regulatory prediction and adaptable to related applications. Their representations encode regulatory logic learned from functional genomics supervision, complementing the sequence patterns learned by self-supervised models from raw DNA. In practice, the two approaches may prove most powerful in combination: self-supervised models provide sequence representations from evolutionary context, while supervised regulatory models provide representations from functional genomics context. Integrating these representations for tasks like variant effect prediction is an active area of development, explored further in @sec-vep-fm.

From a practical standpoint, hybrid regulatory models remain among the most directly useful genomic deep learning systems for variant interpretation. They provide quantitative, tissue-specific predictions for regulatory variants, outperform short-context alternatives on distal regulatory elements, and integrate naturally into variant prioritization workflows. Their limitations are real but understood; their strengths are substantial and empirically validated.


## Toward Next-Generation Architectures

The hybrid CNN-transformer design that defines Enformer and its descendants represents one solution to the long-context modeling problem, but not necessarily the final one. Alternative architectures offer different trade-offs that may prove advantageous as the field evolves.

Hierarchical attention approaches like the 1D-Swin architecture used in Genomic Interpreter [@li_genomic_2023] partition sequences into local windows, applying attention within windows and using shifted-window patterns to propagate information across boundaries. This reduces attention complexity while preserving long-range modeling capacity, potentially enabling even longer context windows or higher resolution within existing compute budgets.

State-space models and related architectures (Hyena, Mamba) replace attention entirely with mechanisms that have sub-quadratic or linear scaling in sequence length. These approaches can process very long sequences efficiently, making them attractive for genome-scale modeling. Early applications to DNA sequence show promise, though integration with multi-task regulatory prediction remains less developed than for hybrid transformers.

The computational landscape continues to evolve. Hardware advances (larger memory, faster attention implementations), algorithmic improvements (more efficient attention variants), and architectural innovations may shift the optimal design point. What remains constant is the underlying biological problem: gene regulation spans long distances, and models that capture this reach provide better predictions than those that do not.


The models examined in this chapter demonstrate that long-range regulatory prediction from sequence is tractable. Enformer established that hybrid architectures could span 200 kilobases and predict expression-related features; Borzoi extended this to full transcriptome coverage; AlphaGenome unified multiple modalities at megabase scale. These models do not explain regulatory mechanism, but they predict regulatory outcomes with sufficient accuracy to guide variant interpretation and hypothesis generation. The integration of these predictions into clinical variant assessment, alongside protein-based and splicing-focused models, is the subject of @sec-vep-fm.