# Confounders and Leakage {#sec-**confounding**}

A variant effect predictor trained on **ClinVar** achieves 0.92 AUC on held-out variants from the same database, yet performance drops to 0.71 when evaluated on a prospectively collected clinical cohort. A **polygenic risk score** for coronary artery disease stratifies European-ancestry individuals with impressive discrimination, then fails almost completely when applied to individuals of African ancestry. A gene expression model trained on GTEx data predicts tissue-specific patterns with apparent **precision**, until deployment reveals it learned to distinguish sequencing centers rather than biological states. Each model worked brilliantly in evaluation and failed quietly in practice.

These failures share a common cause: the models learned shortcuts rather than biology. Genomic datasets encode hidden structure from ancestry and family relatedness to sequencing center, capture kit, and label curation protocol. These factors correlate with both features and labels. When such confounders remain uncontrolled, models exploit them. The central challenge is that confounded models can appear to work, sometimes spectacularly well, until they encounter data where the shortcuts no longer apply.

This problem is not unique to deep learning. Linear regression and logistic models suffer from the same biases when fit on confounded data. What makes confounding particularly dangerous in the foundation model era is scale: larger datasets and more expressive architectures make it easier to discover subtle shortcuts that remain invisible in standard diagnostics but cause dramatic failures when distributions shift at deployment. A shallow model might miss the correlation between sequencing center and disease status; a transformer with hundreds of millions of parameters will find it if that correlation helps optimize the training objective. This chapter examines the major sources of confounding in genomic datasets, methods for detecting and controlling confounders, and strategies for building models robust to the distribution shifts they will inevitably encounter.

::: {#fig-confounding-dag}
![**FIGURE PLACEHOLDER**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[Essential] DAG with genomic example. Main DAG: Confounder (Ancestry) with arrows to Exposure (Genomic features) and Outcome (Disease status); spurious path X ← C → Y highlighted. Annotation boxes explaining how ancestry affects both. Second DAG showing adjustment blocking spurious path. Concrete example: Rare disease clinic → ClinVar submissions → ancestry proxy learned. Key insight: shortcuts appear to work until deployment shifts.
:::

## Confounding, Bias, and Leakage

The terminology of confounding, bias, and leakage describes distinct phenomena that often co-occur and reinforce each other. Precision in language helps clarify what has gone wrong when a model fails.

A **confounder** is a variable that influences both the input features and the label. Ancestry provides a canonical example: it affects allele frequencies across the genome (the features) and disease risk through environmental, socioeconomic, and healthcare pathways (the labels). If ancestry is not explicitly modeled or controlled, a model trained to predict disease may learn to identify ancestry rather than disease biology. The prediction appears accurate because ancestry correlates with outcome, but the model has captured correlation rather than mechanism.

**Bias** refers to systematic deviation from the quantity we intend to estimate or predict. Bias can result from confounding, but also arises from measurement error, label definitions, sampling procedures, or deployment differences. A case-control study with 50% disease prevalence will train models that systematically over-predict risk when deployed in populations where true prevalence is 5%. The model may be perfectly calibrated for the training distribution yet dangerously miscalibrated for clinical use.

**Leakage** occurs when information about the test set inadvertently influences model training or selection. Leakage pathways include overlapping individuals or variants between training and evaluation, shared family members across splits, duplicated samples under different identifiers, and indirect channels such as pretraining on resources that later serve as benchmarks. The circularity between computational predictors and ClinVar annotations discussed in @sec-vep-classical exemplifies this last category: CADD-like scores influence which variants receive pathogenic annotations, and those annotations then become training labels for the next generation of predictors.

**Distribution shift** describes mismatch between training and deployment data distributions. Shift can be driven by changes in ancestry composition, sequencing technology, clinical coding practices, or temporal trends in care. A model that learns hospital-specific coding patterns will fail when deployed at a different institution, not because the biology differs but because the label generation process does.

These phenomena interact. Confounders create biases in estimated effects. Leakage hides those biases by making held-out performance appear better than warranted. Distribution shift then reveals the problem when deployment performance collapses. For foundation models, three features magnify these risks. First, genomes encode ancestry, relatedness, and assay conditions in thousands of subtle features, even when those labels are never explicitly provided. Second, large transformers find shortcuts that smaller models would miss if those shortcuts improve the training objective. Third, complex training regimes involving pretraining on biobank-scale data, fine-tuning on curated labels, and evaluation on community benchmarks create many opportunities for direct and indirect leakage.


## Sources of Confounding in Genomic Data

Confounders in genomic modeling cluster into several categories, though the same underlying variable (such as recruitment site) may simultaneously induce ancestry differences, **batch effect**s, and label bias.

**Population structure and relatedness** encompasses continental and sub-continental ancestry, family relationships (siblings, parent-offspring pairs, cryptic relatedness detectable only through genotype similarity), and founder effects that create local haplotype structure. Ancestry affects both features and many phenotypes of interest, creating classic confounding. Relatedness creates a more subtle problem: when close relatives appear in both training and test sets, models can memorize shared haplotype segments rather than learning generalizable patterns, producing inflated performance estimates that collapse for unrelated individuals.

**Technical batch and platform effects** arise throughout the sequencing and analysis pipeline. Different instruments produce distinct error profiles. Library preparation protocols vary in GC bias, coverage uniformity, and adapter content. Capture kits determine which genomic regions receive adequate coverage. Alignment algorithms and variant callers make different decisions at ambiguous positions. When samples from a particular batch disproportionately represent a specific label class (cases sequenced at one center, controls at another), models learn to distinguish batches rather than biology.

**Cohort and institutional effects** reflect differences in patient populations, clinical practices, and data collection procedures. Hospital systems use distinct coding practices, diagnostic thresholds, and follow-up schedules. Population-based biobanks differ from referral-center cohorts in disease severity, comorbidity patterns, and demographic composition. Individuals who receive genomic testing may be more severely affected, more affluent, or preferentially drawn from particular ancestry groups, introducing selection bias that distorts apparent variant-phenotype relationships.

**Label and curation bias** stems from how ground truth annotations are generated. Clinical labels derived from billing codes or problem lists reflect documentation practices as much as underlying disease. Variant pathogenicity databases exhibit the systematic biases detailed in @sec-data: ClinVar annotations over-represent European ancestry, well-studied genes, and variants submitted by high-volume clinical laboratories [@landrum_clinvar_2018]. Expression, regulatory, or splicing labels derived from specific tissues or cell lines may not generalize to other biological contexts. The circularity problem identified in @sec-vep-classical persists into the foundation model era: when model predictions influence which variants receive expert review, and expert classifications become training labels, feedback loops amplify historical biases.

**Temporal drift** encompasses changes in clinical practice, diagnostic criteria, and coding conventions over time, evolving sequencing technologies and quality control pipelines, and shifts in the patient population served by a healthcare system. A model trained on 2015 data may fail on 2024 data not because biology changed but because documentation practices, coding standards, and available treatments all evolved.

**Knowledge-base and **benchmark** leakage** occurs when resources like gnomAD or UK Biobank appear in both model training and evaluation. A foundation model pretrained on gnomAD allele frequencies, then evaluated on a benchmark that uses gnomAD for population filtering, faces indirect leakage even if specific variants do not overlap. Community benchmarks that reuse widely available variant sets across multiple publications create additional leakage pathways that accumulate over time as the field iterates.


## Population Structure as a Shortcut

Population structure represents one of the most pervasive confounders in genomic modeling. The core issue is that ancestry simultaneously affects genomic features and many phenotypes through pathways that have nothing to do with direct genetic causation.

Human genetic variation is structured by ancestry: allele frequencies, haplotype blocks, and **linkage disequilibrium** patterns differ across populations in ways that reflect demographic history. Principal components computed from genome-wide genotypes provide a low-dimensional summary of this structure and have become standard in genome-wide association study (**GWAS**) to correct for stratification [@patterson_population_2006; @price_pca_2006]. Yet ancestry is not merely a statistical nuisance. It is intertwined with geography, environment, socioeconomic status, and access to healthcare, factors that directly impact disease risk, likelihood of receiving genetic testing, and the quality of phenotyping when testing occurs.

The statistical genetics community developed these corrections precisely because early genome-wide association studies produced spurious signals driven by ancestry differences between cases and controls rather than causal variant effects (see @sec-gwas for detailed treatment of **population stratification** in association testing). Foundation models face the same fundamental problem in a different guise: ancestry structure that confounded linear regression in GWAS now confounds neural network predictions, and the solutions require similar conceptual foundations even when the technical implementations differ.

::: {#fig-population-structure-shortcut layout-ncol=2}
![**FIGURE PLACEHOLDER A**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20A)

![**FIGURE PLACEHOLDER B**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20B)

![**FIGURE PLACEHOLDER C**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20C)

![**FIGURE PLACEHOLDER D**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20D)

[Essential] Multi-panel figure. Panel A (PCA of genomic data): *PC1* vs *PC2*; colored by ancestry; clear clustering. Panel B (Ancestry in k-mer frequencies): Heatmap across populations; even local composition differs. Panel C (The shortcut pathway): Flow diagram (Ancestry → Sequencing → Labels; Ancestry → Allele frequencies → Features; model learns via ancestry). Panel D (Cross-population performance): Bar chart showing 40-75% PGS reduction; "Shortcuts fail when relationship changes."
:::

Consider a rare disease clinic serving primarily individuals of European ancestry. This clinic contributes most pathogenic variant submissions to ClinVar, while variants observed predominantly in other ancestries remain classified as variants of uncertain significance [@landrum_clinvar_2018]. A model trained on ClinVar may learn that European-enriched variants tend to have pathogenic labels and non-European-enriched variants tend to have uncertain or benign labels, not because of any biological difference in pathogenicity but because of differential clinical characterization. The model appears to predict pathogenicity while actually predicting ancestry-correlated ascertainment.

Foundation models trained on nucleotide sequences see ancestry information directly: the distribution of k-mers and haplotypes differs by population. When such models are fine-tuned to predict disease risk or variant effects, they may leverage ancestry as a shortcut. Increasing model capacity does not solve this problem; it often makes it worse by enabling detection of increasingly subtle ancestry-linked features. The polygenic score portability literature provides stark evidence: risk scores derived from European ancestry cohorts show 40-75% reductions in prediction accuracy when applied to African ancestry individuals [@duncan_analysis_2019]. Similar patterns emerge for variant effect predictors and regulatory models, though they are often less thoroughly documented due to limited cross-ancestry evaluation.

**This mismatch between the populations used for model development and the populations that would benefit from genomic medicine creates a fundamental tension between current practice and equitable healthcare.** Models that work primarily for European ancestry individuals perpetuate existing health disparities, regardless of their benchmark performance.


## Technical Artifacts as Biological Signal

Technical pipelines are complex, and each step from sample collection through final variant calls can introduce systematic differences that models may learn.

Sequencing centers differ in instruments, reagents, and quality control thresholds. Library preparation protocols produce distinct coverage profiles and GC bias patterns. Capture kits determine which genomic regions are well-covered and which have systematic dropout. Read length affects the ability to span repetitive regions and call structural variants. Alignment and variant calling algorithms make different decisions at ambiguous genomic positions.

::: {#fig-batch-effects layout-ncol=3}
![**FIGURE PLACEHOLDER A**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20A)

![**FIGURE PLACEHOLDER B**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20B)

![**FIGURE PLACEHOLDER C**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER%20C)

[High] Three-panel figure. Panel A (Batch structure in embeddings): UMAP colored by sequencing center; samples cluster by batch not phenotype. Panel B (Coverage patterns by batch): Genome browser tracks; different centers show systematic differences. Panel C (Batch predicts phenotype): Contingency table (batch × case/control); imbalanced distribution. Warning: "Model predicting disease may actually predict sequencing center."
:::

When samples from a particular batch or platform are disproportionately drawn from a specific phenotype class, models learn to distinguish batches. In high-dimensional feature spaces, even subtle batch-specific artifacts (coverage dips at particular loci, variant density patterns reflecting caller behavior, residual adapter sequences) can become predictive. Foundation models that process raw reads, coverage tracks, or variant streams are particularly vulnerable because batch signatures may be encoded in features that preprocessing would typically remove.

Common patterns suggesting batch confounding include embedding spaces where samples cluster by sequencing center rather than phenotype, strong predictive performance that collapses when evaluated on data from a new platform, and models that can accurately predict batch identity (sequencing center, capture kit, processing date) from inputs that should be batch-independent. When a model designed to predict disease can also predict which laboratory processed the sample, something has gone wrong.


## Label Bias and Circularity

Labels in genomic applications rarely represent ground truth in any absolute sense. They represent the outputs of complex processes involving clinical documentation, expert review, computational prediction, and database curation. These processes introduce biases that models absorb and may amplify.

Clinical phenotypes derived from electronic health records inherit the limitations of medical documentation. Billing codes capture what was reimbursable, not necessarily what was present. Problem lists reflect what clinicians chose to document, which varies by specialty, institution, and individual practice patterns. Diagnostic criteria change over time, creating apparent temporal trends in disease prevalence that reflect evolving definitions rather than changing biology.

::: {#fig-label-circularity}
![**FIGURE PLACEHOLDER**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[High] Circular flow diagram. Steps: (1) Clinical lab submits to ClinVar using *CADD*/REVEL as evidence; (2) ClinVar aggregates (computational evidence influences labels); (3) New model trains on ClinVar (learns to replicate patterns); (4) New model used by labs (influences next submissions); (5) Return to step 1. Annotations: circularity, apparent validation reflects agreement not insight. Side panel: Breaking cycle (prospective, temporal, independent functional).
:::


Variant pathogenicity labels illustrate the problem of circularity. ClinVar aggregates submissions from clinical laboratories, research groups, and expert panels [@landrum_clinvar_2018]. The evidence underlying these submissions often includes computational predictions: a laboratory may cite *CADD*, REVEL, or other predictors as supporting evidence for a pathogenic classification. When the next generation of predictors trains on ClinVar, it learns to replicate the computational predictions that contributed to those labels. Performance on ClinVar-derived benchmarks thus reflects, in part, agreement with previous predictors rather than independent biological insight.

This circularity extends across the ecosystem of genomic resources. gnomAD allele frequencies inform variant filtering in clinical pipelines. UK Biobank genotype-phenotype associations shape which variants receive functional follow-up. Structural annotations from ENCODE and Roadmap Epigenomics influence which regulatory regions are considered biologically important. Foundation models pretrained on these resources, then evaluated against benchmarks derived from the same resources, may achieve impressive scores while learning to reproduce the assumptions and biases of existing annotations rather than discovering new biology.


## Data Splitting

Data splitting is among the primary tools for assessing generalization, yet naive splits can silently permit leakage that inflates apparent performance.

Random individual-level splits assign samples randomly to training, validation, and test sets. This approach fails when samples are not independent: family members may appear on both sides of a split, allowing models to memorize shared haplotypes. Rare variant analysis is particularly vulnerable because disease-causing variants may be private to specific families, and memorizing which families have which variants is far easier than learning generalizable sequence-function relationships.

Family-aware splits address relatedness by ensuring that all members of a family appear in the same split. This prevents direct memorization of family-specific variants but does not address population structure (ancestry groups may remain imbalanced across splits) or other confounders.

Locus-level splits hold out entire genomic positions, ensuring that no variant at a test position appears during training. This stringent approach prevents models from memorizing site-specific patterns and is essential for **variant effect prediction** where the goal is to score novel variants at positions the model has never seen. Many published benchmarks fail to implement locus-level splitting, allowing models to achieve high scores by recognizing familiar positions rather than learning generalizable effects.

Region or chromosome splits hold out entire genomic regions, testing whether models learn biology that transfers across the genome rather than region-specific patterns. This is particularly relevant for regulatory prediction, where local chromatin context may differ between regions.

Cohort or site splits hold out entire institutions, sequencing centers, or biobanks, directly testing robustness to the batch and cohort effects discussed above. Models that perform well only within their training cohort but fail on held-out cohorts have learned institution-specific patterns.

Time-based splits use temporal ordering, training on earlier data and evaluating on later data. This approach simulates prospective deployment and tests robustness to temporal drift. A model trained on 2018 data and evaluated on 2023 data faces realistic distribution shift that random splits would obscure.

Beyond explicit split design, indirect leakage remains a concern. A variant that appears in ClinVar may also appear in gnomAD (with population frequency information), in functional assay datasets (with splicing or expression effects), and in literature-derived databases (with disease associations). Pretraining on any of these resources while evaluating on another creates indirect information flow that standard deduplication would miss.

## Data Leakage

Data leakage takes three principal forms in genomic modeling, each creating distinct pathways for inflated performance.

**Label leakage** occurs when information about the target variable inadvertently enters the feature set. A variant effect predictor that includes conservation scores computed using alignments that incorporated known pathogenic variants has access to label-correlated information that would not exist for truly novel variants. Similarly, expression models trained on features derived from the same samples used to define expression labels face circular information flow. The ClinVar circularity problem represents a particularly insidious form: when computational predictions contributed to the pathogenicity classifications that later become training labels, the new model learns to replicate its predecessors rather than discover independent signal.

**Feature leakage** arises when features encode information about data partitions or batch structure rather than biology. Coverage patterns that differ systematically between cases and controls, quality metrics that correlate with sequencing center, or variant density profiles that reflect caller-specific behavior all constitute feature leakage. Unlike label leakage, feature leakage may persist even when labels are rigorously separated, because the features themselves carry information about which samples belong together.

**Temporal leakage** violates the causal structure of prediction by using future information to predict past events. Training on variants classified in 2023 to predict classifications that were uncertain in 2020 allows models to learn from reclassification patterns rather than intrinsic variant properties. Clinical outcome prediction faces similar risks when laboratory values, imaging results, or clinical notes recorded after the prediction timepoint enter the feature set. Proper temporal splits must respect not only when samples were collected but when each feature became available.
These leakage types interact and compound. A model suffering from all three may achieve extraordinary benchmark performance while learning nothing transferable to prospective clinical use.

## Detecting Confounding

Confounding is often subtle, requiring systematic diagnostics rather than reliance on aggregate performance metrics.

::: {#fig-confounding-detection}
![**FIGURE PLACEHOLDER**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[High] Diagnostic checklist with visualizations. Diagnostic 1 (Confounder-only baseline): Bar chart comparing full model vs ancestry PCs only vs batch only; if simple baseline approaches full → confounding. Diagnostic 2 (Subgroup stratification): Multiple reliability diagrams by ancestry. Diagnostic 3 (Prediction-confounder association): Scatter of predictions vs *PC1*; residual after controlling for label. Diagnostic 4 (Split sensitivity): Table showing performance across split strategies; large drop = memorization. Diagnostic 5 (**Negative control**s): Accuracy on outcomes unrelated to genetics; should be chance.
:::

**Confounder-only baselines** provide the most direct test. Train simple models (logistic regression, gradient boosting) using only potential confounders: ancestry principal components, batch indicators, sequencing center identifiers, recruitment site. If these baselines approach the performance of complex genomic models, confounding likely drives a substantial portion of the signal. Reporting confounder-only baselines alongside genomic model results makes hidden shortcuts visible.

**Subgroup stratification** reveals whether aggregate performance masks heterogeneity. Report metrics stratified by ancestry group, sequencing platform, institution, and time period. Include both discrimination (**AUROC**, area under the precision-**recall** curve (**AUPRC**)) and **calibration** diagnostics for each subgroup. Models may achieve high overall AUC while being poorly calibrated or nearly useless for specific subpopulations. Performance that varies dramatically across subgroups suggests confounding or distribution shift even when overall metrics appear strong.

**Prediction-confounder association** tests whether model outputs encode confounders beyond what the label requires. Plot predictions against ancestry principal components, adjusting for true label status. Compare mean predicted risk across batches or time periods within the same true label class. Conduct formal association tests (regression, mutual information) between predictions and confounders. Strong residual associations indicate the model has learned confounder-related features that go beyond predicting the label itself.

**Split sensitivity analysis** varies the splitting strategy to probe for leakage. Re-evaluate performance under locus-level splits, cohort holdouts, or temporal splits. Large drops in performance under stricter splitting indicate that initial results were inflated. A model that achieves 0.90 AUC with random splits but only 0.75 AUC with locus-level splits has likely memorized site-specific patterns.

**Negative controls** use outcomes known to be unrelated to genomics. If a model trained to predict disease from genotypes can also predict administrative outcomes (insurance type, documentation completeness) with similar accuracy, it has learned confounders. Shuffling labels within batch or ancestry strata should eliminate predictive signal; if it does not, the model exploits structure that transcends any specific outcome.


## Mitigation Strategies

No mitigation strategy eliminates confounding entirely, and each involves trade-offs between bias, variance, and coverage. Nonetheless, several practical approaches substantially reduce reliance on confounders.

::: {#fig-mitigation-strategies}
![**FIGURE PLACEHOLDER**](https://placehold.co/300x200?text=FIGURE%20PLACEHOLDER)

[Enhancing] Strategy comparison table. Strategies: Study design (match cases/controls, before collection, reduces sample), Covariate adjustment (include ancestry/batch as inputs, during training, may remove real signal), Domain adaptation (train invariant to confounders, complex), Robust optimization (minimize worst-group error, requires group labels), Benchmark design (locus-level splits, during evaluation, harder scores by design). Annotation: "Approaches complementary—use multiple."
:::

**Study design and cohort construction** provide the most robust protection. Matching cases and controls on age, sex, ancestry, and recruitment site removes these variables as potential confounders. Balanced sampling (down-sampling majority groups or up-sampling minority groups within mini-batches) prevents models from optimizing primarily for the majority pattern. Prospective collection with diversity targets ensures that training data represent the populations where models will be deployed. These design-based approaches constrain confounding before modeling begins, avoiding the need for post-hoc correction.

**Covariate adjustment** explicitly models confounders rather than ignoring them. Including ancestry principal components, batch indicators, and site variables as covariates in regression or classification models allows estimation of outcome effects that are adjusted for these factors. Residualizing features or phenotypes with respect to known confounders before training genomic models removes confounder-associated variance, though this risks removing genuine biological signal when confounders correlate with causal variants. Mixed models or hierarchical structures model institution or batch as random effects, allowing estimation of genomic effects while accounting for clustering.

**Domain adaptation and invariance learning** aim to learn representations that do not encode confounders. Adversarial training adds a discriminator that attempts to predict batch or ancestry from learned representations; the feature extractor is trained to maximize prediction accuracy while minimizing the discriminator's ability to recover confounder labels, promoting invariance. Domain adversarial networks and importance weighting align distributions across batches or cohorts. Group-robust optimization targets worst-group performance (minimizing maximum error across subgroups) rather than average performance, encouraging models that work for all groups rather than only the majority.

**Data curation and benchmark design** determine what signals are available to learn. Deduplicating individuals, families, and variants across training and evaluation prevents direct memorization. Locus-level or region-level splits prevent site-specific pattern learning. Benchmarks that explicitly include diverse ancestries, institutions, and platforms test generalization rather than fitting to a single distribution. Documentation of overlaps between training resources and benchmarks enables readers to assess potential leakage.

These approaches complement each other. Good design reduces the need for modeling corrections. Adjustment handles residual confounding that design did not eliminate. Invariance learning provides additional protection when explicit confounder measurement is incomplete. Rigorous benchmark construction ensures that evaluation reflects generalization rather than shortcut learning.

### Causal Inference Approaches

When observational confounding cannot be eliminated through design or statistical adjustment, causal inference frameworks offer principled alternatives that leverage the structure of genetic inheritance itself.

**Mendelian randomization** exploits the random assortment of alleles at meiosis to create natural experiments [@davey_smith_mendelian_2003]. Because genotypes are assigned before birth and cannot be influenced by most environmental confounders, genetic variants that affect an exposure (such as a biomarker level or gene expression) can serve as instrumental variables for estimating causal effects on downstream outcomes. A foundation model trained to predict expression levels can be evaluated for causal relevance by testing whether its predictions, instrumented through genetic variants, associate with disease outcomes in ways that survive Mendelian randomization assumptions. This approach has revealed that many observational biomarker-disease associations reflect confounding rather than causation, and similar logic applies to model-derived predictions.

**Directed acyclic graphs (DAGs)** formalize assumptions about causal structure and clarify which variables should be adjusted, which should be left unadjusted, and which adjustments would introduce bias rather than remove it [@pearl_causality_2009]. Conditioning on a collider (a variable caused by both exposure and outcome) induces spurious associations; conditioning on a mediator blocks causal pathways of interest. Explicit DAG construction forces researchers to articulate their causal assumptions, making hidden confounding visible and enabling principled variable selection. For genomic models, DAGs clarify the relationships among ancestry, technical factors, biological mechanisms, and phenotypic outcomes, revealing which adjustment strategies address confounding versus which inadvertently condition on consequences of the outcome.

**Negative control outcomes and exposures** provide empirical tests of residual confounding without requiring complete causal knowledge [@lipsitch_negative_2010]. A negative control outcome is one that should not be causally affected by the exposure of interest; if the model predicts it anyway, confounding is present. A negative control exposure is one that should not causally affect the outcome; association with the outcome again indicates confounding. For a variant effect predictor, administrative outcomes (insurance status, documentation completeness) serve as negative control outcomes that genotypes should not predict. Synonymous variants in non-conserved regions can serve as negative control exposures that should not affect protein function. Strong predictions for negative controls reveal that the model has learned confounders rather than biology.

These causal approaches do not replace careful study design and rigorous splitting, but they provide additional tools for distinguishing genuine biological signal from confounded associations, particularly when the same observational data must serve both training and evaluation purposes.


## Fairness and External Validity

Confounding connects directly to fairness and health equity. Models that achieve high average performance while failing for specific populations may appear successful while exacerbating existing disparities.

Polygenic risk scores illustrate this tension. European ancestry-derived scores predict cardiovascular disease, diabetes, and breast cancer risk reasonably well within European ancestry populations. Applied to African or Asian ancestry individuals, the same scores show substantially worse discrimination and calibration [@duncan_analysis_2019]. Healthcare systems that deploy these scores without ancestry-specific validation risk providing inferior risk stratification to already underserved populations.

Variant interpretation exhibits similar patterns. ClinVar contains many more pathogenic variant classifications for European ancestry individuals than for other populations [@landrum_clinvar_2018]. Predictors trained on ClinVar inherit this imbalance, performing better for variants common in European populations and worse for variants enriched in other ancestries. Clinical deployment of such predictors may reduce diagnostic yield for non-European patients.

The **uncertainty quantification** approaches discussed in @sec-uncertainty provide partial mitigation: models that report high uncertainty for under-represented populations at least flag predictions that should not be trusted. The **interpretability** methods in @sec-interpretability can reveal when models rely on ancestry-correlated features. Yet technical solutions alone are insufficient. Addressing fairness requires intentional data collection that prioritizes under-represented populations, evaluation protocols that mandate subgroup analysis, and deployment decisions that consider equity alongside aggregate accuracy.

External validity asks whether a model's performance in one setting predicts its performance in another. Confounding and distribution shift often cause dramatic external validity failures. A model that achieves excellent metrics in the development cohort may fail when deployed at a different institution, in a different healthcare system, or in a different country. The clinical risk prediction frameworks in @sec-clinical-risk emphasize multi-site validation precisely because single-site performance frequently fails to generalize.

The fairness implications of confounding extend beyond technical model performance into questions of justice in healthcare resource allocation, diagnostic equity, and the distribution of benefits from genomic medicine. Chapter @sec-confounding examines these issues systematically, connecting the technical confounding problems discussed here to frameworks for algorithmic fairness, regulatory requirements for equitable AI deployment, and practical strategies for ensuring that genomic foundation models reduce rather than amplify existing health disparities.

## A Practical Checklist

The following checklist synthesizes the diagnostics and mitigations discussed above. Systematic application during model development and evaluation surfaces confounding that would otherwise remain hidden.

**Population structure and relatedness**: Quantify ancestry via principal components and relatedness via kinship coefficients. Decide explicitly whether to match, stratify, or adjust for these factors, and document the justification. Report performance stratified by ancestry group. When family structure exists in the data, verify that relatives do not appear across train-test boundaries.

**Data splits and leakage**: Ensure individuals, families, and genomic loci do not cross the train-validation-test boundaries for target tasks. Implement stricter splits (locus-level, chromosome-level, cohort-based, time-based) and report the performance differences. Check for overlap with external databases or benchmarks used in evaluation and document any shared resources.

**Batch, platform, and cohort effects**: Catalog technical variables (sequencing center, instrument, protocol, assay) and cohort identifiers. Test whether these variables predict labels or align with subgroups of interest. Use embedding visualizations, principal components, or simple classifiers to detect batch signatures. Apply mitigation (design matching, covariate adjustment, domain adaptation) when batch effects are detected.

**Label quality and curation bias**: Document how labels were defined and what processes (billing codes, expert review, computational prediction, registry inclusion) produced them. Quantify label noise where possible. Consider robust training strategies when labels are noisy. Assess how curated resources like ClinVar reflect historical biases and whether those biases affect evaluation validity.

**Cross-group performance and fairness**: Report metrics for each major subgroup (ancestry, sex, age, cohort, platform) rather than only aggregate performance. Examine calibration across groups, not just discrimination. Discuss clinical implications of residual performance gaps and whether deployment might worsen existing disparities.

**Reproducibility and transparency**: Document dataset construction, inclusion criteria, and splitting strategies completely. Release preprocessing, training, and evaluation code when feasible. Describe which confounders were measured, how they were handled, and what limitations remain.

Models that pass this checklist provide more reliable evidence of genuine biological learning. Models that fail at multiple points may achieve benchmark success while learning shortcuts that will not transfer to new settings.


## Rigor as Response

The confounding and bias problems examined in this chapter are not reasons for despair. They are reasons for rigor. The same expressive capacity that enables foundation models to discover subtle shortcuts also enables them to learn complex biological patterns when training data and evaluation protocols are designed appropriately. The goal is not to abandon powerful models but to create conditions under which their power serves biological discovery rather than benchmark gaming.

Several trends support progress. Multi-ancestry biobanks and international collaborations expand the diversity of available training data. Benchmark developers implement stricter splitting protocols and require subgroup analyses. Pretraining strategies that explicitly promote invariance to technical factors are emerging. Uncertainty quantification methods (@sec-uncertainty) provide mechanisms for models to express appropriate caution when inputs fall outside their training distribution. The problem of confounding is tractable with sustained attention to data provenance, evaluation design, and deployment monitoring.

Yet vigilance remains essential. New datasets bring new confounders. Novel architectures create new opportunities for shortcut learning. Community benchmarks accumulate indirect leakage as resources are reused across studies. Treating confounding as a first-order concern throughout model development, rather than an afterthought addressed only when reviewers complain, distinguishes models that actually work from models that merely perform well on convenient benchmarks. The interpretability methods in @sec-interpretability provide tools for distinguishing genuine regulatory insight from sophisticated pattern matching, while the uncertainty quantification approaches in @sec-uncertainty enable models to communicate when their predictions should not be trusted. Together with rigorous evaluation, these capabilities move the field toward models that reveal genuine biology and behave reliably across the diverse clinical and scientific settings where they will be deployed.