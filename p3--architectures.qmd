::: {.callout-warning .content-visible when-profile="draft"}
"While protein language models like ESM preceded DNA transformers chronologically, we present these architectures following the central dogma to maintain conceptual coherence."
:::

# Part III: Deep Learning Architectures {.unnumbered}

Part I established the data resources, statistical foundations, and pre-deep learning variant scoring methods that preceded the current era of genomic modeling. This part turns to the architectural innovations that transformed what is computationally possible. The chapters that follow trace an arc from early convolutional neural networks through transformer-based approaches to hybrid designs that combine the strengths of both paradigms.

The progression is not merely chronological. Each architecture embodies a different set of assumptions about biological sequence. Convolutional models assume that local motifs and their short-range combinations are the primary carriers of regulatory information. Transformer-based language models treat sequences as structured compositions of tokens whose meaning emerges from context, leveraging self-attention to capture dependencies across arbitrary distances. Hybrid architectures attempt to reconcile these perspectives, using convolutions to extract local features efficiently while deploying attention mechanisms to model long-range interactions that span tens or hundreds of kilobases.

Chapter @sec-cnn begins with the CNN-based models that first demonstrated deep learning could outperform handcrafted features for regulatory genomics. DeepSEA, ExPecto, and SpliceAI established the paradigm of training deep networks on functional genomics data to predict chromatin accessibility, transcription factor binding, gene expression, and splicing from sequence alone. These models remain widely used and provide the conceptual foundation for everything that follows. 

Chapter @sec-dna then examines DNA language models that apply self-supervised pretraining strategies to genomic sequence. Models like DNABERT, Nucleotide Transformer, and HyenaDNA treat DNA as text to be understood through masked token prediction or autoregressive modeling, learning representations that transfer across diverse downstream tasks without task-specific architectures.

Chapter @sec-rna extends this treatment to RNA, covering models that predict secondary structure, capture splicing regulation beyond what CNN-based methods achieve, and represent the emerging frontier of RNA foundation models. RNA sits in the middle of the central dogma as both an information carrier and a structured biochemical object, requiring models that account for base pairing, epitranscriptomic modifications, and the relationship between sequence and structure.

Chapter @sec-prot examines protein language models, where the success of masked language modeling on natural language translated most directly to amino acid sequences. Models like ESM and ProtTrans learn rich representations of protein structure and function without explicit supervision. AlphaFold demonstrated that these representations could revolutionize structure prediction, while AlphaMissense applied them to proteome-wide variant pathogenicity scoring. These models established the foundation model paradigm that inspired subsequent work in DNA and RNA.

Finally, Chapter @sec-hybrid examines hybrid architectures like Enformer, Borzoi, and AlphaGenome that combine convolutional processing with transformer blocks to achieve context windows spanning hundreds of kilobases. These models enable direct prediction of gene expression and chromatin state from sequence, capturing enhancer-promoter interactions and long-range regulatory effects that shorter-context models cannot represent.

By the end of this part, readers will have a working understanding of the major architectural paradigms in genomic deep learning: what each assumes, what each can and cannot capture, and how these design choices translate to practical performance on regulatory prediction tasks.