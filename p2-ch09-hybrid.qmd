# Long-range Hybrid Models {#sec-hybrid}

Hybrid convolution–transformer architectures such as Enformer and Borzoi were developed to tackle one of the hardest supervised problems in regulatory genomics: predicting gene expression and other functional readouts directly from long stretches of DNA sequence. By combining the locality and efficiency of convolutions with the expressiveness of attention, these models extend the effective receptive field from a few kilobases to hundreds of kilobases or more, while still training end-to-end on large functional genomics compendia.

In this chapter, we focus on long-range hybrid models for expression and related tasks. We start with the motivation for long-range context, formalize the problem setting, and then discuss three representative models:

- **Enformer** [@avsec_enformer_2021], which predicts chromatin and CAGE profiles from 200 kb windows using a CNN front-end and transformer trunk.
- **Borzoi** [@linder_borzoi_2025], which extends an Enformer-style backbone to predict base-level RNA-seq coverage and related transcriptomic readouts.
- **AlphaGenome** [@avsec_alphagenome_2025], which pushes context to roughly a megabase and unifies multiple regulatory and transcriptional modalities.

We then briefly survey alternative long-range architectures such as hierarchical and windowed attention (Genomic Interpreter) [@li_genomic_2023], discuss what these models changed relative to earlier CNN-only architectures, and highlight their limitations and role within the broader genomic foundation model landscape.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (overview)**  
Place a small schematic here showing the progression DeepSEA → Basenji2 → Enformer → Borzoi → AlphaGenome with increasing context length and richer outputs.
:::


## Why Expression Needs Long-Range Models

Early sequence-based models like DeepSEA [@zhou_deepsea_2015] and Expecto [@zhou_expecto_2018] demonstrated that local chromatin features and gene expression can be predicted *ab initio* from relatively short windows of DNA around promoters and candidate regulatory elements. These models showed that motif content, nucleosome positioning signals, and short-range sequence context carry substantial information about chromatin accessibility, histone marks, and transcriptional activity.

However, gene regulation in higher eukaryotes is a long-range, three-dimensional phenomenon. Enhancers and silencers can act over hundreds of kilobases or more, often skipping over nearby genes to regulate more distant targets. Chromatin looping and topologically associating domains (TADs) bring promoters into physical proximity with distal regulatory elements. Many disease-associated variants discovered by GWAS sit far from gene bodies and promoters, but still influence gene expression via such long-range interactions.

Short-context models inevitably treat distal sequence as noise. They can capture promoter-proximal elements, but may miss key enhancers, silencers, and insulators that fall outside their receptive field. As a result, they can misattribute regulatory effects or underestimate the impact of variants that act through distal elements.

As functional genomics datasets grew through ENCODE, Roadmap, FANTOM, and GTEx, and sequencing costs dropped, the field accumulated enough data to train models with substantially longer context. At the same time, improvements in hardware and optimization made deeper and wider convolutional architectures feasible. Basenji2 [@kelley_basenji_2018] extended context to tens of kilobases by aggressive pooling, but purely convolutional networks still struggle to propagate information across hundreds of kilobases without very deep stacks.

Hybrid architectures like Enformer and Borzoi emerged as a compromise: use convolutions to condense local sequence into a manageable sequence of latent tokens, then apply attention to propagate information across 100–200 kb. By predicting many signals at once, including chromatin marks, transcription initiation, and RNA coverage, these models can learn a rich representation of regulatory sequence that is particularly useful for variant effect prediction.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (biological motivation)**  
Figure illustrating promoter–enhancer interactions: a gene with several distal enhancers and silencers spanning ~200 kb, with cartoon 3D chromatin loops and annotated GWAS variants.
:::


## Problem Setting: Sequence-to-Expression at Scale

The models in this chapter tackle a demanding version of the classic sequence-to-label problem. Instead of predicting a single scalar from a short sequence, they map a long DNA window to thousands of positional, multi-task outputs.

### Inputs and Outputs

The input is a **one-hot encoded DNA sequence**, typically spanning 100–200 kb for Enformer and Borzoi, and up to ~1 Mb for AlphaGenome. Most implementations use four channels for nucleotides A, C, G, and T, with ambiguous "N" positions either masked or handled by learned embeddings. A fixed reference genome (e.g., GRCh38) provides the sequence; variants are introduced during inference using in silico mutagenesis or by directly encoding alternative alleles.

To make attention computationally tractable, hybrid models use a **convolutional front-end** that progressively downsamples the sequence. For example, Enformer starts at single-base resolution, applies several convolutional and pooling layers, and ends with a latent sequence of a few thousand tokens representing the 200 kb window.

The **outputs** are multi-task, multi-position tracks:

- For Enformer: per-base predictions of chromatin accessibility, histone modifications, and CAGE signal across many cell types and assays.
- For Borzoi: base-level RNA-seq coverage and other transcriptomic signals (e.g., PRO-seq, nascent transcription) across cell types.
- For AlphaGenome: a broader set of outputs spanning chromatin, gene expression, splicing, and 3D contacts.

Outputs are typically arranged as a tensor with axes for **position**, **task/assay**, and **cell type or condition**. Different readouts may be predicted at different resolutions (e.g., downsampled 128 bp bins for chromatin, finer bins around promoters and splice sites).

### Training Objective

The training objective is usually a **count-based likelihood** or loss computed per track and per position. Common choices include:

- **Poisson or negative binomial log-likelihood** for sequencing counts, sometimes with log-link transformations and offsets for library size.
- **Mean squared error (MSE)** or Pearson correlation objectives when predicting normalized, continuous signals (e.g., log-transformed coverage).
- **Classification losses** (e.g., binary cross-entropy) for presence/absence or peak/no-peak tasks.

Because models predict thousands of outputs simultaneously, losses are aggregated across positions, tasks, and cell types. Many implementations use **per-track weighting** to prevent abundant assays or cell types from dominating the gradient. Some models explicitly down-weight noisy tracks or use curriculum strategies to stabilize training.

From a foundation model perspective, these hybrid architectures can be viewed as **supervised multi-task pretraining**: the model learns a shared representation of regulatory sequence by jointly optimizing against many functional genomics readouts. This representation can then be probed directly for variant effect prediction, interpreted using attribution methods, or adapted to downstream prediction tasks.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (problem formulation)**  
Figure showing: 200 kb DNA window → convolutional downsampling → transformer tokens → multi-task output heads producing a stack of tracks (chromatin, CAGE, RNA-seq) across cell types.
:::


## Enformer: CNN Plus Attention for 200 kb Context

Enformer [@avsec_enformer_2021] is a landmark model that directly predicts chromatin and CAGE profiles from 200 kb windows of DNA. It demonstrates that long-range context and cross-species training substantially improve prediction of gene expression and regulatory activity, and it introduces a widely adopted template for hybrid genomic architectures.

### Architectural Overview

Conceptually, Enformer consists of three stages:

1. **Convolutional stem**  
   A stack of one-dimensional convolutions with residual connections and pooling progressively transforms base-level one-hot sequence into a shorter sequence of latent representations. This stem detects local motifs and short-range patterns, applies dilated convolutions to expand the receptive field, and uses pooling to reduce sequence length while increasing channel dimensionality.

2. **Transformer trunk**  
   The downsampled latent sequence (on the order of 1–2k positions) is fed into a stack of multi-head self-attention blocks. These blocks use positional encodings to retain information about relative position within the 200 kb window, allow each position to attend to any other, enabling modeling of long-range dependencies across the window, and include feed-forward layers and normalization to stabilize training.

3. **Multi-task output heads**  
   After the transformer trunk, Enformer applies task-specific linear and convolutional layers to predict coverage tracks for many assays and cell types. Different heads share the same backbone but specialize in different modalities (e.g., DNase, histone marks, CAGE).

This design balances **local pattern recognition** (handled by convolutions) and **global interaction modeling** (handled by attention), while keeping the attention cost manageable by operating on a downsampled sequence.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (architecture)**  
Figure: Enformer architecture schematic — one-hot DNA → convolutional blocks (with pooling) → transformer blocks → multi-task regression heads producing chromatin and CAGE tracks.
:::

Enformer differs from its predecessor Basenji2 [@kelley_basenji_2018] in several key respects. It extends the input window to 200 kb and uses attention instead of relying solely on very deep dilated convolutions to carry long-range information. It unifies many assays and cell types in a single model, rather than training separate models per modality. It explicitly targets gene expression and promoter-level activity, not just local chromatin accessibility. These changes allow Enformer to capture the influence of distal elements on promoters that may be separated by tens or hundreds of kilobases.

### Training Data and Cross-Species Learning

Enformer is trained on a large collection of human and mouse regulatory data, including chromatin accessibility (e.g., DNase, ATAC), histone modifications and other marks (ChIP-seq), CAGE or related measures of transcription initiation, and other functional readouts where sufficient coverage is available.

Mouse data from analogous assays enables **cross-species learning**: by training a single model on both human and mouse genomes, Enformer learns regulatory motifs and patterns that are conserved across mammals. This reduces overfitting to species-specific idiosyncrasies and improves generalization.

Two key design choices shape the training regime. First, genome-wide sampling ensures the model is trained on many windows across the genome, not just promoter-proximal regions, ensuring exposure to diverse regulatory contexts. Second, multi-task learning means all assays, cell types, and output positions contribute to the loss, which encourages the backbone to learn features useful across modalities. Cross-species and multi-task training together push Enformer toward learning **biologically meaningful regulatory syntax** that generalizes beyond any single dataset.

### Variant Effect Prediction

Like DeepSEA and Basenji2 before it, Enformer can be used for *in silico* variant effect prediction. The standard approach is to select a genomic locus and extract a 200 kb window around it, encode the reference allele and compute Enformer's predicted output tracks, encode an alternative allele (or multiple variants) and recompute predictions, and compute differences between reference and alternative predictions for each track and cell type.

This workflow yields per-variant effect estimates on chromatin and CAGE signals across many cell types. Changes can be summarized at the level of gene expression (by aggregating CAGE or chromatin signal around promoters and transcription start sites), regulatory features (by examining specific histone marks or accessibility tracks), and cell-type specificity (by comparing changes across cell types and conditions).

Because Enformer provides **position-resolved, multi-task outputs**, it supports rich analyses of how a variant may alter regulatory landscapes, not just a single scalar expression measure.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (variant effects)**  
Figure: Heatmap of predicted log-fold change in expression across tissues for a set of variants in a gene's promoter, with example highlighted showing a motif-disrupting variant.
:::

### Validation Against GTEx eQTLs

A crucial question is whether Enformer's variant predictions align with observed expression quantitative trait loci (eQTLs), such as those cataloged in GTEx. In the original work [@avsec_enformer_2021], Enformer's predictions were systematically compared to GTEx eQTLs. Variants with large predicted effects on promoter CAGE often coincided with significant eQTLs. Enformer captured **long-range regulation**: variants located tens of kilobases away from a gene's transcription start site still showed predictive power for expression changes when they lay in predicted enhancers.

While not perfect, these analyses showed that **purely sequence-based predictions** from Enformer can recover a substantial fraction of eQTL signal, especially for variants in regulatory regions with strong chromatin and CAGE signals.

### Interpretation and Mechanistic Insight

Despite its size, Enformer is amenable to several interpretation strategies: gradient-based attribution (e.g., saliency maps, integrated gradients) to highlight sequence positions and motifs that contribute most to predicted outputs, in silico saturation mutagenesis systematically testing all possible substitutions in a region to map functional motifs, and attention visualization examining which positions attend to promoters in the transformer layers, providing hints about promoter–enhancer interactions.

These tools have been used to map candidate long-range regulatory interactions, generate hypotheses about motif function, and prioritize variants for experimental follow-up. However, as discussed in Chapter @sec-vep, attribution is not foolproof; it must be interpreted carefully and combined with external evidence.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (attention / attribution)**  
Figure: Side-by-side attributions showing (1) gradient-based importance scores around a promoter, and (2) attention weights connecting distal enhancers to that promoter.
:::


## Borzoi: Transcriptome-Centric Hybrid Modeling

Enformer is primarily trained on chromatin and transcription initiation signals (CAGE). Borzoi [@linder_borzoi_2025] extends this paradigm to **full RNA-seq coverage**, capturing splicing, alternative isoforms, and polyadenylation patterns in a unified framework. Instead of focusing on promoter activity, Borzoi treats the entire **transcript lifecycle** as a modeling target.

### Motivation

RNA-seq carries richer information than a single expression value per gene: exon–intron structure and splice junction usage, alternative transcription start sites and promoter choice, alternative polyadenylation and 3′ UTR usage, and allele-specific expression in heterozygous individuals.

These features encode not only transcriptional regulation but also aspects of RNA processing, stability, localization, and translation efficiency. A model that predicts base-level RNA-seq coverage across the genome can therefore inform diverse downstream analyses, from variant effect prediction on splicing to interpretation of 3′ UTR variants that modulate mRNA stability.

### Architecture

Borzoi builds on an Enformer-style backbone with modifications tailored to RNA readouts. A convolutional stem and transformer trunk similar in spirit to Enformer provide long-range context (hundreds of kilobases). The output heads predict stranded RNA-seq coverage across the window and additional transcriptomic signals such as PRO-seq, CAGE, and other assays when available. The model places special emphasis on splice junctions (acceptor and donor sites), promoter regions with alternative TSS usage, and 3′ ends where polyadenylation and cleavage occur.

Multi-task learning across these signals encourages the backbone to encode regulatory information from chromatin through transcription initiation to processing and degradation.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (Borzoi architecture + outputs)**  
Figure: Comparison diagram — Enformer outputs chromatin + CAGE, Borzoi outputs RNA-seq coverage with highlighted exon–intron structure and alternative 3′ UTR usage on an example gene.
:::

### From Chromatin Signals to RNA Readouts

By predicting RNA-seq coverage instead of just promoter-proximal activity, Borzoi supports several analyses not easily addressed by chromatin-only models.

**Splicing variant effects** can be evaluated by comparing predicted coverage at exons and junctions under reference and alternative alleles. Large changes in junction usage suggest splicing disruption, complementing specialized models like SpliceAI.

**Alternative promoter and TSS usage** becomes visible through coverage predictions. Promoter-proximal variants may alter initiation at alternative TSSs. Borzoi's coverage predictions reveal shifts in the relative usage of upstream versus downstream promoters.

**Alternative polyadenylation and 3′ UTR regulation** can be assessed by measuring shifts in predicted coverage around alternative polyA sites. Variants in 3′ UTRs and downstream regulatory regions may affect mRNA stability and microRNA targeting.

Variant effect prediction follows similar steps as with Enformer (in silico mutagenesis or allelic substitution), but the **outputs now span the entire gene body and flanks**, enabling a unified view of how sequence changes affect transcription, splicing, and polyadenylation simultaneously.

From a foundation model perspective, Borzoi moves closer to modeling a **full transcriptome readout** from sequence. It provides a rich, supervised training signal for representations that encode both regulatory and post-transcriptional features.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (RNA coverage example)**  
Figure: Real vs predicted RNA-seq coverage across a gene with an alternative exon and alternative 3′ UTR. Show how a splice-site variant alters predicted exon inclusion.
:::


## AlphaGenome: Unified Megabase-Scale Regulatory Modeling

AlphaGenome [@avsec_alphagenome_2025] pushes the hybrid modeling paradigm further by expanding context to roughly a megabase and unifying multiple regulatory, transcriptional, and structural readouts in a single model. Instead of focusing on specific modalities like chromatin or RNA, AlphaGenome aims to serve as a **general-purpose regulatory model** of the human genome.

### Motivation: From Specialized Models to Unified Prediction

Enformer and Borzoi demonstrate that long-range, multi-task models can predict chromatin and transcriptional features, respectively. However, variant interpretation and mechanistic understanding often require integrating multiple modalities: chromatin accessibility and histone marks (regulatory potential), promoter activity and gene expression (transcription), splicing outcomes (isoform composition), and three-dimensional contacts (which distal elements can act on which genes).

Running separate models for each modality complicates interpretation and can introduce inconsistencies. AlphaGenome's goal is to **unify these tasks** within a single architecture, so that the backbone representation is informed by data across modalities, variant effect predictions are coherent across chromatin, expression, splicing, and 3D structure, and one can query the model for many types of effects without juggling multiple systems.

### Architecture

At a high level, AlphaGenome follows the hybrid template but at larger scale. The input window spans roughly 1 Mb of DNA sequence, encoded at single-base resolution and then downsampled by a convolutional stem into a sequence of latent tokens. The convolutional stem, similar in spirit to DeepSEA/Basenji lineages, uses stacked convolutions with pooling and nonlinearities to extract local motifs and patterns while reducing sequence length.

A deep stack of self-attention layers (the transformer trunk) operates on the condensed sequence, enabling modeling of interactions across the full megabase window. Positional encodings and architectural choices are tuned to handle the longer context without prohibitive memory use.

Separate output heads predict chromatin signals (accessibility, histone marks), transcriptional readouts (including gene-level expression and promoter activity), splicing-related features (e.g., exon inclusion, splice junction usage), and structural features such as Hi-C or Micro-C contact maps.

The model is trained in a multi-task manner, leveraging large compendia of human functional genomics data. Compared to Enformer and Borzoi, AlphaGenome emphasizes **human data and multi-modal integration**, rather than cross-species training.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (AlphaGenome schematic)**  
Figure: 1 Mb DNA window with icons for outputs: chromatin tracks, expression values per gene, splicing metrics per exon, and a 2D contact map, all predicted from a shared backbone.
:::

### Access and Practical Use

At the time of writing, AlphaGenome is primarily available through an **API interface**, rather than as an openly downloadable model. This has several practical consequences. Users can **score large sets of variants** without running training or heavy inference infrastructure locally. The API abstracts away model complexity but limits **fine-grained customization** (e.g., domain-specific fine-tuning). Data privacy and regulatory requirements may restrict which genomic datasets can be sent to a cloud-hosted API, especially in clinical contexts.

In practice, AlphaGenome can be used to score candidate regulatory variants identified from GWAS or sequencing studies, generate multi-modal hypotheses about how a variant acts (e.g., altered chromatin plus splicing disruption), and provide large-scale annotations for variant effect prioritization pipelines, complementing specialized models and statistical fine-mapping.

### Positioning in the Landscape

AlphaGenome sits at the intersection of long-range hybrid architectures and **multi-modal genomic foundation models**. Compared to Enformer and Borzoi, it extends context from 200 kb to ~1 Mb, broadens outputs from chromatin/RNA to include splicing and 3D structure, and emphasizes a unified, human-centric regulatory model.

Relative to emerging cross-species sequence models such as Evo 2 [@brixi_evo_2025], AlphaGenome is more **task-specific and supervised**, using labeled functional genomics datasets. Evo 2 focuses on **self-supervised pretraining** across diverse genomes, potentially providing more general sequence representations but less direct mechanistic interpretability.

Relative to self-supervised DNA language models and efficient long-context architectures (Hyena, Mamba; see @sec-princ), AlphaGenome can be seen as a **bridge** between specialized supervised models and broad foundation models, offering rich, multi-modal supervision within a long-range hybrid backbone.

From a practical standpoint, AlphaGenome's API and multi-modal outputs make it an attractive candidate for **variant interpretation pipelines**, particularly where a single system that integrates chromatin, expression, splicing, and contacts is desirable.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (positioning figure)**  
Table or figure comparing Enformer, Borzoi, and AlphaGenome on axes: context length, modalities predicted, training regime, access (open model vs API).
:::


## Alternative Architectures: Hierarchical Attention

While Enformer, Borzoi, and AlphaGenome use standard self-attention over a condensed sequence, long-range modeling can also be approached through **hierarchical or windowed attention mechanisms** that reduce computational cost and impose additional inductive biases.

### Genomic Interpreter and 1D-Swin Transformers

Genomic Interpreter [@li_genomic_2023] adapts the shifted window (Swin) transformer paradigm to one-dimensional genomic sequences. The core idea is to partition the downsampled sequence into **local windows**, apply self-attention **within each window** (which is cheaper than full-sequence attention), use **shifted windows** in alternating layers so that information can propagate across window boundaries over depth, and merge representations hierarchically, allowing the model to build progressively more global features.

The 1D-Swin block operates in two alternating phases. First, standard windowed attention where each position attends only to positions within its local window. Second, shifted-window attention where windows are shifted relative to the original partition so that tokens at window boundaries can attend to neighbors in adjacent windows.

By stacking these layers, Genomic Interpreter achieves **effective long-range dependency modeling** while limiting the quadratic cost of attention to smaller windows. This yields better **scaling to longer input sequences** than full self-attention at the same resolution and an inductive bias toward **local-to-global aggregation**, which may align with hierarchical aspects of regulatory architecture (e.g., motifs to enhancers to domains).

Genomic Interpreter has been evaluated on long-range chromatin and expression prediction tasks, often matching or surpassing Enformer-style baselines at similar compute budgets, especially when pushing context lengths beyond a few hundred kilobases.

### Computational Trade-offs

The choice between full attention and hierarchical/windowed attention involves several trade-offs.

**Full attention (Enformer-style)** offers flexibility and expressiveness (any position can attend to any other in a single layer) but has quadratic cost in sequence length, becoming expensive at longer context or higher resolution.

**Hierarchical/windowed attention (Genomic Interpreter, 1D-Swin)** scales better with sequence length and can handle longer inputs at similar compute budgets. However, some long-range interactions require multiple layers to propagate, and the inductive bias may or may not match specific regulatory architectures.

**Alternative efficient mechanisms** (e.g., Hyena, state-space models, Mamba; see @sec-princ) replace attention entirely with architectures that have **sub-quadratic or linear** scaling and strong long-range memory. These are still being actively explored and benchmarked on genomic tasks; integration with multi-task hybrid setups is an open area.

In practice, hybrid models are likely to incorporate a mix of these ideas: convolutional stems for motif-scale features, efficient long-range mechanisms for context propagation, and multi-task heads for rich outputs. The frontier is moving from "Can we model 200 kb?" to "Can we model megabase-scale or chromosomal segments with biologically meaningful inductive biases?"

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (architectural comparison)**  
Figure: Side-by-side schematic comparing full self-attention vs windowed/shifted attention vs a generic efficient long-range mechanism, with curves showing theoretical compute vs context length.
:::


## What Hybrid Models Changed

Hybrid CNN–transformer sequence models like Enformer and Borzoi introduced several conceptual advances over earlier architectures.

### Explicit Long-Range Modeling

By combining convolutional downsampling with attention over latent tokens, hybrid models explicitly model **long-range interactions** within windows of 100–200 kb or more. This enables better representation of enhancer–promoter and enhancer–enhancer interactions, modeling of promoter competition and insulator effects within regulatory neighborhoods, and capture of regulatory context spanning multiple genes and non-coding regions.

Earlier CNN-only models such as DeepSEA and Basenji2 could expand their receptive fields through deeper stacks and dilated convolutions, but the path length between distal positions remained long. Attention shortens this path, making it easier to learn dependencies between distant positions given enough data and capacity.

### Unified Multi-Task Learning Across Modalities

Hybrid models jointly predict multiple modalities (chromatin, transcription initiation, RNA coverage, and more) from a shared backbone. This multi-task setup encourages the model to learn representations that are **consistent across modalities** (e.g., open chromatin plus active histone marks plus high transcription), allows **implicit modeling of relationships** between assays (e.g., chromatin changes that precede expression changes), and provides a form of **regularization**, as the model must simultaneously fit many related outputs.

From a foundation model perspective, this is a supervised analog of multi-modal pretraining: a single model learns from heterogeneous signals, which can then be probed or adapted for downstream tasks.

### Improved Variant Effect Prediction for Expression

Compared to earlier CNN-only models like DeepSEA, Beluga, and Expecto, hybrid models substantially improve variant effect prediction for expression-related outcomes. Longer context allows them to capture the effects of **distal regulatory variants** that would be invisible to short-window models. Multi-modal outputs provide richer evidence for how a variant acts (via chromatin, promoter activity, splicing, or polyadenylation) rather than a single scalar change. Cross-species and multi-task training help filter out noise and emphasize **conserved regulatory mechanisms**.

Borzoi further extends this by connecting sequence changes to the **full RNA life cycle**, offering predictions about splicing, isoform ratios, and 3′ UTR usage. This is particularly valuable for interpreting variants in non-coding regions that regulate isoform-specific expression rather than total gene expression.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (before vs after)**  
Figure: Conceptual diagram comparing "pre-hybrid" vs "hybrid" landscapes for variant effect prediction: short-range scalar outputs vs long-range, multi-modal outputs informing expression, splicing, and chromatin.
:::


## Limitations and Failure Modes

Despite their power, hybrid long-range models are not omniscient and introduce new challenges alongside their capabilities.

### Data and Label Limitations

Training data still imposes strong constraints. Functional genomics assays are **biased toward certain cell types and conditions**, often over-representing well-studied tissues and cancer lines. Many loci, particularly in **non-European ancestries**, are underrepresented or absent from training data. Assays are noisy, with batch effects, sequencing artifacts, and experimental variability.

As a result, models may **underperform in underrepresented cell types or ancestries**, and their predictions should be treated with caution in those settings. Rare regulatory phenomena, such as cell-state-specific enhancers or context-dependent chromatin changes, may be only partially captured. Predictions can reflect **technical artifacts** in training data, not just biology (a theme revisited in the chapters on evaluation, @sec-eval, and confounders, @sec-confound).

### Sequence Context and Generalization

Enformer, Borzoi, and AlphaGenome are trained on fixed window sizes around annotated loci or genome-wide tiles. This introduces several limitations. Even a 1 Mb window (finite context) does not capture whole-chromosome or trans-chromosomal interactions, which can matter for some regulatory events. Models implicitly assume that the relevant information for a readout lies within the chosen window (assumption of local causality). Structural variants, long-range rearrangements, or trans-regulatory effects that fall outside the window are not modeled. Training windows are typically sampled from a reference genome (reference-centric view); complex haplotypes and structural variation are underrepresented.

These constraints mean that predictions are most trustworthy for **cis-acting variants** whose effects are captured within the window and training distribution. Out-of-distribution scenarios (novel structural variants, unusual haplotypes, or highly divergent backgrounds) require special care and, ideally, experimental validation.

### Interpretability and Trust

Although attribution and interpretation methods exist and have yielded biologically plausible insights, several caveats remain. Attribution maps can be **sensitive to model architecture and noise**, and may highlight correlated but non-causal sequence features. Attention weights are not guaranteed to be faithful explanations of the model's reasoning. Multi-modal outputs increase the **complexity of interpretation**, as one must reconcile changes across many tracks.

As discussed in the chapters on evaluation (@sec-eval) and confounders (@sec-confound), hybrid models must be evaluated not only on predictive performance but also on their **robustness, calibration, and susceptibility to dataset biases**. Interpretability tools should be treated as hypothesis-generating, not as definitive proofs of mechanism.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (failure modes)**  
Figure: Cartoon panel showing several failure modes — missing data for some cell types, misinterpreted attribution maps, structural variant outside model context, ancestry mismatch.
:::


## Role in the Genomic Foundation Model Landscape

Hybrid architectures like Enformer, Borzoi, and AlphaGenome occupy an interesting niche in the broader genomic foundation model landscape. They are **high-capacity models trained on large, heterogeneous datasets**, much like foundation models in NLP and vision. However, they are **strongly supervised** by specific assays and tasks, rather than being pre-trained purely self-supervised on raw DNA.

In practice, hybrid models serve multiple roles:

- As **task models**: directly predicting chromatin, expression, RNA coverage, and variant effects from sequence.
- As **feature extractors**: their internal representations can be used as embeddings for downstream models, fine-tuned for specific tasks or cell types.
- As **benchmarks and baselines**: they set a high bar for supervised performance on regulatory tasks, against which newer architectures (state-space models, Hyena, Mamba, and large self-supervised DNA language models) must be compared (@sec-princ).

As the field moves toward large, multi-modal genomic foundation models, hybrid long-range architectures are likely to remain important as **specialized, mechanistically grounded models** for variant effect prediction, provide **training curricula and evaluation tasks** for more general sequence models, and influence the design of future architectures that blend **supervised multi-task learning** with **self-supervised pretraining** on large-scale genomic data.

In other words, hybrid models sit between early CNN-based predictors and fully general genomic foundation models, providing both a stepping stone and a practical tool for current applications.

::: {.callout-note .content-visible when-profile="draft"}
**VISUAL SUGGESTION (ecosystem diagram)**  
Figure: Ecosystem diagram placing hybrid models alongside self-supervised DNA language models, multi-modal GFMs, and downstream clinical models, with arrows showing how representations and predictions flow between them.
:::


## Summary

This chapter examined hybrid CNN–transformer architectures designed for long-range genomic prediction, focusing on Enformer, Borzoi, and AlphaGenome as representative examples.

Enformer combines a convolutional stem with transformer blocks to predict chromatin and CAGE profiles from 200 kb windows, enabling explicit modeling of long-range regulatory interactions and improving variant effect prediction for expression [@avsec_enformer_2021]. Borzoi extends this paradigm to RNA-seq coverage and related transcriptomic signals, providing a unified view of how sequence variation affects transcription, splicing, and polyadenylation [@linder_borzoi_2025]. AlphaGenome pushes context to megabase scale and unifies multiple modalities (chromatin, expression, splicing, and 3D contacts) within a single hybrid model, currently accessible primarily via API [@avsec_alphagenome_2025]. Hierarchical and efficient attention architectures such as Genomic Interpreter's 1D-Swin transformer offer alternative ways to scale long-range modeling while controlling compute [@li_genomic_2023].

The key lessons from this chapter are that long-range context substantially improves our ability to predict expression and regulatory activity from sequence alone, multi-task and multi-modal supervision helps models learn representations that connect chromatin, transcription, and RNA processing, hybrid models are powerful tools for variant effect prediction but remain limited by data biases, finite context, and interpretability challenges, and in the broader genomic foundation model ecosystem, hybrid long-range architectures act as both state-of-the-art task models and stepping stones toward more general, multi-modal genomic foundation models.

In @sec-princ, we step back to consider what makes a model a "genomic foundation model" and how hybrid architectures, self-supervised sequence models, and efficient long-context mechanisms fit together in this rapidly evolving space.