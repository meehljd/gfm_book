::: {.callout-warning .content-visible when-profile="draft"}
**TODO:**
- Flesh-out Enformer; Berzoi; AlphaGenome; 
- Include population specific models, like UKBerzoi
- Add figure: Enformer architecture diagram showing CNN stem → transformer trunk → multi-task heads
- Add figure: Comparison of effective receptive fields across models (DeepSEA 1kb → Basenji2 40kb → Enformer 200kb)
- Add figure: Borzoi RNA-seq coverage prediction example showing transcription, splicing, and polyadenylation signals
- Add visualization: Attention weight patterns showing promoter-enhancer interactions
- Add table: Comprehensive comparison of Basenji2, Enformer, and Borzoi (context length, parameters, training data, output tracks)
- Consider adding Evo2 and HyenaDNA as emerging alternatives to attention-based long-range modeling
- Reference UK Biobank fine-tuned variants (UKEnformer, UKBorzoi) if published by time of print
- Ensure cross-reference to AlphaMissense and AlphaGenome discussion in Chapter 13
:::


# Long-range Hybrid Models {#sec-hybrid}

## Why Expression Needs Long-Range Models

ExPecto (@sec-trans) showed that gene expression can be predicted *ab initio* from sequence by combining a CNN-based chromatin model (Beluga) with a separate regression layer mapping chromatin features to expression across tissues [@zhou_expecto_2018]. This modular strategy worked surprisingly well, but it inherited two key limitations from its DeepSEA-style backbone (@sec-reg). First, the 40 kb input window captures proximal promoters and some nearby enhancers, but many regulatory interactions span 100 kb or more. Second, chromatin prediction and expression prediction are trained separately, leaving no opportunity for the expression objective to shape the representation of sequence.

As genomic datasets grew through ENCODE, Roadmap, FANTOM, GTEx, and other consortia (@sec-data), it became clear that enhancers can regulate genes hundreds of kilobases away, that eQTLs often sit outside the promoter windows traditionally used for expression models, and that chromatin conformation introduces non-local dependencies between DNA segments through loops and topologically associating domains. Pure CNN architectures can expand their receptive field using dilated convolutions and pooling, but doing so at single-nucleotide resolution quickly becomes parameter- and memory-intensive. On the other hand, classic transformer architectures can model long-range dependencies via attention, but their quadratic runtime and memory in sequence length makes naïve application to 200 kb sequences computationally infeasible (@sec-dna).

Hybrid architectures like Enformer and Borzoi emerged as a compromise between these constraints. These models use convolutions to extract local motif features and progressively downsample the sequence into a manageable number of latent positions, then apply self-attention over this compressed representation to capture long-range regulatory interactions across 100 to 200 kilobases. By predicting many signals at once, including chromatin profiles, transcription start site activity, and RNA-seq coverage, they enable multi-task learning and rich variant effect prediction. This chapter focuses on these hybrid designs, particularly Enformer [@avsec_enformer_2021] and Borzoi [@linder_borzoi_2025], and how they changed what sequence-to-expression models can accomplish.


## Problem Setting: Sequence-to-Expression at Scale

The models in this chapter tackle a demanding version of the classic sequence-to-function problem: given a long DNA sequence window around a genomic locus, predict a rich set of regulatory and transcriptional readouts across many cell types.

### Inputs and Outputs

The input to these models is a one-hot encoded DNA sequence, typically around 200 kb centered on a candidate promoter or gene. Each position in the sequence is represented by one of four channels corresponding to the nucleotides A, C, G, and T, with N positions masked or handled by zeroing all channels. Beyond the raw sequence, the model must know where promoter-proximal bases and distal elements sit relative to each other. This positional information is encoded through convolutional receptive fields in the early layers and through explicit positional embeddings for the attention layers.

The outputs of Enformer and Borzoi are multi-task, multi-position predictions spanning multiple assays, multiple cell types, and multiple positions along the input window. The assays include chromatin accessibility measured by DNase-seq or ATAC-seq, histone modifications such as H3K4me3 and H3K27ac, and transcriptional activity measured by CAGE or RNA-seq. Each assay is predicted separately for hundreds of cell types and experimental conditions, and predictions are made at fixed strides across the input window, typically every 128 or 256 base pairs, yielding coverage tracks rather than single scalar values. The result is a dense tensor of predictions with dimensions corresponding to output positions, assay types, and cell types.

### Training Objective

The typical training objective involves per-track, per-position regression using either a Poisson or negative binomial likelihood on read counts, or alternatively a mean-squared error loss on log-transformed counts. All tracks contribute to the loss simultaneously, though some models tune weights to prevent abundant assays like DNase from dominating scarce but potentially important ones like rare histone marks. The learning problem can be written as a function $f_\theta$ that maps a DNA sequence of approximately 200 kb to a tensor of continuous outputs indexed by tracks and positions, with parameters $\theta$ shared across assays, cell types, and genomic loci.


## Enformer: CNN Plus Attention for 200 kb Context

Enformer [@avsec_enformer_2021] is a landmark model that directly integrates long-range sequence context with cell-type-specific expression prediction using a hybrid CNN-transformer architecture. The name, a portmanteau of "enhancer" and "transformer," reflects its primary innovation: using attention mechanisms to capture the relationships between enhancers and promoters that may be separated by tens or hundreds of kilobases.

### Architectural Overview

Conceptually, Enformer consists of three stages. The first stage is a convolutional stem that extracts local motifs and progressively downsamples the sequence. The second stage is a transformer trunk that applies self-attention to model long-range dependencies between the downsampled positions. The third stage comprises output heads that decode the attended representation into assay- and cell-type-specific coverage tracks.

The convolutional front-end takes approximately 200 kb of one-hot encoded sequence as input and applies stacked convolution-normalization-nonlinearity-pooling layers. This progressively compresses the input, with each pooling operation reducing the spatial dimension while the convolutional operations expand the channel dimension. By the end of this stage, the roughly 200,000 base pair input has been reduced to around 1,500 to 2,000 latent tokens, each summarizing a multi-kilobase region and encoding local motif configurations and short-range regulatory patterns. This compression step solves the computational problem of applying attention to raw nucleotides: rather than computing attention over 200,000 positions with quadratic cost, the model operates on a tractable sequence of around 1,500 positions.

The transformer trunk then applies several transformer blocks over this compressed sequence. Multi-head self-attention allows every downsampled position to attend to every other position, capturing relationships between distant enhancers and promoters or between multiple regulatory elements that may all contribute to a gene's expression. Feed-forward networks provide nonlinear mixing of information at each position, and residual connections with layer normalization stabilize training and enable deep stacks. Intuitively, the convolutional layers answer the question of what motifs and local patterns exist in each region, while the attention layers answer the question of how these regions interact across the 200 kb window to shape regulatory activity.

After the transformer blocks, Enformer applies task-specific output heads to each position in the latent sequence, producing coverage predictions for each combination of assay and cell type. For CAGE-based transcription start site activity, the model predicts coverage around TSS positions, and gene-level expression metrics can be obtained by aggregating predictions at positions near annotated transcription start sites through summing or averaging log counts across a small window.

Enformer differs from its predecessor Basenji2 in several key respects. It uses transformer blocks instead of dilated convolutions for long-range modeling, attention pooling instead of max pooling for downsampling, twice as many channels in the network, and 1.5 times longer input sequences (197 kb instead of 131 kb). These changes collectively enable the model to capture regulatory elements up to 100 kb from a gene, compared to only about 20 kb for Basenji2. This expanded receptive field is biologically important: estimates from high-confidence enhancer-gene pairs suggest that 47% of relevant enhancers lie within 20 kb of their target genes, but 84% lie within 100 kb.

### Training Data and Cross-Species Learning

Enformer is trained on a large collection of human and mouse regulatory datasets. The human data includes DNase-seq, histone ChIP-seq, and CAGE across many cell types from ENCODE, Roadmap Epigenomics, and other consortia. Mouse data from analogous assays enables cross-species learning. Two key design choices shape the training regime.

First, joint human-mouse training encourages the model to learn regulatory principles conserved across mammals rather than overfitting to species-specific patterns. This approach also enables zero-shot transfer between species for some tasks, as representations learned from one species can generalize to the other. Second, entire chromosomes are held out for evaluation to avoid overly optimistic performance estimates that might arise from local sequence similarity between training and test examples. The loss aggregates over all targets, all positions in the output window, and all training loci.

### Variant Effect Prediction

Like DeepSEA before it, Enformer can be used for in silico variant effect prediction. The procedure involves extracting a 200 kb window around a locus from the reference genome, running Enformer to obtain predicted coverage tracks, introducing an alternative allele into the window, re-predicting coverage, and computing the difference between alternate and reference predictions. This delta can be computed for each track at each position, and aggregating these differences around transcription start sites quantifies the predicted expression change for genes in each cell type.

This approach allows fine-grained assessment of how a variant might alter promoter-proximal signals and distal enhancer contributions. Because the model sees a 200 kb context, it can in principle detect cases where a variant disrupts a distal enhancer that regulates a gene tens of kilobases away. The variant-level scores can be integrated into downstream tools such as fine-mapping pipelines that require per-variant effect estimates.

### Validation Against GTEx eQTLs

Enformer's variant effect predictions were systematically evaluated using GTEx eQTL data (@sec-data). For each gene-tissue pair, known eQTLs (lead variants from association testing) were compared to non-eQTL variants in linkage disequilibrium. The evaluation used signed LD profile (SLDP) regression, which correlates predicted expression effects with observed eQTL effect sizes while accounting for LD structure. Enformer's predictions showed stronger alignment with observed eQTLs than prior models like Basenji2, with improvement especially notable at distal regulatory variants where long-range attention is crucial [@avsec_enformer_2021].

In practice, this means Enformer can prioritize variants likely to be causal eQTLs rather than merely correlated through linkage disequilibrium. The model provides cell-type-specific effect predictions, which are critical for interpreting tissues with sparse experimental data. If a variant is predicted to have a large effect in a particular tissue, that prediction can inform downstream analyses of tissue-specific disease mechanisms.

### Interpretation and Mechanistic Insight

While Enformer is a complex model, several interpretation strategies provide mechanistic insight. Gradient-based attribution computes gradients of gene-level expression predictions with respect to input sequence, highlighting bases or motifs that drive the predicted expression of a gene in a specific cell type. In silico mutagenesis systematically mutates bases to estimate their impact on target genes or tracks, identifying enhancers and key transcription factor binding sites controlling expression. Analysis of attention weights reveals which positions attend most strongly to a promoter, suggesting candidate long-range enhancers.

These tools have been used to map promoter-enhancer interactions directly from sequence and to suggest causal regulatory elements for disease-associated variants. Contribution scores computed for genes with CRISPRi-validated enhancers correlate with H3K27ac marks and highlight not only local promoter regions but also distal enhancers more than 20 kb away. By contrast, contribution scores from Basenji2 are zero for sequences beyond 20 kb due to its limited receptive field. This provides evidence that Enformer genuinely uses biologically relevant distal sequence when making predictions.


## Borzoi: Transcriptome-Centric Hybrid Modeling

Enformer is primarily trained on chromatin and CAGE profiles, which capture regulatory states and transcription initiation but not the full complexity of RNA processing. Borzoi [@linder_borzoi_2025] extends the hybrid architecture paradigm to model the RNA transcriptome itself, with emphasis on finer-grained transcriptional features including splicing and polyadenylation.

### Motivation

RNA-seq data carries richer information than a single expression scalar per gene. Coverage along exons and introns reflects transcription initiation, elongation, and termination. Splice junction usage reveals alternative splicing patterns, complementing specialized models like SpliceAI (@sec-splice). Coverage patterns around 3' UTRs and polyadenylation sites reflect mRNA stability, localization, and translation efficiency.

A general-purpose model that predicts base-level RNA-seq read coverage from DNA sequence could provide a unified framework for transcript-level variant effect prediction spanning transcription, splicing, and polyadenylation. It could also offer mechanistic insight into how regulatory sequence features shape the full life cycle of transcripts, from initiation through processing to eventual degradation.

### Architecture

Borzoi builds on the Enformer-style backbone with modifications tailored to RNA-seq prediction. The convolutional front-end processes long DNA windows on the order of 100 to 200 kb, learning local motifs and regulatory patterns at single-nucleotide or modestly downsampled resolution. A hybrid long-range module uses attention and/or long-range convolutions to integrate information across the entire context, explicitly designed to capture relationships between promoters, internal exons, and distal elements. Multi-layer output heads predict RNA-seq coverage tracks across the window, with separate tracks for sense versus antisense transcription, splice junction signals, and polyadenylation-related coverage around 3' ends.

Like Enformer, Borzoi is trained in a multi-task regime, but with stronger emphasis on RNA-related readouts. Where DeepSEA, Beluga, and Enformer mapped sequence to chromatin plus transcription start activity, Borzoi maps sequence to full transcriptome coverage.

### From Chromatin Signals to RNA Readouts

This shift to RNA-level prediction supports several analyses not possible with chromatin-focused models. Promoter usage can be assessed by distinguishing alternative promoter transcription start sites based on coverage patterns. Alternative splicing can be predicted through differential exon inclusion or skipping, complementing the splice-site-focused approach of SpliceAI. Coverage drop-offs and polyadenylation-linked patterns enable modeling of 3' UTR and polyA site choice.

Variant effect prediction follows similar steps as with Enformer: predict transcriptome outputs for reference and alternate sequences, compute delta-coverage at exons, splice junctions, and 3' ends, and aggregate into variant-level scores for tasks like eQTL or sQTL prioritization. The richer output enables combined assessment of how a single variant might affect transcription, splicing, and polyadenylation simultaneously.

## AlphaGenome: Unified Megabase-Scale Regulatory Modeling

::: {.callout-warning .content-visible when-profile="draft"}
**UPDATE THIS SECTION**
:::

### Motivation: From Specialized Models to Unified Prediction
- Enformer/Borzoi: chromatin + expression, separate from SpliceAI (splicing)
- AlphaGenome unifies regulatory, transcriptional, splicing, and 3D predictions
- Single model spanning the functional readouts relevant to variant interpretation

### Architecture
- 1 megabase DNA input at single-base resolution
- Convolutional stem for local motif detection (DeepSEA lineage)
- Transformer blocks for full-megabase context propagation
- Task-specific output heads for each modality

### Multi-Modal Outputs
- Chromatin accessibility and histone modifications
- Transcription factor binding across cell types
- Gene expression (CAGE-like signals)
- Splice junctions and splice site usage
- 3D genome contacts (enhancer-promoter communication)

### Variant Effect Prediction Workflow
- Predict tracks for reference sequence
- Predict tracks for variant-bearing sequence
- Compute delta signals across all modalities
- Mechanistic hypotheses: which tracks/tissues disrupted, not just scalar scores

### Benchmarking Performance
- State-of-the-art on unseen functional genomics tracks
- Strong performance on noncoding disease variants
- Splicing disruption prediction
- Regulatory MPRA concordance
- Cross-ref to detailed benchmarks in @sec-veps

### Access and Practical Use
- API access for large-scale variant scoring
- No local training infrastructure required
- Enables mechanistic interpretation at scale

### Positioning in the Landscape
- Comparison table: Enformer (200kb, chromatin+CAGE) → Borzoi (RNA-seq coverage) → AlphaGenome (1Mb, unified multi-modal)
- Human-genome-centric vs Evo 2's cross-species generality
- Supervised multi-task vs self-supervised pretraining

## Alternative Architectures: Hierarchical Attention

While Enformer, Borzoi, and AlphaGenome use standard self-attention over downsampled sequence representations, alternative approaches address the computational cost of long-range modeling through hierarchical or windowed attention mechanisms.

### Genomic Interpreter and 1D-Swin Transformers

Genomic Interpreter adapts the shifted window (Swin) transformer architecture from computer vision to one-dimensional genomic sequences [@li_genomic_2023]. The core insight is that biological regulatory dependencies often exhibit hierarchical structure: local motif interactions operate at different scales than distal enhancer-promoter communication. Rather than computing full self-attention across the entire sequence (which scales quadratically with length), Swin-style architectures restrict attention to local windows and then merge information hierarchically across scales.

The 1D-Swin block operates in two alternating phases. In the first phase, attention is computed within non-overlapping windows of fixed size along the sequence. In the second phase, windows are shifted by half their width, allowing information to flow between the previously separated regions. This alternation creates a hierarchical receptive field that grows with network depth while keeping per-layer attention cost linear in sequence length.

Evaluated on 17 kb DNA segments for chromatin accessibility and gene expression prediction, Genomic Interpreter demonstrated competitive performance with models using full attention while offering computational advantages. The hierarchical structure also provides a natural framework for interpretability, as attention patterns at different levels correspond to regulatory interactions at different genomic scales.

### Computational Trade-offs

The choice between full attention (Enformer-style) and hierarchical attention (Swin-style) involves trade-offs. Full attention can capture arbitrary pairwise interactions but scales as O(n²) in sequence length, limiting context windows or requiring substantial downsampling. Hierarchical approaches scale more favorably but impose an inductive bias that distant interactions must be mediated through intermediate representations. For regulatory genomics, where most enhancer-promoter interactions occur within topologically associating domains of roughly 1 Mb, this hierarchical structure may align well with biological reality.

Other efficient attention mechanisms have been explored for long genomic sequences, including linear attention approximations, sparse attention patterns, and state-space models like Hyena and Mamba (@sec-dna). The diversity of architectural solutions reflects the fundamental tension between modeling capacity and computational tractability that defines the current frontier of long-range genomic modeling.


## What Hybrid Models Changed

Hybrid CNN-transformer sequence models like Enformer and Borzoi introduced several conceptual advances over earlier architectures.

### Explicit Long-Range Modeling

By combining convolutional downsampling with attention over latent tokens, these models achieve hundreds of kilobases of effective context with manageable compute. All positions in the compressed representation can interact, approximating many possible promoter-enhancer relationships. This is crucial for capturing distal enhancers that sit far from genes and for modeling complex regulatory architectures where multiple enhancers and silencers integrate to control expression.

Earlier CNN-only models like DeepSEA and Basenji2 could expand their receptive field through dilated convolutions, but the information flow between distant positions remained indirect, passing through many intermediate layers. Attention allows direct information exchange between any two positions in the compressed sequence, which the original Enformer paper showed outperforms dilated convolutions across model sizes and training data volumes.

### Unified Multi-Task Learning Across Modalities

Hybrid models jointly predict chromatin accessibility, histone marks, and transcriptional activity in a single forward pass. This multi-task learning yields shared representations that capture general regulatory logic, regularization across assays and cell types that reduces overfitting to any single dataset, and a pathway to transfer learning where a single pretrained model can be adapted to downstream tasks.

The multi-task setup also enables consistency checking: a variant predicted to strongly increase H3K27ac (an enhancer mark) but not affect CAGE output would be suspicious, as these signals typically correlate at active regulatory elements. The model implicitly learns these relationships through joint training.

### Improved Variant Effect Prediction for Expression

Compared to earlier CNN-only models like DeepSEA, Beluga, ExPecto, and Basenji2, Enformer demonstrated stronger eQTL concordance and better performance on expression-related benchmarks [@avsec_enformer_2021]. Hybrid designs can identify distal causal variants more reliably because their architecture naturally encodes long-range dependencies. Borzoi extends this further by providing detailed transcriptome-level readouts, enabling combined assessment of transcription, splicing, and polyadenylation for each variant and offering a richer mechanistic understanding of how sequence variation impacts the full RNA life cycle.


## Limitations and Failure Modes

Despite their power, hybrid long-range models are not omniscient and introduce new challenges alongside their capabilities.

### Data and Label Limitations

The training data for these models, drawn primarily from ENCODE, Roadmap Epigenomics, GTEx, and similar resources, have known biases. The assays focus on specific cell types, conditions, and genomic regions. GTEx eQTLs are enriched for individuals of European ancestry (@sec-data). Many regulatory phenomena, such as RNA binding protein effects and 3D chromatin structure beyond simple contact frequency, are only partially captured by the available assays.

As a result, the models may underperform in cell types or ancestries not well represented in the training data. They may also misinterpret patterns that are confounded by technical artifacts such as batch effects or mapping biases. A predicted effect in a rare cell type or an underrepresented population should be treated with appropriate caution.

### Sequence Context and Generalization

Enformer and Borzoi are trained on fixed window sizes around annotated loci, and their behavior outside those canonical windows may be less reliable. Training focuses on reference genome context, meaning large indels, structural variants, or rearrangements may be poorly modeled. The models assume linear genomic context: 3D chromatin architecture is only indirectly captured via sequence patterns correlated with looping, and explicit Hi-C or Micro-C integration remains limited.

These constraints mean that a variant prediction assumes the rest of the genome matches the reference, which is never true for any real individual. Epistatic effects between multiple variants in the same regulatory region, or between a variant and a structural rearrangement, are not captured.

### Interpretability and Trust

Although attribution methods exist and have yielded biologically plausible results, attention weights and gradient-based scores are not direct causal evidence. Attributions can be noisy and sensitive to how targets are aggregated. For clinical use, predictions often require orthogonal validation through CRISPR perturbation, allele-specific expression assays, or other experimental approaches. These interpretability challenges are part of the broader issues discussed in the chapters on evaluation (@sec-eval) and confounders (@sec-confound).


## Role in the Genomic Foundation Model Landscape

Hybrid architectures like Enformer and Borzoi occupy an interesting middle ground between task-specific CNNs and general-purpose genomic foundation models. Compared to earlier CNN systems, they model much longer context and support richer multi-modal outputs, offering significantly improved expression-related variant effect prediction. Compared to the self-supervised genomic language models discussed in @sec-dna, they are specialized and supervised on particular assays rather than trained with broad self-supervision on raw genomes. Their architecture is hand-crafted for specific tasks (chromatin plus expression) rather than serving as a universal pretraining backbone.

In practice, hybrid models serve multiple roles. They function as high-performance baselines for variant effect prediction tasks, especially when expression or RNA readouts are primary endpoints. Their representations can be adapted for downstream tasks or combined with pretrained language models over DNA. Their "convolutional stem plus long-range module plus multi-task heads" pattern has become a design template that newer architectures borrow, substituting attention for alternative long-range mechanisms such as state space models, Hyena, or Mamba (@sec-princ).

As the field moves toward large, multi-modal genomic foundation models that integrate sequence, chromatin, expression, and 3D structure, Enformer and Borzoi represent key waypoints. They demonstrate that long-range context is essential for accurate expression prediction, that hybrid architectures can make such context computationally tractable, and that multi-task supervision across regulatory layers is an effective path from raw DNA to clinically relevant variant effect predictions.


## Summary

This chapter examined hybrid CNN-transformer architectures designed for long-range genomic prediction, focusing on Enformer and Borzoi as representative examples.

Enformer combines a convolutional stem with transformer blocks to achieve 200 kb context windows, enabling the model to capture distal enhancer-promoter interactions that purely convolutional models miss. Joint training on human and mouse data across thousands of chromatin and CAGE tracks produces representations that improve eQTL prioritization and provide cell-type-specific expression effect predictions. Borzoi extends this approach to predict RNA-seq coverage directly, enabling unified assessment of transcription, splicing, and polyadenylation effects.

The key lessons from this chapter are that long-range context substantially improves expression prediction, that hybrid architectures offer a practical solution to the computational constraints of attention over long sequences, and that multi-task learning across regulatory modalities yields representations useful for variant interpretation. At the same time, these models inherit biases from their training data, assume reference genome context, and require experimental validation for clinical applications.

In @sec-princ, we step back to consider what makes a model a "genomic foundation model" more broadly, examining the design dimensions, evaluation frameworks, and emerging architectures that define this rapidly evolving space.