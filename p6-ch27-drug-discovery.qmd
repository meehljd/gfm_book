# Drug Discovery {#sec-drug-discovery}

More than 90% of drug candidates that enter clinical trials fail. They fail because they targeted the wrong gene. They fail because the patient population was too heterogeneous for a single mechanism to succeed. They fail because preclinical models predicted efficacy that did not translate to humans. They fail because safety signals emerged only at scale. The pharmaceutical industry spends billions of dollars on programs that will not produce approved therapies, and the cost of this attrition propagates to the drugs that do succeed: higher prices, longer development timelines, and reduced investment in diseases with smaller markets. The fundamental bottleneck is not generating drug candidates but identifying, early in the process, which targets and which patients offer the highest probability of success.

Human genetics provides one of the strongest predictors of clinical success. Targets with genetic support succeed in trials at roughly twice the rate of targets without such support; human mutations that cause phenotypes resembling the therapeutic goal provide direct evidence that modulating the target affects the disease. Yet exploiting this signal is harder than it sounds. Genome-wide association studies have identified thousands of disease-associated loci, but most point to noncoding regions where the causal gene is unclear. Even when a gene is implicated, the direction of effect often remains ambiguous. Translating a statistical association into a validated therapeutic target typically requires a decade of work and hundreds of millions of dollars in follow-up studies.

Genomic foundation models offer a path through this translational bottleneck. Rather than treating each target identification program as a de novo effort, foundation models encode biological knowledge learned across millions of sequences into reusable representations. A variant effect predictor can score any missense mutation's functional impact without task-specific retraining. A regulatory model can predict expression consequences of noncoding variants across tissues. Network models can propagate genetic signals to identify pathway relationships invisible in single-gene analyses. This chapter examines how these capabilities connect to drug discovery: target prioritization and genetic validation, network-aware approaches that identify modules and repurposing opportunities, foundation model-guided functional genomics screens, and biomarker development for patient stratification.

## The Genetic Foundation of Target Selection

Human genetics provides uniquely causal evidence for target selection. Unlike expression correlations or pathway membership, genetic associations reflect the consequences of lifelong modulation of gene activity in human populations. Multiple analyses over the past decade have demonstrated that genetically supported targets succeed in clinical trials at roughly twice the rate of targets without genetic evidence. Targets implicated by Mendelian disease genetics, GWAS hits, or functional variants show higher probabilities of success in phase II and III trials compared to targets selected through other means.

This empirical observation motivates building pipelines where genetic architecture serves as a first-class input to target identification. Genomic foundation models extend this logic in two directions. First, they provide richer biological context: instead of simple "variants near gene X," foundation models encode regulatory architecture, chromatin state, three-dimensional genome interactions, cell-type specificity, and perturbation responses. Second, they enable transfer across diseases and modalities: a single model trained on diverse genomic and multi-omic data can be reused for multiple diseases and therapeutic areas, analogous to how language models transfer across domains.

### From Variant-Level Predictions to Gene-Level Evidence

Drug discovery teams rarely care about individual variants per se; they care about genes and pathways. The fundamental challenge in target identification is therefore aggregating variant-level information into gene-level evidence that can guide target selection.

Consider a typical workflow. Starting from GWAS summary statistics (see @sec-gwas), statistical fine-mapping methods identify credible sets of potentially causal variants at each locus. Sequence-based foundation models then score each candidate variant for regulatory or coding impact. Protein-centric variant effect predictors such as AlphaMissense, GPN-MSA, and the missense components of AlphaGenome combine protein language models, structural information, and evolutionary conservation to assess coding variants [@cheng_alphamissense_2023; @benegas_gpn-msa_2024; @avsec_alphagenome_2025; @brandes_genome-wide_2023]. Regulatory foundation models including Enformer, Borzoi, and long-context DNA language models predict the consequences of noncoding variants on chromatin accessibility, transcription factor binding, and gene expression [@avsec_enformer_2021; @linder_borzoi_2025; @dalla-torre_nucleotide_2023; @nguyen_hyenadna_2023].

The critical step is connecting variants to genes. For coding variants, this mapping is straightforward: the variant lies within a gene's coding sequence, and protein-level scores directly inform that gene's candidacy. For noncoding variants, the mapping requires integrating chromatin conformation data (Hi-C, promoter-capture Hi-C), enhancer-gene predictions from models like Enformer, and expression quantitative trait locus data that empirically links variants to gene expression changes. Fine-mapping approaches such as MIFM can help distinguish truly causal regulatory variants from correlated passengers, tightening the map from GWAS locus to variant to target gene [@wu_genome-wide_2024; @rakowski_mifm_2025].

Gene-level aggregation proceeds by summarizing variant effects across all variants linked to each gene. For a given gene, this summary might include the burden of predicted loss-of-function variants in cases versus controls, the strongest regulatory variant effect sizes predicted by foundation models, constraint metrics indicating the gene's intolerance to damaging variation, and pleiotropy scores reflecting associations with other traits that might indicate safety liabilities or broader biological importance. From a foundation model perspective, the core idea is to treat gene-level evidence as an aggregation problem over high-dimensional variant embeddings. Rather than manually defining a handful of summary statistics, variant embeddings and predicted functional profiles can feed into downstream models that learn which patterns matter most for disease.

### Linking Genetics to Target Safety and Efficacy

Classical human genetics has established several heuristics for target selection that foundation models can reinforce and extend. Human knockout individuals, people carrying biallelic loss-of-function variants, provide natural experiments on the consequences of gene inactivation. Protective variants that reduce disease risk suggest the directionality of therapeutic intervention: partial inhibition of a protein may be beneficial rather than harmful. Pleiotropy, meaning associations with many unrelated traits, may signal safety liabilities if modulating a target affects multiple physiological systems.

Foundation models sharpen these assessments. Fine-mapping methods combined with regulatory foundation models can distinguish causal variants from those merely in linkage disequilibrium with causal variants. Variant effect scores from protein and regulatory models approximate effect sizes, helping differentiate subtle modulators from catastrophic loss-of-function mutations. Multi-task predictions across chromatin marks, transcription factor binding, expression, and splicing provide mechanistic hypotheses for how risk loci affect biology, moving beyond statistical association toward functional understanding.

The output of this workflow is a ranked list of candidate targets with structured evidence that can be compared across diseases and programs. Each target comes annotated with the strength of genetic evidence (effect sizes, fine-mapping probabilities), predicted mechanisms (coding versus regulatory, affected tissues), constraint information (tolerance to loss-of-function, essentiality), and druggability features (protein family, structural information, existing ligands).

## Network-Aware Target Discovery and Repurposing

Individual genes do not operate in isolation. Proteins interact in complexes, genes participate in pathways, and regulatory networks coordinate cellular responses. Even with excellent variant-to-gene mapping, the biological context of a target shapes its therapeutic potential. Network-aware approaches propagate genetic signals through these relational structures to identify modules, bottleneck nodes, and repurposing opportunities.

### Propagating Genetic Signals Through Networks

The basic intuition is that GWAS signals concentrated in a pathway or protein interaction module provide stronger evidence than isolated hits. A single gene with modest genetic support but tight functional connections to several strongly implicated genes may be a more attractive target than an isolated hit with stronger statistics but unclear biology.

Network-based methods integrate noncoding GWAS loci, regulatory annotations, and protein-protein interactomes to identify disease genes and evaluate drug repurposing opportunities in complex diseases. Graph neural network architectures (see @sec-networks) can learn to propagate genetic evidence through interaction networks, scoring each gene not just by its direct genetic association but by its network context. The key methodological insight is that genes can be embedded jointly with their network neighbors, allowing the model to capture how genetic perturbations in one gene might affect functionally related genes.

Foundation model representations enhance these network approaches. Instead of representing each gene by a sparse vector of annotations, genes can be embedded using features derived from protein language models, regulatory foundation models, and expression-based cell state encoders. These rich representations capture functional similarity beyond what interaction databases alone can provide. Two genes with similar protein language model embeddings likely share functional properties even if no direct interaction has been catalogued.

### Drug Repurposing Through Shared Representations

The same framework enables systematic drug repurposing. By representing drugs via their targets, gene expression signatures, and phenotypic effects, and representing diseases via their genetic architecture and molecular signatures, models can score drug-disease pairs based on representation similarity. If a drug's target sits near genetically implicated genes in representation space, or if the drug's expression signature opposes the disease signature, that drug becomes a repurposing candidate.

Network proximity provides one concrete operationalization: drugs whose targets are enriched near disease-risk genes, as measured by network diffusion or embedding similarity, may have therapeutic potential for that disease. Several retrospective analyses have found that such proximity predicts reduced disease incidence among users of particular drugs, though prospective validation remains limited.

The caution here is fundamental: representation similarity is not causation. A drug that appears near disease genes in embedding space might act through that mechanism, or the association might reflect confounding by indication, survivorship bias, or other artifacts of observational data. Network-based repurposing generates hypotheses; Mendelian randomization, natural experiments, and prospective trials must test them.

## Drug-Target Interaction Prediction

Beyond identifying disease-relevant targets, foundation models can predict which molecules might modulate those targets. Drug-target interaction prediction sits at the interface between genomic and chemical foundation models, using biological representations to inform molecular design decisions.

### Representing Targets for Binding Prediction

Traditional drug-target interaction methods rely on sequence similarity, structural docking, or chemical fingerprint matching. Foundation model approaches replace these hand-crafted features with learned representations. Protein language model embeddings from ESM-2 or similar architectures capture evolutionary and structural information that correlates with binding site properties [@lin_evolutionary-scale_2023]. Ligand representations from chemical foundation models encode molecular properties relevant to binding affinity and selectivity.

The prediction task becomes: given a protein embedding (derived from a protein language model) and a molecule embedding (derived from a chemical language model or graph neural network), predict binding affinity or interaction probability. These models can be trained on large databases of known drug-target interactions and binding affinities, then applied to predict interactions for novel targets or molecules.

For genomics-focused applications, the protein representation is the critical contribution. A target identified through genetic validation can be immediately embedded using protein foundation models, enabling binding prediction without waiting for experimental structures or extensive biochemical characterization. This acceleration is particularly valuable for understudied targets where structural data is sparse.

### Selectivity and Off-Target Prediction

The same framework extends to selectivity prediction. By comparing a drug's predicted binding across all proteins in a proteome-scale embedding space, models can flag potential off-target interactions. A compound predicted to bind its intended target but also showing high affinity for kinases with cardiovascular expression, for example, might warrant additional safety characterization before advancement.

Foundation model representations capture protein family relationships and binding site similarities that inform off-target predictions. Two proteins with similar embeddings likely share structural features that could bind similar molecules. This information, combined with tissue expression data and phenome-wide association data (linking genes to thousands of traits), enables preliminary safety profiling before expensive preclinical experiments.

## Toxicity Prediction from Genomic Context

Safety failures represent a major cause of drug attrition, particularly in late-stage development where failures are most expensive. Genomic information provides several routes to earlier toxicity prediction.

### Genetic Evidence of Target Liabilities

Human genetic data offers direct evidence of target-related toxicity. If loss-of-function variants in a target gene associate with adverse phenotypes in biobank populations, those phenotypes may emerge as on-target toxicities during therapeutic inhibition. Phenome-wide association studies across biobanks link genes to thousands of traits, from laboratory values to disease diagnoses to imaging features. A target strongly associated with QT prolongation, hepatotoxicity markers, or nephrotoxicity phenotypes warrants careful safety evaluation.

Foundation models enhance this analysis by providing more accurate variant effect predictions (distinguishing true loss-of-function from benign variants) and by integrating across evidence types. A gene might show modest individual associations with several safety-relevant traits that, when aggregated using foundation model representations, reveal a concerning pattern.

### Expression-Based Toxicity Prediction

Tissue expression patterns inform toxicity risk. A target expressed highly in hepatocytes poses greater hepatotoxicity risk than one expressed primarily in the target tissue. Single-cell foundation models (see @sec-single-cell) provide cell-type-resolved expression information, enabling predictions about which cell types might be affected by target modulation.

More sophisticated approaches use perturbation-response models trained on CRISPR screens and drug treatment data. Given a target knockdown or drug treatment, these models predict transcriptomic responses across cell types. If the predicted response signature resembles known toxicity signatures (mitochondrial stress, DNA damage response, inflammatory activation), that prediction informs safety risk assessment.

### Integrating Genomic Context with Chemical Properties

Ultimate toxicity prediction requires integrating target information with compound properties. The same molecule might be safe or toxic depending on its selectivity profile, metabolism, and tissue distribution. Foundation models provide the biological context (target properties, off-target predictions, expression patterns) that complements chemical property predictions (metabolism, reactivity, distribution) in integrated toxicity models.

The field remains early: prospective validation of foundation model toxicity predictions against clinical outcomes is limited. Current utility lies in prioritizing compounds for experimental toxicity testing and in generating hypotheses about liability mechanisms, rather than replacing traditional safety pharmacology.

## Functional Genomics Screens and Perturbation Models

While human genetics offers observational evidence, drug discovery relies heavily on perturbation experiments that directly test hypotheses. CRISPR knockout and knockdown screens, saturation mutagenesis of protein domains, massively parallel reporter assays (MPRAs) of regulatory elements, and Perturb-seq experiments linking genetic perturbations to single-cell transcriptomic responses all generate data that both validates targets and improves models.

### Designing Informative Perturbation Libraries

Traditional pooled screens use simple design rules: one guide RNA per exon, or tiling a regulatory region at fixed spacing. Foundation models enable smarter library design by providing priors over which perturbations are likely to be informative.

Variant effect scores from protein foundation models can prioritize which amino acid positions are most likely to reveal functional differences when mutated. Positions predicted to be highly constrained and structurally important warrant more thorough coverage than positions predicted to be mutationally tolerant. Regulatory foundation models can highlight which enhancer or promoter regions are predicted to have the largest expression effects in the cell type of interest, focusing screening effort on high-impact regions.

Beyond prioritization, foundation models can guide combinatorial design. Model uncertainty, the degree to which a model is confident in its predictions, identifies regions where experimental data would be most informative. Positions where the model makes uncertain predictions are precisely those where experimental measurement adds the most value. Active learning strategies that select perturbations to maximize expected information gain can dramatically improve the efficiency of screening campaigns.

### Perturb-seq and Transcriptomic Readouts

Perturb-seq experiments combine pooled genetic screens with single-cell RNA sequencing, linking each perturbation to its transcriptomic consequences. These data are exceptionally rich: rather than a single phenotypic readout (viability, fluorescence), each cell provides a high-dimensional expression profile reflecting how the perturbation affected cellular state.

Foundation models trained on Perturb-seq data learn to predict transcriptomic responses to genetic perturbations (see @sec-single-cell for architectural details). Given a gene knockdown, these models predict which other genes will be up- or down-regulated, providing a functional signature for each target. Similar signatures suggest similar biology; divergent signatures suggest distinct mechanisms even for targets in the same pathway.

The drug discovery application is perturbation matching. Given a disease state characterized by a transcriptomic signature (perhaps derived from patient samples or disease models), foundation models can identify perturbations whose predicted response signature would move the system toward a healthier state. If knocking down gene X reverses the disease signature, X becomes a candidate therapeutic target. If treating with drug Y produces a signature similar to knocking down gene X, Y becomes a candidate molecule for that mechanism.

### Closing the Loop: Lab-in-the-Loop Refinement

Perhaps the most powerful application of foundation models in functional genomics is iterative refinement. Screen outcomes provide labeled examples that can fine-tune sequence-to-function models for the specific biological context of interest.

Consider an MPRA that assays thousands of enhancer variants for their effects on reporter gene expression in a disease-relevant cell type. These sequence-activity pairs directly supervise expression-prediction foundation models, dramatically improving their accuracy for that locus and tissue. The refined model then makes better predictions for the next round of experiments, suggesting which additional variants would be most informative to test.

This lab-in-the-loop cycle accelerates discovery while improving model accuracy in disease-relevant regions of sequence space. Foundation models provide the prior (general knowledge about sequence-function relationships); experiments provide the likelihood (specific measurements in the system of interest); and the posterior (updated model) makes better predictions for subsequent experiments.

## Biomarker Development and Patient Stratification

Even when a target is well validated, many programs fail in clinical trials because the right patients were not enrolled, the right endpoints were not measured, or the treatment effect was diluted across a heterogeneous population. Foundation model representations provide new tools for defining and validating biomarkers.

### From Polygenic Scores to Foundation Model Features

Classical polygenic scores summarize additive effects of common variants on disease risk. These scores have proven useful for patient enrichment in cardiovascular and metabolic disease trials, selecting patients at highest genetic risk who might benefit most from intervention. Deep learning methods extend this approach by learning nonlinear genotype-phenotype mappings that capture interactions and nonadditive effects.

Foundation models enhance polygenic prediction in several ways. Instead of using raw genotypes as inputs, models can use variant effect scores, regulatory predictions, or gene-level embeddings derived from foundation models. This captures biological context that simple genotypes miss. Models trained on variant embeddings rather than binary genotype calls can capture subtle differences between variants at the same position, distinguishing a mildly damaging missense from a severely damaging one even when both are heterozygous.

Transfer across populations represents a particular strength. Foundation models trained on diverse genomes provide representations that may generalize more robustly across ancestries than models trained on individual cohorts. Fine-mapping-aware approaches that use foundation model features can reduce dependence on linkage disequilibrium patterns that vary across populations, potentially improving the portability of genetic risk predictors.

### Multi-Omic Biomarker Discovery

Beyond germline genetics, drug development increasingly leverages somatic genomics, transcriptomics, proteomics, and other molecular readouts. Tumor sequencing combined with expression profiling characterizes the molecular landscape of each patient's cancer. Single-cell multiome data (RNA plus ATAC) reveal cell-state heterogeneity that bulk assays miss.

Foundation models trained on these data types provide embeddings that capture patient-level molecular profiles. Set-based architectures that treat each patient's genomic features as a set (rather than assuming fixed feature positions) can handle the heterogeneity of tumor genomes, where different patients have different mutations. Gene regulatory network inference models trained on atlas-scale single-cell data can extract pathway activity scores that serve as mechanistically interpretable biomarkers.

The key shift is that biomarkers are no longer limited to a handful of hand-picked variants or expression markers. They become functions over high-dimensional genomic and multi-omic embeddings, learned in a data-driven way yet grounded in biological priors from foundation models. A biomarker might be a region of embedding space corresponding to patients with particular molecular subtypes, defined by the model rather than by manual curation.

### Trial Design and Endpoint Selection

Foundation model predictions inform trial design at multiple stages. Patient enrichment uses genetic risk scores or molecular subtypes to select patients most likely to respond, increasing statistical power and reducing required sample sizes. Adaptive designs use intermediate biomarker responses to modify randomization or dosing during the trial. Endpoint selection uses molecular signatures to define pharmacodynamic biomarkers that indicate target engagement, supporting dose selection and early efficacy signals.

Regulatory agencies increasingly accept genomic biomarkers for patient selection in oncology and are developing frameworks for other therapeutic areas. The challenge is validation: demonstrating that foundation model predictions actually stratify patient outcomes requires prospective trials or well-designed retrospective analyses with appropriate controls for confounding.

## Industry Workflows and Infrastructure

For pharmaceutical and biotechnology organizations, the challenge is not whether they can access a foundation model but how to integrate these models into existing data platforms, governance structures, and decision-making processes.

### Building Model Infrastructure

In mature organizations, foundation models should be treated as shared infrastructure rather than ad hoc scripts developed by individual project teams. A well-organized model catalog contains DNA language models (Nucleotide Transformer, HyenaDNA, GENA-LM), sequence-to-function models (Enformer, Borzoi), and variant effect predictors (AlphaMissense, GPN-MSA, CADD) with documented capabilities, limitations, and appropriate use cases [@dalla-torre_nucleotide_2023; @nguyen_hyenadna_2023; @fishman_gena-lm_2025; @avsec_enformer_2021; @linder_borzoi_2025; @cheng_alphamissense_2023; @benegas_gpn-msa_2024; @schubach_cadd_2024].

Feature services provide centralized APIs that accept variants, genomic intervals, or genes as input and return embeddings, predicted functional profiles, or risk features. Centralization enables consistency across programs and avoids redundant computation. Logging and versioning ensure that analyses can be reproduced even as models and data evolve.

Data governance maintains clear separation between models trained on public data versus sensitive internal cohorts. Internal data, including proprietary clinical trial data, patient samples, and collaborator contributions, requires careful handling. Guardrails define where internal data can be used for fine-tuning and how resulting models can be shared or published.

### Strategic Choices: Build, Buy, or Fine-Tune

Organizations face three strategic options when adopting foundation models. Using external models as-is offers low upfront cost and benefits from community benchmarking, but may not capture organization-specific populations, assays, or therapeutic areas. A model trained primarily on European ancestry populations may perform poorly on a company's Asian-focused programs; a model trained on common cell lines may miss biology relevant to rare disease indications.

Fine-tuning open-source foundation models on internal data retains powerful general representations while adapting to local data distributions. This approach requires computational investment and careful privacy controls, but often provides the best balance of generality and specificity. A company with large internal biobank data can fine-tune a general variant effect predictor on that cohort, improving predictions for its patient populations without sacrificing the broad knowledge captured during pretraining.

Training bespoke internal models from scratch offers maximum control and allows alignment of pretraining objectives with specific use cases. A company focused on rare diseases might pretrain on sequences and phenotypes particularly relevant to that space. The cost is substantial: pretraining requires significant compute, data engineering, and machine learning expertise. There is also risk of overfitting to narrow internal datasets if the pretraining corpus is not sufficiently diverse.

In practice, most organizations adopt hybrid strategies. They start with public foundation models for early exploration and non-sensitive applications, gradually fine-tune on internal data as value becomes clear, and reserve from-scratch training for cases where unique data assets justify the investment. Lightweight model-serving infrastructure handles latency-sensitive applications such as clinical decision support, while heavier offline systems support large-scale research workloads.

### Industry Context: Timelines and Decision Gates

Academic machine learning research optimizes benchmark performance; drug discovery optimizes probability of clinical and commercial success under time and resource constraints. Understanding industry context helps foundation model practitioners contribute effectively.

Drug discovery programs progress through gates: target validation, candidate selection, investigational new drug filing, and clinical trial phases. Each gate requires specific evidence: biological rationale, efficacy data, safety data, manufacturing feasibility. Foundation model contributions must align with gate requirements. A beautiful embedding space is valueless if it cannot be translated into evidence that advances a program through its next gate.

Timelines matter. A prediction that takes six months to validate experimentally may be worthless if the program decision must be made in three months. Foundation models that enable faster experiments (through better library design, prioritization, or interpretation) create more value than models that provide incrementally better predictions but require the same experimental timeline to validate.

Biotechnology companies and pharmaceutical companies operate differently. Biotechs often focus on single programs with limited resources, prioritizing speed and risk-taking. Pharma companies manage portfolios across therapeutic areas, prioritizing consistency and scalability. Foundation model infrastructure that serves one context may not serve the other. A boutique biotech might prefer lightweight, single-purpose models; a large pharma might invest in comprehensive infrastructure serving dozens of programs.

### Intellectual Property and Data Considerations

Foundation models raise new questions around intellectual property, data sharing, and regulatory expectations that organizations must navigate.

Models trained on proprietary data can be valuable assets but are difficult to patent directly. The model architecture is typically based on published methods; the weights reflect training data that may include public and proprietary components. Downstream discoveries, including specific targets, biomarkers, and therapeutic hypotheses derived from foundation model analyses, are more clearly protectable but require careful documentation of inventive contribution.

Collaborative model development across institutions may require federated learning or model-to-data paradigms, especially for patient-level data. Genomic data carries re-identification risk; sharing raw data, even for model training, requires appropriate consent and data use agreements. Federated approaches that train on local data without centralizing raw information can enable multi-institutional collaboration while respecting privacy constraints.

For regulatory submissions, foundation models used in drug development create documentation requirements. If a model informed target selection, patient stratification, or safety assessment, regulators may request information about model training, validation, and performance across subgroups. The confounding and interpretability challenges discussed in @sec-confounding and @sec-interpretability become acute when models inform pivotal decisions in drug development. Clear documentation trails from model prediction to program decision support regulatory review.

## Evaluation and Validation

Evaluating foundation model contributions to drug discovery requires carefully separating model performance from scientific and clinical validity. A model that achieves high benchmark scores may still fail to improve drug discovery outcomes; a model with modest benchmarks may provide actionable insights that advance programs.

### Benchmark Limitations

Many published benchmarks draw targets and drugs from the same databases used to pretrain models, creating risk of leakage that inflates performance estimates. Repurposing success stories often rely on retrospective data mining with limited prospective validation. The ultimate test of a foundation model for drug discovery is whether it identifies targets that succeed in clinical trials, a test that takes years and confounds model contribution with countless other factors.

Confounding pervades drug discovery data. Models trained on observational clinical and genomic data inherit confounders from those data. Drug-disease associations learned by foundation models may reflect treatment patterns rather than true causal relationships. Confounding by indication (sicker patients receive different treatments), survivorship bias (only patients who survived long enough enter certain analyses), and healthcare access patterns all threaten validity. Genetic instruments and careful epidemiologic designs remain essential for causal claims that foundation model predictions cannot provide alone.

### From Prediction to Validation

Foundation model predictions are hypotheses, not conclusions. A target ranked highly by genetic evidence and foundation model scoring still requires experimental validation. The value of foundation models lies in prioritizing which hypotheses to test, not in replacing experimental testing.

Validation strategies depend on the application. Target predictions can be validated through functional genomics screens that test whether predicted targets affect disease-relevant phenotypes. Biomarker predictions require retrospective validation on held-out cohorts or prospective validation in clinical trials. Repurposing predictions require real-world evidence analyses or prospective trials.

The timeline for validation matters. Some predictions can be tested in weeks (cell-based assays for target validation); others require years (clinical outcomes for biomarkers). Foundation model contributions should be assessed on timescales relevant to drug discovery decisions, not just immediate benchmark performance.

## Connections to Molecular Design

While this chapter focuses on target identification and indication selection, foundation model representations connect downstream to molecular design. The bridge between genomic and molecular foundation models typically involves using target context as conditioning signals for molecule generation. Gene-level embeddings from genomic foundation models, reflecting genetic evidence, tissue specificity, and network context, can condition chemistry models that propose molecules targeting that gene.

Multi-modal foundation models jointly trained on DNA, RNA, proteins, structures, small molecules, and phenotypic readouts learn representations that span these modalities. Such models can predict not just whether a molecule binds a target, but how target modulation in a particular genetic context might affect cellular phenotypes. Closed-loop optimization uses genomic foundation models to predict target relevance and liability, chemistry and protein foundation models to propose molecules, and experimental feedback to update both model types in active learning cycles.

The detailed treatment of molecular design belongs to @sec-design. From a target identification perspective, the key point is that genomic foundation models determine whether a target is worth pursuing; downstream models then optimize how to hit it. The investment in accurate target identification and validation pays dividends throughout the drug discovery pipeline by ensuring that optimization efforts focus on targets with the highest probability of clinical success.

## Acceleration Through Prioritization

Foundation models connect to drug discovery not as replacements for experimental validation but as tools that reduce risk and focus resources. Target discovery workflows aggregate variant-level predictions into gene-level evidence, integrating fine-mapping, variant effect prediction, and regulatory modeling to prioritize candidates with strong genetic and mechanistic support. Network-aware approaches propagate genetic signals through protein and regulatory networks to identify druggable nodes and repurposing opportunities. Drug-target interaction prediction uses foundation model representations to assess binding, selectivity, and safety liabilities before synthesis. Functional genomics screens leverage foundation models for library design and iterative refinement. Biomarker development uses foundation model features for patient stratification and trial enrichment.

Throughout these applications, the value proposition is acceleration and prioritization, not automation of discovery. Foundation models help identify the most promising targets, design the most informative experiments, and select the patients most likely to benefit. Programs that would have required years of hypothesis testing can reach the right target faster. Screens that would have required exhaustive enumeration can focus on high-priority candidates. The fundamental uncertainties of drug development remain: targets validated genetically may still fail in trials, predictions of binding may not translate to efficacy, and patients selected by biomarkers may still not respond. Foundation models reduce risk without eliminating it.

The subsequent chapter on sequence design (@sec-design) extends these ideas from analysis to generation, where foundation models not only evaluate existing sequences but propose new ones optimized for therapeutic function. The transition from interpretation to design represents the frontier where foundation models become engines for creating biology, not just understanding it.